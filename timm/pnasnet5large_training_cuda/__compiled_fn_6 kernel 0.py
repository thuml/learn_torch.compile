
from ctypes import c_void_p, c_long
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align

from torch import device, empty, empty_strided
from torch._inductor.codecache import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
alloc_from_pool = torch.ops.inductor._alloc_from_pool
reinterpret_tensor = torch.ops.inductor._reinterpret_tensor
async_compile = AsyncCompile()


# kernel path: /tmp/torchinductor_youkaichao/qt/cqtcbt4g57hrviadj7jcv3toxxh4xcnjikjw4nylbesjf4s5onol.py
# Source Nodes: [], Original ATen: [aten.sum]

triton_per_fused_sum_0 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 8],
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_sum_0', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 1000
    rnumel = 8
    RBLOCK: tl.constexpr = 8
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (1000*r1)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')

import triton
import triton.language as tl
from torch._inductor.triton_heuristics import grid, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_cuda_stream


# kernel path: /tmp/torchinductor_youkaichao/qh/cqh4l3l7sduxtdjsjw3i5ixqm5o535h7hrz72mun4w2ivczjl2yk.py
# Source Nodes: [], Original ATen: [aten.div, aten.threshold_backward]

triton_poi_fused_div_threshold_backward_1 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4194304], 
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_div_threshold_backward_1', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 4181760
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x3 = xindex
    x0 = xindex % 4320
    x2 = (xindex // 522720)
    tmp0 = tl.load(in_ptr0 + (x3), xmask).to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (x0 + (4320*x2)), xmask, eviction_policy='evict_last')
    tmp2 = 121.0
    tmp3 = tmp1 / tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zb/czbcpkvyshlbnzj3ngp2yinlvfcudqb24edxw54arcrpi6j7dd7f.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_2 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_2', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp5 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (3456 + x0 + (4320*r2) + (522720*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp0 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rz/crznslu4scqaoxc2k2vl54vvtcedvbfxxjlxtectc6723bqk2tua.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_3 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 8],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_3', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 864
    rnumel = 8
    RBLOCK: tl.constexpr = 8
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (864*r1)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/23/c23tvrgj5lwnzj2fm2wgkn5sfzus432dxnffncy747edygn5fhq4.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_4 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 8],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_4', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 864
    rnumel = 8
    RBLOCK: tl.constexpr = 8
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (864*r1)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/e6/ce6fe2jgvs5vif4qombaq4mo3ub3bn7hb4lzypdrv3mck3jpbogn.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_5 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1048576], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_5', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 836352
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 864
    x1 = (xindex // 864)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (3456 + x0 + (4320*x1)), xmask)
    tmp1 = tl.load(in_ptr1 + (x2), xmask)
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0010330578512396695
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2), tmp17, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/4m/c4mshidmekhvimgfthdeo4m22zv3owdmkhjd6tz7dgqfxc4jt74d.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_6 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_6', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    x3 = xindex
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    tmp9 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp13 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.load(in_ptr1 + (r2 + (121*x3)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp8 = tl.load(in_ptr2 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp4 = tl.where(tmp2, tmp1, tmp3)
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
        tmp10 = tmp8 - tmp9
        tmp11 = tmp4 * tmp10
        tmp12 = tl.broadcast_to(tmp11, [XBLOCK, RBLOCK])
        tmp14 = _tmp13 + tmp12
        _tmp13 = tl.where(rmask & xmask, tmp14, _tmp13)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tmp13 = tl.sum(_tmp13, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp13, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ov/cov3426dwnw6mjmrnxgzkekqoalvo3yzg2pf52siy3j4ggi76vu5.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 968
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 121
    y1 = (yindex // 121)
    tmp0 = tl.load(in_ptr0 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (121*x2) + (104544*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr2 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp7 = tmp5 - tmp6
    tmp9 = 0.0010330578512396695
    tmp10 = tmp8 * tmp9
    tmp12 = tmp11 * tmp11
    tmp13 = tmp10 * tmp12
    tmp14 = tmp7 * tmp13
    tmp15 = tmp4 - tmp14
    tmp17 = tmp16 * tmp9
    tmp18 = tmp15 - tmp17
    tmp20 = tmp11 * tmp19
    tmp21 = tmp18 * tmp20
    tl.store(out_ptr0 + (x2 + (864*y3)), tmp21, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/66/c66mej7fy4v7nwnze2fvjow27ps5n6v52paea7ndak7gkyklaley.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_8 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_8', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp5 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (2592 + x0 + (4320*r2) + (522720*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp0 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zr/czripapodwpcxr27mauewdp55ggjk73sjxcva6dvyae2pupzqxi5.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_9 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1048576], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_9', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 836352
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 864
    x1 = (xindex // 864)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (2592 + x0 + (4320*x1)), xmask)
    tmp1 = tl.load(in_ptr1 + (x2), xmask)
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0010330578512396695
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2), tmp17, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/t4/ct4eulqmeer7gs5fcgmy5bz3k2h3oqqluo3laya2rt55d3lsuzmp.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_10 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_10', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, out_ptr1, out_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    x3 = xindex
    _tmp8 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    tmp11 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp15 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    tmp18 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
    _tmp22 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (1728 + x0 + (4320*r2) + (522720*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp4 = tl.load(in_ptr2 + (r2 + (121*x3)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp10 = tl.load(in_ptr3 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp17 = tl.load(in_ptr5 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = 0.0
        tmp3 = tmp1 <= tmp2
        tmp5 = tl.where(tmp3, tmp2, tmp4)
        tmp6 = tmp0 + tmp5
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(rmask & xmask, tmp9, _tmp8)
        tmp12 = tmp10 - tmp11
        tmp13 = tmp6 * tmp12
        tmp14 = tl.broadcast_to(tmp13, [XBLOCK, RBLOCK])
        tmp16 = _tmp15 + tmp14
        _tmp15 = tl.where(rmask & xmask, tmp16, _tmp15)
        tmp19 = tmp17 - tmp18
        tmp20 = tmp6 * tmp19
        tmp21 = tl.broadcast_to(tmp20, [XBLOCK, RBLOCK])
        tmp23 = _tmp22 + tmp21
        _tmp22 = tl.where(rmask & xmask, tmp23, _tmp22)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp8, xmask)
    tmp15 = tl.sum(_tmp15, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp15, xmask)
    tmp22 = tl.sum(_tmp22, 1)[:, None]
    tl.store(out_ptr2 + (x3), tmp22, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rw/crwxgrmqytthzggmh3hry3p3ljldoy3hxff7h6zrxc7okuydu3vj.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_11 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: '*fp32', 15: '*fp32', 16: 'i32', 17: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(16, 17))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_11', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, in_ptr12, in_ptr13, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 968
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 121
    y1 = (yindex // 121)
    tmp0 = tl.load(in_ptr0 + (1728 + x2 + (4320*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0 + (121*x2) + (104544*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr9 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp27 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr12 + (x2), xmask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr13 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tmp1 <= tmp2
    tmp5 = tl.where(tmp3, tmp2, tmp4)
    tmp6 = tmp0 + tmp5
    tmp9 = tmp7 - tmp8
    tmp11 = 0.0010330578512396695
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp6 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tmp26 = tmp24 - tmp25
    tmp28 = tmp27 * tmp11
    tmp30 = tmp29 * tmp29
    tmp31 = tmp28 * tmp30
    tmp32 = tmp26 * tmp31
    tmp33 = tmp6 - tmp32
    tmp34 = tmp33 - tmp19
    tmp36 = tmp29 * tmp35
    tmp37 = tmp34 * tmp36
    tl.store(out_ptr0 + (x2 + (864*y3)), tmp23, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (864*y3)), tmp37, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/sj/csjkrppcpu6dksvxsl3v76nkkgo6zy56lvht2eu7ihs4b7owjdrb.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_12 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_12', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp5 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (864 + x0 + (4320*r2) + (522720*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp0 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/s2/cs2b6j6assvpwgzjydg55toygaehasgo2qmdneywozpjmfy2sa7k.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_13 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1048576], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_13', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 836352
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 864
    x1 = (xindex // 864)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (864 + x0 + (4320*x1)), xmask)
    tmp1 = tl.load(in_ptr1 + (x2), xmask)
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0010330578512396695
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2), tmp17, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/k4/ck4aond37msujvxdiqkfq3ryrpjsmehnsix5jdajf6sdiwsj65yf.py
# Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]

triton_poi_fused_add_threshold_backward_14 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7, 8))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_threshold_backward_14', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 968
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 121
    y1 = (yindex // 121)
    tmp0 = tl.load(in_ptr0 + (3456 + x2 + (4320*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr2 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (y0 + (121*x2) + (104544*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr4 + (y0 + (121*x2) + (104544*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp12 = tl.load(in_out_ptr0 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr5 + (y0 + (121*x2) + (104544*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp4 = 0.0
    tmp5 = tmp3 <= tmp4
    tmp7 = tl.where(tmp5, tmp4, tmp6)
    tmp8 = tmp2 + tmp7
    tmp10 = tl.where(tmp5, tmp4, tmp9)
    tmp11 = tmp8 + tmp10
    tmp13 = tmp11 + tmp12
    tmp15 = tl.where(tmp5, tmp4, tmp14)
    tmp16 = tmp13 + tmp15
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (864*y3)), tmp16, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zk/czkukrb5kaz7cyuk4jpv7lqvh7frwpiqrwttq4r76y6lg47z7y5k.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_15 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_15', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp5 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (4320*r2) + (522720*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp0 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/g7/cg77lowncqentstmjmh5o2xqcfjgwtg7xhbjh3i6wfvsm2jdpi3o.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_16 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1048576], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_16', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 836352
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 864
    x1 = (xindex // 864)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (4320*x1)), xmask)
    tmp1 = tl.load(in_ptr1 + (x2), xmask)
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0010330578512396695
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2), tmp17, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/a4/ca4fhos6e4jh3n76dygamok5dw4yc36su3f5ki63g37bt4vnaxpc.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_17 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_17', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp5 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp0 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/fj/cfjmg2cjwprpzdtft5zjf3iir5zv7uvwxuhaccra54dncyw6h2sj.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_18 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1048576], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_18', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, xnumel, XBLOCK : tl.constexpr):
    xnumel = 836352
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = xindex % 864
    tmp0 = tl.load(in_out_ptr0 + (x2), xmask)
    tmp1 = tl.load(in_ptr0 + (x2), xmask)
    tmp2 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0010330578512396695
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(in_out_ptr0 + (x2), tmp17, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ma/cmathhh7c464igrid56555iml5fv3jbxioawyxz55csn77x4tka6.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_19 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_19', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    x3 = xindex
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    tmp14 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    _tmp18 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.load(in_ptr1 + (r2 + (121*x3)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp5 = tl.load(in_ptr2 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp7 = tl.load(in_ptr3 + (r2 + (121*x3)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp13 = tl.load(in_ptr4 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp4 = tl.where(tmp2, tmp1, tmp3)
        tmp6 = tmp4 + tmp5
        tmp8 = tl.where(tmp2, tmp1, tmp7)
        tmp9 = tmp6 + tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
        tmp15 = tmp13 - tmp14
        tmp16 = tmp9 * tmp15
        tmp17 = tl.broadcast_to(tmp16, [XBLOCK, RBLOCK])
        tmp19 = _tmp18 + tmp17
        _tmp18 = tl.where(rmask & xmask, tmp19, _tmp18)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
    tmp18 = tl.sum(_tmp18, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp18, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5i/c5idjnkd7ewyd4xdrt3gcbrpplwy2qgwr6yajaqpcwfc2xwik3ak.py
# Source Nodes: [], Original ATen: [aten.add, aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_20 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_20', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 968
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 121
    y1 = (yindex // 121)
    tmp0 = tl.load(in_ptr0 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (121*x2) + (104544*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_out_ptr0 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr2 + (y0 + (121*x2) + (104544*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr3 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp6 = tmp4 + tmp5
    tmp8 = tl.where(tmp2, tmp1, tmp7)
    tmp9 = tmp6 + tmp8
    tmp12 = tmp10 - tmp11
    tmp14 = 0.0010330578512396695
    tmp15 = tmp13 * tmp14
    tmp17 = tmp16 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp12 * tmp18
    tmp20 = tmp9 - tmp19
    tmp22 = tmp21 * tmp14
    tmp23 = tmp20 - tmp22
    tmp25 = tmp16 * tmp24
    tmp26 = tmp23 * tmp25
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (864*y3)), tmp26, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/if/cifabdbvm3otolms3unzxq2rorabywfyjhbvk3yjtjdntezkhvot.py
# Source Nodes: [], Original ATen: [aten.threshold_backward]

triton_poi_fused_threshold_backward_21 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[65536, 128], tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_threshold_backward_21', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 34560
    xnumel = 121
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 4320
    y1 = (yindex // 4320)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (4320*x2) + (522720*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_out_ptr0 + (x2 + (121*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (121*y3)), tmp4, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/vm/cvmaxkjiryaxryln7nljpomalavwja6nzqovmqj2qxl2n3rdsjjb.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_22 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_22', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
    xnumel = 864
    XBLOCK: tl.constexpr = 1
    rnumel = 968
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r1 = rindex % 121
    r2 = (rindex // 121)
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (418176 + r1 + (121*x0) + (522720*r2)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/fc/cfc4awa4dgm4uqcyvd2gqlqhk4s7rwv3y5ehpwwbceji6bemx4tk.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_23 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_23', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (418176 + r2 + (121*x0) + (522720*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/f5/cf5fvr7pub7l4yf7vmmbb3gjufiqmvv3mlrcor5w3r6gryfned5s.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_24 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_24', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 968
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 121
    y1 = (yindex // 121)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (418176 + y0 + (121*x2) + (522720*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0010330578512396695
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (864*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/bb/cbbmjkj5nolr5cfa52m4l2zeeaodbwyqolhodabuvbmc2zh4ehjx.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_25 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_25', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
    xnumel = 864
    XBLOCK: tl.constexpr = 1
    rnumel = 968
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r1 = rindex % 121
    r2 = (rindex // 121)
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (313632 + r1 + (121*x0) + (522720*r2)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/q7/cq7p2z35tj2lcv3n2722osmw6bknzog3uekhr6fasrri4jvfgxlc.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_26 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_26', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (313632 + r2 + (121*x0) + (522720*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/v6/cv6l22tovanktcktyvuuhy5hydwhfy62mb6rpwlagf6zrrjbrkfv.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_27 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_27', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 968
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 121
    y1 = (yindex // 121)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (313632 + y0 + (121*x2) + (522720*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0010330578512396695
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (864*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ds/cdssieryxgznydyqa3tuc7jhcpqr5ixtf4amisrwwtgbswm3m2uk.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_28 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_28', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
    xnumel = 864
    XBLOCK: tl.constexpr = 1
    rnumel = 968
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r1 = rindex % 121
    r2 = (rindex // 121)
    x0 = xindex
    r3 = rindex
    tmp0 = tl.load(in_ptr0 + (209088 + r1 + (121*x0) + (522720*r2)), rmask & xmask, other=0.0)
    tmp1 = tl.load(in_ptr1 + (x0 + (864*r3)), rmask & xmask, other=0.0)
    tmp4 = tl.load(in_ptr2 + (r1 + (121*x0) + (104544*r2)), rmask & xmask, other=0.0)
    tmp2 = 0.0
    tmp3 = tmp1 <= tmp2
    tmp5 = tl.where(tmp3, tmp2, tmp4)
    tmp6 = tmp0 + tmp5
    tmp7 = tl.broadcast_to(tmp6, [RBLOCK])
    tmp9 = tl.where(rmask & xmask, tmp7, 0)
    tmp10 = triton_helpers.promote_to_tensor(tl.sum(tmp9, 0))
    tl.store(out_ptr0 + (x0), tmp10, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/f6/cf6zip7gpp7zlspgztnqgr23uwskmsyp6nxclb7paju3tpaiep6d.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_29 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_29', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    x3 = xindex
    tmp8 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    tmp15 = tl.load(in_ptr6 + (x0), xmask, eviction_policy='evict_last')
    _tmp19 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (209088 + r2 + (121*x0) + (522720*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp4 = tl.load(in_ptr2 + (r2 + (121*x3)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp7 = tl.load(in_ptr3 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp14 = tl.load(in_ptr5 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = 0.0
        tmp3 = tmp1 <= tmp2
        tmp5 = tl.where(tmp3, tmp2, tmp4)
        tmp6 = tmp0 + tmp5
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
        tmp16 = tmp14 - tmp15
        tmp17 = tmp6 * tmp16
        tmp18 = tl.broadcast_to(tmp17, [XBLOCK, RBLOCK])
        tmp20 = _tmp19 + tmp18
        _tmp19 = tl.where(rmask & xmask, tmp20, _tmp19)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp12, xmask)
    tmp19 = tl.sum(_tmp19, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp19, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/n2/cn22nffwnn6ujvr2zydxq34tphnegj5hfhzeuqimqmgluvsz4ick.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_30 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: '*fp32', 15: '*fp32', 16: 'i32', 17: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(16, 17))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_30', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, in_ptr12, in_ptr13, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 968
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 121
    y1 = (yindex // 121)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (209088 + y0 + (121*x2) + (522720*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0 + (121*x2) + (104544*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr9 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp27 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr12 + (x2), xmask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr13 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tmp1 <= tmp2
    tmp5 = tl.where(tmp3, tmp2, tmp4)
    tmp6 = tmp0 + tmp5
    tmp9 = tmp7 - tmp8
    tmp11 = 0.0010330578512396695
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp6 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tmp26 = tmp24 - tmp25
    tmp28 = tmp27 * tmp11
    tmp30 = tmp29 * tmp29
    tmp31 = tmp28 * tmp30
    tmp32 = tmp26 * tmp31
    tmp33 = tmp6 - tmp32
    tmp34 = tmp33 - tmp19
    tmp36 = tmp29 * tmp35
    tmp37 = tmp34 * tmp36
    tl.store(out_ptr0 + (x2 + (864*y3)), tmp23, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (864*y3)), tmp37, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/nv/cnvlrtkkpic4hmeru2zlhravshqdukecoagy46hy6676xksjhzmb.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_31 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_31', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
    xnumel = 864
    XBLOCK: tl.constexpr = 1
    rnumel = 968
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r1 = rindex % 121
    r2 = (rindex // 121)
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (104544 + r1 + (121*x0) + (522720*r2)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5m/c5moc2jknrhqznq2wmtoilwq5s72dskorh2zz6ee6xkql65bna23.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_32 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_32', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (104544 + r2 + (121*x0) + (522720*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ik/cik6yn6gr2j6bnddp3ghfk6v5qrh6tvfgjnacwhjbxozkusflzdm.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_33 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_33', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 968
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 121
    y1 = (yindex // 121)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (104544 + y0 + (121*x2) + (522720*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0010330578512396695
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (864*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/f5/cf5d3r2dzfxpzkq2eijhtqz4rjmuypf7wg5rlg6pndoatwyp3wcr.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_34 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_34', 'mutated_arg_names': ['in_out_ptr0']}
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    x3 = xindex
    tmp18 = tl.load(in_ptr7 + (x0), xmask, eviction_policy='evict_last')
    _tmp22 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (418176 + r2 + (121*x0) + (522720*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.load(in_ptr2 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp6 = tl.load(in_out_ptr0 + (r2 + (121*x3)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp9 = tl.load(in_ptr3 + (r2 + (121*x3)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp12 = tl.load(in_ptr4 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp14 = tl.load(in_ptr5 + (r2 + (121*x3)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp17 = tl.load(in_ptr6 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = tmp0 + tmp1
        tmp4 = 0.0
        tmp5 = tmp3 <= tmp4
        tmp7 = tl.where(tmp5, tmp4, tmp6)
        tmp8 = tmp2 + tmp7
        tmp10 = tl.where(tmp5, tmp4, tmp9)
        tmp11 = tmp8 + tmp10
        tmp13 = tmp11 + tmp12
        tmp15 = tl.where(tmp5, tmp4, tmp14)
        tmp16 = tmp13 + tmp15
        tmp19 = tmp17 - tmp18
        tmp20 = tmp16 * tmp19
        tmp21 = tl.broadcast_to(tmp20, [XBLOCK, RBLOCK])
        tmp23 = _tmp22 + tmp21
        _tmp22 = tl.where(rmask & xmask, tmp23, _tmp22)
        tl.store(in_out_ptr0 + (r2 + (121*x3)), tmp16, rmask & xmask)
    tmp22 = tl.sum(_tmp22, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp22, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/6b/c6b2by563zn5iqo5b4l37aq46mzrengtvzfxct5udfubdricouw6.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_35 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_35', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
    xnumel = 864
    XBLOCK: tl.constexpr = 1
    rnumel = 968
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r1 = rindex % 121
    r2 = (rindex // 121)
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (121*x0) + (522720*r2)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rn/crnjkyc32lvtz3bh53lya7ewunwd7waacd2rmr3fmkc4hdialitx.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_36 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_36', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (r2 + (121*x0) + (522720*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ij/cijmknu4mtgz3h2zt6vowmco7prc55tcmhrj4s22f7oqlzc6zbhf.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_37 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_37', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 968
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 121
    y1 = (yindex // 121)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (121*x2) + (522720*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0010330578512396695
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (864*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ra/craiyoc46jnneojgnteenxvu4xk2cicjvgpss5nuwdkpdhfuexgs.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_38 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_38', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
    xnumel = 864
    XBLOCK: tl.constexpr = 1
    rnumel = 968
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r1 = rindex % 121
    r2 = (rindex // 121)
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (121*x0) + (104544*r2)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/s7/cs7eaxh5sji2c6hbonfbirz667ddw5dgez7gglle7jl2cwt6grcg.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_39 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_39', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 968
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 121
    y1 = (yindex // 121)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (121*x2) + (104544*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0010330578512396695
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (864*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/l3/cl3byhyxp3dskz545kq2fivkqkw3sicbv4z46he6umwxvfgu2tgi.py
# Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]

triton_poi_fused_add_threshold_backward_40 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[65536, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_threshold_backward_40', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 34560
    xnumel = 121
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 4320
    y1 = (yindex // 4320)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (4320*x2) + (522720*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_out_ptr0 + (x2 + (121*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr1 + (x2 + (121*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp6 = tl.where(tmp2, tmp1, tmp5)
    tmp7 = tmp4 + tmp6
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (121*y3)), tmp7, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/q3/cq3ybo5mxbdyitsxsbw4ul2csffm4wrjkyfspjoaozhb2d4juxhn.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_41 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_41', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 968
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 121
    y1 = (yindex // 121)
    tmp0 = tl.load(in_ptr0 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (121*x2) + (104544*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_out_ptr0 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr2 + (y0 + (121*x2) + (104544*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr3 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp6 = tmp4 + tmp5
    tmp8 = tl.where(tmp2, tmp1, tmp7)
    tmp9 = tmp6 + tmp8
    tmp12 = tmp10 - tmp11
    tmp14 = 0.0010330578512396695
    tmp15 = tmp13 * tmp14
    tmp17 = tmp16 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp12 * tmp18
    tmp20 = tmp9 - tmp19
    tmp22 = tmp21 * tmp14
    tmp23 = tmp20 - tmp22
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (864*y3)), tmp23, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/a4/ca4t5arz5ur4nroxfndiuyckayfae5ijghwsyb4tnvgkfpambemn.py
# Source Nodes: [], Original ATen: [aten.convolution_backward]

triton_poi_fused_convolution_backward_42 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[524288], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_42', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 418176
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 432
    x1 = (xindex // 432)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (432 + x0 + (864*x1)), xmask)
    tmp1 = tl.load(in_ptr1 + (432 + x0), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (432 + x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tl.store(out_ptr0 + (x2), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ep/cepydrgh7k2k2wc34hpnlohu7bb56dbbhgjpnx5yftyb3tceoai7.py
# Source Nodes: [], Original ATen: [aten.convolution_backward]

triton_poi_fused_convolution_backward_43 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[524288], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_43', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 418176
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 432
    x1 = (xindex // 432)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (864*x1)), xmask)
    tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tl.store(out_ptr0 + (x2), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/n4/cn4wjymt3d5x2s7626hslyqufkirbhd2dmkvi355mnniyuui4p6l.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_44 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_44', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 6912
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp9 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp13 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (418176 + r2 + (121*x0) + (522720*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp8 = tl.load(in_ptr3 + (x0 + (864*r2) + (104544*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
        tmp10 = tmp8 - tmp9
        tmp11 = tmp0 * tmp10
        tmp12 = tl.broadcast_to(tmp11, [XBLOCK, RBLOCK])
        tmp14 = _tmp13 + tmp12
        _tmp13 = tl.where(rmask & xmask, tmp14, _tmp13)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tmp13 = tl.sum(_tmp13, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp13, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/26/c26fo2dwq7fivu46xj5iey5d7gm46i4qildyyir5jdcfw2ae6vvv.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_45 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: 'i32', 15: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(14, 15))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_45', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 968
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 121
    y1 = (yindex // 121)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (418176 + y0 + (121*x2) + (522720*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr9 + (x2), xmask, eviction_policy='evict_last')
    tmp23 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0010330578512396695
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tmp20 = tmp18 - tmp19
    tmp22 = tmp21 * tmp5
    tmp24 = tmp23 * tmp23
    tmp25 = tmp22 * tmp24
    tmp26 = tmp20 * tmp25
    tmp27 = tmp0 - tmp26
    tmp28 = tmp27 - tmp13
    tmp30 = tmp23 * tmp29
    tmp31 = tmp28 * tmp30
    tl.store(out_ptr0 + (x2 + (864*y3)), tmp17, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (864*y3)), tmp31, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/k5/ck5spxdheewc6lbwaan6qpakn5u2g7i32tmaxsynw3fqdhhdu2g6.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.threshold_backward]

triton_poi_fused_add_constant_pad_nd_threshold_backward_46 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*i1', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_constant_pad_nd_threshold_backward_46', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6912
    xnumel = 441
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x4 = xindex
    y0 = yindex % 864
    y1 = (yindex // 864)
    y5 = yindex
    x3 = (xindex // 21)
    x2 = xindex % 21
    tmp0 = tl.load(in_ptr0 + (y0 + (864*x4) + (381024*y1)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_out_ptr0 + (x4 + (441*y5)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp4 = 1 + x3
    tmp5 = tl.full([1, 1], 0, tl.int64)
    tmp6 = tmp4 >= tmp5
    tmp7 = tl.full([1, 1], 23, tl.int64)
    tmp8 = tmp4 < tmp7
    tmp9 = 1 + x2
    tmp10 = tmp9 >= tmp5
    tmp11 = tmp9 < tmp7
    tmp12 = tmp6 & tmp8
    tmp13 = tmp12 & tmp10
    tmp14 = tmp13 & tmp11
    tmp15 = tl.load(in_ptr1 + (20736 + y0 + (864*x2) + (19872*x3) + (457056*y1)), tmp14 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp16 = tl.full(tmp15.shape, 0.0, tmp15.dtype)
    tmp17 = tl.where(tmp14, tmp15, tmp16)
    tmp18 = tmp3 + tmp17
    tmp19 = tl.load(in_ptr2 + (24 + x2 + (23*x3) + (529*y5)), tmp14 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp20 = tl.full(tmp19.shape, 0.0, tmp19.dtype)
    tmp21 = tl.where(tmp14, tmp19, tmp20)
    tmp22 = tl.where(tmp0, tmp2, tmp21)
    tmp23 = tmp18 + tmp22
    tmp24 = 2 + x3
    tmp25 = tmp24 >= tmp5
    tmp26 = tl.full([1, 1], 25, tl.int64)
    tmp27 = tmp24 < tmp26
    tmp28 = 2 + x2
    tmp29 = tmp28 >= tmp5
    tmp30 = tmp28 < tmp26
    tmp31 = tmp25 & tmp27
    tmp32 = tmp31 & tmp29
    tmp33 = tmp32 & tmp30
    tmp34 = tl.load(in_ptr3 + (52 + x2 + (25*x3) + (625*y5)), tmp33 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp35 = tl.full(tmp34.shape, 0.0, tmp34.dtype)
    tmp36 = tl.where(tmp33, tmp34, tmp35)
    tmp37 = tl.where(tmp0, tmp2, tmp36)
    tmp38 = tmp23 + tmp37
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x4 + (441*y5)), tmp38, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/a7/ca7qegoz4fje4k3mpo3heobhoebus5sgbuu4rb7kw2lyxzrnvn26.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_47 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_47', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 24192
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    _tmp34 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (381024*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp16 = tl.load(in_ptr2 + (x1 + (864*r2) + (108864*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = 1 + (((r2 + (126*x0)) // 21) % 21)
        tmp2 = tl.full([1, 1], 0, tl.int64)
        tmp3 = tmp1 >= tmp2
        tmp4 = tl.full([1, 1], 23, tl.int64)
        tmp5 = tmp1 < tmp4
        tmp6 = 1 + (r2 % 21)
        tmp7 = tmp6 >= tmp2
        tmp8 = tmp6 < tmp4
        tmp9 = tmp3 & tmp5
        tmp10 = tmp9 & tmp7
        tmp11 = tmp10 & tmp8
        tmp12 = tl.load(in_ptr1 + (20736 + x1 + (864*(r2 % 21)) + (19872*(((r2 + (126*x0)) // 21) % 21)) + (457056*((r2 + (126*x0)) // 441))), rmask & tmp11 & xmask, eviction_policy='evict_last', other=0.0)
        tmp13 = tl.full(tmp12.shape, 0.0, tmp12.dtype)
        tmp14 = tl.where(tmp11, tmp12, tmp13)
        tmp15 = tmp0 + tmp14
        tmp17 = 3 + (((r2 + (126*x0)) // 21) % 21)
        tmp18 = tmp17 >= tmp2
        tmp19 = tl.full([1, 1], 27, tl.int64)
        tmp20 = tmp17 < tmp19
        tmp21 = 3 + (r2 % 21)
        tmp22 = tmp21 >= tmp2
        tmp23 = tmp21 < tmp19
        tmp24 = tmp18 & tmp20
        tmp25 = tmp24 & tmp22
        tmp26 = tmp25 & tmp23
        tmp27 = tl.load(in_ptr3 + (84 + (27*(((r2 + (126*x0)) // 21) % 21)) + (729*x1) + (629856*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & tmp26 & xmask, eviction_policy='evict_last', other=0.0)
        tmp28 = tl.full(tmp27.shape, 0.0, tmp27.dtype)
        tmp29 = tl.where(tmp26, tmp27, tmp28)
        tmp30 = 0.0
        tmp31 = tl.where(tmp16, tmp30, tmp29)
        tmp32 = tmp15 + tmp31
        tmp33 = tl.broadcast_to(tmp32, [XBLOCK, RBLOCK])
        tmp35 = _tmp34 + tmp33
        _tmp34 = tl.where(rmask & xmask, tmp35, _tmp34)
    tmp34 = tl.sum(_tmp34, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp34, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ja/cjaabs3lgv57lnj54vlqhjmfmz345v4fajvwdhrkrmzguzu2ulnt.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_48 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 32],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_48', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 864
    rnumel = 28
    RBLOCK: tl.constexpr = 32
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (28*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7a/c7anxgl6eah36xa2hwedu77m2dymenmwnf332v4xwbsbik7e5gih.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_49 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_49', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 24192
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 864
    x1 = (xindex // 864)
    tmp34 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    _tmp38 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((21*(((r2 + (126*x1)) // 21) % 21)) + (441*x0) + (381024*((r2 + (126*x1)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp16 = tl.load(in_ptr2 + (x0 + (864*r2) + (108864*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp33 = tl.load(in_ptr4 + (x0 + (864*r2) + (108864*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = 1 + (((r2 + (126*x1)) // 21) % 21)
        tmp2 = tl.full([1, 1], 0, tl.int64)
        tmp3 = tmp1 >= tmp2
        tmp4 = tl.full([1, 1], 23, tl.int64)
        tmp5 = tmp1 < tmp4
        tmp6 = 1 + (r2 % 21)
        tmp7 = tmp6 >= tmp2
        tmp8 = tmp6 < tmp4
        tmp9 = tmp3 & tmp5
        tmp10 = tmp9 & tmp7
        tmp11 = tmp10 & tmp8
        tmp12 = tl.load(in_ptr1 + (20736 + x0 + (864*(r2 % 21)) + (19872*(((r2 + (126*x1)) // 21) % 21)) + (457056*((r2 + (126*x1)) // 441))), rmask & tmp11 & xmask, eviction_policy='evict_last', other=0.0)
        tmp13 = tl.full(tmp12.shape, 0.0, tmp12.dtype)
        tmp14 = tl.where(tmp11, tmp12, tmp13)
        tmp15 = tmp0 + tmp14
        tmp17 = 3 + (((r2 + (126*x1)) // 21) % 21)
        tmp18 = tmp17 >= tmp2
        tmp19 = tl.full([1, 1], 27, tl.int64)
        tmp20 = tmp17 < tmp19
        tmp21 = 3 + (r2 % 21)
        tmp22 = tmp21 >= tmp2
        tmp23 = tmp21 < tmp19
        tmp24 = tmp18 & tmp20
        tmp25 = tmp24 & tmp22
        tmp26 = tmp25 & tmp23
        tmp27 = tl.load(in_ptr3 + (84 + (27*(((r2 + (126*x1)) // 21) % 21)) + (729*x0) + (629856*((r2 + (126*x1)) // 441)) + (r2 % 21)), rmask & tmp26 & xmask, eviction_policy='evict_last', other=0.0)
        tmp28 = tl.full(tmp27.shape, 0.0, tmp27.dtype)
        tmp29 = tl.where(tmp26, tmp27, tmp28)
        tmp30 = 0.0
        tmp31 = tl.where(tmp16, tmp30, tmp29)
        tmp32 = tmp15 + tmp31
        tmp35 = tmp33 - tmp34
        tmp36 = tmp32 * tmp35
        tmp37 = tl.broadcast_to(tmp36, [XBLOCK, RBLOCK])
        tmp39 = _tmp38 + tmp37
        _tmp38 = tl.where(rmask & xmask, tmp39, _tmp38)
    tmp38 = tl.sum(_tmp38, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp38, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/sx/csx3556h2sxbskzvvjy3qjeoceukh27gvfwqyuwboxx45nqjgqli.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_50 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 32],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_50', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 864
    rnumel = 28
    RBLOCK: tl.constexpr = 32
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (864*r1)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ra/crarqbslwbvnzpkkuhrmgdtglcjyj6yjhq5ppgv7bpvqd6hwrgac.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_51 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_51', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3528
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x3 = xindex
    y2 = (yindex // 441)
    y4 = yindex % 441
    y1 = (yindex // 21) % 21
    y0 = yindex % 21
    y6 = yindex
    tmp0 = tl.load(in_ptr0 + (y4 + (441*x3) + (381024*y2)), xmask & ymask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr2 + (x3 + (864*y6)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp33 = tl.load(in_ptr4 + (x3 + (864*y6)), xmask & ymask, eviction_policy='evict_last')
    tmp34 = tl.load(in_ptr5 + (x3), xmask, eviction_policy='evict_last')
    tmp36 = tl.load(in_ptr6 + (x3), xmask, eviction_policy='evict_last')
    tmp39 = tl.load(in_ptr7 + (x3), xmask, eviction_policy='evict_last')
    tmp1 = 1 + y1
    tmp2 = tl.full([1, 1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1, 1], 23, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = 1 + y0
    tmp7 = tmp6 >= tmp2
    tmp8 = tmp6 < tmp4
    tmp9 = tmp3 & tmp5
    tmp10 = tmp9 & tmp7
    tmp11 = tmp10 & tmp8
    tmp12 = tl.load(in_ptr1 + (20736 + x3 + (864*y0) + (19872*y1) + (457056*y2)), tmp11 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp13 = tl.full(tmp12.shape, 0.0, tmp12.dtype)
    tmp14 = tl.where(tmp11, tmp12, tmp13)
    tmp15 = tmp0 + tmp14
    tmp17 = 3 + y1
    tmp18 = tmp17 >= tmp2
    tmp19 = tl.full([1, 1], 27, tl.int64)
    tmp20 = tmp17 < tmp19
    tmp21 = 3 + y0
    tmp22 = tmp21 >= tmp2
    tmp23 = tmp21 < tmp19
    tmp24 = tmp18 & tmp20
    tmp25 = tmp24 & tmp22
    tmp26 = tmp25 & tmp23
    tmp27 = tl.load(in_ptr3 + (84 + y0 + (27*y1) + (729*x3) + (629856*y2)), tmp26 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp28 = tl.full(tmp27.shape, 0.0, tmp27.dtype)
    tmp29 = tl.where(tmp26, tmp27, tmp28)
    tmp30 = 0.0
    tmp31 = tl.where(tmp16, tmp30, tmp29)
    tmp32 = tmp15 + tmp31
    tmp35 = tmp33 - tmp34
    tmp37 = 0.0002834467120181406
    tmp38 = tmp36 * tmp37
    tmp40 = tmp39 * tmp39
    tmp41 = tmp38 * tmp40
    tmp42 = tmp35 * tmp41
    tmp43 = tmp32 - tmp42
    tl.store(out_ptr0 + (x3 + (864*y6)), tmp43, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7j/c7jkyrapnnan4cnb5ulcyt576toeqbebn3zkoyqhzwqo2itrkaq6.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_52 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_52', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6912
    xnumel = 441
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 864
    y1 = (yindex // 864)
    tmp0 = tl.load(in_out_ptr0 + (y0 + (864*x2) + (381024*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr0 + (y0), ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr1 + (y0), ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    tmp2 = 0.0002834467120181406
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 - tmp3
    tmp7 = tmp5 * tmp6
    tmp8 = tmp4 * tmp7
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (y0 + (864*x2) + (381024*y1)), tmp8, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/kh/ckhekxen5wt5aqce7gqpgkvfdcqxsibivlhzcorzm7bm4asi5zlf.py
# Source Nodes: [], Original ATen: [aten.add, aten.avg_pool2d_backward, aten.constant_pad_nd, aten.threshold_backward]

triton_poi_fused_add_avg_pool2d_backward_constant_pad_nd_threshold_backward_53 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_avg_pool2d_backward_constant_pad_nd_threshold_backward_53', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 17280
    xnumel = 441
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = (xindex // 21)
    x1 = xindex % 21
    y0 = yindex
    x3 = xindex
    y4 = yindex % 2160
    y5 = (yindex // 2160)
    tmp24 = tl.load(in_ptr1 + ((11*(tl.math.min(tl.math.max(0, ((1 + x2) // 2)), (-1) + (tl.math.min(11, 1 + (x2 // 2)))))) + (11*(tl.where((tl.math.min(tl.math.max(0, ((1 + x2) // 2)), (-1) + (tl.math.min(11, 1 + (x2 // 2))))) >= 0, 0, 11))) + (121*y0) + (tl.math.min(tl.math.max(0, ((1 + x1) // 2)), (-1) + (tl.math.min(11, 1 + (x1 // 2))))) + (tl.where((tl.math.min(tl.math.max(0, ((1 + x1) // 2)), (-1) + (tl.math.min(11, 1 + (x1 // 2))))) >= 0, 0, 11))), xmask & ymask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr2 + (y4 + (2160*x3) + (952560*y5)), xmask & ymask, eviction_policy='evict_last')
    tmp38 = tl.load(in_ptr3 + (x3 + (441*y0)), xmask & ymask, eviction_policy='evict_last')
    tmp0 = (-1) + x2
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 21, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = (-1) + x1
    tmp6 = tmp5 >= tmp1
    tmp7 = tmp5 < tmp3
    tmp8 = tmp2 & tmp4
    tmp9 = tmp8 & tmp6
    tmp10 = tmp9 & tmp7
    tmp11 = tl.load(in_ptr0 + ((11*(tl.math.min(tl.math.max(0, (x2 // 2)), (-1) + (tl.math.min(11, 1 + (((-1) + x2) // 2)))))) + (11*(tl.where((tl.math.min(tl.math.max(0, (x2 // 2)), (-1) + (tl.math.min(11, 1 + (((-1) + x2) // 2))))) >= 0, 0, 11))) + (121*y0) + (tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(11, 1 + (((-1) + x1) // 2))))) + (tl.where((tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(11, 1 + (((-1) + x1) // 2))))) >= 0, 0, 11))), tmp10 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp12 = tmp11 / 1
    tmp13 = tl.broadcast_to(tl.math.max(0, (x2 // 2)), [XBLOCK, YBLOCK])
    tmp14 = tl.broadcast_to(tl.math.min(11, 1 + (((-1) + x2) // 2)), [XBLOCK, YBLOCK])
    tmp15 = tmp13 < tmp14
    tmp16 = tl.broadcast_to(tl.math.max(0, (x1 // 2)), [XBLOCK, YBLOCK])
    tmp17 = tl.broadcast_to(tl.math.min(11, 1 + (((-1) + x1) // 2)), [XBLOCK, YBLOCK])
    tmp18 = tmp16 < tmp17
    tmp19 = tmp15 & tmp18
    tmp20 = 0.0
    tmp21 = tl.where(tmp19, tmp12, tmp20)
    tmp22 = tl.full(tmp21.shape, 0.0, tmp21.dtype)
    tmp23 = tl.where(tmp10, tmp21, tmp22)
    tmp25 = tmp24 / 1
    tmp26 = tl.math.max(0, ((1 + x2) // 2))
    tmp27 = tl.math.min(11, 1 + (x2 // 2))
    tmp28 = tmp26 < tmp27
    tmp29 = tl.math.max(0, ((1 + x1) // 2))
    tmp30 = tl.math.min(11, 1 + (x1 // 2))
    tmp31 = tmp29 < tmp30
    tmp32 = tmp28 & tmp31
    tmp33 = tl.where(tmp32, tmp25, tmp20)
    tmp34 = tmp23 + tmp33
    tmp36 = tmp35 <= tmp20
    tmp37 = tl.where(tmp36, tmp20, tmp34)
    tmp39 = tl.where(tmp36, tmp20, tmp38)
    tmp40 = tmp37 + tmp39
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3 + (441*y0)), tmp40, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ie/cieiemdbhqjif5cirshaxhhxcs6wp6ny3bwb7lgr36ohjq2eouqy.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.threshold_backward]

triton_poi_fused_add_constant_pad_nd_threshold_backward_54 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_constant_pad_nd_threshold_backward_54', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6912
    xnumel = 441
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x4 = xindex
    y0 = yindex % 864
    y1 = (yindex // 864)
    x3 = (xindex // 21)
    x2 = xindex % 21
    y5 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (864*x4) + (381024*y1)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = 1 + x3
    tmp2 = tl.full([1, 1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1, 1], 23, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = 1 + x2
    tmp7 = tmp6 >= tmp2
    tmp8 = tmp6 < tmp4
    tmp9 = tmp3 & tmp5
    tmp10 = tmp9 & tmp7
    tmp11 = tmp10 & tmp8
    tmp12 = tl.load(in_ptr1 + (24 + x2 + (23*x3) + (529*y5)), tmp11 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp13 = tl.full(tmp12.shape, 0.0, tmp12.dtype)
    tmp14 = tl.where(tmp11, tmp12, tmp13)
    tmp15 = 0.0
    tmp16 = tl.where(tmp0, tmp15, tmp14)
    tmp17 = tl.load(in_ptr2 + (20736 + y0 + (864*x2) + (19872*x3) + (457056*y1)), tmp11 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp18 = tl.full(tmp17.shape, 0.0, tmp17.dtype)
    tmp19 = tl.where(tmp11, tmp17, tmp18)
    tmp20 = tmp16 + tmp19
    tmp21 = 2 + x3
    tmp22 = tmp21 >= tmp2
    tmp23 = tl.full([1, 1], 25, tl.int64)
    tmp24 = tmp21 < tmp23
    tmp25 = 2 + x2
    tmp26 = tmp25 >= tmp2
    tmp27 = tmp25 < tmp23
    tmp28 = tmp22 & tmp24
    tmp29 = tmp28 & tmp26
    tmp30 = tmp29 & tmp27
    tmp31 = tl.load(in_ptr3 + (52 + x2 + (25*x3) + (625*y5)), tmp30 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp32 = tl.full(tmp31.shape, 0.0, tmp31.dtype)
    tmp33 = tl.where(tmp30, tmp31, tmp32)
    tmp34 = tl.where(tmp0, tmp15, tmp33)
    tmp35 = tmp20 + tmp34
    tl.store(out_ptr0 + (x4 + (441*y5)), tmp35, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/3m/c3mzjohy57vdfxpu642yeah6kdjxwl6cudowa4b47zj5er2gortw.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_55 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 4096],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_55', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 864
    rnumel = 3528
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex % 441
        r2 = (rindex // 441)
        tmp0 = tl.load(in_ptr0 + (r1 + (441*x0) + (381024*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/x6/cx6sz6klbyq4mnkr67qznfjgrkqyq6igsmmc3kdl55slskqotkie.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_56 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_56', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 24192
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp2 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (381024*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x1 + (864*r2) + (108864*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7i/c7ikguys4jb53bu3wdbf6yuto5yltnprqs4337uc3p5jnade5lce.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_57 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 32],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_57', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 864
    rnumel = 28
    RBLOCK: tl.constexpr = 32
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (28*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/h2/ch2vmcgicdy3b6i2itixgtbcxgtiksh4d77o77cvwj3jrbfnml3j.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_58 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_58', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3528
    xnumel = 864
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 441
    y1 = (yindex // 441)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (441*x2) + (381024*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (864*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0002834467120181406
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (864*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/do/cdocbt3fkxlktuwv7dxkkzw22ny4ltrhl53vukmy4vgepmngqatu.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_59 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 4096],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_59', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 3528
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex % 441
        r2 = (rindex // 441)
        tmp0 = tl.load(in_ptr0 + (762048 + r1 + (441*x0) + (952560*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rr/crrd4amzgp6xip3fksi5k35zwomhcxwdk3bvshoccs2chec4k3xd.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_60 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_60', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 12096
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp2 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (762048 + (21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (952560*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/iy/ciy3dq7flp3lk5i3vnspnyide3fo6fbl6v6pyxxnrexz2doq3ys4.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_61 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[512, 32],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_61', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 28
    RBLOCK: tl.constexpr = 32
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (28*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/px/cpxb2s2hs74oikwban3zv6apiimxgtqnm7lzdb3i3lur2ymadzxp.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_62 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_62', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3528
    xnumel = 432
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 441
    y1 = (yindex // 441)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (762048 + y0 + (441*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0002834467120181406
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (432*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/xg/cxgx5arjbrniapmt24zob3rdamwgsjc3irnb57uchpcvis7ydqr4.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_63 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_63', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 12096
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr1 + ((21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (190512*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp4 = tl.where(tmp2, tmp1, tmp3)
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/e3/ce3lf2sispcpay5mxgvvgjkyyz2av3tcg5z2nfx6syca5rmylywa.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_64 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[512, 32],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_64', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 28
    RBLOCK: tl.constexpr = 32
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (28*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/6h/c6hwt3lt66b76ruqbhkg734xpot7xxpjgrsrwohzzkzs43rktmkr.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_65 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_65', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 12096
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 432
    x1 = (xindex // 432)
    tmp6 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp10 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (432*r2) + (54432*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr1 + ((21*(((r2 + (126*x1)) // 21) % 21)) + (441*x0) + (190512*((r2 + (126*x1)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (x0 + (432*r2) + (54432*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp4 = tl.where(tmp2, tmp1, tmp3)
        tmp7 = tmp5 - tmp6
        tmp8 = tmp4 * tmp7
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, RBLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(rmask & xmask, tmp11, _tmp10)
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp10, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ba/cbani5j4fym6zkcqxbixsqyvqfh2ehwj35rclozroguzmq5vp23q.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_66 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[512, 32],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_66', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 28
    RBLOCK: tl.constexpr = 32
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (432*r1)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/oi/coirmdb75xwo6iaxigs5x55fthvdkn4wxeqnlodr52fviqptperj.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3528
    xnumel = 432
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 441
    y1 = (yindex // 441)
    tmp0 = tl.load(in_ptr0 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (441*x2) + (190512*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr2 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp7 = tmp5 - tmp6
    tmp9 = 0.0002834467120181406
    tmp10 = tmp8 * tmp9
    tmp12 = tmp11 * tmp11
    tmp13 = tmp10 * tmp12
    tmp14 = tmp7 * tmp13
    tmp15 = tmp4 - tmp14
    tmp17 = tmp16 * tmp9
    tmp18 = tmp15 - tmp17
    tmp20 = tmp11 * tmp19
    tmp21 = tmp18 * tmp20
    tl.store(out_ptr0 + (x2 + (432*y3)), tmp21, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zi/czingan5ejrqwpxrsb46jyo4ioapqck5dhfqxftom7p3mjgvb75q.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_68 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 4096],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_68', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 3528
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex % 441
        r2 = (rindex // 441)
        tmp0 = tl.load(in_ptr0 + (571536 + r1 + (441*x0) + (952560*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zi/czithnn2pkqh3jqrj7c2ke6ohvivt64ry7ejplwf2tzbcfe6i4zi.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_69 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_69', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 12096
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp2 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (571536 + (21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (952560*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/jx/cjxbo3hz45w3gykife7ip7qzwz3heqfcxb73hjvjymjwl2ccryno.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_70 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_70', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3528
    xnumel = 432
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 441
    y1 = (yindex // 441)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (571536 + y0 + (441*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0002834467120181406
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (432*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/lk/clk67pg773cfa72ui5ivfz7xsrwe4d6qi4wwhpdsalkelxgbmmmy.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_71 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 4096],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_71', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 3528
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp8 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex % 441
        r2 = (rindex // 441)
        r3 = rindex
        tmp0 = tl.load(in_ptr0 + (381024 + r1 + (441*x0) + (952560*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + (432*r3)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (r1 + (441*x0) + (190512*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tmp1 <= tmp2
        tmp5 = tl.where(tmp3, tmp2, tmp4)
        tmp6 = tmp0 + tmp5
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(rmask & xmask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp8, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ky/ckyykogvzwjiiqkx3rsdgo2kyjrjr4ludgapa4yx4nhho6xyx7px.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_72 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_72', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 12096
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp15 = tl.load(in_ptr6 + (x1), xmask, eviction_policy='evict_last')
    _tmp19 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (381024 + (21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (952560*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + ((21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (190512*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.load(in_ptr3 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp14 = tl.load(in_ptr5 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tmp1 <= tmp2
        tmp5 = tl.where(tmp3, tmp2, tmp4)
        tmp6 = tmp0 + tmp5
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
        tmp16 = tmp14 - tmp15
        tmp17 = tmp6 * tmp16
        tmp18 = tl.broadcast_to(tmp17, [XBLOCK, RBLOCK])
        tmp20 = _tmp19 + tmp18
        _tmp19 = tl.where(rmask & xmask, tmp20, _tmp19)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp12, xmask)
    tmp19 = tl.sum(_tmp19, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp19, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/6j/c6j2v3wis5xcr5cfbtjz6hg2agvdxhsf3pl5lg3zp5rjhd53osy6.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_73 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: '*fp32', 15: '*fp32', 16: 'i32', 17: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 17), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(16, 17))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_73', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, in_ptr12, in_ptr13, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3528
    xnumel = 432
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 441
    y1 = (yindex // 441)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (381024 + y0 + (441*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0 + (441*x2) + (190512*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr9 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp27 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr12 + (x2), xmask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr13 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tmp1 <= tmp2
    tmp5 = tl.where(tmp3, tmp2, tmp4)
    tmp6 = tmp0 + tmp5
    tmp9 = tmp7 - tmp8
    tmp11 = 0.0002834467120181406
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp6 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tmp26 = tmp24 - tmp25
    tmp28 = tmp27 * tmp11
    tmp30 = tmp29 * tmp29
    tmp31 = tmp28 * tmp30
    tmp32 = tmp26 * tmp31
    tmp33 = tmp6 - tmp32
    tmp34 = tmp33 - tmp19
    tmp36 = tmp29 * tmp35
    tmp37 = tmp34 * tmp36
    tl.store(out_ptr0 + (x2 + (432*y3)), tmp23, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (432*y3)), tmp37, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/z2/cz2daolhkwq5cj7o7dfsxvqtkuyb2n3lxbnc5jqrj46cjbfzz4qu.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_74 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 4096],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_74', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 3528
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex % 441
        r2 = (rindex // 441)
        tmp0 = tl.load(in_ptr0 + (190512 + r1 + (441*x0) + (952560*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/qc/cqcda3gvanpr2rlrjhpvd5yh4isx37mq6uilvet2oegne55gwqxq.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_75 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_75', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 12096
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp2 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (190512 + (21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (952560*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ya/cyahipbdoa2ffbzxukghpojkrsgbwosejbqnriqqd4nqatungplz.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_76 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_76', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3528
    xnumel = 432
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 441
    y1 = (yindex // 441)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (190512 + y0 + (441*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0002834467120181406
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (432*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/3b/c3btnyxpvlflrq2rct4j42e73sawpc3kmbxclsrbxdwkmwbbhssy.py
# Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]

triton_poi_fused_add_threshold_backward_77 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_threshold_backward_77', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3456
    xnumel = 441
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 432
    y1 = (yindex // 432)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (762048 + x2 + (441*y0) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (y0 + (432*x2) + (190512*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr2 + (y0 + (432*x2) + (190512*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_out_ptr0 + (x2 + (441*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x2 + (441*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr4 + (y0 + (432*x2) + (190512*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr5 + (x2 + (441*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp4 = 0.0
    tmp5 = tmp3 <= tmp4
    tmp7 = tl.where(tmp5, tmp4, tmp6)
    tmp8 = tmp2 + tmp7
    tmp10 = tl.where(tmp5, tmp4, tmp9)
    tmp11 = tmp8 + tmp10
    tmp13 = tmp11 + tmp12
    tmp15 = tl.where(tmp5, tmp4, tmp14)
    tmp16 = tmp13 + tmp15
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (441*y3)), tmp16, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/f7/cf7acz5mhjfsssazzrwnlgasz5t2othzj36tgd3ho6w4oy7gokeu.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_78 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 4096],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_78', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 3528
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex % 441
        r2 = (rindex // 441)
        tmp0 = tl.load(in_ptr0 + (r1 + (441*x0) + (952560*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/e5/ce5negaf6x2lo5mqnay66bsm26covy7gjduiqwdgv5k724cuvn6d.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_79 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_79', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 12096
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp2 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (952560*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/wt/cwttkbrjz7kykbrs2e6wt4fzq3lvigdsdcff7pbuk4hlkrucjns6.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_80 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_80', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3528
    xnumel = 432
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 441
    y1 = (yindex // 441)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (441*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0002834467120181406
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (432*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/qv/cqv62rtxs74ryjuvqei2ins5hmxmrgkz3gnulzna7fzfnwcqtzge.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_81 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 4096],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_81', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 3528
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex % 441
        r2 = (rindex // 441)
        tmp0 = tl.load(in_ptr0 + (r1 + (441*x0) + (190512*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/my/cmya6kct4l27tdykzk5gl2fcurrn5x5c3l57vv6gjeusx2yaowla.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_82 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_82', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 12096
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp2 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (190512*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zw/czwes42uryg4xmchzujtscntbg3tjxmk5mvupu4wrf2cfrrk7jx4.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_83 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_83', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3528
    xnumel = 432
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 441
    y1 = (yindex // 441)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (441*x2) + (190512*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0002834467120181406
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (432*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/pb/cpbqo6xduuh7la55tprnpiqz4mvgula6f4yydkuqjpl66ceiuin7.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_84 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_84', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 12096
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr1 + ((21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (190512*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.load(in_ptr3 + ((21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (190512*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp4 = tl.where(tmp2, tmp1, tmp3)
        tmp6 = tmp4 + tmp5
        tmp8 = tl.where(tmp2, tmp1, tmp7)
        tmp9 = tmp6 + tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ud/cud3zhnbhm4u2b6qe5lecxnoz7jzmqu4m424fm7m2xr73jven2v2.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_85 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_85', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 12096
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 432
    x1 = (xindex // 432)
    tmp11 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    _tmp15 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (432*r2) + (54432*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr1 + ((21*(((r2 + (126*x1)) // 21) % 21)) + (441*x0) + (190512*((r2 + (126*x1)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (x0 + (432*r2) + (54432*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.load(in_ptr3 + ((21*(((r2 + (126*x1)) // 21) % 21)) + (441*x0) + (190512*((r2 + (126*x1)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.load(in_ptr4 + (x0 + (432*r2) + (54432*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp4 = tl.where(tmp2, tmp1, tmp3)
        tmp6 = tmp4 + tmp5
        tmp8 = tl.where(tmp2, tmp1, tmp7)
        tmp9 = tmp6 + tmp8
        tmp12 = tmp10 - tmp11
        tmp13 = tmp9 * tmp12
        tmp14 = tl.broadcast_to(tmp13, [XBLOCK, RBLOCK])
        tmp16 = _tmp15 + tmp14
        _tmp15 = tl.where(rmask & xmask, tmp16, _tmp15)
    tmp15 = tl.sum(_tmp15, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp15, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/bc/cbcp5q3v3uujc3pu2wdpgcrfbhztnsxsccnvztk5mev6xvglbn6y.py
# Source Nodes: [], Original ATen: [aten.add, aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_86 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_86', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3528
    xnumel = 432
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 441
    y1 = (yindex // 441)
    tmp0 = tl.load(in_ptr0 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (441*x2) + (190512*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_out_ptr0 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr2 + (y0 + (441*x2) + (190512*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr3 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp6 = tmp4 + tmp5
    tmp8 = tl.where(tmp2, tmp1, tmp7)
    tmp9 = tmp6 + tmp8
    tmp12 = tmp10 - tmp11
    tmp14 = 0.0002834467120181406
    tmp15 = tmp13 * tmp14
    tmp17 = tmp16 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp12 * tmp18
    tmp20 = tmp9 - tmp19
    tmp22 = tmp21 * tmp14
    tmp23 = tmp20 - tmp22
    tmp25 = tmp16 * tmp24
    tmp26 = tmp23 * tmp25
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (432*y3)), tmp26, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/xb/cxbz5lhehbc6y2mvrvfyhzqc3ezmjtkrxudud4sqvxbedz3dmdvp.py
# Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]

triton_poi_fused_add_threshold_backward_87 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_threshold_backward_87', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 17280
    xnumel = 441
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 2160
    y1 = (yindex // 2160)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (2160*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_out_ptr0 + (x2 + (441*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr1 + (x2 + (441*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp6 = tl.where(tmp2, tmp1, tmp5)
    tmp7 = tmp4 + tmp6
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (441*y3)), tmp7, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/fv/cfvsb6vjgaauzkhmyhwikqbc7qvy4wys5ovtav4mblje5o7enytk.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_88 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_88', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3528
    xnumel = 432
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 441
    y1 = (yindex // 441)
    tmp0 = tl.load(in_ptr0 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (441*x2) + (190512*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_out_ptr0 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr2 + (y0 + (441*x2) + (190512*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr3 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp6 = tmp4 + tmp5
    tmp8 = tl.where(tmp2, tmp1, tmp7)
    tmp9 = tmp6 + tmp8
    tmp12 = tmp10 - tmp11
    tmp14 = 0.0002834467120181406
    tmp15 = tmp13 * tmp14
    tmp17 = tmp16 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp12 * tmp18
    tmp20 = tmp9 - tmp19
    tmp22 = tmp21 * tmp14
    tmp23 = tmp20 - tmp22
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (432*y3)), tmp23, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rr/crr23sr76xquabkkytceyw3k67ah7xvhloos3ugmxt6bf5do4cbj.py
# Source Nodes: [], Original ATen: [aten.convolution_backward]

triton_poi_fused_convolution_backward_89 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1048576], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_89', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 762048
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 216
    x1 = (xindex // 216)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (216 + x0 + (432*x1)), xmask)
    tmp1 = tl.load(in_ptr1 + (216 + x0), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (216 + x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tl.store(out_ptr0 + (x2), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/nb/cnbpztqflxx2524mjmpz5gx7p4ajh2qdxqlhbihvhiscpzp5nypf.py
# Source Nodes: [], Original ATen: [aten.convolution_backward]

triton_poi_fused_convolution_backward_90 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1048576], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_90', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 762048
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 216
    x1 = (xindex // 216)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (432*x1)), xmask)
    tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tl.store(out_ptr0 + (x2), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rp/crps32zszhtazxm3bxvu27t4k6iii5xclrf4qdoqjvrmq4wepk7p.py
# Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]

triton_poi_fused_add_threshold_backward_91 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_threshold_backward_91', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 17280
    xnumel = 441
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 2160
    y1 = (yindex // 2160)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (2160*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (x2 + (441*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_out_ptr0 + (x2 + (441*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp6 = tl.where(tmp2, tmp1, tmp5)
    tmp7 = tmp4 + tmp6
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (441*y3)), tmp7, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/qr/cqrk37kzsopjpn23yswrljhtrjrelchkwtra3qnc5yl3hj64ndbz.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_92 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_92', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 12096
    rnumel = 126
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp2 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp9 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp13 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (762048 + (21*(((r2 + (126*x0)) // 21) % 21)) + (441*x1) + (952560*((r2 + (126*x0)) // 441)) + (r2 % 21)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.load(in_ptr3 + (x1 + (432*r2) + (54432*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
        tmp10 = tmp8 - tmp9
        tmp11 = tmp0 * tmp10
        tmp12 = tl.broadcast_to(tmp11, [XBLOCK, RBLOCK])
        tmp14 = _tmp13 + tmp12
        _tmp13 = tl.where(rmask & xmask, tmp14, _tmp13)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tmp13 = tl.sum(_tmp13, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp13, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/dw/cdwyg22n3vnmwifsox2xy7bnkwjyhwqe6aeekouyyxabtjjhkp6r.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_93 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: 'i32', 15: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(14, 15))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_93', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3528
    xnumel = 432
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 441
    y1 = (yindex // 441)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (762048 + y0 + (441*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2 + (432*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr9 + (x2), xmask, eviction_policy='evict_last')
    tmp23 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0002834467120181406
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tmp20 = tmp18 - tmp19
    tmp22 = tmp21 * tmp5
    tmp24 = tmp23 * tmp23
    tmp25 = tmp22 * tmp24
    tmp26 = tmp20 * tmp25
    tmp27 = tmp0 - tmp26
    tmp28 = tmp27 - tmp13
    tmp30 = tmp23 * tmp29
    tmp31 = tmp28 * tmp30
    tl.store(out_ptr0 + (x2 + (432*y3)), tmp17, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (432*y3)), tmp31, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/qz/cqzf4tbg66zsy6pbjmrzpzg6ahuaxk7ecncabcytcr3ueoix5o4d.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.threshold_backward]

triton_poi_fused_add_constant_pad_nd_threshold_backward_94 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 2048], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*i1', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_constant_pad_nd_threshold_backward_94', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3456
    xnumel = 1764
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x4 = xindex
    y0 = yindex % 432
    y1 = (yindex // 432)
    y5 = yindex
    x3 = (xindex // 42)
    x2 = xindex % 42
    tmp0 = tl.load(in_ptr0 + (y0 + (432*x4) + (762048*y1)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_out_ptr0 + (x4 + (1764*y5)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp4 = x3
    tmp5 = tl.full([1, 1], 43, tl.int64)
    tmp6 = tmp4 < tmp5
    tmp7 = x2
    tmp8 = tmp7 < tmp5
    tmp9 = tmp6 & tmp8
    tmp10 = tl.load(in_ptr1 + (y0 + (432*x2) + (18576*x3) + (798768*y1)), tmp9 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp11 = tl.full(tmp10.shape, 0.0, tmp10.dtype)
    tmp12 = tl.where(tmp9, tmp10, tmp11)
    tmp13 = tmp3 + tmp12
    tmp14 = tl.load(in_ptr2 + (x2 + (43*x3) + (1849*y5)), tmp9 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp15 = tl.full(tmp14.shape, 0.0, tmp14.dtype)
    tmp16 = tl.where(tmp9, tmp14, tmp15)
    tmp17 = tl.where(tmp0, tmp2, tmp16)
    tmp18 = tmp13 + tmp17
    tmp19 = 1 + x3
    tmp20 = tl.full([1, 1], 0, tl.int64)
    tmp21 = tmp19 >= tmp20
    tmp22 = tl.full([1, 1], 45, tl.int64)
    tmp23 = tmp19 < tmp22
    tmp24 = 1 + x2
    tmp25 = tmp24 >= tmp20
    tmp26 = tmp24 < tmp22
    tmp27 = tmp21 & tmp23
    tmp28 = tmp27 & tmp25
    tmp29 = tmp28 & tmp26
    tmp30 = tl.load(in_ptr3 + (46 + x2 + (45*x3) + (2025*y5)), tmp29 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp31 = tl.full(tmp30.shape, 0.0, tmp30.dtype)
    tmp32 = tl.where(tmp29, tmp30, tmp31)
    tmp33 = tl.where(tmp0, tmp2, tmp32)
    tmp34 = tmp18 + tmp33
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x4 + (1764*y5)), tmp34, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5g/c5gvi444ckr25x43ki2cc3phv6jhb6wp6df36kxnhm7crj6klbs2.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_95 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_95', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 47952
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp37 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((1764*x1) + (762048*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = ((r2 + (128*x0)) // 42) % 42
        tmp5 = tl.full([1, 1], 43, tl.int64)
        tmp6 = tmp4 < tmp5
        tmp7 = (r2 + (128*x0)) % 42
        tmp8 = tmp7 < tmp5
        tmp9 = tmp6 & tmp8
        tmp10 = tmp9 & tmp2
        tmp11 = tl.load(in_ptr1 + (x1 + (432*((r2 + (128*x0)) % 42)) + (18576*(((r2 + (128*x0)) // 42) % 42)) + (798768*(((r2 + (128*x0)) // 1764) % 8))), rmask & tmp10 & xmask, eviction_policy='evict_last', other=0.0)
        tmp12 = tl.full(tmp11.shape, 0.0, tmp11.dtype)
        tmp13 = tl.where(tmp10, tmp11, tmp12)
        tmp14 = tmp3 + tmp13
        tmp15 = tl.load(in_ptr2 + (x1 + (432*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp16 = 2 + (((r2 + (128*x0)) // 42) % 42)
        tmp17 = tl.full([1, 1], 0, tl.int64)
        tmp18 = tmp16 >= tmp17
        tmp19 = tl.full([1, 1], 47, tl.int64)
        tmp20 = tmp16 < tmp19
        tmp21 = 2 + ((r2 + (128*x0)) % 42)
        tmp22 = tmp21 >= tmp17
        tmp23 = tmp21 < tmp19
        tmp24 = tmp18 & tmp20
        tmp25 = tmp24 & tmp22
        tmp26 = tmp25 & tmp23
        tmp27 = tmp26 & tmp2
        tmp28 = tl.load(in_ptr3 + (96 + (47*(((r2 + (128*x0)) // 42) % 42)) + (2209*x1) + (954288*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 42)), rmask & tmp27 & xmask, eviction_policy='evict_last', other=0.0)
        tmp29 = tl.full(tmp28.shape, 0.0, tmp28.dtype)
        tmp30 = tl.where(tmp27, tmp28, tmp29)
        tmp31 = 0.0
        tmp32 = tl.where(tmp15, tmp31, tmp30)
        tmp33 = tmp14 + tmp32
        tmp34 = tl.full(tmp33.shape, 0, tmp33.dtype)
        tmp35 = tl.where(tmp2, tmp33, tmp34)
        tmp36 = tl.broadcast_to(tmp35, [XBLOCK, RBLOCK])
        tmp38 = _tmp37 + tmp36
        _tmp37 = tl.where(rmask & xmask, tmp38, _tmp37)
    tmp37 = tl.sum(_tmp37, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp37, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7b/c7b4siy3h7x5tuawj7mjgqz6yxpb4rd4whw4qqt4rqfom7f5u2fh.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_96 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_96', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 111
    RBLOCK: tl.constexpr = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (111*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/w5/cw5d56tp66lntw6p2rnzqpzlgue2vupz3pp53txuif52mgnqtxpf.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_97 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7, 8))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_97', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 47952
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 432)
    x0 = xindex % 432
    _tmp41 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x1)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((1764*x0) + (762048*(((r2 + (128*x1)) // 1764) % 8)) + ((r2 + (128*x1)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = ((r2 + (128*x1)) // 42) % 42
        tmp5 = tl.full([1, 1], 43, tl.int64)
        tmp6 = tmp4 < tmp5
        tmp7 = (r2 + (128*x1)) % 42
        tmp8 = tmp7 < tmp5
        tmp9 = tmp6 & tmp8
        tmp10 = tmp9 & tmp2
        tmp11 = tl.load(in_ptr1 + (x0 + (432*((r2 + (128*x1)) % 42)) + (18576*(((r2 + (128*x1)) // 42) % 42)) + (798768*(((r2 + (128*x1)) // 1764) % 8))), rmask & tmp10 & xmask, eviction_policy='evict_last', other=0.0)
        tmp12 = tl.full(tmp11.shape, 0.0, tmp11.dtype)
        tmp13 = tl.where(tmp10, tmp11, tmp12)
        tmp14 = tmp3 + tmp13
        tmp15 = tl.load(in_ptr2 + (x0 + (432*((r2 + (128*x1)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp16 = 2 + (((r2 + (128*x1)) // 42) % 42)
        tmp17 = tl.full([1, 1], 0, tl.int64)
        tmp18 = tmp16 >= tmp17
        tmp19 = tl.full([1, 1], 47, tl.int64)
        tmp20 = tmp16 < tmp19
        tmp21 = 2 + ((r2 + (128*x1)) % 42)
        tmp22 = tmp21 >= tmp17
        tmp23 = tmp21 < tmp19
        tmp24 = tmp18 & tmp20
        tmp25 = tmp24 & tmp22
        tmp26 = tmp25 & tmp23
        tmp27 = tmp26 & tmp2
        tmp28 = tl.load(in_ptr3 + (96 + (47*(((r2 + (128*x1)) // 42) % 42)) + (2209*x0) + (954288*(((r2 + (128*x1)) // 1764) % 8)) + ((r2 + (128*x1)) % 42)), rmask & tmp27 & xmask, eviction_policy='evict_last', other=0.0)
        tmp29 = tl.full(tmp28.shape, 0.0, tmp28.dtype)
        tmp30 = tl.where(tmp27, tmp28, tmp29)
        tmp31 = 0.0
        tmp32 = tl.where(tmp15, tmp31, tmp30)
        tmp33 = tmp14 + tmp32
        tmp34 = tl.load(in_ptr4 + (x0 + (432*((r2 + (128*x1)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp35 = tl.load(in_ptr5 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp36 = tmp34 - tmp35
        tmp37 = tmp33 * tmp36
        tmp38 = tl.full(tmp37.shape, 0, tmp37.dtype)
        tmp39 = tl.where(tmp2, tmp37, tmp38)
        tmp40 = tl.broadcast_to(tmp39, [XBLOCK, RBLOCK])
        tmp42 = _tmp41 + tmp40
        _tmp41 = tl.where(rmask & xmask, tmp42, _tmp41)
    tmp41 = tl.sum(_tmp41, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp41, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/4e/c4e76wgh5v2jsqxrygsbd46zpfp3snswbnhz7poqjnsfmxoau2pa.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_98 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_98', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 111
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (432*r1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/4s/c4smeswmolzibboiautwcfveoddfuvz5h6ffd4qc2xggbfnvzeus.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_99 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_99', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 432
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x3 = xindex
    y2 = (yindex // 1764)
    y4 = yindex % 1764
    y1 = (yindex // 42) % 42
    y0 = yindex % 42
    y5 = yindex
    tmp0 = tl.load(in_ptr0 + (y4 + (1764*x3) + (762048*y2)), xmask & ymask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr2 + (x3 + (432*y5)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp29 = tl.load(in_ptr4 + (x3 + (432*y5)), xmask & ymask, eviction_policy='evict_last')
    tmp30 = tl.load(in_ptr5 + (x3), xmask, eviction_policy='evict_last')
    tmp32 = tl.load(in_ptr6 + (x3), xmask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr7 + (x3), xmask, eviction_policy='evict_last')
    tmp40 = tl.load(in_ptr8 + (x3), xmask, eviction_policy='evict_last')
    tmp1 = y1
    tmp2 = tl.full([1, 1], 43, tl.int64)
    tmp3 = tmp1 < tmp2
    tmp4 = y0
    tmp5 = tmp4 < tmp2
    tmp6 = tmp3 & tmp5
    tmp7 = tl.load(in_ptr1 + (x3 + (432*y0) + (18576*y1) + (798768*y2)), tmp6 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp8 = tl.full(tmp7.shape, 0.0, tmp7.dtype)
    tmp9 = tl.where(tmp6, tmp7, tmp8)
    tmp10 = tmp0 + tmp9
    tmp12 = 2 + y1
    tmp13 = tl.full([1, 1], 0, tl.int64)
    tmp14 = tmp12 >= tmp13
    tmp15 = tl.full([1, 1], 47, tl.int64)
    tmp16 = tmp12 < tmp15
    tmp17 = 2 + y0
    tmp18 = tmp17 >= tmp13
    tmp19 = tmp17 < tmp15
    tmp20 = tmp14 & tmp16
    tmp21 = tmp20 & tmp18
    tmp22 = tmp21 & tmp19
    tmp23 = tl.load(in_ptr3 + (96 + y0 + (47*y1) + (2209*x3) + (954288*y2)), tmp22 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp24 = tl.full(tmp23.shape, 0.0, tmp23.dtype)
    tmp25 = tl.where(tmp22, tmp23, tmp24)
    tmp26 = 0.0
    tmp27 = tl.where(tmp11, tmp26, tmp25)
    tmp28 = tmp10 + tmp27
    tmp31 = tmp29 - tmp30
    tmp33 = 7.086167800453515e-05
    tmp34 = tmp32 * tmp33
    tmp36 = tmp35 * tmp35
    tmp37 = tmp34 * tmp36
    tmp38 = tmp31 * tmp37
    tmp39 = tmp28 - tmp38
    tmp41 = tmp40 * tmp33
    tmp42 = tmp39 - tmp41
    tl.store(out_ptr0 + (x3 + (432*y5)), tmp42, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/y3/cy3wmbjmcsvdxpes3reh7x63g2jqoodfob62okgsloc4u2bpql6b.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_100 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 2048], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_100', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3456
    xnumel = 1764
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 432
    y1 = (yindex // 432)
    tmp0 = tl.load(in_out_ptr0 + (y0 + (432*x2) + (762048*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr0 + (y0), ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr1 + (y0), ymask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (y0 + (432*x2) + (762048*y1)), tmp4, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/k6/ck6xdxqgd3te2l3bbexldrq77ztnbghr77xcdk7mzrjeingzxe2y.py
# Source Nodes: [], Original ATen: [aten.add, aten.avg_pool2d_backward, aten.constant_pad_nd, aten.threshold_backward]

triton_poi_fused_add_avg_pool2d_backward_constant_pad_nd_threshold_backward_101 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 2048], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_avg_pool2d_backward_constant_pad_nd_threshold_backward_101', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 8640
    xnumel = 1764
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = (xindex // 42)
    x1 = xindex % 42
    y0 = yindex
    x3 = xindex
    y4 = yindex % 1080
    y5 = (yindex // 1080)
    tmp24 = tl.load(in_ptr1 + ((21*(tl.math.min(tl.math.max(0, ((1 + x2) // 2)), (-1) + (tl.math.min(21, 1 + (x2 // 2)))))) + (21*(tl.where((tl.math.min(tl.math.max(0, ((1 + x2) // 2)), (-1) + (tl.math.min(21, 1 + (x2 // 2))))) >= 0, 0, 21))) + (441*y0) + (tl.math.min(tl.math.max(0, ((1 + x1) // 2)), (-1) + (tl.math.min(21, 1 + (x1 // 2))))) + (tl.where((tl.math.min(tl.math.max(0, ((1 + x1) // 2)), (-1) + (tl.math.min(21, 1 + (x1 // 2))))) >= 0, 0, 21))), xmask & ymask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr2 + (y4 + (1080*x3) + (1905120*y5)), xmask & ymask, eviction_policy='evict_last')
    tmp38 = tl.load(in_ptr3 + (x3 + (1764*y0)), xmask & ymask, eviction_policy='evict_last')
    tmp0 = (-1) + x2
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 42, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = (-1) + x1
    tmp6 = tmp5 >= tmp1
    tmp7 = tmp5 < tmp3
    tmp8 = tmp2 & tmp4
    tmp9 = tmp8 & tmp6
    tmp10 = tmp9 & tmp7
    tmp11 = tl.load(in_ptr0 + ((21*(tl.math.min(tl.math.max(0, (x2 // 2)), (-1) + (tl.math.min(21, 1 + (((-1) + x2) // 2)))))) + (21*(tl.where((tl.math.min(tl.math.max(0, (x2 // 2)), (-1) + (tl.math.min(21, 1 + (((-1) + x2) // 2))))) >= 0, 0, 21))) + (441*y0) + (tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(21, 1 + (((-1) + x1) // 2))))) + (tl.where((tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(21, 1 + (((-1) + x1) // 2))))) >= 0, 0, 21))), tmp10 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp12 = tmp11 / 1
    tmp13 = tl.broadcast_to(tl.math.max(0, (x2 // 2)), [XBLOCK, YBLOCK])
    tmp14 = tl.broadcast_to(tl.math.min(21, 1 + (((-1) + x2) // 2)), [XBLOCK, YBLOCK])
    tmp15 = tmp13 < tmp14
    tmp16 = tl.broadcast_to(tl.math.max(0, (x1 // 2)), [XBLOCK, YBLOCK])
    tmp17 = tl.broadcast_to(tl.math.min(21, 1 + (((-1) + x1) // 2)), [XBLOCK, YBLOCK])
    tmp18 = tmp16 < tmp17
    tmp19 = tmp15 & tmp18
    tmp20 = 0.0
    tmp21 = tl.where(tmp19, tmp12, tmp20)
    tmp22 = tl.full(tmp21.shape, 0.0, tmp21.dtype)
    tmp23 = tl.where(tmp10, tmp21, tmp22)
    tmp25 = tmp24 / 1
    tmp26 = tl.math.max(0, ((1 + x2) // 2))
    tmp27 = tl.math.min(21, 1 + (x2 // 2))
    tmp28 = tmp26 < tmp27
    tmp29 = tl.math.max(0, ((1 + x1) // 2))
    tmp30 = tl.math.min(21, 1 + (x1 // 2))
    tmp31 = tmp29 < tmp30
    tmp32 = tmp28 & tmp31
    tmp33 = tl.where(tmp32, tmp25, tmp20)
    tmp34 = tmp23 + tmp33
    tmp36 = tmp35 <= tmp20
    tmp37 = tl.where(tmp36, tmp20, tmp34)
    tmp39 = tl.where(tmp36, tmp20, tmp38)
    tmp40 = tmp37 + tmp39
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3 + (1764*y0)), tmp40, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/mw/cmwjfgg4dgopyl52fhrs2vlth32iv4zritq7he3yqm7utcyg3geo.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_102 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_102', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 47952
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp40 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (432*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = ((r2 + (128*x0)) // 42) % 42
        tmp5 = tl.full([1, 1], 43, tl.int64)
        tmp6 = tmp4 < tmp5
        tmp7 = (r2 + (128*x0)) % 42
        tmp8 = tmp7 < tmp5
        tmp9 = tmp6 & tmp8
        tmp10 = tmp9 & tmp2
        tmp11 = tl.load(in_ptr1 + ((43*(((r2 + (128*x0)) // 42) % 42)) + (1849*x1) + (798768*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 42)), rmask & tmp10 & xmask, eviction_policy='evict_last', other=0.0)
        tmp12 = tl.full(tmp11.shape, 0.0, tmp11.dtype)
        tmp13 = tl.where(tmp10, tmp11, tmp12)
        tmp14 = 0.0
        tmp15 = tl.where(tmp3, tmp14, tmp13)
        tmp16 = tl.load(in_ptr2 + (x1 + (432*((r2 + (128*x0)) % 42)) + (18576*(((r2 + (128*x0)) // 42) % 42)) + (798768*(((r2 + (128*x0)) // 1764) % 8))), rmask & tmp10 & xmask, eviction_policy='evict_last', other=0.0)
        tmp17 = tl.full(tmp16.shape, 0.0, tmp16.dtype)
        tmp18 = tl.where(tmp10, tmp16, tmp17)
        tmp19 = tmp15 + tmp18
        tmp20 = 1 + (((r2 + (128*x0)) // 42) % 42)
        tmp21 = tl.full([1, 1], 0, tl.int64)
        tmp22 = tmp20 >= tmp21
        tmp23 = tl.full([1, 1], 45, tl.int64)
        tmp24 = tmp20 < tmp23
        tmp25 = 1 + ((r2 + (128*x0)) % 42)
        tmp26 = tmp25 >= tmp21
        tmp27 = tmp25 < tmp23
        tmp28 = tmp22 & tmp24
        tmp29 = tmp28 & tmp26
        tmp30 = tmp29 & tmp27
        tmp31 = tmp30 & tmp2
        tmp32 = tl.load(in_ptr3 + (46 + (45*(((r2 + (128*x0)) // 42) % 42)) + (2025*x1) + (874800*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 42)), rmask & tmp31 & xmask, eviction_policy='evict_last', other=0.0)
        tmp33 = tl.full(tmp32.shape, 0.0, tmp32.dtype)
        tmp34 = tl.where(tmp31, tmp32, tmp33)
        tmp35 = tl.where(tmp3, tmp14, tmp34)
        tmp36 = tmp19 + tmp35
        tmp37 = tl.full(tmp36.shape, 0, tmp36.dtype)
        tmp38 = tl.where(tmp2, tmp36, tmp37)
        tmp39 = tl.broadcast_to(tmp38, [XBLOCK, RBLOCK])
        tmp41 = _tmp40 + tmp39
        _tmp40 = tl.where(rmask & xmask, tmp41, _tmp40)
    tmp40 = tl.sum(_tmp40, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp40, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/uc/cucc5sohrvk67ymflp35ssdn7onp2zewjc4ctu7ol53umnohfbqi.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_103 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7, 8))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_103', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 47952
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 432)
    x0 = xindex % 432
    _tmp44 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x1)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (432*((r2 + (128*x1)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = ((r2 + (128*x1)) // 42) % 42
        tmp5 = tl.full([1, 1], 43, tl.int64)
        tmp6 = tmp4 < tmp5
        tmp7 = (r2 + (128*x1)) % 42
        tmp8 = tmp7 < tmp5
        tmp9 = tmp6 & tmp8
        tmp10 = tmp9 & tmp2
        tmp11 = tl.load(in_ptr1 + ((43*(((r2 + (128*x1)) // 42) % 42)) + (1849*x0) + (798768*(((r2 + (128*x1)) // 1764) % 8)) + ((r2 + (128*x1)) % 42)), rmask & tmp10 & xmask, eviction_policy='evict_last', other=0.0)
        tmp12 = tl.full(tmp11.shape, 0.0, tmp11.dtype)
        tmp13 = tl.where(tmp10, tmp11, tmp12)
        tmp14 = 0.0
        tmp15 = tl.where(tmp3, tmp14, tmp13)
        tmp16 = tl.load(in_ptr2 + (x0 + (432*((r2 + (128*x1)) % 42)) + (18576*(((r2 + (128*x1)) // 42) % 42)) + (798768*(((r2 + (128*x1)) // 1764) % 8))), rmask & tmp10 & xmask, eviction_policy='evict_last', other=0.0)
        tmp17 = tl.full(tmp16.shape, 0.0, tmp16.dtype)
        tmp18 = tl.where(tmp10, tmp16, tmp17)
        tmp19 = tmp15 + tmp18
        tmp20 = 1 + (((r2 + (128*x1)) // 42) % 42)
        tmp21 = tl.full([1, 1], 0, tl.int64)
        tmp22 = tmp20 >= tmp21
        tmp23 = tl.full([1, 1], 45, tl.int64)
        tmp24 = tmp20 < tmp23
        tmp25 = 1 + ((r2 + (128*x1)) % 42)
        tmp26 = tmp25 >= tmp21
        tmp27 = tmp25 < tmp23
        tmp28 = tmp22 & tmp24
        tmp29 = tmp28 & tmp26
        tmp30 = tmp29 & tmp27
        tmp31 = tmp30 & tmp2
        tmp32 = tl.load(in_ptr3 + (46 + (45*(((r2 + (128*x1)) // 42) % 42)) + (2025*x0) + (874800*(((r2 + (128*x1)) // 1764) % 8)) + ((r2 + (128*x1)) % 42)), rmask & tmp31 & xmask, eviction_policy='evict_last', other=0.0)
        tmp33 = tl.full(tmp32.shape, 0.0, tmp32.dtype)
        tmp34 = tl.where(tmp31, tmp32, tmp33)
        tmp35 = tl.where(tmp3, tmp14, tmp34)
        tmp36 = tmp19 + tmp35
        tmp37 = tl.load(in_ptr4 + (x0 + (432*((r2 + (128*x1)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp38 = tl.load(in_ptr5 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp39 = tmp37 - tmp38
        tmp40 = tmp36 * tmp39
        tmp41 = tl.full(tmp40.shape, 0, tmp40.dtype)
        tmp42 = tl.where(tmp2, tmp40, tmp41)
        tmp43 = tl.broadcast_to(tmp42, [XBLOCK, RBLOCK])
        tmp45 = _tmp44 + tmp43
        _tmp44 = tl.where(rmask & xmask, tmp45, _tmp44)
    tmp44 = tl.sum(_tmp44, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp44, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/cm/ccmj5y4gvvouve5777ok22nfzkqzj5ws5pv4naosjke2i2hqwflj.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_104 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_104', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 432
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x3 = xindex
    y4 = yindex
    y1 = (yindex // 42) % 42
    y0 = yindex % 42
    y2 = (yindex // 1764)
    tmp0 = tl.load(in_ptr0 + (x3 + (432*y4)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp32 = tl.load(in_ptr4 + (x3 + (432*y4)), xmask & ymask, eviction_policy='evict_last')
    tmp33 = tl.load(in_ptr5 + (x3), xmask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr6 + (x3), xmask, eviction_policy='evict_last')
    tmp38 = tl.load(in_ptr7 + (x3), xmask, eviction_policy='evict_last')
    tmp1 = y1
    tmp2 = tl.full([1, 1], 43, tl.int64)
    tmp3 = tmp1 < tmp2
    tmp4 = y0
    tmp5 = tmp4 < tmp2
    tmp6 = tmp3 & tmp5
    tmp7 = tl.load(in_ptr1 + (y0 + (43*y1) + (1849*x3) + (798768*y2)), tmp6 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp8 = tl.full(tmp7.shape, 0.0, tmp7.dtype)
    tmp9 = tl.where(tmp6, tmp7, tmp8)
    tmp10 = 0.0
    tmp11 = tl.where(tmp0, tmp10, tmp9)
    tmp12 = tl.load(in_ptr2 + (x3 + (432*y0) + (18576*y1) + (798768*y2)), tmp6 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp13 = tl.full(tmp12.shape, 0.0, tmp12.dtype)
    tmp14 = tl.where(tmp6, tmp12, tmp13)
    tmp15 = tmp11 + tmp14
    tmp16 = 1 + y1
    tmp17 = tl.full([1, 1], 0, tl.int64)
    tmp18 = tmp16 >= tmp17
    tmp19 = tl.full([1, 1], 45, tl.int64)
    tmp20 = tmp16 < tmp19
    tmp21 = 1 + y0
    tmp22 = tmp21 >= tmp17
    tmp23 = tmp21 < tmp19
    tmp24 = tmp18 & tmp20
    tmp25 = tmp24 & tmp22
    tmp26 = tmp25 & tmp23
    tmp27 = tl.load(in_ptr3 + (46 + y0 + (45*y1) + (2025*x3) + (874800*y2)), tmp26 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp28 = tl.full(tmp27.shape, 0.0, tmp27.dtype)
    tmp29 = tl.where(tmp26, tmp27, tmp28)
    tmp30 = tl.where(tmp0, tmp10, tmp29)
    tmp31 = tmp15 + tmp30
    tmp34 = tmp32 - tmp33
    tmp36 = 7.086167800453515e-05
    tmp37 = tmp35 * tmp36
    tmp39 = tmp38 * tmp38
    tmp40 = tmp37 * tmp39
    tmp41 = tmp34 * tmp40
    tmp42 = tmp31 - tmp41
    tl.store(out_ptr0 + (x3 + (432*y4)), tmp42, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ri/crijyodezg33dnewlbl6b6nanvnvdxagglwrhlwhq3sqpujmixlv.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_105 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 2048], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_105', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 3456
    xnumel = 1764
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 432
    y1 = (yindex // 432)
    tmp0 = tl.load(in_out_ptr0 + (y0 + (432*x2) + (762048*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr0 + (y0), ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr1 + (y0), ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    tmp2 = 7.086167800453515e-05
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 - tmp3
    tmp7 = tmp5 * tmp6
    tmp8 = tmp4 * tmp7
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (y0 + (432*x2) + (762048*y1)), tmp8, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/kc/ckcfrkojt3nribyvo27grgverrhycpqlsp7akftyejss23ww5a7u.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_106 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_106', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 7056
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 216
    x1 = (xindex // 216)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (1524096 + (1764*x0) + (1905120*(r2 // 1764)) + (7620480*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5d/c5dichjawm7wgazehhwwokagslvpb5x5ptyz3jz2c73byhi5qhra.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_107 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[256, 2],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_107', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 216
    rnumel = 2
    RBLOCK: tl.constexpr = 2
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (216*r1)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/aw/cawlwdfykj7gj2dooyji32n2xei6j4wuk3tmif77kz24oeprm2gt.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_108 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_108', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23976
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (1524096 + (1764*x1) + (1905120*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (216*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zw/czwqjopwijqazkoxkrxpj5q5qsi7vtekmueq23vvorry7ogujm2c.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_109 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[256, 128],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_109', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 216
    rnumel = 111
    RBLOCK: tl.constexpr = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (111*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ih/cihmulwqj4tbfi67r7kszpkrinqnncnndisbdroo3rm7ofbqkpyz.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_110 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_110', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 216
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (1524096 + y0 + (1764*x2) + (1905120*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 7.086167800453515e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (216*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/vs/cvsm4zocx3pkagyve2kxsy6yopilopupuiwyxzi27f653gre4vvl.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_111 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3, 4))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_111', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23976
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (216*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = 0.0
        tmp5 = tmp3 <= tmp4
        tmp6 = tl.load(in_ptr1 + ((1764*x1) + (381024*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.where(tmp5, tmp4, tmp6)
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ci/ccirpuksz4xnjtmzte5jggwv4edwwe4n7fxpkryj2zdymuwout4n.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_112 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[256, 128],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_112', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 216
    rnumel = 111
    RBLOCK: tl.constexpr = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (111*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ls/clsokf2lod2hiaq2wlyfqpj55fyxudrpq54ahfjnxncavm5ayw6l.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_113 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_113', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23976
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 216)
    x0 = xindex % 216
    _tmp15 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x1)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (216*((r2 + (128*x1)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = 0.0
        tmp5 = tmp3 <= tmp4
        tmp6 = tl.load(in_ptr1 + ((1764*x0) + (381024*(((r2 + (128*x1)) // 1764) % 8)) + ((r2 + (128*x1)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.where(tmp5, tmp4, tmp6)
        tmp8 = tl.load(in_ptr2 + (x0 + (216*((r2 + (128*x1)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp10 = tmp8 - tmp9
        tmp11 = tmp7 * tmp10
        tmp12 = tl.full(tmp11.shape, 0, tmp11.dtype)
        tmp13 = tl.where(tmp2, tmp11, tmp12)
        tmp14 = tl.broadcast_to(tmp13, [XBLOCK, RBLOCK])
        tmp16 = _tmp15 + tmp14
        _tmp15 = tl.where(rmask & xmask, tmp16, _tmp15)
    tmp15 = tl.sum(_tmp15, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp15, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/27/c27n2xuc32bakmslv2rfi6u7bqv3xo35grsnos43hncmq7ucojys.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_114 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[256, 128],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_114', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 216
    rnumel = 111
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (216*r1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/vp/cvpxcsttiljywn2wyvuppqm3wth54x76yqmeet7bb64kbw26grgt.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 216
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    tmp0 = tl.load(in_ptr0 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (1764*x2) + (381024*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr2 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp7 = tmp5 - tmp6
    tmp9 = 7.086167800453515e-05
    tmp10 = tmp8 * tmp9
    tmp12 = tmp11 * tmp11
    tmp13 = tmp10 * tmp12
    tmp14 = tmp7 * tmp13
    tmp15 = tmp4 - tmp14
    tmp17 = tmp16 * tmp9
    tmp18 = tmp15 - tmp17
    tmp20 = tmp11 * tmp19
    tmp21 = tmp18 * tmp20
    tl.store(out_ptr0 + (x2 + (216*y3)), tmp21, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/qa/cqarmdav7gxkln3qiaxkv2fb5rz2565umffazbutmon2ctgbctgy.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_116 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_116', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 7056
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 216
    x1 = (xindex // 216)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (1143072 + (1764*x0) + (1905120*(r2 // 1764)) + (7620480*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/jt/cjt3xontyd3y7qua2ogtm64xxkmfx3xusjoozgscinuu3dpjwvlt.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_117 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_117', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23976
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (1143072 + (1764*x1) + (1905120*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (216*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ca/ccapb3oivt3nx5fq26h3jsnnos3hh4c2jx46qzjqultwelrz774w.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_118 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_118', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 216
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (1143072 + y0 + (1764*x2) + (1905120*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 7.086167800453515e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (216*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/sv/csvjs2nwr2cq6inkrw7ats4e3dorjorlc7ujvtib22htx4ctnsce.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_119 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_119', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 7056
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 216
    x1 = (xindex // 216)
    _tmp8 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (762048 + (1764*x0) + (1905120*(r2 // 1764)) + (7620480*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + (216*r2) + (1524096*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + ((1764*x0) + (381024*(r2 // 1764)) + (1524096*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tmp1 <= tmp2
        tmp5 = tl.where(tmp3, tmp2, tmp4)
        tmp6 = tmp0 + tmp5
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(rmask & xmask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp8, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/4g/c4g6qkemdhubxunqu5q27gldwngxmmhoriaff4tal4ihdv7ri33o.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_120 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_120', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23976
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp17 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    _tmp26 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (762048 + (1764*x1) + (1905120*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (216*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tmp4 <= tmp5
        tmp7 = tl.load(in_ptr2 + ((1764*x1) + (381024*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.where(tmp6, tmp5, tmp7)
        tmp9 = tmp3 + tmp8
        tmp10 = tl.load(in_ptr3 + (x1 + (216*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp11 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp12 = tmp10 - tmp11
        tmp13 = tmp9 * tmp12
        tmp14 = tl.full(tmp13.shape, 0, tmp13.dtype)
        tmp15 = tl.where(tmp2, tmp13, tmp14)
        tmp16 = tl.broadcast_to(tmp15, [XBLOCK, RBLOCK])
        tmp18 = _tmp17 + tmp16
        _tmp17 = tl.where(rmask & xmask, tmp18, _tmp17)
        tmp19 = tl.load(in_ptr5 + (x1 + (216*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp20 = tl.load(in_ptr6 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp21 = tmp19 - tmp20
        tmp22 = tmp9 * tmp21
        tmp23 = tl.full(tmp22.shape, 0, tmp22.dtype)
        tmp24 = tl.where(tmp2, tmp22, tmp23)
        tmp25 = tl.broadcast_to(tmp24, [XBLOCK, RBLOCK])
        tmp27 = _tmp26 + tmp25
        _tmp26 = tl.where(rmask & xmask, tmp27, _tmp26)
    tmp17 = tl.sum(_tmp17, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp17, xmask)
    tmp26 = tl.sum(_tmp26, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp26, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5z/c5zyigy3mes2wrel4cvpapswalzwlal37wgcexxs453cf4czwjdt.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_121 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: '*fp32', 15: '*fp32', 16: 'i32', 17: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(16, 17))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_121', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, in_ptr12, in_ptr13, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 216
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (762048 + y0 + (1764*x2) + (1905120*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0 + (1764*x2) + (381024*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr9 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp27 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr12 + (x2), xmask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr13 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tmp1 <= tmp2
    tmp5 = tl.where(tmp3, tmp2, tmp4)
    tmp6 = tmp0 + tmp5
    tmp9 = tmp7 - tmp8
    tmp11 = 7.086167800453515e-05
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp6 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tmp26 = tmp24 - tmp25
    tmp28 = tmp27 * tmp11
    tmp30 = tmp29 * tmp29
    tmp31 = tmp28 * tmp30
    tmp32 = tmp26 * tmp31
    tmp33 = tmp6 - tmp32
    tmp34 = tmp33 - tmp19
    tmp36 = tmp29 * tmp35
    tmp37 = tmp34 * tmp36
    tl.store(out_ptr0 + (x2 + (216*y3)), tmp23, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (216*y3)), tmp37, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/yc/cycn4wuecsd6j7d5p6oypxyrfxhs5i7ubfjmq3txelxhng5tyqdm.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_122 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_122', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 7056
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 216
    x1 = (xindex // 216)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (381024 + (1764*x0) + (1905120*(r2 // 1764)) + (7620480*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/jd/cjdpqjeni3tu7lyinlnmzre55zxnyl7m3ltoxlp27xxnpe5uw5ua.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_123 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_123', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23976
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (381024 + (1764*x1) + (1905120*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (216*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/eq/ceq3rcnwmnni5euq2rizc5gfyntf6kqxkqbs523laheb74txooq7.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_124 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_124', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 216
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (381024 + y0 + (1764*x2) + (1905120*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 7.086167800453515e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (216*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/um/cumb6o5aozim35nwuacsqpbperokxw3wq6eq4aqpxos6zirfposh.py
# Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]

triton_poi_fused_add_threshold_backward_125 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 2048], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_threshold_backward_125', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1728
    xnumel = 1764
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 216
    y1 = (yindex // 216)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (1524096 + x2 + (1764*y0) + (1905120*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (y0 + (216*x2) + (381024*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr2 + (y0 + (216*x2) + (381024*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_out_ptr0 + (x2 + (1764*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x2 + (1764*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr4 + (y0 + (216*x2) + (381024*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr5 + (x2 + (1764*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tmp0 + tmp1
    tmp4 = 0.0
    tmp5 = tmp3 <= tmp4
    tmp7 = tl.where(tmp5, tmp4, tmp6)
    tmp8 = tmp2 + tmp7
    tmp10 = tl.where(tmp5, tmp4, tmp9)
    tmp11 = tmp8 + tmp10
    tmp13 = tmp11 + tmp12
    tmp15 = tl.where(tmp5, tmp4, tmp14)
    tmp16 = tmp13 + tmp15
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (1764*y3)), tmp16, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/sj/csjvieszxsxvfzh2guzjlsimkan2xf34xflwgofnlczi4jcywbdm.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_126 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_126', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 7056
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 216
    x1 = (xindex // 216)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((1764*x0) + (1905120*(r2 // 1764)) + (7620480*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/iw/ciwedftl3s6eukzf6cursu5sjobsxyhx2eh4z5f37okfex5zrl3v.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_127 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_127', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23976
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((1764*x1) + (1905120*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (216*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/dn/cdndepqa5k43fz7weebtpdqoixhm3hny62uy7nxuxywl7pcuyou5.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_128 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_128', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 216
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (1764*x2) + (1905120*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 7.086167800453515e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (216*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5x/c5xv43i6rb6bgji42cwxzqhqf42sje5tuxme7j4ugtnajajjtyyu.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_129 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_129', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 432
    rnumel = 7056
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 216
    x1 = (xindex // 216)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((1764*x0) + (381024*(r2 // 1764)) + (1524096*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/j6/cj6l3bc5kptlzaizpneot7pq55h3idyekvy56jlm75gkopm5obyq.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_130 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_130', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23976
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((1764*x1) + (381024*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (216*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/xh/cxhln6magmclkxytjcbh3ebtgjy6bsgmxxrdm5ztb7r47c7tikie.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_131 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_131', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 216
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (1764*x2) + (381024*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 7.086167800453515e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (216*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/u5/cu5oe277dfgcqdj6kaylcfnf2puoh5xp57qzx4ewpxctzkvnbhsm.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_132 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_132', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23976
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp16 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (216*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = 0.0
        tmp5 = tmp3 <= tmp4
        tmp6 = tl.load(in_ptr1 + ((1764*x1) + (381024*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.where(tmp5, tmp4, tmp6)
        tmp8 = tl.load(in_ptr2 + (x1 + (216*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tmp7 + tmp8
        tmp10 = tl.load(in_ptr3 + ((1764*x1) + (381024*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp11 = tl.where(tmp5, tmp4, tmp10)
        tmp12 = tmp9 + tmp11
        tmp13 = tl.full(tmp12.shape, 0, tmp12.dtype)
        tmp14 = tl.where(tmp2, tmp12, tmp13)
        tmp15 = tl.broadcast_to(tmp14, [XBLOCK, RBLOCK])
        tmp17 = _tmp16 + tmp15
        _tmp16 = tl.where(rmask & xmask, tmp17, _tmp16)
    tmp16 = tl.sum(_tmp16, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp16, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/3z/c3znn7zmtgwhunlt7lcf2pt6padtjv2fwte7z665qz3rfbfpsca4.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_133 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7, 8))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_133', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23976
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 216)
    x0 = xindex % 216
    _tmp20 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x1)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (216*((r2 + (128*x1)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = 0.0
        tmp5 = tmp3 <= tmp4
        tmp6 = tl.load(in_ptr1 + ((1764*x0) + (381024*(((r2 + (128*x1)) // 1764) % 8)) + ((r2 + (128*x1)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.where(tmp5, tmp4, tmp6)
        tmp8 = tl.load(in_ptr2 + (x0 + (216*((r2 + (128*x1)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tmp7 + tmp8
        tmp10 = tl.load(in_ptr3 + ((1764*x0) + (381024*(((r2 + (128*x1)) // 1764) % 8)) + ((r2 + (128*x1)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp11 = tl.where(tmp5, tmp4, tmp10)
        tmp12 = tmp9 + tmp11
        tmp13 = tl.load(in_ptr4 + (x0 + (216*((r2 + (128*x1)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp14 = tl.load(in_ptr5 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp15 = tmp13 - tmp14
        tmp16 = tmp12 * tmp15
        tmp17 = tl.full(tmp16.shape, 0, tmp16.dtype)
        tmp18 = tl.where(tmp2, tmp16, tmp17)
        tmp19 = tl.broadcast_to(tmp18, [XBLOCK, RBLOCK])
        tmp21 = _tmp20 + tmp19
        _tmp20 = tl.where(rmask & xmask, tmp21, _tmp20)
    tmp20 = tl.sum(_tmp20, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp20, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/br/cbryr3764cy6wosxscrmd4epx3lxy3ouxoz2xrpm6usy477lt7gi.py
# Source Nodes: [], Original ATen: [aten.add, aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_134 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_134', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 216
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    tmp0 = tl.load(in_ptr0 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (1764*x2) + (381024*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_out_ptr0 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr2 + (y0 + (1764*x2) + (381024*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr3 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp6 = tmp4 + tmp5
    tmp8 = tl.where(tmp2, tmp1, tmp7)
    tmp9 = tmp6 + tmp8
    tmp12 = tmp10 - tmp11
    tmp14 = 7.086167800453515e-05
    tmp15 = tmp13 * tmp14
    tmp17 = tmp16 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp12 * tmp18
    tmp20 = tmp9 - tmp19
    tmp22 = tmp21 * tmp14
    tmp23 = tmp20 - tmp22
    tmp25 = tmp16 * tmp24
    tmp26 = tmp23 * tmp25
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (216*y3)), tmp26, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/g7/cg733ttnsh2soxkiob2scxdwyf63lit5xkwlszatuww2f7wrgpj3.py
# Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]

triton_poi_fused_add_threshold_backward_135 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 2048], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_threshold_backward_135', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 8640
    xnumel = 1764
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1080
    y1 = (yindex // 1080)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (1080*x2) + (1905120*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_out_ptr0 + (x2 + (1764*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr1 + (x2 + (1764*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp6 = tl.where(tmp2, tmp1, tmp5)
    tmp7 = tmp4 + tmp6
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (1764*y3)), tmp7, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ne/cneen4wlqm2qicradulueackircrv5qxnbor3xyuz5vqezzc7glp.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_136 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_136', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 216
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    tmp0 = tl.load(in_ptr0 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (1764*x2) + (381024*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_out_ptr0 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr2 + (y0 + (1764*x2) + (381024*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr3 + (x2 + (216*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp6 = tmp4 + tmp5
    tmp8 = tl.where(tmp2, tmp1, tmp7)
    tmp9 = tmp6 + tmp8
    tmp12 = tmp10 - tmp11
    tmp14 = 7.086167800453515e-05
    tmp15 = tmp13 * tmp14
    tmp17 = tmp16 * tmp16
    tmp18 = tmp15 * tmp17
    tmp19 = tmp12 * tmp18
    tmp20 = tmp9 - tmp19
    tmp22 = tmp21 * tmp14
    tmp23 = tmp20 - tmp22
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (216*y3)), tmp23, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ov/covzygof6hvedzofss4dcpbttfcvwkuqa3owhsxbwxsea3pywdkh.py
# Source Nodes: [], Original ATen: [aten.convolution_backward]

triton_poi_fused_convolution_backward_137 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2097152], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_137', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1524096
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 108
    x1 = (xindex // 108)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (108 + x0 + (216*x1)), xmask)
    tmp1 = tl.load(in_ptr1 + (108 + x0), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (108 + x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tl.store(out_ptr0 + (x2), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/al/calxf3ad77ecyvndr3tw5dkg3q62g45enf6zusvqlsvyriqtxktc.py
# Source Nodes: [], Original ATen: [aten.convolution_backward]

triton_poi_fused_convolution_backward_138 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2097152], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_138', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1524096
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 108
    x1 = (xindex // 108)
    x2 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (216*x1)), xmask)
    tmp1 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x0), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 * tmp3
    tl.store(out_ptr0 + (x2), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/l6/cl6l6np5agzg4ovjm22u2p6ma6ky33du7kmhewqfxdn7u6c4vrxk.py
# Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]

triton_poi_fused_add_threshold_backward_139 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 2048], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_threshold_backward_139', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 4320
    xnumel = 1764
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 540
    y1 = (yindex // 540)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (540*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_out_ptr0 + (x2 + (1764*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr1 + (x2 + (1764*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp6 = tl.where(tmp2, tmp1, tmp5)
    tmp7 = tmp4 + tmp6
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (1764*y3)), tmp7, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/t7/ct7ckdmgtcbirauqhgtxodmmomi5snpn5uoazhczzt7jhwcoyhkg.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_140 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[256, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_140', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 216
    rnumel = 7056
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 108
    x1 = (xindex // 108)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (762048 + (1764*x0) + (952560*(r2 // 1764)) + (3810240*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/64/c64sjf2q7gwmx65sb4ozqaqkyzpkmc2v7ps66isvzrrk2eztpkcx.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_141 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 2],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_141', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 108
    rnumel = 2
    RBLOCK: tl.constexpr = 2
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (108*r1)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rk/crko3dd2sgdxho5kmrd7myonqorhzwrxuwse4wcatjevybd73irx.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_142 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_142', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 11988
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    _tmp20 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (762048 + (1764*x1) + (952560*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (108*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
        tmp13 = tl.load(in_ptr3 + (x1 + (108*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp14 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp15 = tmp13 - tmp14
        tmp16 = tmp3 * tmp15
        tmp17 = tl.full(tmp16.shape, 0, tmp16.dtype)
        tmp18 = tl.where(tmp2, tmp16, tmp17)
        tmp19 = tl.broadcast_to(tmp18, [XBLOCK, RBLOCK])
        tmp21 = _tmp20 + tmp19
        _tmp20 = tl.where(rmask & xmask, tmp21, _tmp20)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
    tmp20 = tl.sum(_tmp20, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp20, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rz/crzyiyy5zshd7ivxnovqiipsdqsnym2ouwsx6ldemicngxe5uosg.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_143 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 128],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_143', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 108
    rnumel = 111
    RBLOCK: tl.constexpr = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (111*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/vu/cvuctcvdfbq5lr64xsmfyhvltqqvpiojzouowvfla4phm2ijdcj3.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_144 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: 'i32', 15: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(14,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_144', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 108
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (762048 + y0 + (1764*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (108*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2 + (108*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr9 + (x2), xmask, eviction_policy='evict_last')
    tmp23 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 7.086167800453515e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tmp20 = tmp18 - tmp19
    tmp22 = tmp21 * tmp5
    tmp24 = tmp23 * tmp23
    tmp25 = tmp22 * tmp24
    tmp26 = tmp20 * tmp25
    tmp27 = tmp0 - tmp26
    tmp28 = tmp27 - tmp13
    tmp30 = tmp23 * tmp29
    tmp31 = tmp28 * tmp30
    tl.store(out_ptr0 + (x2 + (108*y3)), tmp17, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (108*y3)), tmp31, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ep/cepiu5vmsjxuwm42xnjp2odasoitjqi7rzqcshazlvhxrghphvyh.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_145 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[256, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_145', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 216
    rnumel = 7056
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 108
    x1 = (xindex // 108)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (571536 + (1764*x0) + (952560*(r2 // 1764)) + (3810240*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/dx/cdx2q2xuqbvvkvlxjaehv4dir3nk6qub26ownzu5pf2qcjj7wnmy.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_146 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_146', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 11988
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (571536 + (1764*x1) + (952560*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (108*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/c6/cc6toknmlkuveionsppu5xb2d5obrjgpnh4xbylvljhlke7uwku3.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_147 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_147', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 108
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (571536 + y0 + (1764*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (108*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 7.086167800453515e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (108*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/g4/cg4dfsdr4d7mufapu5t5osfvwka4bqfx42yho4ketexql6tl3zer.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_148 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_148', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 11988
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (108*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = 0.0
        tmp5 = tmp3 <= tmp4
        tmp6 = tl.load(in_ptr1 + ((1764*x1) + (190512*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.where(tmp5, tmp4, tmp6)
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/bt/cbtc4mrvrwqy34rtxw2dcjwgd3bdroymy6g7eqagx7yzckncvknt.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_149 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 128],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_149', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 108
    rnumel = 111
    RBLOCK: tl.constexpr = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (111*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/yl/cyla5pjmsapvd7hcr4627dhtdaml2wfkkqw7knem4bpdtydxq3zc.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_150 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_150', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 11988
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 108)
    x0 = xindex % 108
    _tmp15 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x1)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (108*((r2 + (128*x1)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = 0.0
        tmp5 = tmp3 <= tmp4
        tmp6 = tl.load(in_ptr1 + ((1764*x0) + (190512*(((r2 + (128*x1)) // 1764) % 8)) + ((r2 + (128*x1)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.where(tmp5, tmp4, tmp6)
        tmp8 = tl.load(in_ptr2 + (x0 + (108*((r2 + (128*x1)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp10 = tmp8 - tmp9
        tmp11 = tmp7 * tmp10
        tmp12 = tl.full(tmp11.shape, 0, tmp11.dtype)
        tmp13 = tl.where(tmp2, tmp11, tmp12)
        tmp14 = tl.broadcast_to(tmp13, [XBLOCK, RBLOCK])
        tmp16 = _tmp15 + tmp14
        _tmp15 = tl.where(rmask & xmask, tmp16, _tmp15)
    tmp15 = tl.sum(_tmp15, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp15, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/lv/clvyqwnfztjl5ncirpslbbqrl2a4gefm6noz5jn2ytjtoi6mstwc.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_151 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[128, 128],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_151', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 108
    rnumel = 111
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (108*r1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ju/cjunkdywzhmks7zfcpl33nijbd5viqos6wfz4w2ihro76atvxn72.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_152 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_152', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 108
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    tmp0 = tl.load(in_ptr0 + (x2 + (108*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (1764*x2) + (190512*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr2 + (x2 + (108*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp7 = tmp5 - tmp6
    tmp9 = 7.086167800453515e-05
    tmp10 = tmp8 * tmp9
    tmp12 = tmp11 * tmp11
    tmp13 = tmp10 * tmp12
    tmp14 = tmp7 * tmp13
    tmp15 = tmp4 - tmp14
    tmp17 = tmp16 * tmp9
    tmp18 = tmp15 - tmp17
    tmp20 = tmp11 * tmp19
    tmp21 = tmp18 * tmp20
    tl.store(out_ptr0 + (x2 + (108*y3)), tmp21, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/6p/c6pnq5vr3c2p6swxkchmfamghrn6oakqjymhmuvfrps4hkyrlmpo.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_153 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[256, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_153', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 216
    rnumel = 7056
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 108
    x1 = (xindex // 108)
    _tmp8 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (381024 + (1764*x0) + (952560*(r2 // 1764)) + (3810240*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + (108*r2) + (762048*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + ((1764*x0) + (190512*(r2 // 1764)) + (762048*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tmp1 <= tmp2
        tmp5 = tl.where(tmp3, tmp2, tmp4)
        tmp6 = tmp0 + tmp5
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(rmask & xmask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp8, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ny/cny46xjemsecqnmygr6zx5doxiivmzf54dgocck6lvvm56wcs7d6.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_154 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_154', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 11988
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp17 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    _tmp26 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (381024 + (1764*x1) + (952560*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (108*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tmp4 <= tmp5
        tmp7 = tl.load(in_ptr2 + ((1764*x1) + (190512*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.where(tmp6, tmp5, tmp7)
        tmp9 = tmp3 + tmp8
        tmp10 = tl.load(in_ptr3 + (x1 + (108*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp11 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp12 = tmp10 - tmp11
        tmp13 = tmp9 * tmp12
        tmp14 = tl.full(tmp13.shape, 0, tmp13.dtype)
        tmp15 = tl.where(tmp2, tmp13, tmp14)
        tmp16 = tl.broadcast_to(tmp15, [XBLOCK, RBLOCK])
        tmp18 = _tmp17 + tmp16
        _tmp17 = tl.where(rmask & xmask, tmp18, _tmp17)
        tmp19 = tl.load(in_ptr5 + (x1 + (108*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp20 = tl.load(in_ptr6 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp21 = tmp19 - tmp20
        tmp22 = tmp9 * tmp21
        tmp23 = tl.full(tmp22.shape, 0, tmp22.dtype)
        tmp24 = tl.where(tmp2, tmp22, tmp23)
        tmp25 = tl.broadcast_to(tmp24, [XBLOCK, RBLOCK])
        tmp27 = _tmp26 + tmp25
        _tmp26 = tl.where(rmask & xmask, tmp27, _tmp26)
    tmp17 = tl.sum(_tmp17, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp17, xmask)
    tmp26 = tl.sum(_tmp26, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp26, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/m2/cm2iwzhcdqob4hvt4gyog2uxqiouo7uom5fuudvtlspqdkojz6xz.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_155 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: '*fp32', 15: '*fp32', 16: 'i32', 17: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(16,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_155', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, in_ptr12, in_ptr13, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 108
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (381024 + y0 + (1764*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (108*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0 + (1764*x2) + (190512*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (x2 + (108*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr9 + (x2 + (108*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp27 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr12 + (x2), xmask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr13 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tmp1 <= tmp2
    tmp5 = tl.where(tmp3, tmp2, tmp4)
    tmp6 = tmp0 + tmp5
    tmp9 = tmp7 - tmp8
    tmp11 = 7.086167800453515e-05
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp6 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tmp26 = tmp24 - tmp25
    tmp28 = tmp27 * tmp11
    tmp30 = tmp29 * tmp29
    tmp31 = tmp28 * tmp30
    tmp32 = tmp26 * tmp31
    tmp33 = tmp6 - tmp32
    tmp34 = tmp33 - tmp19
    tmp36 = tmp29 * tmp35
    tmp37 = tmp34 * tmp36
    tl.store(out_ptr0 + (x2 + (108*y3)), tmp23, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (108*y3)), tmp37, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/dz/cdzb7n2wsitrdiox6ev6mlqdiqqfhtv7zrbglyot26k2si3f5iaj.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.threshold_backward]

triton_poi_fused_add_constant_pad_nd_threshold_backward_156 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 8192], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*i1', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_constant_pad_nd_threshold_backward_156', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 864
    xnumel = 6889
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x4 = xindex
    y0 = yindex % 108
    y1 = (yindex // 108)
    y5 = yindex
    x3 = (xindex // 83)
    x2 = xindex % 83
    tmp0 = tl.load(in_ptr0 + (y0 + (108*x4) + (744012*y1)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_out_ptr0 + (x4 + (6889*y5)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp4 = 1 + x3
    tmp5 = tl.full([1, 1], 0, tl.int64)
    tmp6 = tmp4 >= tmp5
    tmp7 = tl.full([1, 1], 85, tl.int64)
    tmp8 = tmp4 < tmp7
    tmp9 = 1 + x2
    tmp10 = tmp9 >= tmp5
    tmp11 = tmp9 < tmp7
    tmp12 = tmp6 & tmp8
    tmp13 = tmp12 & tmp10
    tmp14 = tmp13 & tmp11
    tmp15 = tl.load(in_ptr1 + (9288 + y0 + (108*x2) + (9180*x3) + (780300*y1)), tmp14 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp16 = tl.full(tmp15.shape, 0.0, tmp15.dtype)
    tmp17 = tl.where(tmp14, tmp15, tmp16)
    tmp18 = tmp3 + tmp17
    tmp19 = tl.load(in_ptr2 + (86 + x2 + (85*x3) + (7225*y5)), tmp14 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp20 = tl.full(tmp19.shape, 0.0, tmp19.dtype)
    tmp21 = tl.where(tmp14, tmp19, tmp20)
    tmp22 = tl.where(tmp0, tmp2, tmp21)
    tmp23 = tmp18 + tmp22
    tmp24 = 2 + x3
    tmp25 = tmp24 >= tmp5
    tmp26 = tl.full([1, 1], 87, tl.int64)
    tmp27 = tmp24 < tmp26
    tmp28 = 2 + x2
    tmp29 = tmp28 >= tmp5
    tmp30 = tmp28 < tmp26
    tmp31 = tmp25 & tmp27
    tmp32 = tmp31 & tmp29
    tmp33 = tmp32 & tmp30
    tmp34 = tl.load(in_ptr3 + (176 + x2 + (87*x3) + (7569*y5)), tmp33 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp35 = tl.full(tmp34.shape, 0.0, tmp34.dtype)
    tmp36 = tl.where(tmp33, tmp34, tmp35)
    tmp37 = tl.where(tmp0, tmp2, tmp36)
    tmp38 = tmp23 + tmp37
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x4 + (6889*y5)), tmp38, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ue/cuelwuz6j4q3j7vblslzx72vsawy37bwynxia6ugxeymfgn6jwxe.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_157 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[256, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_157', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 216
    rnumel = 7056
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 108
    x1 = (xindex // 108)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (190512 + (1764*x0) + (952560*(r2 // 1764)) + (3810240*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/4n/c4no6o5hwjrc7fpcry3s5u7js4tiqs4tespv4mm2xwrkpfdcnh7f.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_158 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_158', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 11988
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (190512 + (1764*x1) + (952560*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (108*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/tb/ctbmnrcho6w6elts7v6umqtrh3jlbqljekogyto7zw7iviieqrbv.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_159 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_159', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 108
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (190512 + y0 + (1764*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (108*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 7.086167800453515e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (108*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/un/cunswzjhg722rwwt7icm6d5vnx64onhbo2wph6g4slb5fcuexwky.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_160 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_160', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 46548
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 431
    x1 = (xindex // 431)
    _tmp41 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((6889*x1) + (744012*(((r2 + (128*x0)) // 6889) % 8)) + ((r2 + (128*x0)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = 1 + (((r2 + (128*x0)) // 83) % 83)
        tmp5 = tl.full([1, 1], 0, tl.int64)
        tmp6 = tmp4 >= tmp5
        tmp7 = tl.full([1, 1], 85, tl.int64)
        tmp8 = tmp4 < tmp7
        tmp9 = 1 + ((r2 + (128*x0)) % 83)
        tmp10 = tmp9 >= tmp5
        tmp11 = tmp9 < tmp7
        tmp12 = tmp6 & tmp8
        tmp13 = tmp12 & tmp10
        tmp14 = tmp13 & tmp11
        tmp15 = tmp14 & tmp2
        tmp16 = tl.load(in_ptr1 + (9288 + x1 + (108*((r2 + (128*x0)) % 83)) + (9180*(((r2 + (128*x0)) // 83) % 83)) + (780300*(((r2 + (128*x0)) // 6889) % 8))), rmask & tmp15 & xmask, eviction_policy='evict_last', other=0.0)
        tmp17 = tl.full(tmp16.shape, 0.0, tmp16.dtype)
        tmp18 = tl.where(tmp15, tmp16, tmp17)
        tmp19 = tmp3 + tmp18
        tmp20 = tl.load(in_ptr2 + (x1 + (108*((r2 + (128*x0)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp21 = 3 + (((r2 + (128*x0)) // 83) % 83)
        tmp22 = tmp21 >= tmp5
        tmp23 = tl.full([1, 1], 89, tl.int64)
        tmp24 = tmp21 < tmp23
        tmp25 = 3 + ((r2 + (128*x0)) % 83)
        tmp26 = tmp25 >= tmp5
        tmp27 = tmp25 < tmp23
        tmp28 = tmp22 & tmp24
        tmp29 = tmp28 & tmp26
        tmp30 = tmp29 & tmp27
        tmp31 = tmp30 & tmp2
        tmp32 = tl.load(in_ptr3 + (270 + (89*(((r2 + (128*x0)) // 83) % 83)) + (7921*x1) + (855468*(((r2 + (128*x0)) // 6889) % 8)) + ((r2 + (128*x0)) % 83)), rmask & tmp31 & xmask, eviction_policy='evict_last', other=0.0)
        tmp33 = tl.full(tmp32.shape, 0.0, tmp32.dtype)
        tmp34 = tl.where(tmp31, tmp32, tmp33)
        tmp35 = 0.0
        tmp36 = tl.where(tmp20, tmp35, tmp34)
        tmp37 = tmp19 + tmp36
        tmp38 = tl.full(tmp37.shape, 0, tmp37.dtype)
        tmp39 = tl.where(tmp2, tmp37, tmp38)
        tmp40 = tl.broadcast_to(tmp39, [XBLOCK, RBLOCK])
        tmp42 = _tmp41 + tmp40
        _tmp41 = tl.where(rmask & xmask, tmp42, _tmp41)
    tmp41 = tl.sum(_tmp41, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp41, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ad/caddjzr546rlsnchisqjrqzsmlk7ldolp7du2u7yco56w43qctfi.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_161 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 512],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_161', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
    xnumel = 108
    XBLOCK: tl.constexpr = 1
    rnumel = 431
    RBLOCK: tl.constexpr = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (431*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/bi/cbi3imbtdfzprkczsupvgria3o6rsk2zries4n6js3ane3vtbiav.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_162 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_162', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 46548
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 108)
    x0 = xindex % 108
    _tmp45 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x1)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((6889*x0) + (744012*(((r2 + (128*x1)) // 6889) % 8)) + ((r2 + (128*x1)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = 1 + (((r2 + (128*x1)) // 83) % 83)
        tmp5 = tl.full([1, 1], 0, tl.int64)
        tmp6 = tmp4 >= tmp5
        tmp7 = tl.full([1, 1], 85, tl.int64)
        tmp8 = tmp4 < tmp7
        tmp9 = 1 + ((r2 + (128*x1)) % 83)
        tmp10 = tmp9 >= tmp5
        tmp11 = tmp9 < tmp7
        tmp12 = tmp6 & tmp8
        tmp13 = tmp12 & tmp10
        tmp14 = tmp13 & tmp11
        tmp15 = tmp14 & tmp2
        tmp16 = tl.load(in_ptr1 + (9288 + x0 + (108*((r2 + (128*x1)) % 83)) + (9180*(((r2 + (128*x1)) // 83) % 83)) + (780300*(((r2 + (128*x1)) // 6889) % 8))), rmask & tmp15 & xmask, eviction_policy='evict_last', other=0.0)
        tmp17 = tl.full(tmp16.shape, 0.0, tmp16.dtype)
        tmp18 = tl.where(tmp15, tmp16, tmp17)
        tmp19 = tmp3 + tmp18
        tmp20 = tl.load(in_ptr2 + (x0 + (108*((r2 + (128*x1)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp21 = 3 + (((r2 + (128*x1)) // 83) % 83)
        tmp22 = tmp21 >= tmp5
        tmp23 = tl.full([1, 1], 89, tl.int64)
        tmp24 = tmp21 < tmp23
        tmp25 = 3 + ((r2 + (128*x1)) % 83)
        tmp26 = tmp25 >= tmp5
        tmp27 = tmp25 < tmp23
        tmp28 = tmp22 & tmp24
        tmp29 = tmp28 & tmp26
        tmp30 = tmp29 & tmp27
        tmp31 = tmp30 & tmp2
        tmp32 = tl.load(in_ptr3 + (270 + (89*(((r2 + (128*x1)) // 83) % 83)) + (7921*x0) + (855468*(((r2 + (128*x1)) // 6889) % 8)) + ((r2 + (128*x1)) % 83)), rmask & tmp31 & xmask, eviction_policy='evict_last', other=0.0)
        tmp33 = tl.full(tmp32.shape, 0.0, tmp32.dtype)
        tmp34 = tl.where(tmp31, tmp32, tmp33)
        tmp35 = 0.0
        tmp36 = tl.where(tmp20, tmp35, tmp34)
        tmp37 = tmp19 + tmp36
        tmp38 = tl.load(in_ptr4 + (x0 + (108*((r2 + (128*x1)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp39 = tl.load(in_ptr5 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp40 = tmp38 - tmp39
        tmp41 = tmp37 * tmp40
        tmp42 = tl.full(tmp41.shape, 0, tmp41.dtype)
        tmp43 = tl.where(tmp2, tmp41, tmp42)
        tmp44 = tl.broadcast_to(tmp43, [XBLOCK, RBLOCK])
        tmp46 = _tmp45 + tmp44
        _tmp45 = tl.where(rmask & xmask, tmp46, _tmp45)
    tmp45 = tl.sum(_tmp45, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp45, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rx/crxl5lwqqxj4bwv4u7rygwhriduuv5ozwqijjkphinmhmyap2eoo.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_163 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[128, 512],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_163', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 108
    rnumel = 431
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (108*r1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/sd/csd43gtut5c5mtpk3az4z2iibo2vpttnab6smv6mabt5plr45o74.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_164 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[65536, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_164', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 55112
    xnumel = 108
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x3 = xindex
    y2 = (yindex // 6889)
    y4 = yindex % 6889
    y1 = (yindex // 83) % 83
    y0 = yindex % 83
    y6 = yindex
    tmp0 = tl.load(in_ptr0 + (y4 + (6889*x3) + (744012*y2)), xmask & ymask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr2 + (x3 + (108*y6)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp33 = tl.load(in_ptr4 + (x3 + (108*y6)), xmask & ymask, eviction_policy='evict_last')
    tmp34 = tl.load(in_ptr5 + (x3), xmask, eviction_policy='evict_last')
    tmp36 = tl.load(in_ptr6 + (x3), xmask, eviction_policy='evict_last')
    tmp39 = tl.load(in_ptr7 + (x3), xmask, eviction_policy='evict_last')
    tmp1 = 1 + y1
    tmp2 = tl.full([1, 1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1, 1], 85, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = 1 + y0
    tmp7 = tmp6 >= tmp2
    tmp8 = tmp6 < tmp4
    tmp9 = tmp3 & tmp5
    tmp10 = tmp9 & tmp7
    tmp11 = tmp10 & tmp8
    tmp12 = tl.load(in_ptr1 + (9288 + x3 + (108*y0) + (9180*y1) + (780300*y2)), tmp11 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp13 = tl.full(tmp12.shape, 0.0, tmp12.dtype)
    tmp14 = tl.where(tmp11, tmp12, tmp13)
    tmp15 = tmp0 + tmp14
    tmp17 = 3 + y1
    tmp18 = tmp17 >= tmp2
    tmp19 = tl.full([1, 1], 89, tl.int64)
    tmp20 = tmp17 < tmp19
    tmp21 = 3 + y0
    tmp22 = tmp21 >= tmp2
    tmp23 = tmp21 < tmp19
    tmp24 = tmp18 & tmp20
    tmp25 = tmp24 & tmp22
    tmp26 = tmp25 & tmp23
    tmp27 = tl.load(in_ptr3 + (270 + y0 + (89*y1) + (7921*x3) + (855468*y2)), tmp26 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp28 = tl.full(tmp27.shape, 0.0, tmp27.dtype)
    tmp29 = tl.where(tmp26, tmp27, tmp28)
    tmp30 = 0.0
    tmp31 = tl.where(tmp16, tmp30, tmp29)
    tmp32 = tmp15 + tmp31
    tmp35 = tmp33 - tmp34
    tmp37 = 1.814486863115111e-05
    tmp38 = tmp36 * tmp37
    tmp40 = tmp39 * tmp39
    tmp41 = tmp38 * tmp40
    tmp42 = tmp35 * tmp41
    tmp43 = tmp32 - tmp42
    tl.store(out_ptr0 + (x3 + (108*y6)), tmp43, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/op/coptwwx4lt6z47zy6q7j7vm76iu5po6e2xctje747buzksdxxows.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_165 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 8192], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_165', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 864
    xnumel = 6889
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 108
    y1 = (yindex // 108)
    tmp0 = tl.load(in_out_ptr0 + (y0 + (108*x2) + (744012*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr0 + (y0), ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr1 + (y0), ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    tmp2 = 1.814486863115111e-05
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 - tmp3
    tmp7 = tmp5 * tmp6
    tmp8 = tmp4 * tmp7
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (y0 + (108*x2) + (744012*y1)), tmp8, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zu/czurkmk3e7dqi5bbpgx2hemhqljo6r266gsbsg7wm6fqjuwa6zsw.py
# Source Nodes: [], Original ATen: [aten.add, aten.avg_pool2d_backward, aten.constant_pad_nd, aten.threshold_backward]

triton_poi_fused_add_avg_pool2d_backward_constant_pad_nd_threshold_backward_166 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 8192], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_avg_pool2d_backward_constant_pad_nd_threshold_backward_166', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 2160
    xnumel = 6889
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = (xindex // 83)
    x1 = xindex % 83
    y0 = yindex
    x3 = xindex
    y4 = yindex % 270
    y5 = (yindex // 270)
    tmp24 = tl.load(in_ptr1 + ((42*(tl.math.min(tl.math.max(0, ((1 + x2) // 2)), (-1) + (tl.math.min(42, 1 + (x2 // 2)))))) + (42*(tl.where((tl.math.min(tl.math.max(0, ((1 + x2) // 2)), (-1) + (tl.math.min(42, 1 + (x2 // 2))))) >= 0, 0, 42))) + (1764*y0) + (tl.math.min(tl.math.max(0, ((1 + x1) // 2)), (-1) + (tl.math.min(42, 1 + (x1 // 2))))) + (tl.where((tl.math.min(tl.math.max(0, ((1 + x1) // 2)), (-1) + (tl.math.min(42, 1 + (x1 // 2))))) >= 0, 0, 42))), xmask & ymask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr2 + (y4 + (270*x3) + (1860030*y5)), xmask & ymask, eviction_policy='evict_last')
    tmp38 = tl.load(in_ptr3 + (x3 + (6889*y0)), xmask & ymask, eviction_policy='evict_last')
    tmp0 = (-1) + x2
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 83, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = (-1) + x1
    tmp6 = tmp5 >= tmp1
    tmp7 = tmp5 < tmp3
    tmp8 = tmp2 & tmp4
    tmp9 = tmp8 & tmp6
    tmp10 = tmp9 & tmp7
    tmp11 = tl.load(in_ptr0 + ((42*(tl.math.min(tl.math.max(0, (x2 // 2)), (-1) + (tl.math.min(42, 1 + (((-1) + x2) // 2)))))) + (42*(tl.where((tl.math.min(tl.math.max(0, (x2 // 2)), (-1) + (tl.math.min(42, 1 + (((-1) + x2) // 2))))) >= 0, 0, 42))) + (1764*y0) + (tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(42, 1 + (((-1) + x1) // 2))))) + (tl.where((tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(42, 1 + (((-1) + x1) // 2))))) >= 0, 0, 42))), tmp10 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp12 = tmp11 / 1
    tmp13 = tl.broadcast_to(tl.math.max(0, (x2 // 2)), [XBLOCK, YBLOCK])
    tmp14 = tl.broadcast_to(tl.math.min(42, 1 + (((-1) + x2) // 2)), [XBLOCK, YBLOCK])
    tmp15 = tmp13 < tmp14
    tmp16 = tl.broadcast_to(tl.math.max(0, (x1 // 2)), [XBLOCK, YBLOCK])
    tmp17 = tl.broadcast_to(tl.math.min(42, 1 + (((-1) + x1) // 2)), [XBLOCK, YBLOCK])
    tmp18 = tmp16 < tmp17
    tmp19 = tmp15 & tmp18
    tmp20 = 0.0
    tmp21 = tl.where(tmp19, tmp12, tmp20)
    tmp22 = tl.full(tmp21.shape, 0.0, tmp21.dtype)
    tmp23 = tl.where(tmp10, tmp21, tmp22)
    tmp25 = tmp24 / 1
    tmp26 = tl.math.max(0, ((1 + x2) // 2))
    tmp27 = tl.math.min(42, 1 + (x2 // 2))
    tmp28 = tmp26 < tmp27
    tmp29 = tl.math.max(0, ((1 + x1) // 2))
    tmp30 = tl.math.min(42, 1 + (x1 // 2))
    tmp31 = tmp29 < tmp30
    tmp32 = tmp28 & tmp31
    tmp33 = tl.where(tmp32, tmp25, tmp20)
    tmp34 = tmp23 + tmp33
    tmp36 = tmp35 <= tmp20
    tmp37 = tl.where(tmp36, tmp20, tmp34)
    tmp39 = tl.where(tmp36, tmp20, tmp38)
    tmp40 = tmp37 + tmp39
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3 + (6889*y0)), tmp40, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/73/c73fzeec3s2btmztnyhf3r2y64jvfu6stfq3b4rzfqdlarrlzbib.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_167 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[256, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_167', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 216
    rnumel = 7056
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 108
    x1 = (xindex // 108)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((1764*x0) + (952560*(r2 // 1764)) + (3810240*x1) + (r2 % 1764)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/dn/cdnwcf54netehmiwrumbfdqx5cvyxc2iz3l6ovyo2zixgp67scti.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_168 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_168', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 11988
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 111
    x1 = (xindex // 111)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 14112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((1764*x1) + (952560*(((r2 + (128*x0)) // 1764) % 8)) + ((r2 + (128*x0)) % 1764)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (108*((r2 + (128*x0)) % 14112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7y/c7y3h25iik5hejm4lzqvxsm4f2eoilof7ypvkdso6vf34ormgemq.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_169 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[16384, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_169', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 14112
    xnumel = 108
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1764
    y1 = (yindex // 1764)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (1764*x2) + (952560*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (108*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 7.086167800453515e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (108*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/da/cdahy7cw46pn4afqickhq7kvvzlqwlmqwcxwsfwetrohpyg6ziuu.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.threshold_backward]

triton_poi_fused_add_constant_pad_nd_threshold_backward_170 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 8192], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_constant_pad_nd_threshold_backward_170', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 864
    xnumel = 6889
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x4 = xindex
    y0 = yindex % 108
    y1 = (yindex // 108)
    x3 = (xindex // 83)
    x2 = xindex % 83
    y5 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (108*x4) + (744012*y1)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = 1 + x3
    tmp2 = tl.full([1, 1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1, 1], 85, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = 1 + x2
    tmp7 = tmp6 >= tmp2
    tmp8 = tmp6 < tmp4
    tmp9 = tmp3 & tmp5
    tmp10 = tmp9 & tmp7
    tmp11 = tmp10 & tmp8
    tmp12 = tl.load(in_ptr1 + (86 + x2 + (85*x3) + (7225*y5)), tmp11 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp13 = tl.full(tmp12.shape, 0.0, tmp12.dtype)
    tmp14 = tl.where(tmp11, tmp12, tmp13)
    tmp15 = 0.0
    tmp16 = tl.where(tmp0, tmp15, tmp14)
    tmp17 = tl.load(in_ptr2 + (9288 + y0 + (108*x2) + (9180*x3) + (780300*y1)), tmp11 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp18 = tl.full(tmp17.shape, 0.0, tmp17.dtype)
    tmp19 = tl.where(tmp11, tmp17, tmp18)
    tmp20 = tmp16 + tmp19
    tmp21 = 2 + x3
    tmp22 = tmp21 >= tmp2
    tmp23 = tl.full([1, 1], 87, tl.int64)
    tmp24 = tmp21 < tmp23
    tmp25 = 2 + x2
    tmp26 = tmp25 >= tmp2
    tmp27 = tmp25 < tmp23
    tmp28 = tmp22 & tmp24
    tmp29 = tmp28 & tmp26
    tmp30 = tmp29 & tmp27
    tmp31 = tl.load(in_ptr3 + (176 + x2 + (87*x3) + (7569*y5)), tmp30 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp32 = tl.full(tmp31.shape, 0.0, tmp31.dtype)
    tmp33 = tl.where(tmp30, tmp31, tmp32)
    tmp34 = tl.where(tmp0, tmp15, tmp33)
    tmp35 = tmp20 + tmp34
    tl.store(out_ptr0 + (x4 + (6889*y5)), tmp35, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/fb/cfbqfcg5evaimechyjfboaakfl2bny7qa6oplrmqqnogygemr3p7.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_171 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_171', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 756
    rnumel = 7874
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 108)
    x0 = xindex % 108
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (7874*x1)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((6889*x0) + (744012*(((r2 + (7874*x1)) // 6889) % 8)) + ((r2 + (7874*x1)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.full(tmp3.shape, 0, tmp3.dtype)
        tmp5 = tl.where(tmp2, tmp3, tmp4)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/vl/cvlio4ugsdedrkbcdzewp75llepr2wgg7kkejbiuysvhledu4352.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_172 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 8],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_172', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 108
    rnumel = 7
    RBLOCK: tl.constexpr = 8
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (108*r1)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/iv/civ7u6m2prytlxnesoum47ahnopeefbemfskzp47e2ko3gmjjmqf.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_173 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_173', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 46548
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 431
    x1 = (xindex // 431)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((6889*x1) + (744012*(((r2 + (128*x0)) // 6889) % 8)) + ((r2 + (128*x0)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (108*((r2 + (128*x0)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/p4/cp4vmsfdnwclyf4jyj72jn3lueshcpivpogv4cecwzvr5cox3cyb.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_174 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 512],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_174', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel):
    xnumel = 108
    XBLOCK: tl.constexpr = 1
    rnumel = 431
    RBLOCK: tl.constexpr = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (431*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ow/cowzfm6rrlm5l2qqkkdu6ko3atxkgnmw4vwt7b4wfzhmmvila3pt.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_poi_fused_native_batch_norm_backward_175 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 8192], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_native_batch_norm_backward_175', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 864
    xnumel = 6889
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 108
    y1 = (yindex // 108)
    tmp0 = tl.load(in_out_ptr0 + (x2 + (6889*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr0 + (y0 + (108*x2) + (744012*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr1 + (y0), ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (y0), ymask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr4 + (y0), ymask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr5 + (y0), ymask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 1.814486863115111e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (6889*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/t7/ct7wmllw2ohcbjolihra5t6qu5csojgkzl7ofbcjrlkmzphsk7mu.py
# Source Nodes: [], Original ATen: [aten.convolution_backward]

triton_poi_fused_convolution_backward_176 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 8192], tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_176', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 432
    xnumel = 6889
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 54
    y1 = (yindex // 54)
    tmp0 = tl.load(in_ptr0 + (372006 + x2 + (6889*y0) + (744012*y1)), xmask & ymask, eviction_policy='evict_last')
    tl.store(out_ptr0 + (y0 + (54*x2) + (372006*y1)), tmp0, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/mg/cmgczf3aoxxu4zgsaga5uvfjsvn5arsrvcuf7vvabpkylt45lycl.py
# Source Nodes: [], Original ATen: [aten.convolution_backward]

triton_poi_fused_convolution_backward_177 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 8192], tile_hint=TileHint.SQUARE,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_177', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 432
    xnumel = 6889
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 54
    y1 = (yindex // 54)
    tmp0 = tl.load(in_ptr0 + (x2 + (6889*y0) + (744012*y1)), xmask & ymask, eviction_policy='evict_last')
    tl.store(out_ptr0 + (y0 + (54*x2) + (372006*y1)), tmp0, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/qa/cqacsiv5zq75jxt4a4pgv5yznxtcaqjtc5kuysppetatn5iifuhi.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_178 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_178', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 378
    rnumel = 7874
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 54)
    x0 = xindex % 54
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (7874*x1)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (1488024 + (6889*x0) + (1860030*(((r2 + (7874*x1)) // 6889) % 8)) + ((r2 + (7874*x1)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.full(tmp3.shape, 0, tmp3.dtype)
        tmp5 = tl.where(tmp2, tmp3, tmp4)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/he/cheyebiglv6ihqdoqo23pt6yxzzekb6ur5law6grvmenoy76km3g.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_179 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[64, 8],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_179', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 54
    rnumel = 7
    RBLOCK: tl.constexpr = 8
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (54*r1)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/hq/chqwucfertcm23nao2brefryutgksknmqfepzehbqkfa47lil2fd.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_180 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_180', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23274
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 431
    x1 = (xindex // 431)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    _tmp20 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (1488024 + (6889*x1) + (1860030*(((r2 + (128*x0)) // 6889) % 8)) + ((r2 + (128*x0)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (54*((r2 + (128*x0)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
        tmp13 = tl.load(in_ptr3 + (x1 + (54*((r2 + (128*x0)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp14 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp15 = tmp13 - tmp14
        tmp16 = tmp3 * tmp15
        tmp17 = tl.full(tmp16.shape, 0, tmp16.dtype)
        tmp18 = tl.where(tmp2, tmp16, tmp17)
        tmp19 = tl.broadcast_to(tmp18, [XBLOCK, RBLOCK])
        tmp21 = _tmp20 + tmp19
        _tmp20 = tl.where(rmask & xmask, tmp21, _tmp20)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
    tmp20 = tl.sum(_tmp20, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp20, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/mm/cmmthqoffqeifqrki3kbiroiz3ynwaxemzdvbrxc4x3yhmh5r5tg.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_per_fused_native_batch_norm_backward_181 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[64, 512],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_181', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel):
    xnumel = 54
    XBLOCK: tl.constexpr = 1
    rnumel = 431
    RBLOCK: tl.constexpr = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (431*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/cr/ccr4mmgtvwo4nv5dbtdqv2way45qlqyn4qsepng5m7hajowj657f.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_182 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[65536, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: 'i32', 15: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(14,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_182', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 55112
    xnumel = 54
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 6889
    y1 = (yindex // 6889)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (1488024 + y0 + (6889*x2) + (1860030*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (54*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2 + (54*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr9 + (x2), xmask, eviction_policy='evict_last')
    tmp23 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 1.814486863115111e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tmp20 = tmp18 - tmp19
    tmp22 = tmp21 * tmp5
    tmp24 = tmp23 * tmp23
    tmp25 = tmp22 * tmp24
    tmp26 = tmp20 * tmp25
    tmp27 = tmp0 - tmp26
    tmp28 = tmp27 - tmp13
    tmp30 = tmp23 * tmp29
    tmp31 = tmp28 * tmp30
    tl.store(out_ptr0 + (x2 + (54*y3)), tmp17, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (54*y3)), tmp31, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ez/cezz4cze3exr4dmhizq5f5j44eaidpryuisodk7j4q7ksz44mw4b.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_183 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_183', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23274
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 431
    x1 = (xindex // 431)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (54*((r2 + (128*x0)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = 0.0
        tmp5 = tmp3 <= tmp4
        tmp6 = tl.load(in_ptr1 + ((6889*x1) + (372006*(((r2 + (128*x0)) // 6889) % 8)) + ((r2 + (128*x0)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.where(tmp5, tmp4, tmp6)
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/3e/c3e5ydvfyb42erqzzq5ihid54anaufqp4kg6tpw5jix445mv6ip2.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_184 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[64, 512],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_184', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
    xnumel = 54
    XBLOCK: tl.constexpr = 1
    rnumel = 431
    RBLOCK: tl.constexpr = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (431*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/cv/ccv2slu6ldevdfi5mlx35wzvnb5u5eeivwts6bugdisnsfoa45fo.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_185 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_185', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23274
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 54)
    x0 = xindex % 54
    _tmp15 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x1)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (54*((r2 + (128*x1)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = 0.0
        tmp5 = tmp3 <= tmp4
        tmp6 = tl.load(in_ptr1 + ((6889*x0) + (372006*(((r2 + (128*x1)) // 6889) % 8)) + ((r2 + (128*x1)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.where(tmp5, tmp4, tmp6)
        tmp8 = tl.load(in_ptr2 + (x0 + (54*((r2 + (128*x1)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp10 = tmp8 - tmp9
        tmp11 = tmp7 * tmp10
        tmp12 = tl.full(tmp11.shape, 0, tmp11.dtype)
        tmp13 = tl.where(tmp2, tmp11, tmp12)
        tmp14 = tl.broadcast_to(tmp13, [XBLOCK, RBLOCK])
        tmp16 = _tmp15 + tmp14
        _tmp15 = tl.where(rmask & xmask, tmp16, _tmp15)
    tmp15 = tl.sum(_tmp15, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp15, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/vl/cvlrlfxgkxxb5anndcc45tkt4nldmko6b33r7fg57pnen2653csu.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_186 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 512],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_186', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 54
    rnumel = 431
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (54*r1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/26/c26jvuci7mzmmq3ronjrqkyj5bbrd2z3c4bs6umvnibz45aghkn5.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_187 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[65536, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_187', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 55112
    xnumel = 54
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 6889
    y1 = (yindex // 6889)
    tmp0 = tl.load(in_ptr0 + (x2 + (54*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (6889*x2) + (372006*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr2 + (x2 + (54*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp7 = tmp5 - tmp6
    tmp9 = 1.814486863115111e-05
    tmp10 = tmp8 * tmp9
    tmp12 = tmp11 * tmp11
    tmp13 = tmp10 * tmp12
    tmp14 = tmp7 * tmp13
    tmp15 = tmp4 - tmp14
    tmp17 = tmp16 * tmp9
    tmp18 = tmp15 - tmp17
    tmp20 = tmp11 * tmp19
    tmp21 = tmp18 * tmp20
    tl.store(out_ptr0 + (x2 + (54*y3)), tmp21, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zc/czcjoty32n5gzbjtx6bvkzwguymmtkqrjy7m6rxsdxeib7hynxdx.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_188 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_188', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 378
    rnumel = 7874
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 54)
    x0 = xindex % 54
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (7874*x1)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((6889*x0) + (1860030*(((r2 + (7874*x1)) // 6889) % 8)) + ((r2 + (7874*x1)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.full(tmp3.shape, 0, tmp3.dtype)
        tmp5 = tl.where(tmp2, tmp3, tmp4)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/tj/ctjusewugin3imm5rotxadaazr6xtj7vlbsctn32bxlf4il7wr2a.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_189 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_189', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23274
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 431
    x1 = (xindex // 431)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    _tmp20 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((6889*x1) + (1860030*(((r2 + (128*x0)) // 6889) % 8)) + ((r2 + (128*x0)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (54*((r2 + (128*x0)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
        tmp13 = tl.load(in_ptr3 + (x1 + (54*((r2 + (128*x0)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp14 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp15 = tmp13 - tmp14
        tmp16 = tmp3 * tmp15
        tmp17 = tl.full(tmp16.shape, 0, tmp16.dtype)
        tmp18 = tl.where(tmp2, tmp16, tmp17)
        tmp19 = tl.broadcast_to(tmp18, [XBLOCK, RBLOCK])
        tmp21 = _tmp20 + tmp19
        _tmp20 = tl.where(rmask & xmask, tmp21, _tmp20)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
    tmp20 = tl.sum(_tmp20, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp20, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/34/c3446nvhlnncruzomazyxspzcbl77akpswzeordrk5tdpcgchcvb.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_190 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[65536, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: 'i32', 15: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(14,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_190', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 55112
    xnumel = 54
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 6889
    y1 = (yindex // 6889)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (6889*x2) + (1860030*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (54*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2 + (54*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr9 + (x2), xmask, eviction_policy='evict_last')
    tmp23 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 1.814486863115111e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tmp20 = tmp18 - tmp19
    tmp22 = tmp21 * tmp5
    tmp24 = tmp23 * tmp23
    tmp25 = tmp22 * tmp24
    tmp26 = tmp20 * tmp25
    tmp27 = tmp0 - tmp26
    tmp28 = tmp27 - tmp13
    tmp30 = tmp23 * tmp29
    tmp31 = tmp28 * tmp30
    tl.store(out_ptr0 + (x2 + (54*y3)), tmp17, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (54*y3)), tmp31, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/yl/cylh6gc3jam5pj234iggxsq5lunklnla2anlo5l77bzogxzc2ka6.py
# Source Nodes: [], Original ATen: [aten.add, aten.avg_pool2d_backward, aten.constant_pad_nd, aten.threshold_backward]

triton_poi_fused_add_avg_pool2d_backward_constant_pad_nd_threshold_backward_191 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1024, 32768], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_avg_pool2d_backward_constant_pad_nd_threshold_backward_191', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 768
    xnumel = 27225
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = (xindex // 165)
    x1 = xindex % 165
    y0 = yindex
    x3 = xindex
    y4 = yindex % 96
    y5 = (yindex // 96)
    tmp24 = tl.load(in_ptr1 + ((83*(tl.math.min(tl.math.max(0, ((1 + x2) // 2)), (-1) + (tl.math.min(83, 1 + (x2 // 2)))))) + (83*(tl.where((tl.math.min(tl.math.max(0, ((1 + x2) // 2)), (-1) + (tl.math.min(83, 1 + (x2 // 2))))) >= 0, 0, 83))) + (6889*y0) + (tl.math.min(tl.math.max(0, ((1 + x1) // 2)), (-1) + (tl.math.min(83, 1 + (x1 // 2))))) + (tl.where((tl.math.min(tl.math.max(0, ((1 + x1) // 2)), (-1) + (tl.math.min(83, 1 + (x1 // 2))))) >= 0, 0, 83))), xmask & ymask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr2 + (y4 + (96*x3) + (2613600*y5)), xmask & ymask, eviction_policy='evict_last')
    tmp0 = (-1) + x2
    tmp1 = tl.full([1, 1], 0, tl.int64)
    tmp2 = tmp0 >= tmp1
    tmp3 = tl.full([1, 1], 165, tl.int64)
    tmp4 = tmp0 < tmp3
    tmp5 = (-1) + x1
    tmp6 = tmp5 >= tmp1
    tmp7 = tmp5 < tmp3
    tmp8 = tmp2 & tmp4
    tmp9 = tmp8 & tmp6
    tmp10 = tmp9 & tmp7
    tmp11 = tl.load(in_ptr0 + ((83*(tl.math.min(tl.math.max(0, (x2 // 2)), (-1) + (tl.math.min(83, 1 + (((-1) + x2) // 2)))))) + (83*(tl.where((tl.math.min(tl.math.max(0, (x2 // 2)), (-1) + (tl.math.min(83, 1 + (((-1) + x2) // 2))))) >= 0, 0, 83))) + (6889*y0) + (tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(83, 1 + (((-1) + x1) // 2))))) + (tl.where((tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(83, 1 + (((-1) + x1) // 2))))) >= 0, 0, 83))), tmp10 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp12 = tmp11 / 1
    tmp13 = tl.broadcast_to(tl.math.max(0, (x2 // 2)), [XBLOCK, YBLOCK])
    tmp14 = tl.broadcast_to(tl.math.min(83, 1 + (((-1) + x2) // 2)), [XBLOCK, YBLOCK])
    tmp15 = tmp13 < tmp14
    tmp16 = tl.broadcast_to(tl.math.max(0, (x1 // 2)), [XBLOCK, YBLOCK])
    tmp17 = tl.broadcast_to(tl.math.min(83, 1 + (((-1) + x1) // 2)), [XBLOCK, YBLOCK])
    tmp18 = tmp16 < tmp17
    tmp19 = tmp15 & tmp18
    tmp20 = 0.0
    tmp21 = tl.where(tmp19, tmp12, tmp20)
    tmp22 = tl.full(tmp21.shape, 0.0, tmp21.dtype)
    tmp23 = tl.where(tmp10, tmp21, tmp22)
    tmp25 = tmp24 / 1
    tmp26 = tl.math.max(0, ((1 + x2) // 2))
    tmp27 = tl.math.min(83, 1 + (x2 // 2))
    tmp28 = tmp26 < tmp27
    tmp29 = tl.math.max(0, ((1 + x1) // 2))
    tmp30 = tl.math.min(83, 1 + (x1 // 2))
    tmp31 = tmp29 < tmp30
    tmp32 = tmp28 & tmp31
    tmp33 = tl.where(tmp32, tmp25, tmp20)
    tmp34 = tmp23 + tmp33
    tmp36 = tmp35 <= tmp20
    tmp37 = tl.where(tmp36, tmp20, tmp34)
    tmp38 = 1 + x2
    tmp39 = tmp38 >= tmp1
    tmp40 = tl.full([1, 1], 167, tl.int64)
    tmp41 = tmp38 < tmp40
    tmp42 = 1 + x1
    tmp43 = tmp42 >= tmp1
    tmp44 = tmp42 < tmp40
    tmp45 = tmp39 & tmp41
    tmp46 = tmp45 & tmp43
    tmp47 = tmp46 & tmp44
    tmp48 = tl.load(in_ptr3 + (168 + x1 + (167*x2) + (27889*y0)), tmp47 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp49 = tl.full(tmp48.shape, 0.0, tmp48.dtype)
    tmp50 = tl.where(tmp47, tmp48, tmp49)
    tmp51 = tl.where(tmp36, tmp20, tmp50)
    tmp52 = tmp37 + tmp51
    tmp53 = tl.load(in_ptr4 + (16128 + y4 + (96*x1) + (16032*x2) + (2677344*y5)), tmp47 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp54 = tl.full(tmp53.shape, 0.0, tmp53.dtype)
    tmp55 = tl.where(tmp47, tmp53, tmp54)
    tmp56 = tmp52 + tmp55
    tmp57 = 2 + x2
    tmp58 = tmp57 >= tmp1
    tmp59 = tl.full([1, 1], 169, tl.int64)
    tmp60 = tmp57 < tmp59
    tmp61 = 2 + x1
    tmp62 = tmp61 >= tmp1
    tmp63 = tmp61 < tmp59
    tmp64 = tmp58 & tmp60
    tmp65 = tmp64 & tmp62
    tmp66 = tmp65 & tmp63
    tmp67 = tl.load(in_ptr5 + (340 + x1 + (169*x2) + (28561*y0)), tmp66 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp68 = tl.full(tmp67.shape, 0.0, tmp67.dtype)
    tmp69 = tl.where(tmp66, tmp67, tmp68)
    tmp70 = tl.where(tmp36, tmp20, tmp69)
    tmp71 = tmp56 + tmp70
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x3 + (27225*y0)), tmp71, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/y4/cy4w5xux6p5fwgnn6m3zlr3iv5bqonaiymv5ufzwu2rpekrl2c7u.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_192 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_192', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 378
    rnumel = 7874
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 54)
    x0 = xindex % 54
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (7874*x1)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (1116018 + (6889*x0) + (1860030*(((r2 + (7874*x1)) // 6889) % 8)) + ((r2 + (7874*x1)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.full(tmp3.shape, 0, tmp3.dtype)
        tmp5 = tl.where(tmp2, tmp3, tmp4)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ai/cai4mebko4j5okdnlzfismbv7qvw4tmdhctm3x2ibikfi6qbxsmq.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_193 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_193', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23274
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 431
    x1 = (xindex // 431)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (1116018 + (6889*x1) + (1860030*(((r2 + (128*x0)) // 6889) % 8)) + ((r2 + (128*x0)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (54*((r2 + (128*x0)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/dt/cdtrlsvtdm6mfb4l3g4kuzkoxxhaiba7cdw2lswz22dps4oieip2.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_194 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[65536, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_194', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 55112
    xnumel = 54
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 6889
    y1 = (yindex // 6889)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (1116018 + y0 + (6889*x2) + (1860030*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (54*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 1.814486863115111e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (54*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/f7/cf754js7tnzz7txgnlypk2joidw625gslnjxhbvxadqhznhmepvl.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_195 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_195', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 378
    rnumel = 7874
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 54)
    x0 = xindex % 54
    _tmp13 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (7874*x1)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (744012 + (6889*x0) + (1860030*(((r2 + (7874*x1)) // 6889) % 8)) + ((r2 + (7874*x1)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x0 + (54*((r2 + (7874*x1)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tmp4 <= tmp5
        tmp7 = tl.load(in_ptr2 + ((6889*x0) + (372006*(((r2 + (7874*x1)) // 6889) % 8)) + ((r2 + (7874*x1)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.where(tmp6, tmp5, tmp7)
        tmp9 = tmp3 + tmp8
        tmp10 = tl.full(tmp9.shape, 0, tmp9.dtype)
        tmp11 = tl.where(tmp2, tmp9, tmp10)
        tmp12 = tl.broadcast_to(tmp11, [XBLOCK, RBLOCK])
        tmp14 = _tmp13 + tmp12
        _tmp13 = tl.where(rmask & xmask, tmp14, _tmp13)
    tmp13 = tl.sum(_tmp13, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp13, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/i4/ci47lubmtcshq4qufg2fefoivckhuzg4l723pos3q7ehntkmnv7o.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_196 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_196', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23274
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 431
    x1 = (xindex // 431)
    _tmp17 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    _tmp26 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (744012 + (6889*x1) + (1860030*(((r2 + (128*x0)) // 6889) % 8)) + ((r2 + (128*x0)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (54*((r2 + (128*x0)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tmp4 <= tmp5
        tmp7 = tl.load(in_ptr2 + ((6889*x1) + (372006*(((r2 + (128*x0)) // 6889) % 8)) + ((r2 + (128*x0)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.where(tmp6, tmp5, tmp7)
        tmp9 = tmp3 + tmp8
        tmp10 = tl.load(in_ptr3 + (x1 + (54*((r2 + (128*x0)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp11 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp12 = tmp10 - tmp11
        tmp13 = tmp9 * tmp12
        tmp14 = tl.full(tmp13.shape, 0, tmp13.dtype)
        tmp15 = tl.where(tmp2, tmp13, tmp14)
        tmp16 = tl.broadcast_to(tmp15, [XBLOCK, RBLOCK])
        tmp18 = _tmp17 + tmp16
        _tmp17 = tl.where(rmask & xmask, tmp18, _tmp17)
        tmp19 = tl.load(in_ptr5 + (x1 + (54*((r2 + (128*x0)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp20 = tl.load(in_ptr6 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp21 = tmp19 - tmp20
        tmp22 = tmp9 * tmp21
        tmp23 = tl.full(tmp22.shape, 0, tmp22.dtype)
        tmp24 = tl.where(tmp2, tmp22, tmp23)
        tmp25 = tl.broadcast_to(tmp24, [XBLOCK, RBLOCK])
        tmp27 = _tmp26 + tmp25
        _tmp26 = tl.where(rmask & xmask, tmp27, _tmp26)
    tmp17 = tl.sum(_tmp17, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp17, xmask)
    tmp26 = tl.sum(_tmp26, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp26, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/kc/ckcl2o7ntkohl5dpixlmd2f4pvvcs4yoyaihy74npcwomeihqdkk.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_197 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[65536, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: '*fp32', 15: '*fp32', 16: 'i32', 17: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(16,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_197', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, in_ptr12, in_ptr13, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 55112
    xnumel = 54
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 6889
    y1 = (yindex // 6889)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (744012 + y0 + (6889*x2) + (1860030*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (54*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0 + (6889*x2) + (372006*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (x2 + (54*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr9 + (x2 + (54*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp27 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr12 + (x2), xmask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr13 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tmp1 <= tmp2
    tmp5 = tl.where(tmp3, tmp2, tmp4)
    tmp6 = tmp0 + tmp5
    tmp9 = tmp7 - tmp8
    tmp11 = 1.814486863115111e-05
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp6 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tmp26 = tmp24 - tmp25
    tmp28 = tmp27 * tmp11
    tmp30 = tmp29 * tmp29
    tmp31 = tmp28 * tmp30
    tmp32 = tmp26 * tmp31
    tmp33 = tmp6 - tmp32
    tmp34 = tmp33 - tmp19
    tmp36 = tmp29 * tmp35
    tmp37 = tmp34 * tmp36
    tl.store(out_ptr0 + (x2 + (54*y3)), tmp23, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (54*y3)), tmp37, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/m3/cm3usomfi33ygbvsrmzlidb4p5a627cgvyjvqkrn6267n2o2vlqp.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.threshold_backward]

triton_poi_fused_add_constant_pad_nd_threshold_backward_198 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 32768], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*i1', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_constant_pad_nd_threshold_backward_198', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 432
    xnumel = 27225
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x4 = xindex
    y0 = yindex % 54
    y1 = (yindex // 54)
    y5 = yindex
    x3 = (xindex // 165)
    x2 = xindex % 165
    tmp0 = tl.load(in_ptr0 + (y0 + (54*x4) + (1470150*y1)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_out_ptr0 + (x4 + (27225*y5)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp4 = 1 + x3
    tmp5 = tl.full([1, 1], 0, tl.int64)
    tmp6 = tmp4 >= tmp5
    tmp7 = tl.full([1, 1], 167, tl.int64)
    tmp8 = tmp4 < tmp7
    tmp9 = 1 + x2
    tmp10 = tmp9 >= tmp5
    tmp11 = tmp9 < tmp7
    tmp12 = tmp6 & tmp8
    tmp13 = tmp12 & tmp10
    tmp14 = tmp13 & tmp11
    tmp15 = tl.load(in_ptr1 + (9072 + y0 + (54*x2) + (9018*x3) + (1506006*y1)), tmp14 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp16 = tl.full(tmp15.shape, 0.0, tmp15.dtype)
    tmp17 = tl.where(tmp14, tmp15, tmp16)
    tmp18 = tmp3 + tmp17
    tmp19 = tl.load(in_ptr2 + (168 + x2 + (167*x3) + (27889*y5)), tmp14 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp20 = tl.full(tmp19.shape, 0.0, tmp19.dtype)
    tmp21 = tl.where(tmp14, tmp19, tmp20)
    tmp22 = tl.where(tmp0, tmp2, tmp21)
    tmp23 = tmp18 + tmp22
    tmp24 = 2 + x3
    tmp25 = tmp24 >= tmp5
    tmp26 = tl.full([1, 1], 169, tl.int64)
    tmp27 = tmp24 < tmp26
    tmp28 = 2 + x2
    tmp29 = tmp28 >= tmp5
    tmp30 = tmp28 < tmp26
    tmp31 = tmp25 & tmp27
    tmp32 = tmp31 & tmp29
    tmp33 = tmp32 & tmp30
    tmp34 = tl.load(in_ptr3 + (340 + x2 + (169*x3) + (28561*y5)), tmp33 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp35 = tl.full(tmp34.shape, 0.0, tmp34.dtype)
    tmp36 = tl.where(tmp33, tmp34, tmp35)
    tmp37 = tl.where(tmp0, tmp2, tmp36)
    tmp38 = tmp23 + tmp37
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x4 + (27225*y5)), tmp38, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/t3/ct32u2353slecvx73mofzdqsericadrlpxbf3xsyhf275aaeactn.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_199 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_199', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 378
    rnumel = 7874
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 54)
    x0 = xindex % 54
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (7874*x1)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (372006 + (6889*x0) + (1860030*(((r2 + (7874*x1)) // 6889) % 8)) + ((r2 + (7874*x1)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.full(tmp3.shape, 0, tmp3.dtype)
        tmp5 = tl.where(tmp2, tmp3, tmp4)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/d6/cd6oz27xe7sqv3jyqm4kxkyc2xx2iwb52wwsb2newqt7ihqps7pd.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_200 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_200', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 23274
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 431
    x1 = (xindex // 431)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (128*x0)
        tmp1 = tl.full([1, 1], 55112, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (372006 + (6889*x1) + (1860030*(((r2 + (128*x0)) // 6889) % 8)) + ((r2 + (128*x0)) % 6889)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (54*((r2 + (128*x0)) % 55112))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/vd/cvd5rugzhsrqiddbgiwgvpoomm5f7y6ojrsklmb6xkgrd2s75sdl.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_201 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[65536, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_201', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 55112
    xnumel = 54
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 6889
    y1 = (yindex // 6889)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (372006 + y0 + (6889*x2) + (1860030*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (54*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 1.814486863115111e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (54*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/xu/cxuol3fq4ywp4xhclx2v7jdct3gwndc4j77laupxazb7zaahtq5z.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_202 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 256],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_202', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 53460
    rnumel = 220
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 990
    x1 = (xindex // 990)
    _tmp34 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((27225*x1) + (1470150*((r2 + (220*x0)) // 27225)) + ((r2 + (220*x0)) % 27225)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp16 = tl.load(in_ptr2 + (x1 + (54*r2) + (11880*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = 1 + (((r2 + (220*x0)) // 165) % 165)
        tmp2 = tl.full([1, 1], 0, tl.int64)
        tmp3 = tmp1 >= tmp2
        tmp4 = tl.full([1, 1], 167, tl.int64)
        tmp5 = tmp1 < tmp4
        tmp6 = 1 + ((r2 + (220*x0)) % 165)
        tmp7 = tmp6 >= tmp2
        tmp8 = tmp6 < tmp4
        tmp9 = tmp3 & tmp5
        tmp10 = tmp9 & tmp7
        tmp11 = tmp10 & tmp8
        tmp12 = tl.load(in_ptr1 + (9072 + x1 + (54*((r2 + (220*x0)) % 165)) + (9018*(((r2 + (220*x0)) // 165) % 165)) + (1506006*((r2 + (220*x0)) // 27225))), rmask & tmp11 & xmask, eviction_policy='evict_last', other=0.0)
        tmp13 = tl.full(tmp12.shape, 0.0, tmp12.dtype)
        tmp14 = tl.where(tmp11, tmp12, tmp13)
        tmp15 = tmp0 + tmp14
        tmp17 = 3 + (((r2 + (220*x0)) // 165) % 165)
        tmp18 = tmp17 >= tmp2
        tmp19 = tl.full([1, 1], 171, tl.int64)
        tmp20 = tmp17 < tmp19
        tmp21 = 3 + ((r2 + (220*x0)) % 165)
        tmp22 = tmp21 >= tmp2
        tmp23 = tmp21 < tmp19
        tmp24 = tmp18 & tmp20
        tmp25 = tmp24 & tmp22
        tmp26 = tmp25 & tmp23
        tmp27 = tl.load(in_ptr3 + (516 + (171*(((r2 + (220*x0)) // 165) % 165)) + (29241*x1) + (1579014*((r2 + (220*x0)) // 27225)) + ((r2 + (220*x0)) % 165)), rmask & tmp26 & xmask, eviction_policy='evict_last', other=0.0)
        tmp28 = tl.full(tmp27.shape, 0.0, tmp27.dtype)
        tmp29 = tl.where(tmp26, tmp27, tmp28)
        tmp30 = 0.0
        tmp31 = tl.where(tmp16, tmp30, tmp29)
        tmp32 = tmp15 + tmp31
        tmp33 = tl.broadcast_to(tmp32, [XBLOCK, RBLOCK])
        tmp35 = _tmp34 + tmp33
        _tmp34 = tl.where(rmask & xmask, tmp35, _tmp34)
    tmp34 = tl.sum(_tmp34, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp34, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/go/cgohrzqclamcqz4vm4n42ncxxsi3m7wdwdkmblkm5xss7wlspv3e.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_203 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[64, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_203', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel):
    xnumel = 54
    XBLOCK: tl.constexpr = 1
    rnumel = 990
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (990*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/jj/cjjsno4clfbrtlndwuhshycy66v3aorlribx274rwmmzacjwskqx.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_204 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 256],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_204', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 53460
    rnumel = 220
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 54
    x1 = (xindex // 54)
    tmp34 = tl.load(in_ptr5 + (x0), xmask, eviction_policy='evict_last')
    _tmp38 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((27225*x0) + (1470150*((r2 + (220*x1)) // 27225)) + ((r2 + (220*x1)) % 27225)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp16 = tl.load(in_ptr2 + (x0 + (54*r2) + (11880*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp33 = tl.load(in_ptr4 + (x0 + (54*r2) + (11880*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = 1 + (((r2 + (220*x1)) // 165) % 165)
        tmp2 = tl.full([1, 1], 0, tl.int64)
        tmp3 = tmp1 >= tmp2
        tmp4 = tl.full([1, 1], 167, tl.int64)
        tmp5 = tmp1 < tmp4
        tmp6 = 1 + ((r2 + (220*x1)) % 165)
        tmp7 = tmp6 >= tmp2
        tmp8 = tmp6 < tmp4
        tmp9 = tmp3 & tmp5
        tmp10 = tmp9 & tmp7
        tmp11 = tmp10 & tmp8
        tmp12 = tl.load(in_ptr1 + (9072 + x0 + (54*((r2 + (220*x1)) % 165)) + (9018*(((r2 + (220*x1)) // 165) % 165)) + (1506006*((r2 + (220*x1)) // 27225))), rmask & tmp11 & xmask, eviction_policy='evict_last', other=0.0)
        tmp13 = tl.full(tmp12.shape, 0.0, tmp12.dtype)
        tmp14 = tl.where(tmp11, tmp12, tmp13)
        tmp15 = tmp0 + tmp14
        tmp17 = 3 + (((r2 + (220*x1)) // 165) % 165)
        tmp18 = tmp17 >= tmp2
        tmp19 = tl.full([1, 1], 171, tl.int64)
        tmp20 = tmp17 < tmp19
        tmp21 = 3 + ((r2 + (220*x1)) % 165)
        tmp22 = tmp21 >= tmp2
        tmp23 = tmp21 < tmp19
        tmp24 = tmp18 & tmp20
        tmp25 = tmp24 & tmp22
        tmp26 = tmp25 & tmp23
        tmp27 = tl.load(in_ptr3 + (516 + (171*(((r2 + (220*x1)) // 165) % 165)) + (29241*x0) + (1579014*((r2 + (220*x1)) // 27225)) + ((r2 + (220*x1)) % 165)), rmask & tmp26 & xmask, eviction_policy='evict_last', other=0.0)
        tmp28 = tl.full(tmp27.shape, 0.0, tmp27.dtype)
        tmp29 = tl.where(tmp26, tmp27, tmp28)
        tmp30 = 0.0
        tmp31 = tl.where(tmp16, tmp30, tmp29)
        tmp32 = tmp15 + tmp31
        tmp35 = tmp33 - tmp34
        tmp36 = tmp32 * tmp35
        tmp37 = tl.broadcast_to(tmp36, [XBLOCK, RBLOCK])
        tmp39 = _tmp38 + tmp37
        _tmp38 = tl.where(rmask & xmask, tmp39, _tmp38)
    tmp38 = tl.sum(_tmp38, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp38, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/j7/cj7c2f6msakaalcc2zsseoifdwh5iwm4ilumqnlvcuodaj3usrdy.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_205 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 1024],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_205', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 54
    rnumel = 990
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (54*r1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/2s/c2sllsplbqvqxpzvrahqcmeqcycap3vcxesxsqzygsexrvja23il.py
# Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_206 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[262144, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_206', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 217800
    xnumel = 54
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x3 = xindex
    y2 = (yindex // 27225)
    y4 = yindex % 27225
    y1 = (yindex // 165) % 165
    y0 = yindex % 165
    y6 = yindex
    tmp0 = tl.load(in_ptr0 + (y4 + (27225*x3) + (1470150*y2)), xmask & ymask, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr2 + (x3 + (54*y6)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp33 = tl.load(in_ptr4 + (x3 + (54*y6)), xmask & ymask, eviction_policy='evict_last')
    tmp34 = tl.load(in_ptr5 + (x3), xmask, eviction_policy='evict_last')
    tmp36 = tl.load(in_ptr6 + (x3), xmask, eviction_policy='evict_last')
    tmp39 = tl.load(in_ptr7 + (x3), xmask, eviction_policy='evict_last')
    tmp1 = 1 + y1
    tmp2 = tl.full([1, 1], 0, tl.int64)
    tmp3 = tmp1 >= tmp2
    tmp4 = tl.full([1, 1], 167, tl.int64)
    tmp5 = tmp1 < tmp4
    tmp6 = 1 + y0
    tmp7 = tmp6 >= tmp2
    tmp8 = tmp6 < tmp4
    tmp9 = tmp3 & tmp5
    tmp10 = tmp9 & tmp7
    tmp11 = tmp10 & tmp8
    tmp12 = tl.load(in_ptr1 + (9072 + x3 + (54*y0) + (9018*y1) + (1506006*y2)), tmp11 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp13 = tl.full(tmp12.shape, 0.0, tmp12.dtype)
    tmp14 = tl.where(tmp11, tmp12, tmp13)
    tmp15 = tmp0 + tmp14
    tmp17 = 3 + y1
    tmp18 = tmp17 >= tmp2
    tmp19 = tl.full([1, 1], 171, tl.int64)
    tmp20 = tmp17 < tmp19
    tmp21 = 3 + y0
    tmp22 = tmp21 >= tmp2
    tmp23 = tmp21 < tmp19
    tmp24 = tmp18 & tmp20
    tmp25 = tmp24 & tmp22
    tmp26 = tmp25 & tmp23
    tmp27 = tl.load(in_ptr3 + (516 + y0 + (171*y1) + (29241*x3) + (1579014*y2)), tmp26 & xmask & ymask, eviction_policy='evict_last', other=0.0)
    tmp28 = tl.full(tmp27.shape, 0.0, tmp27.dtype)
    tmp29 = tl.where(tmp26, tmp27, tmp28)
    tmp30 = 0.0
    tmp31 = tl.where(tmp16, tmp30, tmp29)
    tmp32 = tmp15 + tmp31
    tmp35 = tmp33 - tmp34
    tmp37 = 4.591368227731864e-06
    tmp38 = tmp36 * tmp37
    tmp40 = tmp39 * tmp39
    tmp41 = tmp38 * tmp40
    tmp42 = tmp35 * tmp41
    tmp43 = tmp32 - tmp42
    tl.store(out_ptr0 + (x3 + (54*y6)), tmp43, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/3u/c3u36yx7saqayhqmrz5j67v5jzlqeyv4ynrt7iw2evy4kplttaq3.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_207 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 32768], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_207', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 432
    xnumel = 27225
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 54
    y1 = (yindex // 54)
    tmp0 = tl.load(in_out_ptr0 + (y0 + (54*x2) + (1470150*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr0 + (y0), ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr1 + (y0), ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (y0), ymask, eviction_policy='evict_last')
    tmp2 = 4.591368227731864e-06
    tmp3 = tmp1 * tmp2
    tmp4 = tmp0 - tmp3
    tmp7 = tmp5 * tmp6
    tmp8 = tmp4 * tmp7
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (y0 + (54*x2) + (1470150*y1)), tmp8, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rj/crjflgvm5ejlvm6hrpna5ruptfcqx5ablzw6wiufibsdupwry2fk.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_208 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 65536],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_208', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 576
    rnumel = 36300
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 96
    x1 = (xindex // 96)
    _tmp8 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((165*(((r2 + (36300*x1)) // 165) % 165)) + (27225*x0) + (2613600*((r2 + (36300*x1)) // 27225)) + (r2 % 165)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x0 + (96*r2) + (3484800*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + ((165*(((r2 + (36300*x1)) // 165) % 165)) + (27225*x0) + (2613600*((r2 + (36300*x1)) // 27225)) + (r2 % 165)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tmp1 <= tmp2
        tmp5 = tl.where(tmp3, tmp2, tmp4)
        tmp6 = tmp0 + tmp5
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(rmask & xmask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp8, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/kl/cklxxl43gjvg44csfn73tojnpfaxtbzaj7jcprjcprdbwqa4h5pb.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_209 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 8],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_209', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 96
    rnumel = 6
    RBLOCK: tl.constexpr = 8
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (96*r1)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7n/c7ncb4oefoysnvb466dmzwddazhvvqfvohzejihhimnsk4zafsbw.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_210 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[131072, 256],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_210', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 95040
    rnumel = 220
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 990
    x1 = (xindex // 990)
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((27225*x1) + (2613600*((r2 + (220*x0)) // 27225)) + ((r2 + (220*x0)) % 27225)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x1 + (96*r2) + (21120*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + ((27225*x1) + (2613600*((r2 + (220*x0)) // 27225)) + ((r2 + (220*x0)) % 27225)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.load(in_ptr3 + (x1 + (96*r2) + (21120*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tmp1 <= tmp2
        tmp5 = tl.where(tmp3, tmp2, tmp4)
        tmp6 = tmp0 + tmp5
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp12, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/af/cafpeqx5x6p76zpzo6qmcaclhkzecvuxfbxltdwripnh2hvxe7an.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_211 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 1024],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_211', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel):
    xnumel = 96
    XBLOCK: tl.constexpr = 1
    rnumel = 990
    RBLOCK: tl.constexpr = 1024
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (990*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = triton_helpers.promote_to_tensor(tl.sum(tmp3, 0))
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7j/c7j4wl3vbll54ocijkh75oxs2dz7vj2yyghrto46c5fq5aqxkhbz.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_212 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[262144, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_212', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 217800
    xnumel = 96
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 27225
    y1 = (yindex // 27225)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (27225*x2) + (2613600*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (96*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0 + (27225*x2) + (2613600*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (x2 + (96*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tmp1 <= tmp2
    tmp5 = tl.where(tmp3, tmp2, tmp4)
    tmp6 = tmp0 + tmp5
    tmp9 = tmp7 - tmp8
    tmp11 = 4.591368227731864e-06
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp6 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tl.store(out_ptr0 + (x2 + (96*y3)), tmp23, xmask & ymask)
''')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_31, primals_32, primals_34, primals_35, primals_36, primals_38, primals_39, primals_41, primals_42, primals_44, primals_45, primals_46, primals_48, primals_49, primals_51, primals_52, primals_53, primals_55, primals_56, primals_58, primals_59, primals_60, primals_62, primals_63, primals_64, primals_66, primals_67, primals_68, primals_70, primals_71, primals_73, primals_74, primals_75, primals_77, primals_79, primals_80, primals_81, primals_83, primals_84, primals_86, primals_87, primals_89, primals_90, primals_91, primals_93, primals_94, primals_96, primals_97, primals_98, primals_100, primals_101, primals_103, primals_104, primals_105, primals_107, primals_108, primals_110, primals_111, primals_112, primals_114, primals_115, primals_116, primals_118, primals_119, primals_120, primals_122, primals_123, primals_125, primals_126, primals_127, primals_129, primals_131, primals_132, primals_133, primals_135, primals_136, primals_138, primals_139, primals_140, primals_142, primals_143, primals_144, primals_146, primals_147, primals_148, primals_150, primals_151, primals_152, primals_154, primals_155, primals_156, primals_158, primals_159, primals_160, primals_162, primals_163, primals_164, primals_166, primals_167, primals_168, primals_170, primals_171, primals_172, primals_174, primals_175, primals_176, primals_178, primals_179, primals_180, primals_182, primals_183, primals_184, primals_186, primals_187, primals_189, primals_190, primals_192, primals_193, primals_194, primals_196, primals_197, primals_198, primals_200, primals_201, primals_202, primals_204, primals_205, primals_206, primals_208, primals_209, primals_210, primals_212, primals_213, primals_214, primals_216, primals_217, primals_218, primals_220, primals_221, primals_222, primals_224, primals_225, primals_226, primals_228, primals_229, primals_230, primals_232, primals_233, primals_234, primals_236, primals_237, primals_238, primals_240, primals_241, primals_243, primals_244, primals_246, primals_247, primals_248, primals_250, primals_251, primals_252, primals_254, primals_255, primals_256, primals_258, primals_259, primals_260, primals_262, primals_263, primals_264, primals_266, primals_267, primals_268, primals_270, primals_271, primals_272, primals_274, primals_275, primals_276, primals_278, primals_279, primals_280, primals_282, primals_283, primals_284, primals_286, primals_287, primals_288, primals_290, primals_291, primals_292, primals_294, primals_295, primals_297, primals_298, primals_300, primals_301, primals_302, primals_304, primals_305, primals_306, primals_308, primals_309, primals_310, primals_312, primals_313, primals_314, primals_316, primals_317, primals_318, primals_320, primals_321, primals_322, primals_324, primals_325, primals_326, primals_328, primals_329, primals_330, primals_332, primals_333, primals_334, primals_336, primals_337, primals_338, primals_340, primals_341, primals_342, primals_344, primals_345, primals_346, primals_348, primals_349, primals_351, primals_352, primals_354, primals_355, primals_357, primals_358, primals_359, primals_361, primals_362, primals_364, primals_365, primals_366, primals_368, primals_369, primals_371, primals_372, primals_373, primals_375, primals_376, primals_378, primals_379, primals_380, primals_382, primals_383, primals_384, primals_386, primals_387, primals_388, primals_390, primals_391, primals_393, primals_394, primals_395, primals_397, primals_399, primals_400, primals_401, primals_403, primals_404, primals_406, primals_407, primals_408, primals_410, primals_411, primals_412, primals_414, primals_415, primals_416, primals_418, primals_419, primals_420, primals_422, primals_423, primals_424, primals_426, primals_427, primals_428, primals_430, primals_431, primals_432, primals_434, primals_435, primals_436, primals_438, primals_439, primals_440, primals_442, primals_443, primals_444, primals_446, primals_447, primals_448, primals_450, primals_451, primals_452, primals_454, primals_455, primals_457, primals_458, primals_460, primals_461, primals_462, primals_464, primals_465, primals_466, primals_468, primals_469, primals_470, primals_472, primals_473, primals_474, primals_476, primals_477, primals_478, primals_480, primals_481, primals_482, primals_484, primals_485, primals_486, primals_488, primals_489, primals_490, primals_492, primals_493, primals_494, primals_496, primals_497, primals_498, primals_500, primals_501, primals_502, primals_504, primals_505, primals_506, primals_508, primals_509, primals_511, primals_512, primals_514, primals_515, primals_516, primals_518, primals_519, primals_520, primals_522, primals_523, primals_524, primals_526, primals_527, primals_528, primals_530, primals_531, primals_532, primals_534, primals_535, primals_536, primals_538, primals_539, primals_540, primals_542, primals_543, primals_544, primals_546, primals_547, primals_548, primals_550, primals_551, primals_552, primals_554, primals_555, primals_556, primals_558, primals_559, primals_560, primals_562, primals_563, primals_565, primals_566, primals_568, primals_569, primals_571, primals_572, primals_573, primals_575, primals_576, primals_578, primals_579, primals_580, primals_582, primals_583, primals_585, primals_586, primals_587, primals_589, primals_590, primals_592, primals_593, primals_594, primals_596, primals_597, primals_598, primals_600, primals_601, primals_602, primals_604, primals_605, primals_607, primals_608, primals_609, primals_611, primals_613, primals_614, primals_615, primals_617, primals_618, primals_620, primals_621, primals_622, primals_624, primals_625, primals_626, primals_628, primals_629, primals_630, primals_632, primals_633, primals_634, primals_636, primals_637, primals_638, primals_640, primals_641, primals_642, primals_644, primals_645, primals_646, primals_648, primals_649, primals_650, primals_652, primals_653, primals_654, primals_656, primals_657, primals_658, primals_660, primals_661, primals_662, primals_664, primals_665, primals_666, primals_668, primals_669, primals_671, primals_672, primals_674, primals_675, primals_676, primals_678, primals_679, primals_680, primals_682, primals_683, primals_684, primals_686, primals_687, primals_688, primals_690, primals_691, primals_692, primals_694, primals_695, primals_696, primals_698, primals_699, primals_700, primals_702, primals_703, primals_704, primals_706, primals_707, primals_708, primals_710, primals_711, primals_712, primals_714, primals_715, primals_716, primals_718, primals_719, primals_720, primals_722, primals_723, primals_725, primals_726, primals_728, primals_729, primals_730, primals_732, primals_733, primals_734, primals_736, primals_737, primals_738, primals_740, primals_741, primals_742, primals_744, primals_745, primals_746, primals_748, primals_749, primals_750, primals_752, primals_753, primals_754, primals_756, primals_757, primals_758, primals_760, primals_761, primals_762, primals_764, primals_765, primals_766, primals_768, primals_769, primals_770, primals_772, primals_773, primals_774, primals_1381, convolution, squeeze_1, relu, convolution_1, squeeze_4, constant_pad_nd, convolution_2, convolution_3, squeeze_7, relu_2, convolution_4, convolution_5, squeeze_10, constant_pad_nd_1, getitem_8, getitem_9, convolution_6, squeeze_13, constant_pad_nd_2, convolution_7, convolution_8, squeeze_16, relu_4, convolution_9, convolution_10, squeeze_19, constant_pad_nd_3, getitem_17, constant_pad_nd_4, convolution_11, convolution_12, squeeze_22, relu_6, convolution_13, convolution_14, squeeze_25, constant_pad_nd_5, convolution_15, convolution_16, squeeze_28, relu_8, convolution_17, convolution_18, squeeze_31, relu_9, convolution_19, convolution_20, squeeze_34, relu_10, convolution_21, convolution_22, squeeze_37, constant_pad_nd_7, convolution_23, convolution_24, squeeze_40, relu_12, convolution_25, convolution_26, squeeze_43, constant_pad_nd_8, convolution_27, squeeze_46, avg_pool2d, constant_pad_nd_9, avg_pool2d_1, cat_1, squeeze_49, relu_15, convolution_30, squeeze_52, constant_pad_nd_10, convolution_31, convolution_32, squeeze_55, relu_17, convolution_33, convolution_34, squeeze_58, constant_pad_nd_11, getitem_47, constant_pad_nd_12, convolution_35, convolution_36, squeeze_61, relu_19, convolution_37, convolution_38, squeeze_64, constant_pad_nd_13, getitem_53, constant_pad_nd_14, convolution_39, convolution_40, squeeze_67, relu_21, convolution_41, convolution_42, squeeze_70, constant_pad_nd_15, convolution_43, convolution_44, squeeze_73, relu_23, convolution_45, convolution_46, squeeze_76, relu_24, convolution_47, convolution_48, squeeze_79, relu_25, convolution_49, convolution_50, squeeze_82, constant_pad_nd_17, convolution_51, convolution_52, squeeze_85, relu_27, convolution_53, convolution_54, squeeze_88, constant_pad_nd_18, convolution_55, squeeze_91, avg_pool2d_2, constant_pad_nd_19, avg_pool2d_3, cat_3, squeeze_94, add_169, relu_30, convolution_58, squeeze_97, add_174, relu_31, convolution_59, convolution_60, squeeze_100, relu_32, convolution_61, convolution_62, squeeze_103, getitem_83, relu_33, convolution_63, convolution_64, squeeze_106, relu_34, convolution_65, convolution_66, squeeze_109, getitem_89, convolution_67, convolution_68, squeeze_112, relu_36, convolution_69, convolution_70, squeeze_115, convolution_71, convolution_72, squeeze_118, relu_38, convolution_73, convolution_74, squeeze_121, relu_39, convolution_75, convolution_76, squeeze_124, relu_40, convolution_77, convolution_78, squeeze_127, convolution_79, convolution_80, squeeze_130, relu_42, convolution_81, convolution_82, squeeze_133, convolution_83, squeeze_136, add_244, relu_44, convolution_84, squeeze_139, add_249, relu_45, convolution_85, convolution_86, squeeze_142, relu_46, convolution_87, convolution_88, squeeze_145, getitem_117, relu_47, convolution_89, convolution_90, squeeze_148, relu_48, convolution_91, convolution_92, squeeze_151, getitem_123, convolution_93, convolution_94, squeeze_154, relu_50, convolution_95, convolution_96, squeeze_157, convolution_97, convolution_98, squeeze_160, relu_52, convolution_99, convolution_100, squeeze_163, relu_53, convolution_101, convolution_102, squeeze_166, relu_54, convolution_103, convolution_104, squeeze_169, convolution_105, convolution_106, squeeze_172, relu_56, convolution_107, convolution_108, squeeze_175, convolution_109, squeeze_178, add_319, relu_58, convolution_110, squeeze_181, add_324, relu_59, convolution_111, convolution_112, squeeze_184, relu_60, convolution_113, convolution_114, squeeze_187, getitem_151, relu_61, convolution_115, convolution_116, squeeze_190, relu_62, convolution_117, convolution_118, squeeze_193, getitem_157, convolution_119, convolution_120, squeeze_196, relu_64, convolution_121, convolution_122, squeeze_199, convolution_123, convolution_124, squeeze_202, relu_66, convolution_125, convolution_126, squeeze_205, relu_67, convolution_127, convolution_128, squeeze_208, relu_68, convolution_129, convolution_130, squeeze_211, convolution_131, convolution_132, squeeze_214, relu_70, convolution_133, convolution_134, squeeze_217, convolution_135, squeeze_220, add_394, relu_72, convolution_136, squeeze_223, add_399, relu_73, convolution_137, convolution_138, squeeze_226, relu_74, convolution_139, convolution_140, squeeze_229, getitem_185, relu_75, convolution_141, convolution_142, squeeze_232, relu_76, convolution_143, convolution_144, squeeze_235, getitem_191, convolution_145, convolution_146, squeeze_238, relu_78, convolution_147, convolution_148, squeeze_241, convolution_149, convolution_150, squeeze_244, relu_80, convolution_151, convolution_152, squeeze_247, relu_81, convolution_153, convolution_154, squeeze_250, relu_82, convolution_155, convolution_156, squeeze_253, convolution_157, convolution_158, squeeze_256, relu_84, convolution_159, convolution_160, squeeze_259, convolution_161, squeeze_262, relu_86, convolution_162, squeeze_265, constant_pad_nd_20, convolution_163, convolution_164, squeeze_268, relu_88, convolution_165, convolution_166, squeeze_271, constant_pad_nd_21, getitem_219, constant_pad_nd_22, convolution_167, convolution_168, squeeze_274, relu_90, convolution_169, convolution_170, squeeze_277, constant_pad_nd_23, getitem_225, constant_pad_nd_24, convolution_171, convolution_172, squeeze_280, relu_92, convolution_173, convolution_174, squeeze_283, constant_pad_nd_25, convolution_175, convolution_176, squeeze_286, relu_94, convolution_177, convolution_178, squeeze_289, relu_95, convolution_179, convolution_180, squeeze_292, relu_96, convolution_181, convolution_182, squeeze_295, constant_pad_nd_27, convolution_183, convolution_184, squeeze_298, relu_98, convolution_185, convolution_186, squeeze_301, constant_pad_nd_28, convolution_187, squeeze_304, avg_pool2d_4, constant_pad_nd_29, avg_pool2d_5, cat_9, squeeze_307, add_549, relu_101, convolution_190, squeeze_310, add_554, relu_102, convolution_191, convolution_192, squeeze_313, relu_103, convolution_193, convolution_194, squeeze_316, getitem_255, relu_104, convolution_195, convolution_196, squeeze_319, relu_105, convolution_197, convolution_198, squeeze_322, getitem_261, convolution_199, convolution_200, squeeze_325, relu_107, convolution_201, convolution_202, squeeze_328, convolution_203, convolution_204, squeeze_331, relu_109, convolution_205, convolution_206, squeeze_334, relu_110, convolution_207, convolution_208, squeeze_337, relu_111, convolution_209, convolution_210, squeeze_340, convolution_211, convolution_212, squeeze_343, relu_113, convolution_213, convolution_214, squeeze_346, convolution_215, squeeze_349, add_624, relu_115, convolution_216, squeeze_352, add_629, relu_116, convolution_217, convolution_218, squeeze_355, relu_117, convolution_219, convolution_220, squeeze_358, getitem_289, relu_118, convolution_221, convolution_222, squeeze_361, relu_119, convolution_223, convolution_224, squeeze_364, getitem_295, convolution_225, convolution_226, squeeze_367, relu_121, convolution_227, convolution_228, squeeze_370, convolution_229, convolution_230, squeeze_373, relu_123, convolution_231, convolution_232, squeeze_376, relu_124, convolution_233, convolution_234, squeeze_379, relu_125, convolution_235, convolution_236, squeeze_382, convolution_237, convolution_238, squeeze_385, relu_127, convolution_239, convolution_240, squeeze_388, convolution_241, squeeze_391, add_699, relu_129, convolution_242, squeeze_394, add_704, relu_130, convolution_243, convolution_244, squeeze_397, relu_131, convolution_245, convolution_246, squeeze_400, getitem_323, relu_132, convolution_247, convolution_248, squeeze_403, relu_133, convolution_249, convolution_250, squeeze_406, getitem_329, convolution_251, convolution_252, squeeze_409, relu_135, convolution_253, convolution_254, squeeze_412, convolution_255, convolution_256, squeeze_415, relu_137, convolution_257, convolution_258, squeeze_418, relu_138, convolution_259, convolution_260, squeeze_421, relu_139, convolution_261, convolution_262, squeeze_424, convolution_263, convolution_264, squeeze_427, relu_141, convolution_265, convolution_266, squeeze_430, convolution_267, squeeze_433, relu_143, convolution_268, squeeze_436, constant_pad_nd_30, convolution_269, convolution_270, squeeze_439, relu_145, convolution_271, convolution_272, squeeze_442, constant_pad_nd_31, getitem_357, constant_pad_nd_32, convolution_273, convolution_274, squeeze_445, relu_147, convolution_275, convolution_276, squeeze_448, constant_pad_nd_33, getitem_363, constant_pad_nd_34, convolution_277, convolution_278, squeeze_451, relu_149, convolution_279, convolution_280, squeeze_454, constant_pad_nd_35, convolution_281, convolution_282, squeeze_457, relu_151, convolution_283, convolution_284, squeeze_460, relu_152, convolution_285, convolution_286, squeeze_463, relu_153, convolution_287, convolution_288, squeeze_466, constant_pad_nd_37, convolution_289, convolution_290, squeeze_469, relu_155, convolution_291, convolution_292, squeeze_472, constant_pad_nd_38, convolution_293, squeeze_475, avg_pool2d_6, constant_pad_nd_39, avg_pool2d_7, cat_14, squeeze_478, add_854, relu_158, convolution_296, squeeze_481, add_859, relu_159, convolution_297, convolution_298, squeeze_484, relu_160, convolution_299, convolution_300, squeeze_487, getitem_393, relu_161, convolution_301, convolution_302, squeeze_490, relu_162, convolution_303, convolution_304, squeeze_493, getitem_399, convolution_305, convolution_306, squeeze_496, relu_164, convolution_307, convolution_308, squeeze_499, convolution_309, convolution_310, squeeze_502, relu_166, convolution_311, convolution_312, squeeze_505, relu_167, convolution_313, convolution_314, squeeze_508, relu_168, convolution_315, convolution_316, squeeze_511, convolution_317, convolution_318, squeeze_514, relu_170, convolution_319, convolution_320, squeeze_517, convolution_321, squeeze_520, add_929, relu_172, convolution_322, squeeze_523, add_934, relu_173, convolution_323, convolution_324, squeeze_526, relu_174, convolution_325, convolution_326, squeeze_529, getitem_427, relu_175, convolution_327, convolution_328, squeeze_532, relu_176, convolution_329, convolution_330, squeeze_535, getitem_433, convolution_331, convolution_332, squeeze_538, relu_178, convolution_333, convolution_334, squeeze_541, convolution_335, convolution_336, squeeze_544, relu_180, convolution_337, convolution_338, squeeze_547, relu_181, convolution_339, convolution_340, squeeze_550, relu_182, convolution_341, convolution_342, squeeze_553, convolution_343, convolution_344, squeeze_556, relu_184, convolution_345, convolution_346, squeeze_559, convolution_347, squeeze_562, add_1004, relu_186, convolution_348, squeeze_565, add_1009, relu_187, convolution_349, convolution_350, squeeze_568, relu_188, convolution_351, convolution_352, squeeze_571, getitem_461, relu_189, convolution_353, convolution_354, squeeze_574, relu_190, convolution_355, convolution_356, squeeze_577, getitem_467, convolution_357, convolution_358, squeeze_580, relu_192, convolution_359, convolution_360, squeeze_583, convolution_361, convolution_362, squeeze_586, relu_194, convolution_363, convolution_364, squeeze_589, relu_195, convolution_365, convolution_366, squeeze_592, relu_196, convolution_367, convolution_368, squeeze_595, convolution_369, convolution_370, squeeze_598, relu_198, convolution_371, convolution_372, squeeze_601, clone, permute_1, le, unsqueeze_806, unsqueeze_818, unsqueeze_830, unsqueeze_842, unsqueeze_854, unsqueeze_866, unsqueeze_878, unsqueeze_890, unsqueeze_902, unsqueeze_914, unsqueeze_926, unsqueeze_938, unsqueeze_950, unsqueeze_962, unsqueeze_974, unsqueeze_986, unsqueeze_998, unsqueeze_1010, unsqueeze_1022, unsqueeze_1034, unsqueeze_1046, unsqueeze_1058, unsqueeze_1070, unsqueeze_1082, unsqueeze_1094, unsqueeze_1106, unsqueeze_1118, unsqueeze_1130, unsqueeze_1142, unsqueeze_1154, unsqueeze_1166, unsqueeze_1178, unsqueeze_1190, unsqueeze_1202, unsqueeze_1214, unsqueeze_1226, unsqueeze_1238, unsqueeze_1250, unsqueeze_1262, unsqueeze_1274, unsqueeze_1286, unsqueeze_1298, unsqueeze_1310, le_43, unsqueeze_1322, unsqueeze_1334, le_45, unsqueeze_1346, unsqueeze_1358, unsqueeze_1370, unsqueeze_1382, unsqueeze_1394, unsqueeze_1406, unsqueeze_1418, unsqueeze_1430, unsqueeze_1442, unsqueeze_1454, unsqueeze_1466, unsqueeze_1478, unsqueeze_1490, unsqueeze_1502, unsqueeze_1514, unsqueeze_1526, unsqueeze_1538, unsqueeze_1550, unsqueeze_1562, unsqueeze_1574, unsqueeze_1586, unsqueeze_1598, unsqueeze_1610, unsqueeze_1622, unsqueeze_1634, unsqueeze_1646, unsqueeze_1658, unsqueeze_1670, unsqueeze_1682, unsqueeze_1694, unsqueeze_1706, unsqueeze_1718, unsqueeze_1730, unsqueeze_1742, unsqueeze_1754, unsqueeze_1766, unsqueeze_1778, unsqueeze_1790, unsqueeze_1802, unsqueeze_1814, unsqueeze_1826, unsqueeze_1838, unsqueeze_1850, unsqueeze_1862, unsqueeze_1874, unsqueeze_1886, unsqueeze_1898, unsqueeze_1910, unsqueeze_1922, unsqueeze_1934, unsqueeze_1946, unsqueeze_1958, unsqueeze_1970, unsqueeze_1982, unsqueeze_1994, le_100, unsqueeze_2006, unsqueeze_2018, le_102, unsqueeze_2030, unsqueeze_2042, unsqueeze_2054, unsqueeze_2066, unsqueeze_2078, unsqueeze_2090, unsqueeze_2102, unsqueeze_2114, unsqueeze_2126, unsqueeze_2138, unsqueeze_2150, unsqueeze_2162, unsqueeze_2174, unsqueeze_2186, unsqueeze_2198, unsqueeze_2210, unsqueeze_2222, unsqueeze_2234, unsqueeze_2246, unsqueeze_2258, unsqueeze_2270, unsqueeze_2282, unsqueeze_2294, unsqueeze_2306, unsqueeze_2318, unsqueeze_2330, unsqueeze_2342, unsqueeze_2354, unsqueeze_2366, unsqueeze_2378, unsqueeze_2390, unsqueeze_2402, unsqueeze_2414, unsqueeze_2426, unsqueeze_2438, unsqueeze_2450, unsqueeze_2462, unsqueeze_2474, unsqueeze_2486, unsqueeze_2498, unsqueeze_2510, unsqueeze_2522, unsqueeze_2534, unsqueeze_2546, unsqueeze_2558, unsqueeze_2570, unsqueeze_2582, unsqueeze_2594, unsqueeze_2606, unsqueeze_2618, unsqueeze_2630, unsqueeze_2642, unsqueeze_2654, unsqueeze_2666, unsqueeze_2678, unsqueeze_2690, unsqueeze_2702, unsqueeze_2714, unsqueeze_2726, unsqueeze_2738, unsqueeze_2750, unsqueeze_2762, unsqueeze_2774, unsqueeze_2786, unsqueeze_2798, unsqueeze_2810, unsqueeze_2822, unsqueeze_2834, unsqueeze_2846, le_171, unsqueeze_2858, unsqueeze_2870, le_173, unsqueeze_2882, unsqueeze_2894, unsqueeze_2906, unsqueeze_2918, unsqueeze_2930, unsqueeze_2942, unsqueeze_2954, unsqueeze_2966, unsqueeze_2978, unsqueeze_2990, unsqueeze_3002, unsqueeze_3014, unsqueeze_3026, le_186, unsqueeze_3038, unsqueeze_3050, unsqueeze_3062, unsqueeze_3074, unsqueeze_3086, unsqueeze_3098, unsqueeze_3110, unsqueeze_3122, unsqueeze_3134, unsqueeze_3146, unsqueeze_3158, unsqueeze_3170, unsqueeze_3182, unsqueeze_3194, unsqueeze_3206, tangents_1 = args
    args.clear()
    assert_size_stride(primals_1, (96, ), (1, ))
    assert_size_stride(primals_3, (96, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_4, (54, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_5, (54, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_6, (54, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_7, (96, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_8, (54, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(primals_9, (108, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_10, (108, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_11, (108, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_12, (108, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_13, (108, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_14, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_15, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_16, (432, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_17, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_18, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_19, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_20, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_21, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_22, (864, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_23, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_24, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_25, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_26, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_27, (96, 3, 3, 3), (27, 1, 9, 3))
    assert_size_stride(primals_28, (54, 96, 1, 1), (96, 1, 1, 1))
    assert_size_stride(primals_29, (54, ), (1, ))
    assert_size_stride(primals_31, (54, 96, 1, 1), (96, 1, 1, 1))
    assert_size_stride(primals_32, (54, ), (1, ))
    assert_size_stride(primals_34, (54, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_35, (54, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(primals_36, (54, ), (1, ))
    assert_size_stride(primals_38, (54, 96, 1, 1), (96, 1, 1, 1))
    assert_size_stride(primals_39, (54, ), (1, ))
    assert_size_stride(primals_41, (54, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(primals_42, (54, ), (1, ))
    assert_size_stride(primals_44, (54, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_45, (54, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(primals_46, (54, ), (1, ))
    assert_size_stride(primals_48, (54, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(primals_49, (54, ), (1, ))
    assert_size_stride(primals_51, (54, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_52, (54, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(primals_53, (54, ), (1, ))
    assert_size_stride(primals_55, (54, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(primals_56, (54, ), (1, ))
    assert_size_stride(primals_58, (54, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_59, (54, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(primals_60, (54, ), (1, ))
    assert_size_stride(primals_62, (54, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_63, (54, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(primals_64, (54, ), (1, ))
    assert_size_stride(primals_66, (54, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_67, (54, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(primals_68, (54, ), (1, ))
    assert_size_stride(primals_70, (54, 96, 1, 1), (96, 1, 1, 1))
    assert_size_stride(primals_71, (54, ), (1, ))
    assert_size_stride(primals_73, (54, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_74, (54, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(primals_75, (54, ), (1, ))
    assert_size_stride(primals_77, (54, ), (1, ))
    assert_size_stride(primals_79, (54, 96, 1, 1), (96, 1, 1, 1))
    assert_size_stride(primals_80, (54, 96, 1, 1), (96, 1, 1, 1))
    assert_size_stride(primals_81, (108, ), (1, ))
    assert_size_stride(primals_83, (108, 270, 1, 1), (270, 1, 1, 1))
    assert_size_stride(primals_84, (108, ), (1, ))
    assert_size_stride(primals_86, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_87, (108, ), (1, ))
    assert_size_stride(primals_89, (108, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_90, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_91, (108, ), (1, ))
    assert_size_stride(primals_93, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_94, (108, ), (1, ))
    assert_size_stride(primals_96, (108, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_97, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_98, (108, ), (1, ))
    assert_size_stride(primals_100, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_101, (108, ), (1, ))
    assert_size_stride(primals_103, (108, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_104, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_105, (108, ), (1, ))
    assert_size_stride(primals_107, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_108, (108, ), (1, ))
    assert_size_stride(primals_110, (108, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_111, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_112, (108, ), (1, ))
    assert_size_stride(primals_114, (108, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_115, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_116, (108, ), (1, ))
    assert_size_stride(primals_118, (108, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_119, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_120, (108, ), (1, ))
    assert_size_stride(primals_122, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_123, (108, ), (1, ))
    assert_size_stride(primals_125, (108, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_126, (108, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(primals_127, (108, ), (1, ))
    assert_size_stride(primals_129, (108, ), (1, ))
    assert_size_stride(primals_131, (108, 270, 1, 1), (270, 1, 1, 1))
    assert_size_stride(primals_132, (108, 270, 1, 1), (270, 1, 1, 1))
    assert_size_stride(primals_133, (216, ), (1, ))
    assert_size_stride(primals_135, (216, 540, 1, 1), (540, 1, 1, 1))
    assert_size_stride(primals_136, (216, ), (1, ))
    assert_size_stride(primals_138, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_139, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_140, (216, ), (1, ))
    assert_size_stride(primals_142, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_143, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_144, (216, ), (1, ))
    assert_size_stride(primals_146, (216, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_147, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_148, (216, ), (1, ))
    assert_size_stride(primals_150, (216, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_151, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_152, (216, ), (1, ))
    assert_size_stride(primals_154, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_155, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_156, (216, ), (1, ))
    assert_size_stride(primals_158, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_159, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_160, (216, ), (1, ))
    assert_size_stride(primals_162, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_163, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_164, (216, ), (1, ))
    assert_size_stride(primals_166, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_167, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_168, (216, ), (1, ))
    assert_size_stride(primals_170, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_171, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_172, (216, ), (1, ))
    assert_size_stride(primals_174, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_175, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_176, (216, ), (1, ))
    assert_size_stride(primals_178, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_179, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_180, (216, ), (1, ))
    assert_size_stride(primals_182, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_183, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_184, (216, ), (1, ))
    assert_size_stride(primals_186, (216, 540, 1, 1), (540, 1, 1, 1))
    assert_size_stride(primals_187, (216, ), (1, ))
    assert_size_stride(primals_189, (216, 1080, 1, 1), (1080, 1, 1, 1))
    assert_size_stride(primals_190, (216, ), (1, ))
    assert_size_stride(primals_192, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_193, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_194, (216, ), (1, ))
    assert_size_stride(primals_196, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_197, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_198, (216, ), (1, ))
    assert_size_stride(primals_200, (216, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_201, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_202, (216, ), (1, ))
    assert_size_stride(primals_204, (216, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_205, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_206, (216, ), (1, ))
    assert_size_stride(primals_208, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_209, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_210, (216, ), (1, ))
    assert_size_stride(primals_212, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_213, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_214, (216, ), (1, ))
    assert_size_stride(primals_216, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_217, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_218, (216, ), (1, ))
    assert_size_stride(primals_220, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_221, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_222, (216, ), (1, ))
    assert_size_stride(primals_224, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_225, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_226, (216, ), (1, ))
    assert_size_stride(primals_228, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_229, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_230, (216, ), (1, ))
    assert_size_stride(primals_232, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_233, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_234, (216, ), (1, ))
    assert_size_stride(primals_236, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_237, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_238, (216, ), (1, ))
    assert_size_stride(primals_240, (216, 1080, 1, 1), (1080, 1, 1, 1))
    assert_size_stride(primals_241, (216, ), (1, ))
    assert_size_stride(primals_243, (216, 1080, 1, 1), (1080, 1, 1, 1))
    assert_size_stride(primals_244, (216, ), (1, ))
    assert_size_stride(primals_246, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_247, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_248, (216, ), (1, ))
    assert_size_stride(primals_250, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_251, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_252, (216, ), (1, ))
    assert_size_stride(primals_254, (216, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_255, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_256, (216, ), (1, ))
    assert_size_stride(primals_258, (216, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_259, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_260, (216, ), (1, ))
    assert_size_stride(primals_262, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_263, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_264, (216, ), (1, ))
    assert_size_stride(primals_266, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_267, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_268, (216, ), (1, ))
    assert_size_stride(primals_270, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_271, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_272, (216, ), (1, ))
    assert_size_stride(primals_274, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_275, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_276, (216, ), (1, ))
    assert_size_stride(primals_278, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_279, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_280, (216, ), (1, ))
    assert_size_stride(primals_282, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_283, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_284, (216, ), (1, ))
    assert_size_stride(primals_286, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_287, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_288, (216, ), (1, ))
    assert_size_stride(primals_290, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_291, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_292, (216, ), (1, ))
    assert_size_stride(primals_294, (216, 1080, 1, 1), (1080, 1, 1, 1))
    assert_size_stride(primals_295, (216, ), (1, ))
    assert_size_stride(primals_297, (216, 1080, 1, 1), (1080, 1, 1, 1))
    assert_size_stride(primals_298, (216, ), (1, ))
    assert_size_stride(primals_300, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_301, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_302, (216, ), (1, ))
    assert_size_stride(primals_304, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_305, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_306, (216, ), (1, ))
    assert_size_stride(primals_308, (216, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_309, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_310, (216, ), (1, ))
    assert_size_stride(primals_312, (216, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_313, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_314, (216, ), (1, ))
    assert_size_stride(primals_316, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_317, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_318, (216, ), (1, ))
    assert_size_stride(primals_320, (216, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_321, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_322, (216, ), (1, ))
    assert_size_stride(primals_324, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_325, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_326, (216, ), (1, ))
    assert_size_stride(primals_328, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_329, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_330, (216, ), (1, ))
    assert_size_stride(primals_332, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_333, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_334, (216, ), (1, ))
    assert_size_stride(primals_336, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_337, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_338, (216, ), (1, ))
    assert_size_stride(primals_340, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_341, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_342, (216, ), (1, ))
    assert_size_stride(primals_344, (216, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_345, (216, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(primals_346, (216, ), (1, ))
    assert_size_stride(primals_348, (432, 1080, 1, 1), (1080, 1, 1, 1))
    assert_size_stride(primals_349, (432, ), (1, ))
    assert_size_stride(primals_351, (432, 1080, 1, 1), (1080, 1, 1, 1))
    assert_size_stride(primals_352, (432, ), (1, ))
    assert_size_stride(primals_354, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_355, (432, ), (1, ))
    assert_size_stride(primals_357, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_358, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_359, (432, ), (1, ))
    assert_size_stride(primals_361, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_362, (432, ), (1, ))
    assert_size_stride(primals_364, (432, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_365, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_366, (432, ), (1, ))
    assert_size_stride(primals_368, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_369, (432, ), (1, ))
    assert_size_stride(primals_371, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_372, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_373, (432, ), (1, ))
    assert_size_stride(primals_375, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_376, (432, ), (1, ))
    assert_size_stride(primals_378, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_379, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_380, (432, ), (1, ))
    assert_size_stride(primals_382, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_383, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_384, (432, ), (1, ))
    assert_size_stride(primals_386, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_387, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_388, (432, ), (1, ))
    assert_size_stride(primals_390, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_391, (432, ), (1, ))
    assert_size_stride(primals_393, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_394, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_395, (432, ), (1, ))
    assert_size_stride(primals_397, (432, ), (1, ))
    assert_size_stride(primals_399, (216, 1080, 1, 1), (1080, 1, 1, 1))
    assert_size_stride(primals_400, (216, 1080, 1, 1), (1080, 1, 1, 1))
    assert_size_stride(primals_401, (432, ), (1, ))
    assert_size_stride(primals_403, (432, 2160, 1, 1), (2160, 1, 1, 1))
    assert_size_stride(primals_404, (432, ), (1, ))
    assert_size_stride(primals_406, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_407, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_408, (432, ), (1, ))
    assert_size_stride(primals_410, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_411, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_412, (432, ), (1, ))
    assert_size_stride(primals_414, (432, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_415, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_416, (432, ), (1, ))
    assert_size_stride(primals_418, (432, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_419, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_420, (432, ), (1, ))
    assert_size_stride(primals_422, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_423, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_424, (432, ), (1, ))
    assert_size_stride(primals_426, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_427, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_428, (432, ), (1, ))
    assert_size_stride(primals_430, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_431, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_432, (432, ), (1, ))
    assert_size_stride(primals_434, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_435, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_436, (432, ), (1, ))
    assert_size_stride(primals_438, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_439, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_440, (432, ), (1, ))
    assert_size_stride(primals_442, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_443, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_444, (432, ), (1, ))
    assert_size_stride(primals_446, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_447, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_448, (432, ), (1, ))
    assert_size_stride(primals_450, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_451, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_452, (432, ), (1, ))
    assert_size_stride(primals_454, (432, 2160, 1, 1), (2160, 1, 1, 1))
    assert_size_stride(primals_455, (432, ), (1, ))
    assert_size_stride(primals_457, (432, 2160, 1, 1), (2160, 1, 1, 1))
    assert_size_stride(primals_458, (432, ), (1, ))
    assert_size_stride(primals_460, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_461, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_462, (432, ), (1, ))
    assert_size_stride(primals_464, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_465, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_466, (432, ), (1, ))
    assert_size_stride(primals_468, (432, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_469, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_470, (432, ), (1, ))
    assert_size_stride(primals_472, (432, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_473, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_474, (432, ), (1, ))
    assert_size_stride(primals_476, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_477, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_478, (432, ), (1, ))
    assert_size_stride(primals_480, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_481, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_482, (432, ), (1, ))
    assert_size_stride(primals_484, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_485, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_486, (432, ), (1, ))
    assert_size_stride(primals_488, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_489, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_490, (432, ), (1, ))
    assert_size_stride(primals_492, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_493, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_494, (432, ), (1, ))
    assert_size_stride(primals_496, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_497, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_498, (432, ), (1, ))
    assert_size_stride(primals_500, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_501, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_502, (432, ), (1, ))
    assert_size_stride(primals_504, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_505, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_506, (432, ), (1, ))
    assert_size_stride(primals_508, (432, 2160, 1, 1), (2160, 1, 1, 1))
    assert_size_stride(primals_509, (432, ), (1, ))
    assert_size_stride(primals_511, (432, 2160, 1, 1), (2160, 1, 1, 1))
    assert_size_stride(primals_512, (432, ), (1, ))
    assert_size_stride(primals_514, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_515, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_516, (432, ), (1, ))
    assert_size_stride(primals_518, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_519, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_520, (432, ), (1, ))
    assert_size_stride(primals_522, (432, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_523, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_524, (432, ), (1, ))
    assert_size_stride(primals_526, (432, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_527, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_528, (432, ), (1, ))
    assert_size_stride(primals_530, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_531, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_532, (432, ), (1, ))
    assert_size_stride(primals_534, (432, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_535, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_536, (432, ), (1, ))
    assert_size_stride(primals_538, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_539, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_540, (432, ), (1, ))
    assert_size_stride(primals_542, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_543, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_544, (432, ), (1, ))
    assert_size_stride(primals_546, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_547, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_548, (432, ), (1, ))
    assert_size_stride(primals_550, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_551, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_552, (432, ), (1, ))
    assert_size_stride(primals_554, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_555, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_556, (432, ), (1, ))
    assert_size_stride(primals_558, (432, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_559, (432, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(primals_560, (432, ), (1, ))
    assert_size_stride(primals_562, (864, 2160, 1, 1), (2160, 1, 1, 1))
    assert_size_stride(primals_563, (864, ), (1, ))
    assert_size_stride(primals_565, (864, 2160, 1, 1), (2160, 1, 1, 1))
    assert_size_stride(primals_566, (864, ), (1, ))
    assert_size_stride(primals_568, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_569, (864, ), (1, ))
    assert_size_stride(primals_571, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_572, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_573, (864, ), (1, ))
    assert_size_stride(primals_575, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_576, (864, ), (1, ))
    assert_size_stride(primals_578, (864, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_579, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_580, (864, ), (1, ))
    assert_size_stride(primals_582, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_583, (864, ), (1, ))
    assert_size_stride(primals_585, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_586, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_587, (864, ), (1, ))
    assert_size_stride(primals_589, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_590, (864, ), (1, ))
    assert_size_stride(primals_592, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_593, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_594, (864, ), (1, ))
    assert_size_stride(primals_596, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_597, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_598, (864, ), (1, ))
    assert_size_stride(primals_600, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_601, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_602, (864, ), (1, ))
    assert_size_stride(primals_604, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_605, (864, ), (1, ))
    assert_size_stride(primals_607, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_608, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_609, (864, ), (1, ))
    assert_size_stride(primals_611, (864, ), (1, ))
    assert_size_stride(primals_613, (432, 2160, 1, 1), (2160, 1, 1, 1))
    assert_size_stride(primals_614, (432, 2160, 1, 1), (2160, 1, 1, 1))
    assert_size_stride(primals_615, (864, ), (1, ))
    assert_size_stride(primals_617, (864, 4320, 1, 1), (4320, 1, 1, 1))
    assert_size_stride(primals_618, (864, ), (1, ))
    assert_size_stride(primals_620, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_621, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_622, (864, ), (1, ))
    assert_size_stride(primals_624, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_625, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_626, (864, ), (1, ))
    assert_size_stride(primals_628, (864, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_629, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_630, (864, ), (1, ))
    assert_size_stride(primals_632, (864, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_633, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_634, (864, ), (1, ))
    assert_size_stride(primals_636, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_637, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_638, (864, ), (1, ))
    assert_size_stride(primals_640, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_641, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_642, (864, ), (1, ))
    assert_size_stride(primals_644, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_645, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_646, (864, ), (1, ))
    assert_size_stride(primals_648, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_649, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_650, (864, ), (1, ))
    assert_size_stride(primals_652, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_653, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_654, (864, ), (1, ))
    assert_size_stride(primals_656, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_657, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_658, (864, ), (1, ))
    assert_size_stride(primals_660, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_661, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_662, (864, ), (1, ))
    assert_size_stride(primals_664, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_665, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_666, (864, ), (1, ))
    assert_size_stride(primals_668, (864, 4320, 1, 1), (4320, 1, 1, 1))
    assert_size_stride(primals_669, (864, ), (1, ))
    assert_size_stride(primals_671, (864, 4320, 1, 1), (4320, 1, 1, 1))
    assert_size_stride(primals_672, (864, ), (1, ))
    assert_size_stride(primals_674, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_675, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_676, (864, ), (1, ))
    assert_size_stride(primals_678, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_679, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_680, (864, ), (1, ))
    assert_size_stride(primals_682, (864, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_683, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_684, (864, ), (1, ))
    assert_size_stride(primals_686, (864, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_687, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_688, (864, ), (1, ))
    assert_size_stride(primals_690, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_691, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_692, (864, ), (1, ))
    assert_size_stride(primals_694, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_695, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_696, (864, ), (1, ))
    assert_size_stride(primals_698, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_699, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_700, (864, ), (1, ))
    assert_size_stride(primals_702, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_703, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_704, (864, ), (1, ))
    assert_size_stride(primals_706, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_707, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_708, (864, ), (1, ))
    assert_size_stride(primals_710, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_711, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_712, (864, ), (1, ))
    assert_size_stride(primals_714, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_715, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_716, (864, ), (1, ))
    assert_size_stride(primals_718, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_719, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_720, (864, ), (1, ))
    assert_size_stride(primals_722, (864, 4320, 1, 1), (4320, 1, 1, 1))
    assert_size_stride(primals_723, (864, ), (1, ))
    assert_size_stride(primals_725, (864, 4320, 1, 1), (4320, 1, 1, 1))
    assert_size_stride(primals_726, (864, ), (1, ))
    assert_size_stride(primals_728, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_729, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_730, (864, ), (1, ))
    assert_size_stride(primals_732, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_733, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_734, (864, ), (1, ))
    assert_size_stride(primals_736, (864, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_737, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_738, (864, ), (1, ))
    assert_size_stride(primals_740, (864, 1, 7, 7), (49, 49, 7, 1))
    assert_size_stride(primals_741, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_742, (864, ), (1, ))
    assert_size_stride(primals_744, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_745, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_746, (864, ), (1, ))
    assert_size_stride(primals_748, (864, 1, 5, 5), (25, 25, 5, 1))
    assert_size_stride(primals_749, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_750, (864, ), (1, ))
    assert_size_stride(primals_752, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_753, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_754, (864, ), (1, ))
    assert_size_stride(primals_756, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_757, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_758, (864, ), (1, ))
    assert_size_stride(primals_760, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_761, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_762, (864, ), (1, ))
    assert_size_stride(primals_764, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_765, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_766, (864, ), (1, ))
    assert_size_stride(primals_768, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_769, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_770, (864, ), (1, ))
    assert_size_stride(primals_772, (864, 1, 3, 3), (9, 9, 3, 1))
    assert_size_stride(primals_773, (864, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(primals_774, (864, ), (1, ))
    assert_size_stride(primals_1381, (8, 3, 331, 331), (328683, 1, 993, 3))
    assert_size_stride(convolution, (8, 96, 165, 165), (2613600, 1, 15840, 96))
    assert_size_stride(squeeze_1, (96, ), (1, ))
    assert_size_stride(relu, (8, 96, 165, 165), (2613600, 1, 15840, 96))
    assert_size_stride(convolution_1, (8, 54, 165, 165), (1470150, 1, 8910, 54))
    assert_size_stride(squeeze_4, (54, ), (1, ))
    assert_size_stride(constant_pad_nd, (8, 96, 169, 169), (2741856, 1, 16224, 96))
    assert_size_stride(convolution_2, (8, 96, 83, 83), (661344, 1, 7968, 96))
    assert_size_stride(convolution_3, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_7, (54, ), (1, ))
    assert_size_stride(relu_2, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_4, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_5, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_10, (54, ), (1, ))
    assert_size_stride(constant_pad_nd_1, (8, 96, 167, 167), (2677344, 1, 16032, 96))
    assert_size_stride(getitem_8, (8, 96, 83, 83), (661344, 1, 7968, 96))
    assert_size_stride(getitem_9, (8, 96, 83, 83), (661344, 1, 7968, 96))
    assert_size_stride(convolution_6, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_13, (54, ), (1, ))
    assert_size_stride(constant_pad_nd_2, (8, 54, 171, 171), (1579014, 1, 9234, 54))
    assert_size_stride(convolution_7, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_8, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_16, (54, ), (1, ))
    assert_size_stride(relu_4, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_9, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_10, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_19, (54, ), (1, ))
    assert_size_stride(constant_pad_nd_3, (8, 54, 167, 167), (1506006, 1, 9018, 54))
    assert_size_stride(getitem_17, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(constant_pad_nd_4, (8, 54, 169, 169), (1542294, 1, 9126, 54))
    assert_size_stride(convolution_11, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_12, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_22, (54, ), (1, ))
    assert_size_stride(relu_6, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_13, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_14, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_25, (54, ), (1, ))
    assert_size_stride(constant_pad_nd_5, (8, 54, 167, 167), (1506006, 1, 9018, 54))
    assert_size_stride(convolution_15, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_16, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_28, (54, ), (1, ))
    assert_size_stride(relu_8, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_17, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_18, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_31, (54, ), (1, ))
    assert_size_stride(relu_9, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_19, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_20, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_34, (54, ), (1, ))
    assert_size_stride(relu_10, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_21, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_22, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_37, (54, ), (1, ))
    assert_size_stride(constant_pad_nd_7, (8, 96, 167, 167), (2677344, 1, 16032, 96))
    assert_size_stride(convolution_23, (8, 96, 83, 83), (661344, 1, 7968, 96))
    assert_size_stride(convolution_24, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_40, (54, ), (1, ))
    assert_size_stride(relu_12, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_25, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(convolution_26, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_43, (54, ), (1, ))
    assert_size_stride(constant_pad_nd_8, (8, 54, 165, 165), (1470150, 1, 8910, 54))
    assert_size_stride(convolution_27, (8, 54, 83, 83), (372006, 1, 4482, 54))
    assert_size_stride(squeeze_46, (54, ), (1, ))
    assert_size_stride(avg_pool2d, (8, 96, 83, 83), (661344, 1, 7968, 96))
    assert_size_stride(constant_pad_nd_9, (8, 96, 165, 165), (2613600, 1, 15840, 96))
    assert_size_stride(avg_pool2d_1, (8, 96, 83, 83), (661344, 1, 7968, 96))
    assert_size_stride(cat_1, (8, 108, 83, 83), (744012, 1, 8964, 108))
    assert_size_stride(squeeze_49, (108, ), (1, ))
    assert_size_stride(relu_15, (8, 270, 83, 83), (1860030, 1, 22410, 270))
    assert_size_stride(convolution_30, (8, 108, 83, 83), (744012, 1, 8964, 108))
    assert_size_stride(squeeze_52, (108, ), (1, ))
    assert_size_stride(constant_pad_nd_10, (8, 108, 87, 87), (817452, 1, 9396, 108))
    assert_size_stride(convolution_31, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_32, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_55, (108, ), (1, ))
    assert_size_stride(relu_17, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_33, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_34, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_58, (108, ), (1, ))
    assert_size_stride(constant_pad_nd_11, (8, 108, 85, 85), (780300, 1, 9180, 108))
    assert_size_stride(getitem_47, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(constant_pad_nd_12, (8, 108, 89, 89), (855468, 1, 9612, 108))
    assert_size_stride(convolution_35, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_36, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_61, (108, ), (1, ))
    assert_size_stride(relu_19, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_37, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_38, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_64, (108, ), (1, ))
    assert_size_stride(constant_pad_nd_13, (8, 108, 85, 85), (780300, 1, 9180, 108))
    assert_size_stride(getitem_53, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(constant_pad_nd_14, (8, 108, 87, 87), (817452, 1, 9396, 108))
    assert_size_stride(convolution_39, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_40, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_67, (108, ), (1, ))
    assert_size_stride(relu_21, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_41, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_42, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_70, (108, ), (1, ))
    assert_size_stride(constant_pad_nd_15, (8, 108, 85, 85), (780300, 1, 9180, 108))
    assert_size_stride(convolution_43, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_44, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_73, (108, ), (1, ))
    assert_size_stride(relu_23, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_45, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_46, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_76, (108, ), (1, ))
    assert_size_stride(relu_24, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_47, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_48, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_79, (108, ), (1, ))
    assert_size_stride(relu_25, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_49, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_50, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_82, (108, ), (1, ))
    assert_size_stride(constant_pad_nd_17, (8, 108, 85, 85), (780300, 1, 9180, 108))
    assert_size_stride(convolution_51, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_52, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_85, (108, ), (1, ))
    assert_size_stride(relu_27, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_53, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(convolution_54, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_88, (108, ), (1, ))
    assert_size_stride(constant_pad_nd_18, (8, 108, 83, 83), (744012, 1, 8964, 108))
    assert_size_stride(convolution_55, (8, 108, 42, 42), (190512, 1, 4536, 108))
    assert_size_stride(squeeze_91, (108, ), (1, ))
    assert_size_stride(avg_pool2d_2, (8, 270, 42, 42), (476280, 1, 11340, 270))
    assert_size_stride(constant_pad_nd_19, (8, 270, 83, 83), (1860030, 1, 22410, 270))
    assert_size_stride(avg_pool2d_3, (8, 270, 42, 42), (476280, 1, 11340, 270))
    assert_size_stride(cat_3, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_94, (216, ), (1, ))
    assert_size_stride(add_169, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(relu_30, (8, 540, 42, 42), (952560, 1, 22680, 540))
    assert_size_stride(convolution_58, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_97, (216, ), (1, ))
    assert_size_stride(add_174, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(relu_31, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_59, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_60, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_100, (216, ), (1, ))
    assert_size_stride(relu_32, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_61, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_62, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_103, (216, ), (1, ))
    assert_size_stride(getitem_83, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(relu_33, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_63, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_64, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_106, (216, ), (1, ))
    assert_size_stride(relu_34, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_65, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_66, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_109, (216, ), (1, ))
    assert_size_stride(getitem_89, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_67, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_68, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_112, (216, ), (1, ))
    assert_size_stride(relu_36, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_69, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_70, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_115, (216, ), (1, ))
    assert_size_stride(convolution_71, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_72, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_118, (216, ), (1, ))
    assert_size_stride(relu_38, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_73, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_74, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_121, (216, ), (1, ))
    assert_size_stride(relu_39, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_75, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_76, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_124, (216, ), (1, ))
    assert_size_stride(relu_40, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_77, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_78, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_127, (216, ), (1, ))
    assert_size_stride(convolution_79, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_80, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_130, (216, ), (1, ))
    assert_size_stride(relu_42, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_81, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_82, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_133, (216, ), (1, ))
    assert_size_stride(convolution_83, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_136, (216, ), (1, ))
    assert_size_stride(add_244, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(relu_44, (8, 1080, 42, 42), (1905120, 1, 45360, 1080))
    assert_size_stride(convolution_84, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_139, (216, ), (1, ))
    assert_size_stride(add_249, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(relu_45, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_85, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_86, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_142, (216, ), (1, ))
    assert_size_stride(relu_46, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_87, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_88, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_145, (216, ), (1, ))
    assert_size_stride(getitem_117, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(relu_47, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_89, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_90, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_148, (216, ), (1, ))
    assert_size_stride(relu_48, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_91, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_92, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_151, (216, ), (1, ))
    assert_size_stride(getitem_123, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_93, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_94, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_154, (216, ), (1, ))
    assert_size_stride(relu_50, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_95, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_96, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_157, (216, ), (1, ))
    assert_size_stride(convolution_97, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_98, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_160, (216, ), (1, ))
    assert_size_stride(relu_52, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_99, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_100, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_163, (216, ), (1, ))
    assert_size_stride(relu_53, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_101, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_102, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_166, (216, ), (1, ))
    assert_size_stride(relu_54, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_103, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_104, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_169, (216, ), (1, ))
    assert_size_stride(convolution_105, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_106, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_172, (216, ), (1, ))
    assert_size_stride(relu_56, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_107, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_108, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_175, (216, ), (1, ))
    assert_size_stride(convolution_109, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_178, (216, ), (1, ))
    assert_size_stride(add_319, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(relu_58, (8, 1080, 42, 42), (1905120, 1, 45360, 1080))
    assert_size_stride(convolution_110, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_181, (216, ), (1, ))
    assert_size_stride(add_324, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(relu_59, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_111, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_112, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_184, (216, ), (1, ))
    assert_size_stride(relu_60, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_113, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_114, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_187, (216, ), (1, ))
    assert_size_stride(getitem_151, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(relu_61, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_115, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_116, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_190, (216, ), (1, ))
    assert_size_stride(relu_62, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_117, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_118, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_193, (216, ), (1, ))
    assert_size_stride(getitem_157, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_119, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_120, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_196, (216, ), (1, ))
    assert_size_stride(relu_64, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_121, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_122, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_199, (216, ), (1, ))
    assert_size_stride(convolution_123, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_124, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_202, (216, ), (1, ))
    assert_size_stride(relu_66, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_125, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_126, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_205, (216, ), (1, ))
    assert_size_stride(relu_67, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_127, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_128, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_208, (216, ), (1, ))
    assert_size_stride(relu_68, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_129, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_130, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_211, (216, ), (1, ))
    assert_size_stride(convolution_131, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_132, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_214, (216, ), (1, ))
    assert_size_stride(relu_70, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_133, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_134, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_217, (216, ), (1, ))
    assert_size_stride(convolution_135, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_220, (216, ), (1, ))
    assert_size_stride(add_394, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(relu_72, (8, 1080, 42, 42), (1905120, 1, 45360, 1080))
    assert_size_stride(convolution_136, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_223, (216, ), (1, ))
    assert_size_stride(add_399, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(relu_73, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_137, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_138, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_226, (216, ), (1, ))
    assert_size_stride(relu_74, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_139, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_140, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_229, (216, ), (1, ))
    assert_size_stride(getitem_185, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(relu_75, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_141, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_142, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_232, (216, ), (1, ))
    assert_size_stride(relu_76, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_143, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_144, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_235, (216, ), (1, ))
    assert_size_stride(getitem_191, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_145, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_146, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_238, (216, ), (1, ))
    assert_size_stride(relu_78, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_147, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_148, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_241, (216, ), (1, ))
    assert_size_stride(convolution_149, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_150, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_244, (216, ), (1, ))
    assert_size_stride(relu_80, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_151, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_152, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_247, (216, ), (1, ))
    assert_size_stride(relu_81, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_153, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_154, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_250, (216, ), (1, ))
    assert_size_stride(relu_82, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_155, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_156, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_253, (216, ), (1, ))
    assert_size_stride(convolution_157, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_158, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_256, (216, ), (1, ))
    assert_size_stride(relu_84, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_159, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(convolution_160, (8, 216, 42, 42), (381024, 1, 9072, 216))
    assert_size_stride(squeeze_259, (216, ), (1, ))
    assert_size_stride(convolution_161, (8, 432, 42, 42), (762048, 1, 18144, 432))
    assert_size_stride(squeeze_262, (432, ), (1, ))
    assert_size_stride(relu_86, (8, 1080, 42, 42), (1905120, 1, 45360, 1080))
    assert_size_stride(convolution_162, (8, 432, 42, 42), (762048, 1, 18144, 432))
    assert_size_stride(squeeze_265, (432, ), (1, ))
    assert_size_stride(constant_pad_nd_20, (8, 432, 45, 45), (874800, 1, 19440, 432))
    assert_size_stride(convolution_163, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_164, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_268, (432, ), (1, ))
    assert_size_stride(relu_88, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_165, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_166, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_271, (432, ), (1, ))
    assert_size_stride(constant_pad_nd_21, (8, 432, 43, 43), (798768, 1, 18576, 432))
    assert_size_stride(getitem_219, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(constant_pad_nd_22, (8, 432, 47, 47), (954288, 1, 20304, 432))
    assert_size_stride(convolution_167, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_168, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_274, (432, ), (1, ))
    assert_size_stride(relu_90, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_169, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_170, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_277, (432, ), (1, ))
    assert_size_stride(constant_pad_nd_23, (8, 432, 43, 43), (798768, 1, 18576, 432))
    assert_size_stride(getitem_225, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(constant_pad_nd_24, (8, 432, 45, 45), (874800, 1, 19440, 432))
    assert_size_stride(convolution_171, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_172, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_280, (432, ), (1, ))
    assert_size_stride(relu_92, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_173, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_174, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_283, (432, ), (1, ))
    assert_size_stride(constant_pad_nd_25, (8, 432, 43, 43), (798768, 1, 18576, 432))
    assert_size_stride(convolution_175, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_176, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_286, (432, ), (1, ))
    assert_size_stride(relu_94, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_177, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_178, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_289, (432, ), (1, ))
    assert_size_stride(relu_95, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_179, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_180, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_292, (432, ), (1, ))
    assert_size_stride(relu_96, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_181, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_182, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_295, (432, ), (1, ))
    assert_size_stride(constant_pad_nd_27, (8, 432, 43, 43), (798768, 1, 18576, 432))
    assert_size_stride(convolution_183, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_184, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_298, (432, ), (1, ))
    assert_size_stride(relu_98, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_185, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_186, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_301, (432, ), (1, ))
    assert_size_stride(constant_pad_nd_28, (8, 432, 42, 42), (762048, 1, 18144, 432))
    assert_size_stride(convolution_187, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_304, (432, ), (1, ))
    assert_size_stride(avg_pool2d_4, (8, 1080, 21, 21), (476280, 1, 22680, 1080))
    assert_size_stride(constant_pad_nd_29, (8, 1080, 42, 42), (1905120, 1, 45360, 1080))
    assert_size_stride(avg_pool2d_5, (8, 1080, 21, 21), (476280, 1, 22680, 1080))
    assert_size_stride(cat_9, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_307, (432, ), (1, ))
    assert_size_stride(add_549, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(relu_101, (8, 2160, 21, 21), (952560, 1, 45360, 2160))
    assert_size_stride(convolution_190, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_310, (432, ), (1, ))
    assert_size_stride(add_554, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(relu_102, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_191, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_192, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_313, (432, ), (1, ))
    assert_size_stride(relu_103, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_193, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_194, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_316, (432, ), (1, ))
    assert_size_stride(getitem_255, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(relu_104, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_195, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_196, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_319, (432, ), (1, ))
    assert_size_stride(relu_105, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_197, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_198, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_322, (432, ), (1, ))
    assert_size_stride(getitem_261, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_199, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_200, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_325, (432, ), (1, ))
    assert_size_stride(relu_107, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_201, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_202, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_328, (432, ), (1, ))
    assert_size_stride(convolution_203, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_204, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_331, (432, ), (1, ))
    assert_size_stride(relu_109, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_205, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_206, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_334, (432, ), (1, ))
    assert_size_stride(relu_110, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_207, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_208, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_337, (432, ), (1, ))
    assert_size_stride(relu_111, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_209, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_210, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_340, (432, ), (1, ))
    assert_size_stride(convolution_211, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_212, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_343, (432, ), (1, ))
    assert_size_stride(relu_113, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_213, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_214, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_346, (432, ), (1, ))
    assert_size_stride(convolution_215, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_349, (432, ), (1, ))
    assert_size_stride(add_624, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(relu_115, (8, 2160, 21, 21), (952560, 1, 45360, 2160))
    assert_size_stride(convolution_216, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_352, (432, ), (1, ))
    assert_size_stride(add_629, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(relu_116, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_217, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_218, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_355, (432, ), (1, ))
    assert_size_stride(relu_117, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_219, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_220, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_358, (432, ), (1, ))
    assert_size_stride(getitem_289, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(relu_118, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_221, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_222, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_361, (432, ), (1, ))
    assert_size_stride(relu_119, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_223, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_224, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_364, (432, ), (1, ))
    assert_size_stride(getitem_295, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_225, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_226, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_367, (432, ), (1, ))
    assert_size_stride(relu_121, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_227, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_228, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_370, (432, ), (1, ))
    assert_size_stride(convolution_229, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_230, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_373, (432, ), (1, ))
    assert_size_stride(relu_123, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_231, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_232, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_376, (432, ), (1, ))
    assert_size_stride(relu_124, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_233, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_234, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_379, (432, ), (1, ))
    assert_size_stride(relu_125, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_235, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_236, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_382, (432, ), (1, ))
    assert_size_stride(convolution_237, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_238, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_385, (432, ), (1, ))
    assert_size_stride(relu_127, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_239, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_240, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_388, (432, ), (1, ))
    assert_size_stride(convolution_241, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_391, (432, ), (1, ))
    assert_size_stride(add_699, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(relu_129, (8, 2160, 21, 21), (952560, 1, 45360, 2160))
    assert_size_stride(convolution_242, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_394, (432, ), (1, ))
    assert_size_stride(add_704, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(relu_130, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_243, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_244, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_397, (432, ), (1, ))
    assert_size_stride(relu_131, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_245, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_246, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_400, (432, ), (1, ))
    assert_size_stride(getitem_323, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(relu_132, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_247, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_248, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_403, (432, ), (1, ))
    assert_size_stride(relu_133, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_249, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_250, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_406, (432, ), (1, ))
    assert_size_stride(getitem_329, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_251, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_252, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_409, (432, ), (1, ))
    assert_size_stride(relu_135, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_253, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_254, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_412, (432, ), (1, ))
    assert_size_stride(convolution_255, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_256, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_415, (432, ), (1, ))
    assert_size_stride(relu_137, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_257, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_258, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_418, (432, ), (1, ))
    assert_size_stride(relu_138, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_259, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_260, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_421, (432, ), (1, ))
    assert_size_stride(relu_139, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_261, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_262, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_424, (432, ), (1, ))
    assert_size_stride(convolution_263, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_264, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_427, (432, ), (1, ))
    assert_size_stride(relu_141, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_265, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(convolution_266, (8, 432, 21, 21), (190512, 1, 9072, 432))
    assert_size_stride(squeeze_430, (432, ), (1, ))
    assert_size_stride(convolution_267, (8, 864, 21, 21), (381024, 1, 18144, 864))
    assert_size_stride(squeeze_433, (864, ), (1, ))
    assert_size_stride(relu_143, (8, 2160, 21, 21), (952560, 1, 45360, 2160))
    assert_size_stride(convolution_268, (8, 864, 21, 21), (381024, 1, 18144, 864))
    assert_size_stride(squeeze_436, (864, ), (1, ))
    assert_size_stride(constant_pad_nd_30, (8, 864, 25, 25), (540000, 1, 21600, 864))
    assert_size_stride(convolution_269, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_270, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_439, (864, ), (1, ))
    assert_size_stride(relu_145, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_271, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_272, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_442, (864, ), (1, ))
    assert_size_stride(constant_pad_nd_31, (8, 864, 23, 23), (457056, 1, 19872, 864))
    assert_size_stride(getitem_357, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(constant_pad_nd_32, (8, 864, 27, 27), (629856, 1, 23328, 864))
    assert_size_stride(convolution_273, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_274, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_445, (864, ), (1, ))
    assert_size_stride(relu_147, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_275, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_276, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_448, (864, ), (1, ))
    assert_size_stride(constant_pad_nd_33, (8, 864, 23, 23), (457056, 1, 19872, 864))
    assert_size_stride(getitem_363, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(constant_pad_nd_34, (8, 864, 25, 25), (540000, 1, 21600, 864))
    assert_size_stride(convolution_277, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_278, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_451, (864, ), (1, ))
    assert_size_stride(relu_149, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_279, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_280, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_454, (864, ), (1, ))
    assert_size_stride(constant_pad_nd_35, (8, 864, 23, 23), (457056, 1, 19872, 864))
    assert_size_stride(convolution_281, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_282, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_457, (864, ), (1, ))
    assert_size_stride(relu_151, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_283, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_284, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_460, (864, ), (1, ))
    assert_size_stride(relu_152, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_285, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_286, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_463, (864, ), (1, ))
    assert_size_stride(relu_153, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_287, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_288, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_466, (864, ), (1, ))
    assert_size_stride(constant_pad_nd_37, (8, 864, 23, 23), (457056, 1, 19872, 864))
    assert_size_stride(convolution_289, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_290, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_469, (864, ), (1, ))
    assert_size_stride(relu_155, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_291, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_292, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_472, (864, ), (1, ))
    assert_size_stride(constant_pad_nd_38, (8, 864, 21, 21), (381024, 1, 18144, 864))
    assert_size_stride(convolution_293, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_475, (864, ), (1, ))
    assert_size_stride(avg_pool2d_6, (8, 2160, 11, 11), (261360, 1, 23760, 2160))
    assert_size_stride(constant_pad_nd_39, (8, 2160, 21, 21), (952560, 1, 45360, 2160))
    assert_size_stride(avg_pool2d_7, (8, 2160, 11, 11), (261360, 1, 23760, 2160))
    assert_size_stride(cat_14, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_478, (864, ), (1, ))
    assert_size_stride(add_854, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(relu_158, (8, 4320, 11, 11), (522720, 1, 47520, 4320))
    assert_size_stride(convolution_296, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_481, (864, ), (1, ))
    assert_size_stride(add_859, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(relu_159, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_297, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_298, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_484, (864, ), (1, ))
    assert_size_stride(relu_160, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_299, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_300, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_487, (864, ), (1, ))
    assert_size_stride(getitem_393, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(relu_161, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_301, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_302, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_490, (864, ), (1, ))
    assert_size_stride(relu_162, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_303, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_304, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_493, (864, ), (1, ))
    assert_size_stride(getitem_399, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_305, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_306, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_496, (864, ), (1, ))
    assert_size_stride(relu_164, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_307, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_308, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_499, (864, ), (1, ))
    assert_size_stride(convolution_309, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_310, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_502, (864, ), (1, ))
    assert_size_stride(relu_166, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_311, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_312, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_505, (864, ), (1, ))
    assert_size_stride(relu_167, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_313, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_314, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_508, (864, ), (1, ))
    assert_size_stride(relu_168, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_315, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_316, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_511, (864, ), (1, ))
    assert_size_stride(convolution_317, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_318, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_514, (864, ), (1, ))
    assert_size_stride(relu_170, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_319, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_320, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_517, (864, ), (1, ))
    assert_size_stride(convolution_321, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_520, (864, ), (1, ))
    assert_size_stride(add_929, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(relu_172, (8, 4320, 11, 11), (522720, 1, 47520, 4320))
    assert_size_stride(convolution_322, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_523, (864, ), (1, ))
    assert_size_stride(add_934, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(relu_173, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_323, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_324, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_526, (864, ), (1, ))
    assert_size_stride(relu_174, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_325, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_326, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_529, (864, ), (1, ))
    assert_size_stride(getitem_427, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(relu_175, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_327, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_328, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_532, (864, ), (1, ))
    assert_size_stride(relu_176, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_329, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_330, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_535, (864, ), (1, ))
    assert_size_stride(getitem_433, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_331, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_332, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_538, (864, ), (1, ))
    assert_size_stride(relu_178, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_333, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_334, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_541, (864, ), (1, ))
    assert_size_stride(convolution_335, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_336, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_544, (864, ), (1, ))
    assert_size_stride(relu_180, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_337, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_338, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_547, (864, ), (1, ))
    assert_size_stride(relu_181, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_339, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_340, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_550, (864, ), (1, ))
    assert_size_stride(relu_182, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_341, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_342, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_553, (864, ), (1, ))
    assert_size_stride(convolution_343, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_344, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_556, (864, ), (1, ))
    assert_size_stride(relu_184, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_345, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_346, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_559, (864, ), (1, ))
    assert_size_stride(convolution_347, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_562, (864, ), (1, ))
    assert_size_stride(add_1004, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(relu_186, (8, 4320, 11, 11), (522720, 1, 47520, 4320))
    assert_size_stride(convolution_348, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_565, (864, ), (1, ))
    assert_size_stride(add_1009, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(relu_187, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_349, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_350, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_568, (864, ), (1, ))
    assert_size_stride(relu_188, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_351, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_352, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_571, (864, ), (1, ))
    assert_size_stride(getitem_461, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(relu_189, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_353, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_354, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_574, (864, ), (1, ))
    assert_size_stride(relu_190, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_355, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_356, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_577, (864, ), (1, ))
    assert_size_stride(getitem_467, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_357, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_358, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_580, (864, ), (1, ))
    assert_size_stride(relu_192, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_359, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_360, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_583, (864, ), (1, ))
    assert_size_stride(convolution_361, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_362, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_586, (864, ), (1, ))
    assert_size_stride(relu_194, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_363, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_364, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_589, (864, ), (1, ))
    assert_size_stride(relu_195, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_365, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_366, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_592, (864, ), (1, ))
    assert_size_stride(relu_196, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_367, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_368, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_595, (864, ), (1, ))
    assert_size_stride(convolution_369, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_370, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_598, (864, ), (1, ))
    assert_size_stride(relu_198, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_371, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(convolution_372, (8, 864, 11, 11), (104544, 1, 9504, 864))
    assert_size_stride(squeeze_601, (864, ), (1, ))
    assert_size_stride(clone, (8, 4320), (4320, 1))
    assert_size_stride(permute_1, (1000, 4320), (4320, 1))
    assert_size_stride(le, (8, 4320, 11, 11), (522720, 1, 47520, 4320))
    assert_size_stride(unsqueeze_806, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_818, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_830, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_842, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_854, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_866, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_878, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_890, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_902, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_914, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_926, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_938, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_950, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_962, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_974, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_986, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_998, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1010, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1022, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1034, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1046, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1058, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1070, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1082, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1094, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1106, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1118, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1130, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1142, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1154, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1166, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1178, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1190, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1202, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1214, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1226, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1238, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1250, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1262, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1274, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1286, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1298, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1310, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(le_43, (8, 864, 21, 21), (381024, 1, 18144, 864))
    assert_size_stride(unsqueeze_1322, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1334, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(le_45, (8, 864, 21, 21), (381024, 1, 18144, 864))
    assert_size_stride(unsqueeze_1346, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1358, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1370, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1382, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1394, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1406, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1418, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1430, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1442, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1454, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1466, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1478, (1, 864, 1, 1), (864, 1, 1, 1))
    assert_size_stride(unsqueeze_1490, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1502, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1514, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1526, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1538, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1550, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1562, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1574, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1586, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1598, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1610, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1622, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1634, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1646, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1658, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1670, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1682, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1694, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1706, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1718, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1730, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1742, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1754, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1766, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1778, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1790, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1802, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1814, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1826, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1838, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1850, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1862, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1874, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1886, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1898, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1910, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1922, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1934, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1946, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1958, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1970, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1982, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_1994, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(le_100, (8, 432, 42, 42), (762048, 1, 18144, 432))
    assert_size_stride(unsqueeze_2006, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2018, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(le_102, (8, 432, 42, 42), (762048, 1, 18144, 432))
    assert_size_stride(unsqueeze_2030, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2042, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2054, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2066, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2078, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2090, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2102, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2114, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2126, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2138, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2150, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2162, (1, 432, 1, 1), (432, 1, 1, 1))
    assert_size_stride(unsqueeze_2174, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2186, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2198, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2210, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2222, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2234, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2246, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2258, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2270, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2282, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2294, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2306, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2318, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2330, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2342, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2354, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2366, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2378, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2390, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2402, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2414, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2426, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2438, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2450, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2462, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2474, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2486, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2498, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2510, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2522, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2534, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2546, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2558, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2570, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2582, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2594, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2606, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2618, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2630, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2642, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2654, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2666, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2678, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2690, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2702, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2714, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2726, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2738, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2750, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2762, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2774, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2786, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2798, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2810, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2822, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2834, (1, 216, 1, 1), (216, 1, 1, 1))
    assert_size_stride(unsqueeze_2846, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(le_171, (8, 108, 83, 83), (744012, 1, 8964, 108))
    assert_size_stride(unsqueeze_2858, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_2870, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(le_173, (8, 108, 83, 83), (744012, 1, 8964, 108))
    assert_size_stride(unsqueeze_2882, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_2894, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_2906, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_2918, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_2930, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_2942, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_2954, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_2966, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_2978, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_2990, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_3002, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_3014, (1, 108, 1, 1), (108, 1, 1, 1))
    assert_size_stride(unsqueeze_3026, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(le_186, (8, 54, 165, 165), (1470150, 1, 8910, 54))
    assert_size_stride(unsqueeze_3038, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3050, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3062, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3074, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3086, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3098, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3110, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3122, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3134, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3146, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3158, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3170, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3182, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3194, (1, 54, 1, 1), (54, 1, 1, 1))
    assert_size_stride(unsqueeze_3206, (1, 96, 1, 1), (96, 1, 1, 1))
    assert_size_stride(tangents_1, (8, 1000), (1000, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0) # no-op to ensure context
        buf0 = empty((8, 4320), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.mm]
        extern_kernels.mm(tangents_1, permute_1, out=buf0)
        del permute_1
        buf1 = empty((1000, 4320), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.mm]
        extern_kernels.mm(reinterpret_tensor(tangents_1, (1000, 8), (1, 1000), 0), clone, out=buf1)
        del clone
        buf2 = empty((1, 1000), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.sum]
        stream0 = get_cuda_stream(0)
        triton_per_fused_sum_0.run(tangents_1, buf2, 1000, 8, grid=grid(1000), stream=stream0)
        del tangents_1
        buf3 = empty_strided((8, 4320, 11, 11), (522720, 1, 47520, 4320), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.div, aten.threshold_backward]
        triton_poi_fused_div_threshold_backward_1.run(le, buf0, buf3, 4181760, grid=grid(4181760), stream=stream0)
        del buf0
        del le
        buf4 = empty_strided((864, 8), (1, 864), device='cuda', dtype=torch.float32)
        buf6 = empty_strided((864, 8), (1, 864), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_2.run(buf3, convolution_372, unsqueeze_806, buf4, buf6, 6912, 121, grid=grid(6912), stream=stream0)
        buf5 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf4, buf5, 864, 8, grid=grid(864), stream=stream0)
        buf7 = empty((864, ), device='cuda', dtype=torch.float32)
        buf8 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf6, squeeze_601, buf7, buf8, 864, 8, grid=grid(864), stream=stream0)
        buf9 = empty_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_5.run(buf3, convolution_372, unsqueeze_806, buf7, squeeze_601, buf5, primals_774, buf9, 836352, grid=grid(836352), stream=stream0)
        del convolution_372
        del primals_774
        del squeeze_601
        del unsqueeze_806
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf10 = aten.convolution_backward(buf9, convolution_371, primals_773, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf9
        del convolution_371
        del primals_773
        buf11 = buf10[0]
        buf12 = buf10[1]
        del buf10
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf13 = aten.convolution_backward(buf11, relu_198, primals_772, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_772
        buf14 = buf13[0]
        buf15 = buf13[1]
        del buf13
        buf16 = buf6; del buf6  # reuse
        buf18 = buf4; del buf4  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_198, buf14, convolution_370, unsqueeze_818, buf16, buf18, 6912, 121, grid=grid(6912), stream=stream0)
        buf17 = buf7; del buf7  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf16, buf17, 864, 8, grid=grid(864), stream=stream0)
        buf19 = empty((864, ), device='cuda', dtype=torch.float32)
        buf20 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf18, squeeze_598, buf19, buf20, 864, 8, grid=grid(864), stream=stream0)
        buf21 = reinterpret_tensor(buf11, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf11  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_198, buf14, convolution_370, unsqueeze_818, buf19, squeeze_598, buf17, primals_770, buf21, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf14
        del convolution_370
        del primals_770
        del relu_198
        del squeeze_598
        del unsqueeze_818
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf22 = aten.convolution_backward(buf21, convolution_369, primals_769, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf21
        del convolution_369
        del primals_769
        buf23 = buf22[0]
        buf24 = buf22[1]
        del buf22
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf25 = aten.convolution_backward(buf23, relu_187, primals_768, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_768
        buf26 = buf25[0]
        buf27 = buf25[1]
        del buf25
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf28 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf3, (8, 864, 11, 11), (522720, 1, 47520, 4320), 2592), add_1009, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_467)
        buf29 = buf28
        del buf28
        buf30 = buf18; del buf18  # reuse
        buf32 = buf16; del buf16  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_8.run(buf3, convolution_368, unsqueeze_830, buf30, buf32, 6912, 121, grid=grid(6912), stream=stream0)
        buf31 = buf19; del buf19  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf30, buf31, 864, 8, grid=grid(864), stream=stream0)
        buf33 = empty((864, ), device='cuda', dtype=torch.float32)
        buf34 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf32, squeeze_595, buf33, buf34, 864, 8, grid=grid(864), stream=stream0)
        buf35 = reinterpret_tensor(buf23, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf23  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_9.run(buf3, convolution_368, unsqueeze_830, buf33, squeeze_595, buf31, primals_766, buf35, 836352, grid=grid(836352), stream=stream0)
        del convolution_368
        del primals_766
        del squeeze_595
        del unsqueeze_830
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf36 = aten.convolution_backward(buf35, convolution_367, primals_765, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf35
        del convolution_367
        del primals_765
        buf37 = buf36[0]
        buf38 = buf36[1]
        del buf36
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf39 = aten.convolution_backward(buf37, relu_196, primals_764, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_764
        buf40 = buf39[0]
        buf41 = buf39[1]
        del buf39
        buf42 = buf32; del buf32  # reuse
        buf44 = buf30; del buf30  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_196, buf40, convolution_366, unsqueeze_842, buf42, buf44, 6912, 121, grid=grid(6912), stream=stream0)
        buf43 = buf33; del buf33  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf42, buf43, 864, 8, grid=grid(864), stream=stream0)
        buf45 = empty((864, ), device='cuda', dtype=torch.float32)
        buf46 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf44, squeeze_592, buf45, buf46, 864, 8, grid=grid(864), stream=stream0)
        buf47 = reinterpret_tensor(buf37, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf37  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_196, buf40, convolution_366, unsqueeze_842, buf45, squeeze_592, buf43, primals_762, buf47, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf40
        del convolution_366
        del primals_762
        del relu_196
        del squeeze_592
        del unsqueeze_842
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf48 = aten.convolution_backward(buf47, convolution_365, primals_761, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_365
        del primals_761
        buf49 = buf48[0]
        buf50 = buf48[1]
        del buf48
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf51 = aten.convolution_backward(buf49, relu_195, primals_760, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_760
        buf52 = buf51[0]
        buf53 = buf51[1]
        del buf51
        buf54 = buf44; del buf44  # reuse
        buf56 = buf42; del buf42  # reuse
        buf78 = empty_strided((864, 8), (1, 864), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_10.run(buf3, relu_195, buf52, convolution_364, unsqueeze_854, convolution_360, unsqueeze_878, buf54, buf56, buf78, 6912, 121, grid=grid(6912), stream=stream0)
        buf55 = buf45; del buf45  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf54, buf55, 864, 8, grid=grid(864), stream=stream0)
        buf57 = empty((864, ), device='cuda', dtype=torch.float32)
        buf59 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf56, squeeze_589, buf57, buf59, 864, 8, grid=grid(864), stream=stream0)
        buf79 = empty((864, ), device='cuda', dtype=torch.float32)
        buf81 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf78, squeeze_583, buf79, buf81, 864, 8, grid=grid(864), stream=stream0)
        buf58 = reinterpret_tensor(buf49, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf49  # reuse
        buf80 = buf47; del buf47  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_11.run(buf3, relu_195, buf52, convolution_364, unsqueeze_854, buf57, squeeze_589, buf55, primals_758, convolution_360, unsqueeze_878, buf79, squeeze_583, primals_750, buf58, buf80, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf52
        del convolution_360
        del convolution_364
        del primals_750
        del primals_758
        del relu_195
        del squeeze_583
        del squeeze_589
        del unsqueeze_854
        del unsqueeze_878
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf60 = aten.convolution_backward(buf58, convolution_363, primals_757, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf58
        del convolution_363
        del primals_757
        buf61 = buf60[0]
        buf62 = buf60[1]
        del buf60
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf63 = aten.convolution_backward(buf61, relu_194, primals_756, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_756
        buf64 = buf63[0]
        buf65 = buf63[1]
        del buf63
        buf66 = buf78; del buf78  # reuse
        buf68 = buf56; del buf56  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_194, buf64, convolution_362, unsqueeze_866, buf66, buf68, 6912, 121, grid=grid(6912), stream=stream0)
        buf67 = buf79; del buf79  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf66, buf67, 864, 8, grid=grid(864), stream=stream0)
        buf69 = buf57; del buf57  # reuse
        buf70 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf68, squeeze_586, buf69, buf70, 864, 8, grid=grid(864), stream=stream0)
        buf71 = reinterpret_tensor(buf61, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf61  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_194, buf64, convolution_362, unsqueeze_866, buf69, squeeze_586, buf67, primals_754, buf71, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf64
        del convolution_362
        del primals_754
        del relu_194
        del squeeze_586
        del unsqueeze_866
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf72 = aten.convolution_backward(buf71, convolution_361, primals_753, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf71
        del convolution_361
        del primals_753
        buf73 = buf72[0]
        buf74 = buf72[1]
        del buf72
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf75 = aten.convolution_backward(buf73, relu_189, primals_752, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del buf73
        del primals_752
        buf76 = buf75[0]
        buf77 = buf75[1]
        del buf75
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf82 = aten.convolution_backward(buf80, convolution_359, primals_749, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf80
        del convolution_359
        del primals_749
        buf83 = buf82[0]
        buf84 = buf82[1]
        del buf82
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf85 = aten.convolution_backward(buf83, relu_192, primals_748, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_748
        buf86 = buf85[0]
        buf87 = buf85[1]
        del buf85
        buf88 = buf68; del buf68  # reuse
        buf90 = buf66; del buf66  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_192, buf86, convolution_358, unsqueeze_890, buf88, buf90, 6912, 121, grid=grid(6912), stream=stream0)
        buf89 = buf69; del buf69  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf88, buf89, 864, 8, grid=grid(864), stream=stream0)
        buf91 = empty((864, ), device='cuda', dtype=torch.float32)
        buf92 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf90, squeeze_580, buf91, buf92, 864, 8, grid=grid(864), stream=stream0)
        buf93 = reinterpret_tensor(buf83, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf83  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_192, buf86, convolution_358, unsqueeze_890, buf91, squeeze_580, buf89, primals_746, buf93, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf86
        del convolution_358
        del primals_746
        del relu_192
        del squeeze_580
        del unsqueeze_890
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf94 = aten.convolution_backward(buf93, convolution_357, primals_745, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf93
        del convolution_357
        del primals_745
        buf95 = buf94[0]
        buf96 = buf94[1]
        del buf94
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf97 = aten.convolution_backward(buf95, relu_189, primals_744, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_744
        buf98 = buf97[0]
        buf99 = buf97[1]
        del buf97
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf100 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf3, (8, 864, 11, 11), (522720, 1, 47520, 4320), 864), add_1009, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_467)
        del add_1009
        del getitem_467
        buf101 = buf100
        del buf100
        buf102 = buf90; del buf90  # reuse
        buf104 = buf88; del buf88  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_12.run(buf3, convolution_356, unsqueeze_902, buf102, buf104, 6912, 121, grid=grid(6912), stream=stream0)
        buf103 = buf91; del buf91  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf102, buf103, 864, 8, grid=grid(864), stream=stream0)
        buf105 = empty((864, ), device='cuda', dtype=torch.float32)
        buf106 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf104, squeeze_577, buf105, buf106, 864, 8, grid=grid(864), stream=stream0)
        buf107 = reinterpret_tensor(buf95, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf95  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_13.run(buf3, convolution_356, unsqueeze_902, buf105, squeeze_577, buf103, primals_742, buf107, 836352, grid=grid(836352), stream=stream0)
        del convolution_356
        del primals_742
        del squeeze_577
        del unsqueeze_902
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf108 = aten.convolution_backward(buf107, convolution_355, primals_741, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf107
        del convolution_355
        del primals_741
        buf109 = buf108[0]
        buf110 = buf108[1]
        del buf108
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf111 = aten.convolution_backward(buf109, relu_190, primals_740, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_740
        buf112 = buf111[0]
        buf113 = buf111[1]
        del buf111
        buf114 = buf104; del buf104  # reuse
        buf116 = buf102; del buf102  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_190, buf112, convolution_354, unsqueeze_914, buf114, buf116, 6912, 121, grid=grid(6912), stream=stream0)
        buf115 = buf105; del buf105  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf114, buf115, 864, 8, grid=grid(864), stream=stream0)
        buf117 = empty((864, ), device='cuda', dtype=torch.float32)
        buf118 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf116, squeeze_574, buf117, buf118, 864, 8, grid=grid(864), stream=stream0)
        buf119 = reinterpret_tensor(buf109, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf109  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_190, buf112, convolution_354, unsqueeze_914, buf117, squeeze_574, buf115, primals_738, buf119, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf112
        del convolution_354
        del primals_738
        del relu_190
        del squeeze_574
        del unsqueeze_914
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf120 = aten.convolution_backward(buf119, convolution_353, primals_737, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf119
        del convolution_353
        del primals_737
        buf121 = buf120[0]
        buf122 = buf120[1]
        del buf120
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf123 = aten.convolution_backward(buf121, relu_189, primals_736, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 864, [True, True, False])
        del buf121
        del primals_736
        buf124 = buf123[0]
        buf125 = buf123[1]
        del buf123
        buf126 = buf101; del buf101  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_14.run(buf126, buf3, buf29, relu_189, buf76, buf98, buf124, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf124
        del buf29
        del buf76
        del relu_189
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf127 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf3, (8, 864, 11, 11), (522720, 1, 47520, 4320), 0), add_1004, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_461)
        del add_1004
        del getitem_461
        buf128 = buf127
        del buf127
        buf129 = buf116; del buf116  # reuse
        buf131 = buf114; del buf114  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_15.run(buf3, convolution_352, unsqueeze_926, buf129, buf131, 6912, 121, grid=grid(6912), stream=stream0)
        buf130 = buf117; del buf117  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf129, buf130, 864, 8, grid=grid(864), stream=stream0)
        buf132 = empty((864, ), device='cuda', dtype=torch.float32)
        buf133 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf131, squeeze_571, buf132, buf133, 864, 8, grid=grid(864), stream=stream0)
        buf134 = reinterpret_tensor(buf98, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf98  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_16.run(buf3, convolution_352, unsqueeze_926, buf132, squeeze_571, buf130, primals_734, buf134, 836352, grid=grid(836352), stream=stream0)
        del buf3
        del convolution_352
        del primals_734
        del squeeze_571
        del unsqueeze_926
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf135 = aten.convolution_backward(buf134, convolution_351, primals_733, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf134
        del convolution_351
        del primals_733
        buf136 = buf135[0]
        buf137 = buf135[1]
        del buf135
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf138 = aten.convolution_backward(buf136, relu_188, primals_732, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_732
        buf139 = buf138[0]
        buf140 = buf138[1]
        del buf138
        buf141 = buf131; del buf131  # reuse
        buf143 = buf129; del buf129  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_188, buf139, convolution_350, unsqueeze_938, buf141, buf143, 6912, 121, grid=grid(6912), stream=stream0)
        buf142 = buf132; del buf132  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf141, buf142, 864, 8, grid=grid(864), stream=stream0)
        buf144 = empty((864, ), device='cuda', dtype=torch.float32)
        buf145 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf143, squeeze_568, buf144, buf145, 864, 8, grid=grid(864), stream=stream0)
        buf146 = reinterpret_tensor(buf136, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf136  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_188, buf139, convolution_350, unsqueeze_938, buf144, squeeze_568, buf142, primals_730, buf146, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf139
        del convolution_350
        del primals_730
        del relu_188
        del squeeze_568
        del unsqueeze_938
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf147 = aten.convolution_backward(buf146, convolution_349, primals_729, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf146
        del convolution_349
        del primals_729
        buf148 = buf147[0]
        buf149 = buf147[1]
        del buf147
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf150 = aten.convolution_backward(buf148, relu_187, primals_728, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del buf148
        del primals_728
        buf151 = buf150[0]
        buf152 = buf150[1]
        del buf150
        buf153 = buf143; del buf143  # reuse
        buf155 = buf141; del buf141  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_17.run(buf126, convolution_348, unsqueeze_950, buf153, buf155, 6912, 121, grid=grid(6912), stream=stream0)
        buf154 = buf144; del buf144  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf153, buf154, 864, 8, grid=grid(864), stream=stream0)
        buf156 = empty((864, ), device='cuda', dtype=torch.float32)
        buf157 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf155, squeeze_565, buf156, buf157, 864, 8, grid=grid(864), stream=stream0)
        buf158 = buf126; del buf126  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_18.run(buf158, convolution_348, unsqueeze_950, buf156, squeeze_565, buf154, primals_726, 836352, grid=grid(836352), stream=stream0)
        del convolution_348
        del primals_726
        del squeeze_565
        del unsqueeze_950
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf159 = aten.convolution_backward(buf158, relu_186, primals_725, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf158
        del primals_725
        buf160 = buf159[0]
        buf161 = buf159[1]
        del buf159
        buf162 = buf155; del buf155  # reuse
        buf164 = buf153; del buf153  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_19.run(relu_187, buf26, buf128, buf151, convolution_347, unsqueeze_962, buf162, buf164, 6912, 121, grid=grid(6912), stream=stream0)
        buf163 = buf156; del buf156  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf162, buf163, 864, 8, grid=grid(864), stream=stream0)
        buf165 = empty((864, ), device='cuda', dtype=torch.float32)
        buf167 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf164, squeeze_562, buf165, buf167, 864, 8, grid=grid(864), stream=stream0)
        buf166 = buf128; del buf128  # reuse
        buf168 = buf166; del buf166  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_20.run(buf168, relu_187, buf26, buf151, convolution_347, unsqueeze_962, buf165, squeeze_562, buf163, primals_723, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf151
        del buf26
        del convolution_347
        del primals_723
        del relu_187
        del squeeze_562
        del unsqueeze_962
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf169 = aten.convolution_backward(buf168, relu_172, primals_722, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_722
        buf170 = buf169[0]
        buf171 = buf169[1]
        del buf169
        buf172 = buf160; del buf160  # reuse
        # Source Nodes: [], Original ATen: [aten.threshold_backward]
        triton_poi_fused_threshold_backward_21.run(buf172, relu_186, 34560, 121, grid=grid(34560, 121), stream=stream0)
        del relu_186
        buf173 = buf165; del buf165  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_22.run(buf172, buf173, 864, 968, grid=grid(864), stream=stream0)
        buf174 = buf164; del buf164  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_23.run(buf172, convolution_346, unsqueeze_974, buf174, 6912, 121, grid=grid(6912), stream=stream0)
        buf175 = empty((864, ), device='cuda', dtype=torch.float32)
        buf176 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf174, squeeze_559, buf175, buf176, 864, 8, grid=grid(864), stream=stream0)
        buf177 = buf168; del buf168  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_24.run(buf172, convolution_346, unsqueeze_974, buf175, squeeze_559, buf173, primals_720, buf177, 968, 864, grid=grid(968, 864), stream=stream0)
        del convolution_346
        del primals_720
        del squeeze_559
        del unsqueeze_974
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf178 = aten.convolution_backward(buf177, convolution_345, primals_719, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf177
        del convolution_345
        del primals_719
        buf179 = buf178[0]
        buf180 = buf178[1]
        del buf178
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf181 = aten.convolution_backward(buf179, relu_184, primals_718, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_718
        buf182 = buf181[0]
        buf183 = buf181[1]
        del buf181
        buf184 = buf174; del buf174  # reuse
        buf186 = buf162; del buf162  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_184, buf182, convolution_344, unsqueeze_986, buf184, buf186, 6912, 121, grid=grid(6912), stream=stream0)
        buf185 = buf175; del buf175  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf184, buf185, 864, 8, grid=grid(864), stream=stream0)
        buf187 = empty((864, ), device='cuda', dtype=torch.float32)
        buf188 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf186, squeeze_556, buf187, buf188, 864, 8, grid=grid(864), stream=stream0)
        buf189 = reinterpret_tensor(buf179, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf179  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_184, buf182, convolution_344, unsqueeze_986, buf187, squeeze_556, buf185, primals_716, buf189, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf182
        del convolution_344
        del primals_716
        del relu_184
        del squeeze_556
        del unsqueeze_986
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf190 = aten.convolution_backward(buf189, convolution_343, primals_715, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf189
        del convolution_343
        del primals_715
        buf191 = buf190[0]
        buf192 = buf190[1]
        del buf190
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf193 = aten.convolution_backward(buf191, relu_173, primals_714, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_714
        buf194 = buf193[0]
        buf195 = buf193[1]
        del buf193
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf196 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf172, (8, 864, 11, 11), (522720, 121, 11, 1), 313632), add_934, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_433)
        buf197 = buf196
        del buf196
        buf198 = buf187; del buf187  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_25.run(buf172, buf198, 864, 968, grid=grid(864), stream=stream0)
        buf199 = buf186; del buf186  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_26.run(buf172, convolution_342, unsqueeze_998, buf199, 6912, 121, grid=grid(6912), stream=stream0)
        buf200 = empty((864, ), device='cuda', dtype=torch.float32)
        buf201 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf199, squeeze_553, buf200, buf201, 864, 8, grid=grid(864), stream=stream0)
        buf202 = reinterpret_tensor(buf191, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf191  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_27.run(buf172, convolution_342, unsqueeze_998, buf200, squeeze_553, buf198, primals_712, buf202, 968, 864, grid=grid(968, 864), stream=stream0)
        del convolution_342
        del primals_712
        del squeeze_553
        del unsqueeze_998
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf203 = aten.convolution_backward(buf202, convolution_341, primals_711, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf202
        del convolution_341
        del primals_711
        buf204 = buf203[0]
        buf205 = buf203[1]
        del buf203
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf206 = aten.convolution_backward(buf204, relu_182, primals_710, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_710
        buf207 = buf206[0]
        buf208 = buf206[1]
        del buf206
        buf209 = buf199; del buf199  # reuse
        buf211 = buf184; del buf184  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_182, buf207, convolution_340, unsqueeze_1010, buf209, buf211, 6912, 121, grid=grid(6912), stream=stream0)
        buf210 = buf200; del buf200  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf209, buf210, 864, 8, grid=grid(864), stream=stream0)
        buf212 = empty((864, ), device='cuda', dtype=torch.float32)
        buf213 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf211, squeeze_550, buf212, buf213, 864, 8, grid=grid(864), stream=stream0)
        buf214 = reinterpret_tensor(buf204, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf204  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_182, buf207, convolution_340, unsqueeze_1010, buf212, squeeze_550, buf210, primals_708, buf214, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf207
        del convolution_340
        del primals_708
        del relu_182
        del squeeze_550
        del unsqueeze_1010
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf215 = aten.convolution_backward(buf214, convolution_339, primals_707, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_339
        del primals_707
        buf216 = buf215[0]
        buf217 = buf215[1]
        del buf215
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf218 = aten.convolution_backward(buf216, relu_181, primals_706, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_706
        buf219 = buf218[0]
        buf220 = buf218[1]
        del buf218
        buf221 = buf212; del buf212  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_28.run(buf172, relu_181, buf219, buf221, 864, 968, grid=grid(864), stream=stream0)
        buf222 = buf211; del buf211  # reuse
        buf244 = buf209; del buf209  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_29.run(buf172, relu_181, buf219, convolution_338, unsqueeze_1022, convolution_334, unsqueeze_1046, buf222, buf244, 6912, 121, grid=grid(6912), stream=stream0)
        buf223 = empty((864, ), device='cuda', dtype=torch.float32)
        buf225 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf222, squeeze_547, buf223, buf225, 864, 8, grid=grid(864), stream=stream0)
        buf245 = empty((864, ), device='cuda', dtype=torch.float32)
        buf247 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf244, squeeze_541, buf245, buf247, 864, 8, grid=grid(864), stream=stream0)
        buf224 = reinterpret_tensor(buf216, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf216  # reuse
        buf246 = buf214; del buf214  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_30.run(buf172, relu_181, buf219, convolution_338, unsqueeze_1022, buf223, squeeze_547, buf221, primals_704, convolution_334, unsqueeze_1046, buf245, squeeze_541, primals_696, buf224, buf246, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf219
        del convolution_334
        del convolution_338
        del primals_696
        del primals_704
        del relu_181
        del squeeze_541
        del squeeze_547
        del unsqueeze_1022
        del unsqueeze_1046
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf226 = aten.convolution_backward(buf224, convolution_337, primals_703, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf224
        del convolution_337
        del primals_703
        buf227 = buf226[0]
        buf228 = buf226[1]
        del buf226
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf229 = aten.convolution_backward(buf227, relu_180, primals_702, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_702
        buf230 = buf229[0]
        buf231 = buf229[1]
        del buf229
        buf232 = buf244; del buf244  # reuse
        buf234 = buf222; del buf222  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_180, buf230, convolution_336, unsqueeze_1034, buf232, buf234, 6912, 121, grid=grid(6912), stream=stream0)
        buf233 = buf245; del buf245  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf232, buf233, 864, 8, grid=grid(864), stream=stream0)
        buf235 = buf223; del buf223  # reuse
        buf236 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf234, squeeze_544, buf235, buf236, 864, 8, grid=grid(864), stream=stream0)
        buf237 = reinterpret_tensor(buf227, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf227  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_180, buf230, convolution_336, unsqueeze_1034, buf235, squeeze_544, buf233, primals_700, buf237, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf230
        del convolution_336
        del primals_700
        del relu_180
        del squeeze_544
        del unsqueeze_1034
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf238 = aten.convolution_backward(buf237, convolution_335, primals_699, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf237
        del convolution_335
        del primals_699
        buf239 = buf238[0]
        buf240 = buf238[1]
        del buf238
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf241 = aten.convolution_backward(buf239, relu_175, primals_698, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del buf239
        del primals_698
        buf242 = buf241[0]
        buf243 = buf241[1]
        del buf241
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf248 = aten.convolution_backward(buf246, convolution_333, primals_695, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf246
        del convolution_333
        del primals_695
        buf249 = buf248[0]
        buf250 = buf248[1]
        del buf248
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf251 = aten.convolution_backward(buf249, relu_178, primals_694, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_694
        buf252 = buf251[0]
        buf253 = buf251[1]
        del buf251
        buf254 = buf234; del buf234  # reuse
        buf256 = buf232; del buf232  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_178, buf252, convolution_332, unsqueeze_1058, buf254, buf256, 6912, 121, grid=grid(6912), stream=stream0)
        buf255 = buf235; del buf235  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf254, buf255, 864, 8, grid=grid(864), stream=stream0)
        buf257 = empty((864, ), device='cuda', dtype=torch.float32)
        buf258 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf256, squeeze_538, buf257, buf258, 864, 8, grid=grid(864), stream=stream0)
        buf259 = reinterpret_tensor(buf249, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf249  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_178, buf252, convolution_332, unsqueeze_1058, buf257, squeeze_538, buf255, primals_692, buf259, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf252
        del convolution_332
        del primals_692
        del relu_178
        del squeeze_538
        del unsqueeze_1058
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf260 = aten.convolution_backward(buf259, convolution_331, primals_691, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf259
        del convolution_331
        del primals_691
        buf261 = buf260[0]
        buf262 = buf260[1]
        del buf260
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf263 = aten.convolution_backward(buf261, relu_175, primals_690, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_690
        buf264 = buf263[0]
        buf265 = buf263[1]
        del buf263
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf266 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf172, (8, 864, 11, 11), (522720, 121, 11, 1), 104544), add_934, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_433)
        del add_934
        del getitem_433
        buf267 = buf266
        del buf266
        buf268 = buf257; del buf257  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_31.run(buf172, buf268, 864, 968, grid=grid(864), stream=stream0)
        buf269 = buf256; del buf256  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_32.run(buf172, convolution_330, unsqueeze_1070, buf269, 6912, 121, grid=grid(6912), stream=stream0)
        buf270 = empty((864, ), device='cuda', dtype=torch.float32)
        buf271 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf269, squeeze_535, buf270, buf271, 864, 8, grid=grid(864), stream=stream0)
        buf272 = reinterpret_tensor(buf261, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf261  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_33.run(buf172, convolution_330, unsqueeze_1070, buf270, squeeze_535, buf268, primals_688, buf272, 968, 864, grid=grid(968, 864), stream=stream0)
        del convolution_330
        del primals_688
        del squeeze_535
        del unsqueeze_1070
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf273 = aten.convolution_backward(buf272, convolution_329, primals_687, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf272
        del convolution_329
        del primals_687
        buf274 = buf273[0]
        buf275 = buf273[1]
        del buf273
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf276 = aten.convolution_backward(buf274, relu_176, primals_686, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_686
        buf277 = buf276[0]
        buf278 = buf276[1]
        del buf276
        buf279 = buf269; del buf269  # reuse
        buf281 = buf254; del buf254  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_176, buf277, convolution_328, unsqueeze_1082, buf279, buf281, 6912, 121, grid=grid(6912), stream=stream0)
        buf280 = buf270; del buf270  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf279, buf280, 864, 8, grid=grid(864), stream=stream0)
        buf282 = empty((864, ), device='cuda', dtype=torch.float32)
        buf283 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf281, squeeze_532, buf282, buf283, 864, 8, grid=grid(864), stream=stream0)
        buf284 = reinterpret_tensor(buf274, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf274  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_176, buf277, convolution_328, unsqueeze_1082, buf282, squeeze_532, buf280, primals_684, buf284, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf277
        del convolution_328
        del primals_684
        del relu_176
        del squeeze_532
        del unsqueeze_1082
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf285 = aten.convolution_backward(buf284, convolution_327, primals_683, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf284
        del convolution_327
        del primals_683
        buf286 = buf285[0]
        buf287 = buf285[1]
        del buf285
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf288 = aten.convolution_backward(buf286, relu_175, primals_682, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 864, [True, True, False])
        del buf286
        del primals_682
        buf289 = buf288[0]
        buf290 = buf288[1]
        del buf288
        buf291 = buf242; del buf242  # reuse
        buf318 = buf281; del buf281  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_34.run(buf291, buf172, buf197, relu_175, buf264, buf267, buf289, convolution_322, unsqueeze_1118, buf318, 6912, 121, grid=grid(6912), stream=stream0)
        del buf197
        del buf264
        del buf267
        del relu_175
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf292 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf172, (8, 864, 11, 11), (522720, 121, 11, 1), 0), add_929, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_427)
        del add_929
        del getitem_427
        buf293 = buf292
        del buf292
        buf294 = buf282; del buf282  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_35.run(buf172, buf294, 864, 968, grid=grid(864), stream=stream0)
        buf295 = buf279; del buf279  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_36.run(buf172, convolution_326, unsqueeze_1094, buf295, 6912, 121, grid=grid(6912), stream=stream0)
        buf296 = empty((864, ), device='cuda', dtype=torch.float32)
        buf297 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf295, squeeze_529, buf296, buf297, 864, 8, grid=grid(864), stream=stream0)
        buf298 = reinterpret_tensor(buf289, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf289  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_37.run(buf172, convolution_326, unsqueeze_1094, buf296, squeeze_529, buf294, primals_680, buf298, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf172
        del convolution_326
        del primals_680
        del squeeze_529
        del unsqueeze_1094
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf299 = aten.convolution_backward(buf298, convolution_325, primals_679, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf298
        del convolution_325
        del primals_679
        buf300 = buf299[0]
        buf301 = buf299[1]
        del buf299
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf302 = aten.convolution_backward(buf300, relu_174, primals_678, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_678
        buf303 = buf302[0]
        buf304 = buf302[1]
        del buf302
        buf305 = buf295; del buf295  # reuse
        buf307 = buf54; del buf54  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_174, buf303, convolution_324, unsqueeze_1106, buf305, buf307, 6912, 121, grid=grid(6912), stream=stream0)
        buf306 = buf296; del buf296  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf305, buf306, 864, 8, grid=grid(864), stream=stream0)
        buf308 = empty((864, ), device='cuda', dtype=torch.float32)
        buf309 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf307, squeeze_526, buf308, buf309, 864, 8, grid=grid(864), stream=stream0)
        buf310 = reinterpret_tensor(buf300, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf300  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_174, buf303, convolution_324, unsqueeze_1106, buf308, squeeze_526, buf306, primals_676, buf310, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf303
        del convolution_324
        del primals_676
        del relu_174
        del squeeze_526
        del unsqueeze_1106
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf311 = aten.convolution_backward(buf310, convolution_323, primals_675, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf310
        del convolution_323
        del primals_675
        buf312 = buf311[0]
        buf313 = buf311[1]
        del buf311
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf314 = aten.convolution_backward(buf312, relu_173, primals_674, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_674
        buf315 = buf314[0]
        buf316 = buf314[1]
        del buf314
        buf317 = buf308; del buf308  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_38.run(buf291, buf317, 864, 968, grid=grid(864), stream=stream0)
        buf319 = empty((864, ), device='cuda', dtype=torch.float32)
        buf320 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf318, squeeze_523, buf319, buf320, 864, 8, grid=grid(864), stream=stream0)
        buf321 = reinterpret_tensor(buf312, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf312  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_39.run(buf291, convolution_322, unsqueeze_1118, buf319, squeeze_523, buf317, primals_672, buf321, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf291
        del convolution_322
        del primals_672
        del squeeze_523
        del unsqueeze_1118
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf322 = aten.convolution_backward(buf321, relu_172, primals_671, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf321
        del primals_671
        buf323 = buf322[0]
        buf324 = buf322[1]
        del buf322
        buf325 = buf318; del buf318  # reuse
        buf327 = buf307; del buf307  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_19.run(relu_173, buf194, buf293, buf315, convolution_321, unsqueeze_1130, buf325, buf327, 6912, 121, grid=grid(6912), stream=stream0)
        buf326 = buf319; del buf319  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf325, buf326, 864, 8, grid=grid(864), stream=stream0)
        buf328 = empty((864, ), device='cuda', dtype=torch.float32)
        buf330 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf327, squeeze_520, buf328, buf330, 864, 8, grid=grid(864), stream=stream0)
        buf329 = buf293; del buf293  # reuse
        buf331 = buf329; del buf329  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_20.run(buf331, relu_173, buf194, buf315, convolution_321, unsqueeze_1130, buf328, squeeze_520, buf326, primals_669, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf194
        del buf315
        del convolution_321
        del primals_669
        del relu_173
        del squeeze_520
        del unsqueeze_1130
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf332 = aten.convolution_backward(buf331, relu_158, primals_668, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_668
        buf333 = buf332[0]
        buf334 = buf332[1]
        del buf332
        buf335 = buf170; del buf170  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_40.run(buf335, relu_172, buf323, 34560, 121, grid=grid(34560, 121), stream=stream0)
        del buf323
        del relu_172
        buf336 = buf328; del buf328  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_22.run(buf335, buf336, 864, 968, grid=grid(864), stream=stream0)
        buf337 = buf327; del buf327  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_23.run(buf335, convolution_320, unsqueeze_1142, buf337, 6912, 121, grid=grid(6912), stream=stream0)
        buf338 = empty((864, ), device='cuda', dtype=torch.float32)
        buf339 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf337, squeeze_517, buf338, buf339, 864, 8, grid=grid(864), stream=stream0)
        buf340 = buf331; del buf331  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_24.run(buf335, convolution_320, unsqueeze_1142, buf338, squeeze_517, buf336, primals_666, buf340, 968, 864, grid=grid(968, 864), stream=stream0)
        del convolution_320
        del primals_666
        del squeeze_517
        del unsqueeze_1142
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf341 = aten.convolution_backward(buf340, convolution_319, primals_665, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf340
        del convolution_319
        del primals_665
        buf342 = buf341[0]
        buf343 = buf341[1]
        del buf341
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf344 = aten.convolution_backward(buf342, relu_170, primals_664, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_664
        buf345 = buf344[0]
        buf346 = buf344[1]
        del buf344
        buf347 = buf337; del buf337  # reuse
        buf349 = buf325; del buf325  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_170, buf345, convolution_318, unsqueeze_1154, buf347, buf349, 6912, 121, grid=grid(6912), stream=stream0)
        buf348 = buf338; del buf338  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf347, buf348, 864, 8, grid=grid(864), stream=stream0)
        buf350 = empty((864, ), device='cuda', dtype=torch.float32)
        buf351 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf349, squeeze_514, buf350, buf351, 864, 8, grid=grid(864), stream=stream0)
        buf352 = reinterpret_tensor(buf342, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf342  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_170, buf345, convolution_318, unsqueeze_1154, buf350, squeeze_514, buf348, primals_662, buf352, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf345
        del convolution_318
        del primals_662
        del relu_170
        del squeeze_514
        del unsqueeze_1154
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf353 = aten.convolution_backward(buf352, convolution_317, primals_661, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf352
        del convolution_317
        del primals_661
        buf354 = buf353[0]
        buf355 = buf353[1]
        del buf353
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf356 = aten.convolution_backward(buf354, relu_159, primals_660, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_660
        buf357 = buf356[0]
        buf358 = buf356[1]
        del buf356
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf359 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf335, (8, 864, 11, 11), (522720, 121, 11, 1), 313632), add_859, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_399)
        buf360 = buf359
        del buf359
        buf361 = buf350; del buf350  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_25.run(buf335, buf361, 864, 968, grid=grid(864), stream=stream0)
        buf362 = buf349; del buf349  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_26.run(buf335, convolution_316, unsqueeze_1166, buf362, 6912, 121, grid=grid(6912), stream=stream0)
        buf363 = empty((864, ), device='cuda', dtype=torch.float32)
        buf364 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf362, squeeze_511, buf363, buf364, 864, 8, grid=grid(864), stream=stream0)
        buf365 = reinterpret_tensor(buf354, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf354  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_27.run(buf335, convolution_316, unsqueeze_1166, buf363, squeeze_511, buf361, primals_658, buf365, 968, 864, grid=grid(968, 864), stream=stream0)
        del convolution_316
        del primals_658
        del squeeze_511
        del unsqueeze_1166
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf366 = aten.convolution_backward(buf365, convolution_315, primals_657, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf365
        del convolution_315
        del primals_657
        buf367 = buf366[0]
        buf368 = buf366[1]
        del buf366
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf369 = aten.convolution_backward(buf367, relu_168, primals_656, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_656
        buf370 = buf369[0]
        buf371 = buf369[1]
        del buf369
        buf372 = buf362; del buf362  # reuse
        buf374 = buf347; del buf347  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_168, buf370, convolution_314, unsqueeze_1178, buf372, buf374, 6912, 121, grid=grid(6912), stream=stream0)
        buf373 = buf363; del buf363  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf372, buf373, 864, 8, grid=grid(864), stream=stream0)
        buf375 = empty((864, ), device='cuda', dtype=torch.float32)
        buf376 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf374, squeeze_508, buf375, buf376, 864, 8, grid=grid(864), stream=stream0)
        buf377 = reinterpret_tensor(buf367, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf367  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_168, buf370, convolution_314, unsqueeze_1178, buf375, squeeze_508, buf373, primals_654, buf377, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf370
        del convolution_314
        del primals_654
        del relu_168
        del squeeze_508
        del unsqueeze_1178
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf378 = aten.convolution_backward(buf377, convolution_313, primals_653, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_313
        del primals_653
        buf379 = buf378[0]
        buf380 = buf378[1]
        del buf378
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf381 = aten.convolution_backward(buf379, relu_167, primals_652, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_652
        buf382 = buf381[0]
        buf383 = buf381[1]
        del buf381
        buf384 = buf375; del buf375  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_28.run(buf335, relu_167, buf382, buf384, 864, 968, grid=grid(864), stream=stream0)
        buf385 = buf374; del buf374  # reuse
        buf407 = buf372; del buf372  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_29.run(buf335, relu_167, buf382, convolution_312, unsqueeze_1190, convolution_308, unsqueeze_1214, buf385, buf407, 6912, 121, grid=grid(6912), stream=stream0)
        buf386 = empty((864, ), device='cuda', dtype=torch.float32)
        buf388 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf385, squeeze_505, buf386, buf388, 864, 8, grid=grid(864), stream=stream0)
        buf408 = empty((864, ), device='cuda', dtype=torch.float32)
        buf410 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf407, squeeze_499, buf408, buf410, 864, 8, grid=grid(864), stream=stream0)
        buf387 = reinterpret_tensor(buf379, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf379  # reuse
        buf409 = buf377; del buf377  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_30.run(buf335, relu_167, buf382, convolution_312, unsqueeze_1190, buf386, squeeze_505, buf384, primals_650, convolution_308, unsqueeze_1214, buf408, squeeze_499, primals_642, buf387, buf409, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf382
        del convolution_308
        del convolution_312
        del primals_642
        del primals_650
        del relu_167
        del squeeze_499
        del squeeze_505
        del unsqueeze_1190
        del unsqueeze_1214
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf389 = aten.convolution_backward(buf387, convolution_311, primals_649, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf387
        del convolution_311
        del primals_649
        buf390 = buf389[0]
        buf391 = buf389[1]
        del buf389
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf392 = aten.convolution_backward(buf390, relu_166, primals_648, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_648
        buf393 = buf392[0]
        buf394 = buf392[1]
        del buf392
        buf395 = buf407; del buf407  # reuse
        buf397 = buf385; del buf385  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_166, buf393, convolution_310, unsqueeze_1202, buf395, buf397, 6912, 121, grid=grid(6912), stream=stream0)
        buf396 = buf408; del buf408  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf395, buf396, 864, 8, grid=grid(864), stream=stream0)
        buf398 = buf386; del buf386  # reuse
        buf399 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf397, squeeze_502, buf398, buf399, 864, 8, grid=grid(864), stream=stream0)
        buf400 = reinterpret_tensor(buf390, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf390  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_166, buf393, convolution_310, unsqueeze_1202, buf398, squeeze_502, buf396, primals_646, buf400, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf393
        del convolution_310
        del primals_646
        del relu_166
        del squeeze_502
        del unsqueeze_1202
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf401 = aten.convolution_backward(buf400, convolution_309, primals_645, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf400
        del convolution_309
        del primals_645
        buf402 = buf401[0]
        buf403 = buf401[1]
        del buf401
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf404 = aten.convolution_backward(buf402, relu_161, primals_644, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del buf402
        del primals_644
        buf405 = buf404[0]
        buf406 = buf404[1]
        del buf404
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf411 = aten.convolution_backward(buf409, convolution_307, primals_641, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf409
        del convolution_307
        del primals_641
        buf412 = buf411[0]
        buf413 = buf411[1]
        del buf411
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf414 = aten.convolution_backward(buf412, relu_164, primals_640, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_640
        buf415 = buf414[0]
        buf416 = buf414[1]
        del buf414
        buf417 = buf397; del buf397  # reuse
        buf419 = buf395; del buf395  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_164, buf415, convolution_306, unsqueeze_1226, buf417, buf419, 6912, 121, grid=grid(6912), stream=stream0)
        buf418 = buf398; del buf398  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf417, buf418, 864, 8, grid=grid(864), stream=stream0)
        buf420 = empty((864, ), device='cuda', dtype=torch.float32)
        buf421 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf419, squeeze_496, buf420, buf421, 864, 8, grid=grid(864), stream=stream0)
        buf422 = reinterpret_tensor(buf412, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf412  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_164, buf415, convolution_306, unsqueeze_1226, buf420, squeeze_496, buf418, primals_638, buf422, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf415
        del convolution_306
        del primals_638
        del relu_164
        del squeeze_496
        del unsqueeze_1226
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf423 = aten.convolution_backward(buf422, convolution_305, primals_637, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf422
        del convolution_305
        del primals_637
        buf424 = buf423[0]
        buf425 = buf423[1]
        del buf423
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf426 = aten.convolution_backward(buf424, relu_161, primals_636, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_636
        buf427 = buf426[0]
        buf428 = buf426[1]
        del buf426
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf429 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf335, (8, 864, 11, 11), (522720, 121, 11, 1), 104544), add_859, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_399)
        del add_859
        del getitem_399
        buf430 = buf429
        del buf429
        buf431 = buf420; del buf420  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_31.run(buf335, buf431, 864, 968, grid=grid(864), stream=stream0)
        buf432 = buf419; del buf419  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_32.run(buf335, convolution_304, unsqueeze_1238, buf432, 6912, 121, grid=grid(6912), stream=stream0)
        buf433 = empty((864, ), device='cuda', dtype=torch.float32)
        buf434 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf432, squeeze_493, buf433, buf434, 864, 8, grid=grid(864), stream=stream0)
        buf435 = reinterpret_tensor(buf424, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf424  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_33.run(buf335, convolution_304, unsqueeze_1238, buf433, squeeze_493, buf431, primals_634, buf435, 968, 864, grid=grid(968, 864), stream=stream0)
        del convolution_304
        del primals_634
        del squeeze_493
        del unsqueeze_1238
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf436 = aten.convolution_backward(buf435, convolution_303, primals_633, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf435
        del convolution_303
        del primals_633
        buf437 = buf436[0]
        buf438 = buf436[1]
        del buf436
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf439 = aten.convolution_backward(buf437, relu_162, primals_632, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_632
        buf440 = buf439[0]
        buf441 = buf439[1]
        del buf439
        buf442 = buf432; del buf432  # reuse
        buf444 = buf417; del buf417  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_162, buf440, convolution_302, unsqueeze_1250, buf442, buf444, 6912, 121, grid=grid(6912), stream=stream0)
        buf443 = buf433; del buf433  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf442, buf443, 864, 8, grid=grid(864), stream=stream0)
        buf445 = empty((864, ), device='cuda', dtype=torch.float32)
        buf446 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf444, squeeze_490, buf445, buf446, 864, 8, grid=grid(864), stream=stream0)
        buf447 = reinterpret_tensor(buf437, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf437  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_162, buf440, convolution_302, unsqueeze_1250, buf445, squeeze_490, buf443, primals_630, buf447, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf440
        del convolution_302
        del primals_630
        del relu_162
        del squeeze_490
        del unsqueeze_1250
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf448 = aten.convolution_backward(buf447, convolution_301, primals_629, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf447
        del convolution_301
        del primals_629
        buf449 = buf448[0]
        buf450 = buf448[1]
        del buf448
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf451 = aten.convolution_backward(buf449, relu_161, primals_628, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 864, [True, True, False])
        del buf449
        del primals_628
        buf452 = buf451[0]
        buf453 = buf451[1]
        del buf451
        buf454 = buf405; del buf405  # reuse
        buf481 = buf444; del buf444  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_34.run(buf454, buf335, buf360, relu_161, buf427, buf430, buf452, convolution_296, unsqueeze_1286, buf481, 6912, 121, grid=grid(6912), stream=stream0)
        del buf360
        del buf427
        del buf430
        del relu_161
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf455 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf335, (8, 864, 11, 11), (522720, 121, 11, 1), 0), add_854, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_393)
        del add_854
        del getitem_393
        buf456 = buf455
        del buf455
        buf457 = buf445; del buf445  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_35.run(buf335, buf457, 864, 968, grid=grid(864), stream=stream0)
        buf458 = buf442; del buf442  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_36.run(buf335, convolution_300, unsqueeze_1262, buf458, 6912, 121, grid=grid(6912), stream=stream0)
        buf459 = empty((864, ), device='cuda', dtype=torch.float32)
        buf460 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf458, squeeze_487, buf459, buf460, 864, 8, grid=grid(864), stream=stream0)
        buf461 = reinterpret_tensor(buf452, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf452  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_37.run(buf335, convolution_300, unsqueeze_1262, buf459, squeeze_487, buf457, primals_626, buf461, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf335
        del convolution_300
        del primals_626
        del squeeze_487
        del unsqueeze_1262
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf462 = aten.convolution_backward(buf461, convolution_299, primals_625, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf461
        del convolution_299
        del primals_625
        buf463 = buf462[0]
        buf464 = buf462[1]
        del buf462
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf465 = aten.convolution_backward(buf463, relu_160, primals_624, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_624
        buf466 = buf465[0]
        buf467 = buf465[1]
        del buf465
        buf468 = buf458; del buf458  # reuse
        buf470 = buf305; del buf305  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_160, buf466, convolution_298, unsqueeze_1274, buf468, buf470, 6912, 121, grid=grid(6912), stream=stream0)
        buf469 = buf459; del buf459  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf468, buf469, 864, 8, grid=grid(864), stream=stream0)
        del buf468
        buf471 = empty((864, ), device='cuda', dtype=torch.float32)
        buf472 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf470, squeeze_484, buf471, buf472, 864, 8, grid=grid(864), stream=stream0)
        buf473 = reinterpret_tensor(buf463, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf463  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_160, buf466, convolution_298, unsqueeze_1274, buf471, squeeze_484, buf469, primals_622, buf473, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf466
        del convolution_298
        del primals_622
        del relu_160
        del squeeze_484
        del unsqueeze_1274
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf474 = aten.convolution_backward(buf473, convolution_297, primals_621, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf473
        del convolution_297
        del primals_621
        buf475 = buf474[0]
        buf476 = buf474[1]
        del buf474
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf477 = aten.convolution_backward(buf475, relu_159, primals_620, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_620
        buf478 = buf477[0]
        buf479 = buf477[1]
        del buf477
        buf480 = buf471; del buf471  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_38.run(buf454, buf480, 864, 968, grid=grid(864), stream=stream0)
        buf482 = empty((864, ), device='cuda', dtype=torch.float32)
        buf483 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf481, squeeze_481, buf482, buf483, 864, 8, grid=grid(864), stream=stream0)
        buf484 = reinterpret_tensor(buf475, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf475  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_39.run(buf454, convolution_296, unsqueeze_1286, buf482, squeeze_481, buf480, primals_618, buf484, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf454
        del convolution_296
        del primals_618
        del squeeze_481
        del unsqueeze_1286
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf485 = aten.convolution_backward(buf484, relu_158, primals_617, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf484
        del primals_617
        buf486 = buf485[0]
        buf487 = buf485[1]
        del buf485
        buf488 = buf481; del buf481  # reuse
        buf490 = buf470; del buf470  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_19.run(relu_159, buf357, buf456, buf478, cat_14, unsqueeze_1298, buf488, buf490, 6912, 121, grid=grid(6912), stream=stream0)
        buf489 = buf482; del buf482  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf488, buf489, 864, 8, grid=grid(864), stream=stream0)
        buf491 = empty((864, ), device='cuda', dtype=torch.float32)
        buf493 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf490, squeeze_478, buf491, buf493, 864, 8, grid=grid(864), stream=stream0)
        buf492 = buf456; del buf456  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_41.run(buf492, relu_159, buf357, buf478, cat_14, unsqueeze_1298, buf491, squeeze_478, buf489, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf357
        del cat_14
        del relu_159
        del unsqueeze_1298
        buf494 = empty_strided((8, 432, 11, 11), (52272, 1, 4752, 432), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        triton_poi_fused_convolution_backward_42.run(buf492, squeeze_478, primals_615, buf494, 418176, grid=grid(418176), stream=stream0)
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf495 = aten.convolution_backward(buf494, avg_pool2d_7, primals_614, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del avg_pool2d_7
        del primals_614
        buf496 = buf495[0]
        buf497 = buf495[1]
        del buf495
        buf498 = buf494; del buf494  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        triton_poi_fused_convolution_backward_43.run(buf492, squeeze_478, primals_615, buf498, 418176, grid=grid(418176), stream=stream0)
        del primals_615
        del squeeze_478
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf499 = aten.convolution_backward(buf498, avg_pool2d_6, primals_613, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del avg_pool2d_6
        del buf498
        del primals_613
        buf500 = buf499[0]
        buf501 = buf499[1]
        del buf499
        buf503 = buf333; del buf333  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_40.run(buf503, relu_158, buf486, 34560, 121, grid=grid(34560, 121), stream=stream0)
        del buf486
        del relu_158
        buf504 = buf491; del buf491  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_22.run(buf503, buf504, 864, 968, grid=grid(864), stream=stream0)
        buf505 = buf490; del buf490  # reuse
        buf512 = buf488; del buf488  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_44.run(buf503, convolution_293, unsqueeze_1310, convolution_292, unsqueeze_1322, buf505, buf512, 6912, 121, grid=grid(6912), stream=stream0)
        buf506 = empty((864, ), device='cuda', dtype=torch.float32)
        buf507 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf505, squeeze_475, buf506, buf507, 864, 8, grid=grid(864), stream=stream0)
        buf513 = empty((864, ), device='cuda', dtype=torch.float32)
        buf514 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf512, squeeze_472, buf513, buf514, 864, 8, grid=grid(864), stream=stream0)
        buf508 = buf492; del buf492  # reuse
        buf515 = reinterpret_tensor(buf478, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf478  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_45.run(buf503, convolution_293, unsqueeze_1310, buf506, squeeze_475, buf504, primals_611, convolution_292, unsqueeze_1322, buf513, squeeze_472, primals_609, buf508, buf515, 968, 864, grid=grid(968, 864), stream=stream0)
        del convolution_292
        del convolution_293
        del primals_609
        del primals_611
        del squeeze_472
        del squeeze_475
        del unsqueeze_1310
        del unsqueeze_1322
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf509 = aten.convolution_backward(buf508, constant_pad_nd_38, primals_26, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del constant_pad_nd_38
        del primals_26
        buf510 = buf509[0]
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf534 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf503, (8, 864, 11, 11), (522720, 121, 11, 1), 313632), constant_pad_nd_33, [3, 3], [2, 2], [0, 0], [1, 1], False, getitem_363)
        buf535 = buf534
        del buf534
        buf536 = buf513; del buf513  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_25.run(buf503, buf536, 864, 968, grid=grid(864), stream=stream0)
        buf537 = buf512; del buf512  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_26.run(buf503, convolution_288, unsqueeze_1346, buf537, 6912, 121, grid=grid(6912), stream=stream0)
        buf538 = buf506; del buf506  # reuse
        buf539 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf537, squeeze_466, buf538, buf539, 864, 8, grid=grid(864), stream=stream0)
        buf540 = buf508; del buf508  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_27.run(buf503, convolution_288, unsqueeze_1346, buf538, squeeze_466, buf536, primals_602, buf540, 968, 864, grid=grid(968, 864), stream=stream0)
        del convolution_288
        del primals_602
        del squeeze_466
        del unsqueeze_1346
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf541 = aten.convolution_backward(buf540, convolution_287, primals_601, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf540
        del convolution_287
        del primals_601
        buf542 = buf541[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf544 = aten.convolution_backward(buf542, relu_153, primals_600, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_600
        buf545 = buf544[0]
        buf547 = buf537; del buf537  # reuse
        buf549 = buf505; del buf505  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_153, buf545, convolution_286, unsqueeze_1358, buf547, buf549, 6912, 121, grid=grid(6912), stream=stream0)
        buf548 = buf538; del buf538  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf547, buf548, 864, 8, grid=grid(864), stream=stream0)
        buf550 = empty((864, ), device='cuda', dtype=torch.float32)
        buf551 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf549, squeeze_463, buf550, buf551, 864, 8, grid=grid(864), stream=stream0)
        buf552 = reinterpret_tensor(buf542, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf542  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_153, buf545, convolution_286, unsqueeze_1358, buf550, squeeze_463, buf548, primals_598, buf552, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf545
        del convolution_286
        del primals_598
        del relu_153
        del squeeze_463
        del unsqueeze_1358
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf553 = aten.convolution_backward(buf552, convolution_285, primals_597, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_285
        del primals_597
        buf554 = buf553[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf556 = aten.convolution_backward(buf554, relu_152, primals_596, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_596
        buf557 = buf556[0]
        buf559 = buf550; del buf550  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_28.run(buf503, relu_152, buf557, buf559, 864, 968, grid=grid(864), stream=stream0)
        buf560 = buf549; del buf549  # reuse
        buf582 = buf547; del buf547  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_29.run(buf503, relu_152, buf557, convolution_284, unsqueeze_1370, convolution_280, unsqueeze_1394, buf560, buf582, 6912, 121, grid=grid(6912), stream=stream0)
        buf561 = empty((864, ), device='cuda', dtype=torch.float32)
        buf563 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf560, squeeze_460, buf561, buf563, 864, 8, grid=grid(864), stream=stream0)
        buf583 = empty((864, ), device='cuda', dtype=torch.float32)
        buf585 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf582, squeeze_454, buf583, buf585, 864, 8, grid=grid(864), stream=stream0)
        buf562 = reinterpret_tensor(buf554, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf554  # reuse
        buf584 = buf552; del buf552  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_30.run(buf503, relu_152, buf557, convolution_284, unsqueeze_1370, buf561, squeeze_460, buf559, primals_594, convolution_280, unsqueeze_1394, buf583, squeeze_454, primals_587, buf562, buf584, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf557
        del convolution_280
        del convolution_284
        del primals_587
        del primals_594
        del relu_152
        del squeeze_454
        del squeeze_460
        del unsqueeze_1370
        del unsqueeze_1394
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf564 = aten.convolution_backward(buf562, convolution_283, primals_593, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf562
        del convolution_283
        del primals_593
        buf565 = buf564[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf567 = aten.convolution_backward(buf565, relu_151, primals_592, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_592
        buf568 = buf567[0]
        buf570 = buf582; del buf582  # reuse
        buf572 = buf560; del buf560  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_151, buf568, convolution_282, unsqueeze_1382, buf570, buf572, 6912, 121, grid=grid(6912), stream=stream0)
        buf571 = buf583; del buf583  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf570, buf571, 864, 8, grid=grid(864), stream=stream0)
        buf573 = buf561; del buf561  # reuse
        buf574 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf572, squeeze_457, buf573, buf574, 864, 8, grid=grid(864), stream=stream0)
        buf575 = reinterpret_tensor(buf565, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf565  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_151, buf568, convolution_282, unsqueeze_1382, buf573, squeeze_457, buf571, primals_590, buf575, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf568
        del convolution_282
        del primals_590
        del relu_151
        del squeeze_457
        del unsqueeze_1382
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf576 = aten.convolution_backward(buf575, convolution_281, primals_589, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf575
        del convolution_281
        del primals_589
        buf577 = buf576[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf579 = aten.convolution_backward(buf577, constant_pad_nd_35, primals_24, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 864, [True, True, False])
        del buf577
        del constant_pad_nd_35
        del primals_24
        buf580 = buf579[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf586 = aten.convolution_backward(buf584, convolution_279, primals_586, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf584
        del convolution_279
        del primals_586
        buf587 = buf586[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf589 = aten.convolution_backward(buf587, relu_149, primals_585, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_585
        buf590 = buf589[0]
        buf592 = buf572; del buf572  # reuse
        buf594 = buf570; del buf570  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_149, buf590, convolution_278, unsqueeze_1406, buf592, buf594, 6912, 121, grid=grid(6912), stream=stream0)
        buf593 = buf573; del buf573  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf592, buf593, 864, 8, grid=grid(864), stream=stream0)
        buf595 = empty((864, ), device='cuda', dtype=torch.float32)
        buf596 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf594, squeeze_451, buf595, buf596, 864, 8, grid=grid(864), stream=stream0)
        buf597 = reinterpret_tensor(buf587, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf587  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_149, buf590, convolution_278, unsqueeze_1406, buf595, squeeze_451, buf593, primals_583, buf597, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf590
        del convolution_278
        del primals_583
        del relu_149
        del squeeze_451
        del unsqueeze_1406
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf598 = aten.convolution_backward(buf597, convolution_277, primals_582, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf597
        del convolution_277
        del primals_582
        buf599 = buf598[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf601 = aten.convolution_backward(buf599, constant_pad_nd_34, primals_23, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 864, [True, True, False])
        del constant_pad_nd_34
        del primals_23
        buf602 = buf601[0]
        buf604 = buf510; del buf510  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.threshold_backward]
        triton_poi_fused_add_constant_pad_nd_threshold_backward_46.run(buf604, le_43, buf535, buf580, buf602, 6912, 441, grid=grid(6912, 441), stream=stream0)
        del buf535
        del buf580
        del buf602
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf605 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf503, (8, 864, 11, 11), (522720, 121, 11, 1), 104544), constant_pad_nd_33, [3, 3], [2, 2], [0, 0], [1, 1], False, getitem_363)
        del constant_pad_nd_33
        del getitem_363
        buf606 = buf605
        del buf605
        buf607 = buf595; del buf595  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_31.run(buf503, buf607, 864, 968, grid=grid(864), stream=stream0)
        buf608 = buf594; del buf594  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_32.run(buf503, convolution_276, unsqueeze_1418, buf608, 6912, 121, grid=grid(6912), stream=stream0)
        buf609 = empty((864, ), device='cuda', dtype=torch.float32)
        buf610 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf608, squeeze_448, buf609, buf610, 864, 8, grid=grid(864), stream=stream0)
        buf611 = reinterpret_tensor(buf599, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf599  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_33.run(buf503, convolution_276, unsqueeze_1418, buf609, squeeze_448, buf607, primals_580, buf611, 968, 864, grid=grid(968, 864), stream=stream0)
        del convolution_276
        del primals_580
        del squeeze_448
        del unsqueeze_1418
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf612 = aten.convolution_backward(buf611, convolution_275, primals_579, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf611
        del convolution_275
        del primals_579
        buf613 = buf612[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf615 = aten.convolution_backward(buf613, relu_147, primals_578, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_578
        buf616 = buf615[0]
        buf618 = buf608; del buf608  # reuse
        buf620 = buf592; del buf592  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_147, buf616, convolution_274, unsqueeze_1430, buf618, buf620, 6912, 121, grid=grid(6912), stream=stream0)
        buf619 = buf609; del buf609  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf618, buf619, 864, 8, grid=grid(864), stream=stream0)
        buf621 = empty((864, ), device='cuda', dtype=torch.float32)
        buf622 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf620, squeeze_445, buf621, buf622, 864, 8, grid=grid(864), stream=stream0)
        buf623 = reinterpret_tensor(buf613, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf613  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_147, buf616, convolution_274, unsqueeze_1430, buf621, squeeze_445, buf619, primals_576, buf623, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf616
        del convolution_274
        del primals_576
        del relu_147
        del squeeze_445
        del unsqueeze_1430
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf624 = aten.convolution_backward(buf623, convolution_273, primals_575, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf623
        del convolution_273
        del primals_575
        buf625 = buf624[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf627 = aten.convolution_backward(buf625, constant_pad_nd_32, primals_22, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 864, [True, True, False])
        del buf625
        del constant_pad_nd_32
        del primals_22
        buf628 = buf627[0]
        buf656 = empty((864, 28), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_47.run(buf604, buf606, le_43, buf628, buf656, 24192, 126, grid=grid(24192), stream=stream0)
        buf657 = buf621; del buf621  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_48.run(buf656, buf657, 864, 28, grid=grid(864), stream=stream0)
        buf658 = reinterpret_tensor(buf656, (864, 28), (1, 864), 0); del buf656  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_49.run(buf604, buf606, le_43, buf628, convolution_268, unsqueeze_1466, buf658, 24192, 126, grid=grid(24192), stream=stream0)
        buf659 = empty((864, ), device='cuda', dtype=torch.float32)
        buf661 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_50.run(buf658, squeeze_436, buf659, buf661, 864, 28, grid=grid(864), stream=stream0)
        buf660 = empty_strided((8, 864, 21, 21), (381024, 1, 18144, 864), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_51.run(buf604, buf606, le_43, buf628, convolution_268, unsqueeze_1466, buf659, squeeze_436, buf660, 3528, 864, grid=grid(3528, 864), stream=stream0)
        del buf606
        del buf628
        del convolution_268
        del le_43
        del unsqueeze_1466
        buf662 = buf660; del buf660  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_52.run(buf662, buf657, squeeze_436, primals_566, 6912, 441, grid=grid(6912, 441), stream=stream0)
        del primals_566
        del squeeze_436
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf663 = aten.convolution_backward(buf662, relu_143, primals_565, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_565
        buf664 = buf663[0]
        buf502 = empty((8, 2160, 21, 21), device='cuda', dtype=torch.float32)
        buf674 = buf502; del buf502  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.avg_pool2d_backward, aten.constant_pad_nd, aten.threshold_backward]
        triton_poi_fused_add_avg_pool2d_backward_constant_pad_nd_threshold_backward_53.run(buf674, buf496, buf500, relu_143, buf664, 17280, 441, grid=grid(17280, 441), stream=stream0)
        del buf496
        del buf500
        del buf664
        del relu_143
        buf511 = buf509[1]
        del buf509
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf516 = aten.convolution_backward(buf515, convolution_291, primals_608, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf515
        del convolution_291
        del primals_608
        buf517 = buf516[0]
        buf518 = buf516[1]
        del buf516
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf519 = aten.convolution_backward(buf517, relu_155, primals_607, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_607
        buf520 = buf519[0]
        buf521 = buf519[1]
        del buf519
        buf522 = buf620; del buf620  # reuse
        buf524 = buf618; del buf618  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_155, buf520, convolution_290, unsqueeze_1334, buf522, buf524, 6912, 121, grid=grid(6912), stream=stream0)
        buf523 = buf659; del buf659  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf522, buf523, 864, 8, grid=grid(864), stream=stream0)
        buf525 = empty((864, ), device='cuda', dtype=torch.float32)
        buf526 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf524, squeeze_469, buf525, buf526, 864, 8, grid=grid(864), stream=stream0)
        buf527 = reinterpret_tensor(buf517, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf517  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_155, buf520, convolution_290, unsqueeze_1334, buf525, squeeze_469, buf523, primals_605, buf527, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf520
        del convolution_290
        del primals_605
        del relu_155
        del squeeze_469
        del unsqueeze_1334
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf528 = aten.convolution_backward(buf527, convolution_289, primals_604, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf527
        del convolution_289
        del primals_604
        buf529 = buf528[0]
        buf530 = buf528[1]
        del buf528
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf531 = aten.convolution_backward(buf529, constant_pad_nd_37, primals_25, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 864, [True, True, False])
        del constant_pad_nd_37
        del primals_25
        buf532 = buf531[0]
        buf533 = buf531[1]
        del buf531
        buf543 = buf541[1]
        del buf541
        buf546 = buf544[1]
        del buf544
        buf555 = buf553[1]
        del buf553
        buf558 = buf556[1]
        del buf556
        buf566 = buf564[1]
        del buf564
        buf569 = buf567[1]
        del buf567
        buf578 = buf576[1]
        del buf576
        buf581 = buf579[1]
        del buf579
        buf588 = buf586[1]
        del buf586
        buf591 = buf589[1]
        del buf589
        buf600 = buf598[1]
        del buf598
        buf603 = buf601[1]
        del buf601
        buf614 = buf612[1]
        del buf612
        buf617 = buf615[1]
        del buf615
        buf626 = buf624[1]
        del buf624
        buf629 = buf627[1]
        del buf627
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf630 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf503, (8, 864, 11, 11), (522720, 121, 11, 1), 0), constant_pad_nd_31, [3, 3], [2, 2], [0, 0], [1, 1], False, getitem_357)
        del constant_pad_nd_31
        del getitem_357
        buf631 = buf630
        del buf630
        buf632 = buf525; del buf525  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_35.run(buf503, buf632, 864, 968, grid=grid(864), stream=stream0)
        buf633 = buf524; del buf524  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_36.run(buf503, convolution_272, unsqueeze_1442, buf633, 6912, 121, grid=grid(6912), stream=stream0)
        buf634 = empty((864, ), device='cuda', dtype=torch.float32)
        buf635 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf633, squeeze_442, buf634, buf635, 864, 8, grid=grid(864), stream=stream0)
        buf636 = reinterpret_tensor(buf529, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf529  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_37.run(buf503, convolution_272, unsqueeze_1442, buf634, squeeze_442, buf632, primals_573, buf636, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf503
        del convolution_272
        del primals_573
        del squeeze_442
        del unsqueeze_1442
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf637 = aten.convolution_backward(buf636, convolution_271, primals_572, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf636
        del convolution_271
        del primals_572
        buf638 = buf637[0]
        buf639 = buf637[1]
        del buf637
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf640 = aten.convolution_backward(buf638, relu_145, primals_571, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 864, [True, True, False])
        del primals_571
        buf641 = buf640[0]
        buf642 = buf640[1]
        del buf640
        buf643 = buf633; del buf633  # reuse
        buf645 = buf522; del buf522  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_6.run(relu_145, buf641, convolution_270, unsqueeze_1454, buf643, buf645, 6912, 121, grid=grid(6912), stream=stream0)
        buf644 = buf634; del buf634  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_3.run(buf643, buf644, 864, 8, grid=grid(864), stream=stream0)
        del buf643
        buf646 = empty((864, ), device='cuda', dtype=torch.float32)
        buf647 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_4.run(buf645, squeeze_439, buf646, buf647, 864, 8, grid=grid(864), stream=stream0)
        del buf645
        buf648 = reinterpret_tensor(buf638, (8, 864, 11, 11), (104544, 1, 9504, 864), 0); del buf638  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_7.run(relu_145, buf641, convolution_270, unsqueeze_1454, buf646, squeeze_439, buf644, primals_569, buf648, 968, 864, grid=grid(968, 864), stream=stream0)
        del buf641
        del convolution_270
        del primals_569
        del relu_145
        del squeeze_439
        del unsqueeze_1454
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf649 = aten.convolution_backward(buf648, convolution_269, primals_568, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf648
        del convolution_269
        del primals_568
        buf650 = buf649[0]
        buf651 = buf649[1]
        del buf649
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf652 = aten.convolution_backward(buf650, constant_pad_nd_30, primals_21, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 864, [True, True, False])
        del buf650
        del constant_pad_nd_30
        del primals_21
        buf653 = buf652[0]
        buf654 = buf652[1]
        del buf652
        buf655 = reinterpret_tensor(buf662, (8, 864, 21, 21), (381024, 441, 21, 1), 0); del buf662  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.threshold_backward]
        triton_poi_fused_add_constant_pad_nd_threshold_backward_54.run(le_45, buf532, buf631, buf653, buf655, 6912, 441, grid=grid(6912, 441), stream=stream0)
        del buf532
        del buf631
        del buf653
        del le_45
        buf665 = buf663[1]
        del buf663
        buf666 = buf646; del buf646  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_55.run(buf655, buf666, 864, 3528, grid=grid(864), stream=stream0)
        buf667 = reinterpret_tensor(buf658, (864, 28), (28, 1), 0); del buf658  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_56.run(buf655, convolution_267, unsqueeze_1478, buf667, 24192, 126, grid=grid(24192), stream=stream0)
        buf668 = empty((864, ), device='cuda', dtype=torch.float32)
        buf669 = empty((864, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_57.run(buf667, squeeze_433, buf668, buf669, 864, 28, grid=grid(864), stream=stream0)
        del buf667
        buf670 = reinterpret_tensor(buf604, (8, 864, 21, 21), (381024, 1, 18144, 864), 0); del buf604  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_58.run(buf655, convolution_267, unsqueeze_1478, buf668, squeeze_433, buf666, primals_563, buf670, 3528, 864, grid=grid(3528, 864), stream=stream0)
        del buf655
        del buf668
        del convolution_267
        del primals_563
        del squeeze_433
        del unsqueeze_1478
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf671 = aten.convolution_backward(buf670, relu_129, primals_562, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_562
        buf672 = buf671[0]
        buf673 = buf671[1]
        del buf671
        buf675 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_59.run(buf674, buf675, 432, 3528, grid=grid(432), stream=stream0)
        buf676 = empty((432, 28), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_60.run(buf674, convolution_266, unsqueeze_1490, buf676, 12096, 126, grid=grid(12096), stream=stream0)
        buf677 = empty((432, ), device='cuda', dtype=torch.float32)
        buf678 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf676, squeeze_430, buf677, buf678, 432, 28, grid=grid(432), stream=stream0)
        buf679 = empty_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_62.run(buf674, convolution_266, unsqueeze_1490, buf677, squeeze_430, buf675, primals_560, buf679, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del convolution_266
        del primals_560
        del squeeze_430
        del unsqueeze_1490
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf680 = aten.convolution_backward(buf679, convolution_265, primals_559, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf679
        del convolution_265
        del primals_559
        buf681 = buf680[0]
        buf682 = buf680[1]
        del buf680
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf683 = aten.convolution_backward(buf681, relu_141, primals_558, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_558
        buf684 = buf683[0]
        buf685 = buf683[1]
        del buf683
        buf686 = buf676; del buf676  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_141, buf684, buf686, 12096, 126, grid=grid(12096), stream=stream0)
        buf687 = buf677; del buf677  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf686, buf687, 432, 28, grid=grid(432), stream=stream0)
        buf688 = reinterpret_tensor(buf686, (432, 28), (1, 432), 0); del buf686  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_141, buf684, convolution_264, unsqueeze_1502, buf688, 12096, 126, grid=grid(12096), stream=stream0)
        buf689 = empty((432, ), device='cuda', dtype=torch.float32)
        buf690 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf688, squeeze_427, buf689, buf690, 432, 28, grid=grid(432), stream=stream0)
        buf691 = reinterpret_tensor(buf681, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf681  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_141, buf684, convolution_264, unsqueeze_1502, buf689, squeeze_427, buf687, primals_556, buf691, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf684
        del convolution_264
        del primals_556
        del relu_141
        del squeeze_427
        del unsqueeze_1502
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf692 = aten.convolution_backward(buf691, convolution_263, primals_555, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf691
        del convolution_263
        del primals_555
        buf693 = buf692[0]
        buf694 = buf692[1]
        del buf692
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf695 = aten.convolution_backward(buf693, relu_130, primals_554, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_554
        buf696 = buf695[0]
        buf697 = buf695[1]
        del buf695
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf698 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf674, (8, 432, 21, 21), (952560, 441, 21, 1), 571536), add_704, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_329)
        buf699 = buf698
        del buf698
        buf700 = buf689; del buf689  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_68.run(buf674, buf700, 432, 3528, grid=grid(432), stream=stream0)
        buf701 = reinterpret_tensor(buf688, (432, 28), (28, 1), 0); del buf688  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_69.run(buf674, convolution_262, unsqueeze_1514, buf701, 12096, 126, grid=grid(12096), stream=stream0)
        buf702 = empty((432, ), device='cuda', dtype=torch.float32)
        buf703 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf701, squeeze_424, buf702, buf703, 432, 28, grid=grid(432), stream=stream0)
        buf704 = reinterpret_tensor(buf693, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf693  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_70.run(buf674, convolution_262, unsqueeze_1514, buf702, squeeze_424, buf700, primals_552, buf704, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del convolution_262
        del primals_552
        del squeeze_424
        del unsqueeze_1514
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf705 = aten.convolution_backward(buf704, convolution_261, primals_551, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf704
        del convolution_261
        del primals_551
        buf706 = buf705[0]
        buf707 = buf705[1]
        del buf705
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf708 = aten.convolution_backward(buf706, relu_139, primals_550, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_550
        buf709 = buf708[0]
        buf710 = buf708[1]
        del buf708
        buf711 = buf701; del buf701  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_139, buf709, buf711, 12096, 126, grid=grid(12096), stream=stream0)
        buf712 = buf702; del buf702  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf711, buf712, 432, 28, grid=grid(432), stream=stream0)
        buf713 = reinterpret_tensor(buf711, (432, 28), (1, 432), 0); del buf711  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_139, buf709, convolution_260, unsqueeze_1526, buf713, 12096, 126, grid=grid(12096), stream=stream0)
        buf714 = empty((432, ), device='cuda', dtype=torch.float32)
        buf715 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf713, squeeze_421, buf714, buf715, 432, 28, grid=grid(432), stream=stream0)
        buf716 = reinterpret_tensor(buf706, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf706  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_139, buf709, convolution_260, unsqueeze_1526, buf714, squeeze_421, buf712, primals_548, buf716, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf709
        del convolution_260
        del primals_548
        del relu_139
        del squeeze_421
        del unsqueeze_1526
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf717 = aten.convolution_backward(buf716, convolution_259, primals_547, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_259
        del primals_547
        buf718 = buf717[0]
        buf719 = buf717[1]
        del buf717
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf720 = aten.convolution_backward(buf718, relu_138, primals_546, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_546
        buf721 = buf720[0]
        buf722 = buf720[1]
        del buf720
        buf723 = buf714; del buf714  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_71.run(buf674, relu_138, buf721, buf723, 432, 3528, grid=grid(432), stream=stream0)
        buf724 = reinterpret_tensor(buf713, (432, 28), (28, 1), 0); del buf713  # reuse
        buf746 = empty((432, 28), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_72.run(buf674, relu_138, buf721, convolution_258, unsqueeze_1538, convolution_254, unsqueeze_1562, buf724, buf746, 12096, 126, grid=grid(12096), stream=stream0)
        buf725 = empty((432, ), device='cuda', dtype=torch.float32)
        buf727 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf724, squeeze_418, buf725, buf727, 432, 28, grid=grid(432), stream=stream0)
        buf747 = empty((432, ), device='cuda', dtype=torch.float32)
        buf749 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf746, squeeze_412, buf747, buf749, 432, 28, grid=grid(432), stream=stream0)
        buf726 = reinterpret_tensor(buf718, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf718  # reuse
        buf748 = buf716; del buf716  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_73.run(buf674, relu_138, buf721, convolution_258, unsqueeze_1538, buf725, squeeze_418, buf723, primals_544, convolution_254, unsqueeze_1562, buf747, squeeze_412, primals_536, buf726, buf748, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf721
        del convolution_254
        del convolution_258
        del primals_536
        del primals_544
        del relu_138
        del squeeze_412
        del squeeze_418
        del unsqueeze_1538
        del unsqueeze_1562
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf728 = aten.convolution_backward(buf726, convolution_257, primals_543, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf726
        del convolution_257
        del primals_543
        buf729 = buf728[0]
        buf730 = buf728[1]
        del buf728
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf731 = aten.convolution_backward(buf729, relu_137, primals_542, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_542
        buf732 = buf731[0]
        buf733 = buf731[1]
        del buf731
        buf734 = buf746; del buf746  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_137, buf732, buf734, 12096, 126, grid=grid(12096), stream=stream0)
        buf735 = buf747; del buf747  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf734, buf735, 432, 28, grid=grid(432), stream=stream0)
        buf736 = reinterpret_tensor(buf734, (432, 28), (1, 432), 0); del buf734  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_137, buf732, convolution_256, unsqueeze_1550, buf736, 12096, 126, grid=grid(12096), stream=stream0)
        buf737 = buf725; del buf725  # reuse
        buf738 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf736, squeeze_415, buf737, buf738, 432, 28, grid=grid(432), stream=stream0)
        buf739 = reinterpret_tensor(buf729, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf729  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_137, buf732, convolution_256, unsqueeze_1550, buf737, squeeze_415, buf735, primals_540, buf739, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf732
        del convolution_256
        del primals_540
        del relu_137
        del squeeze_415
        del unsqueeze_1550
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf740 = aten.convolution_backward(buf739, convolution_255, primals_539, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf739
        del convolution_255
        del primals_539
        buf741 = buf740[0]
        buf742 = buf740[1]
        del buf740
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf743 = aten.convolution_backward(buf741, relu_132, primals_538, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del buf741
        del primals_538
        buf744 = buf743[0]
        buf745 = buf743[1]
        del buf743
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf750 = aten.convolution_backward(buf748, convolution_253, primals_535, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf748
        del convolution_253
        del primals_535
        buf751 = buf750[0]
        buf752 = buf750[1]
        del buf750
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf753 = aten.convolution_backward(buf751, relu_135, primals_534, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_534
        buf754 = buf753[0]
        buf755 = buf753[1]
        del buf753
        buf756 = reinterpret_tensor(buf736, (432, 28), (28, 1), 0); del buf736  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_135, buf754, buf756, 12096, 126, grid=grid(12096), stream=stream0)
        buf757 = buf737; del buf737  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf756, buf757, 432, 28, grid=grid(432), stream=stream0)
        buf758 = reinterpret_tensor(buf756, (432, 28), (1, 432), 0); del buf756  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_135, buf754, convolution_252, unsqueeze_1574, buf758, 12096, 126, grid=grid(12096), stream=stream0)
        buf759 = empty((432, ), device='cuda', dtype=torch.float32)
        buf760 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf758, squeeze_409, buf759, buf760, 432, 28, grid=grid(432), stream=stream0)
        buf761 = reinterpret_tensor(buf751, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf751  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_135, buf754, convolution_252, unsqueeze_1574, buf759, squeeze_409, buf757, primals_532, buf761, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf754
        del convolution_252
        del primals_532
        del relu_135
        del squeeze_409
        del unsqueeze_1574
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf762 = aten.convolution_backward(buf761, convolution_251, primals_531, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf761
        del convolution_251
        del primals_531
        buf763 = buf762[0]
        buf764 = buf762[1]
        del buf762
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf765 = aten.convolution_backward(buf763, relu_132, primals_530, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_530
        buf766 = buf765[0]
        buf767 = buf765[1]
        del buf765
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf768 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf674, (8, 432, 21, 21), (952560, 441, 21, 1), 190512), add_704, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_329)
        del add_704
        del getitem_329
        buf769 = buf768
        del buf768
        buf770 = buf759; del buf759  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_74.run(buf674, buf770, 432, 3528, grid=grid(432), stream=stream0)
        buf771 = reinterpret_tensor(buf758, (432, 28), (28, 1), 0); del buf758  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_75.run(buf674, convolution_250, unsqueeze_1586, buf771, 12096, 126, grid=grid(12096), stream=stream0)
        buf772 = empty((432, ), device='cuda', dtype=torch.float32)
        buf773 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf771, squeeze_406, buf772, buf773, 432, 28, grid=grid(432), stream=stream0)
        buf774 = reinterpret_tensor(buf763, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf763  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_76.run(buf674, convolution_250, unsqueeze_1586, buf772, squeeze_406, buf770, primals_528, buf774, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del convolution_250
        del primals_528
        del squeeze_406
        del unsqueeze_1586
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf775 = aten.convolution_backward(buf774, convolution_249, primals_527, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf774
        del convolution_249
        del primals_527
        buf776 = buf775[0]
        buf777 = buf775[1]
        del buf775
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf778 = aten.convolution_backward(buf776, relu_133, primals_526, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_526
        buf779 = buf778[0]
        buf780 = buf778[1]
        del buf778
        buf781 = buf771; del buf771  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_133, buf779, buf781, 12096, 126, grid=grid(12096), stream=stream0)
        buf782 = buf772; del buf772  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf781, buf782, 432, 28, grid=grid(432), stream=stream0)
        buf783 = reinterpret_tensor(buf781, (432, 28), (1, 432), 0); del buf781  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_133, buf779, convolution_248, unsqueeze_1598, buf783, 12096, 126, grid=grid(12096), stream=stream0)
        buf784 = empty((432, ), device='cuda', dtype=torch.float32)
        buf785 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf783, squeeze_403, buf784, buf785, 432, 28, grid=grid(432), stream=stream0)
        buf786 = reinterpret_tensor(buf776, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf776  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_133, buf779, convolution_248, unsqueeze_1598, buf784, squeeze_403, buf782, primals_524, buf786, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf779
        del convolution_248
        del primals_524
        del relu_133
        del squeeze_403
        del unsqueeze_1598
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf787 = aten.convolution_backward(buf786, convolution_247, primals_523, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf786
        del convolution_247
        del primals_523
        buf788 = buf787[0]
        buf789 = buf787[1]
        del buf787
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf790 = aten.convolution_backward(buf788, relu_132, primals_522, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 432, [True, True, False])
        del buf788
        del primals_522
        buf791 = buf790[0]
        buf792 = buf790[1]
        del buf790
        buf793 = buf744; del buf744  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_77.run(buf793, buf674, buf699, relu_132, buf766, buf769, buf791, 3456, 441, grid=grid(3456, 441), stream=stream0)
        del buf699
        del buf766
        del buf769
        del relu_132
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf794 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf674, (8, 432, 21, 21), (952560, 441, 21, 1), 0), add_699, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_323)
        del add_699
        del getitem_323
        buf795 = buf794
        del buf794
        buf796 = buf784; del buf784  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_78.run(buf674, buf796, 432, 3528, grid=grid(432), stream=stream0)
        buf797 = reinterpret_tensor(buf783, (432, 28), (28, 1), 0); del buf783  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_79.run(buf674, convolution_246, unsqueeze_1610, buf797, 12096, 126, grid=grid(12096), stream=stream0)
        buf798 = empty((432, ), device='cuda', dtype=torch.float32)
        buf799 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf797, squeeze_400, buf798, buf799, 432, 28, grid=grid(432), stream=stream0)
        buf800 = reinterpret_tensor(buf791, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf791  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_80.run(buf674, convolution_246, unsqueeze_1610, buf798, squeeze_400, buf796, primals_520, buf800, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf674
        del convolution_246
        del primals_520
        del squeeze_400
        del unsqueeze_1610
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf801 = aten.convolution_backward(buf800, convolution_245, primals_519, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf800
        del convolution_245
        del primals_519
        buf802 = buf801[0]
        buf803 = buf801[1]
        del buf801
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf804 = aten.convolution_backward(buf802, relu_131, primals_518, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_518
        buf805 = buf804[0]
        buf806 = buf804[1]
        del buf804
        buf807 = buf797; del buf797  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_131, buf805, buf807, 12096, 126, grid=grid(12096), stream=stream0)
        buf808 = buf798; del buf798  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf807, buf808, 432, 28, grid=grid(432), stream=stream0)
        buf809 = reinterpret_tensor(buf807, (432, 28), (1, 432), 0); del buf807  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_131, buf805, convolution_244, unsqueeze_1622, buf809, 12096, 126, grid=grid(12096), stream=stream0)
        buf810 = empty((432, ), device='cuda', dtype=torch.float32)
        buf811 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf809, squeeze_397, buf810, buf811, 432, 28, grid=grid(432), stream=stream0)
        buf812 = reinterpret_tensor(buf802, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf802  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_131, buf805, convolution_244, unsqueeze_1622, buf810, squeeze_397, buf808, primals_516, buf812, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf805
        del convolution_244
        del primals_516
        del relu_131
        del squeeze_397
        del unsqueeze_1622
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf813 = aten.convolution_backward(buf812, convolution_243, primals_515, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf812
        del convolution_243
        del primals_515
        buf814 = buf813[0]
        buf815 = buf813[1]
        del buf813
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf816 = aten.convolution_backward(buf814, relu_130, primals_514, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_514
        buf817 = buf816[0]
        buf818 = buf816[1]
        del buf816
        buf819 = buf810; del buf810  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_81.run(buf793, buf819, 432, 3528, grid=grid(432), stream=stream0)
        buf820 = reinterpret_tensor(buf809, (432, 28), (28, 1), 0); del buf809  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_82.run(buf793, convolution_242, unsqueeze_1634, buf820, 12096, 126, grid=grid(12096), stream=stream0)
        buf821 = empty((432, ), device='cuda', dtype=torch.float32)
        buf822 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf820, squeeze_394, buf821, buf822, 432, 28, grid=grid(432), stream=stream0)
        buf823 = reinterpret_tensor(buf814, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf814  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_83.run(buf793, convolution_242, unsqueeze_1634, buf821, squeeze_394, buf819, primals_512, buf823, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf793
        del convolution_242
        del primals_512
        del squeeze_394
        del unsqueeze_1634
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf824 = aten.convolution_backward(buf823, relu_129, primals_511, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf823
        del primals_511
        buf825 = buf824[0]
        buf826 = buf824[1]
        del buf824
        buf827 = buf820; del buf820  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_84.run(relu_130, buf696, buf795, buf817, buf827, 12096, 126, grid=grid(12096), stream=stream0)
        buf828 = buf821; del buf821  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf827, buf828, 432, 28, grid=grid(432), stream=stream0)
        buf829 = reinterpret_tensor(buf827, (432, 28), (1, 432), 0); del buf827  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_85.run(relu_130, buf696, buf795, buf817, convolution_241, unsqueeze_1646, buf829, 12096, 126, grid=grid(12096), stream=stream0)
        buf830 = empty((432, ), device='cuda', dtype=torch.float32)
        buf832 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf829, squeeze_391, buf830, buf832, 432, 28, grid=grid(432), stream=stream0)
        buf831 = buf795; del buf795  # reuse
        buf833 = buf831; del buf831  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_86.run(buf833, relu_130, buf696, buf817, convolution_241, unsqueeze_1646, buf830, squeeze_391, buf828, primals_509, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf696
        del buf817
        del convolution_241
        del primals_509
        del relu_130
        del squeeze_391
        del unsqueeze_1646
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf834 = aten.convolution_backward(buf833, relu_115, primals_508, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_508
        buf835 = buf834[0]
        buf836 = buf834[1]
        del buf834
        buf837 = buf672; del buf672  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_87.run(buf837, relu_129, buf825, 17280, 441, grid=grid(17280, 441), stream=stream0)
        del buf825
        del relu_129
        buf838 = buf830; del buf830  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_59.run(buf837, buf838, 432, 3528, grid=grid(432), stream=stream0)
        buf839 = reinterpret_tensor(buf829, (432, 28), (28, 1), 0); del buf829  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_60.run(buf837, convolution_240, unsqueeze_1658, buf839, 12096, 126, grid=grid(12096), stream=stream0)
        buf840 = empty((432, ), device='cuda', dtype=torch.float32)
        buf841 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf839, squeeze_388, buf840, buf841, 432, 28, grid=grid(432), stream=stream0)
        buf842 = buf833; del buf833  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_62.run(buf837, convolution_240, unsqueeze_1658, buf840, squeeze_388, buf838, primals_506, buf842, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del convolution_240
        del primals_506
        del squeeze_388
        del unsqueeze_1658
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf843 = aten.convolution_backward(buf842, convolution_239, primals_505, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf842
        del convolution_239
        del primals_505
        buf844 = buf843[0]
        buf845 = buf843[1]
        del buf843
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf846 = aten.convolution_backward(buf844, relu_127, primals_504, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_504
        buf847 = buf846[0]
        buf848 = buf846[1]
        del buf846
        buf849 = buf839; del buf839  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_127, buf847, buf849, 12096, 126, grid=grid(12096), stream=stream0)
        buf850 = buf840; del buf840  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf849, buf850, 432, 28, grid=grid(432), stream=stream0)
        buf851 = reinterpret_tensor(buf849, (432, 28), (1, 432), 0); del buf849  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_127, buf847, convolution_238, unsqueeze_1670, buf851, 12096, 126, grid=grid(12096), stream=stream0)
        buf852 = empty((432, ), device='cuda', dtype=torch.float32)
        buf853 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf851, squeeze_385, buf852, buf853, 432, 28, grid=grid(432), stream=stream0)
        buf854 = reinterpret_tensor(buf844, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf844  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_127, buf847, convolution_238, unsqueeze_1670, buf852, squeeze_385, buf850, primals_502, buf854, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf847
        del convolution_238
        del primals_502
        del relu_127
        del squeeze_385
        del unsqueeze_1670
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf855 = aten.convolution_backward(buf854, convolution_237, primals_501, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf854
        del convolution_237
        del primals_501
        buf856 = buf855[0]
        buf857 = buf855[1]
        del buf855
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf858 = aten.convolution_backward(buf856, relu_116, primals_500, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_500
        buf859 = buf858[0]
        buf860 = buf858[1]
        del buf858
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf861 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf837, (8, 432, 21, 21), (952560, 441, 21, 1), 571536), add_629, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_295)
        buf862 = buf861
        del buf861
        buf863 = buf852; del buf852  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_68.run(buf837, buf863, 432, 3528, grid=grid(432), stream=stream0)
        buf864 = reinterpret_tensor(buf851, (432, 28), (28, 1), 0); del buf851  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_69.run(buf837, convolution_236, unsqueeze_1682, buf864, 12096, 126, grid=grid(12096), stream=stream0)
        buf865 = empty((432, ), device='cuda', dtype=torch.float32)
        buf866 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf864, squeeze_382, buf865, buf866, 432, 28, grid=grid(432), stream=stream0)
        buf867 = reinterpret_tensor(buf856, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf856  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_70.run(buf837, convolution_236, unsqueeze_1682, buf865, squeeze_382, buf863, primals_498, buf867, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del convolution_236
        del primals_498
        del squeeze_382
        del unsqueeze_1682
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf868 = aten.convolution_backward(buf867, convolution_235, primals_497, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf867
        del convolution_235
        del primals_497
        buf869 = buf868[0]
        buf870 = buf868[1]
        del buf868
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf871 = aten.convolution_backward(buf869, relu_125, primals_496, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_496
        buf872 = buf871[0]
        buf873 = buf871[1]
        del buf871
        buf874 = buf864; del buf864  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_125, buf872, buf874, 12096, 126, grid=grid(12096), stream=stream0)
        buf875 = buf865; del buf865  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf874, buf875, 432, 28, grid=grid(432), stream=stream0)
        buf876 = reinterpret_tensor(buf874, (432, 28), (1, 432), 0); del buf874  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_125, buf872, convolution_234, unsqueeze_1694, buf876, 12096, 126, grid=grid(12096), stream=stream0)
        buf877 = empty((432, ), device='cuda', dtype=torch.float32)
        buf878 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf876, squeeze_379, buf877, buf878, 432, 28, grid=grid(432), stream=stream0)
        buf879 = reinterpret_tensor(buf869, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf869  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_125, buf872, convolution_234, unsqueeze_1694, buf877, squeeze_379, buf875, primals_494, buf879, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf872
        del convolution_234
        del primals_494
        del relu_125
        del squeeze_379
        del unsqueeze_1694
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf880 = aten.convolution_backward(buf879, convolution_233, primals_493, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_233
        del primals_493
        buf881 = buf880[0]
        buf882 = buf880[1]
        del buf880
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf883 = aten.convolution_backward(buf881, relu_124, primals_492, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_492
        buf884 = buf883[0]
        buf885 = buf883[1]
        del buf883
        buf886 = buf877; del buf877  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_71.run(buf837, relu_124, buf884, buf886, 432, 3528, grid=grid(432), stream=stream0)
        buf887 = reinterpret_tensor(buf876, (432, 28), (28, 1), 0); del buf876  # reuse
        buf909 = buf724; del buf724  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_72.run(buf837, relu_124, buf884, convolution_232, unsqueeze_1706, convolution_228, unsqueeze_1730, buf887, buf909, 12096, 126, grid=grid(12096), stream=stream0)
        buf888 = empty((432, ), device='cuda', dtype=torch.float32)
        buf890 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf887, squeeze_376, buf888, buf890, 432, 28, grid=grid(432), stream=stream0)
        buf910 = empty((432, ), device='cuda', dtype=torch.float32)
        buf912 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf909, squeeze_370, buf910, buf912, 432, 28, grid=grid(432), stream=stream0)
        buf889 = reinterpret_tensor(buf881, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf881  # reuse
        buf911 = buf879; del buf879  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_73.run(buf837, relu_124, buf884, convolution_232, unsqueeze_1706, buf888, squeeze_376, buf886, primals_490, convolution_228, unsqueeze_1730, buf910, squeeze_370, primals_482, buf889, buf911, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf884
        del convolution_228
        del convolution_232
        del primals_482
        del primals_490
        del relu_124
        del squeeze_370
        del squeeze_376
        del unsqueeze_1706
        del unsqueeze_1730
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf891 = aten.convolution_backward(buf889, convolution_231, primals_489, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf889
        del convolution_231
        del primals_489
        buf892 = buf891[0]
        buf893 = buf891[1]
        del buf891
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf894 = aten.convolution_backward(buf892, relu_123, primals_488, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_488
        buf895 = buf894[0]
        buf896 = buf894[1]
        del buf894
        buf897 = buf909; del buf909  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_123, buf895, buf897, 12096, 126, grid=grid(12096), stream=stream0)
        buf898 = buf910; del buf910  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf897, buf898, 432, 28, grid=grid(432), stream=stream0)
        buf899 = reinterpret_tensor(buf897, (432, 28), (1, 432), 0); del buf897  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_123, buf895, convolution_230, unsqueeze_1718, buf899, 12096, 126, grid=grid(12096), stream=stream0)
        buf900 = buf888; del buf888  # reuse
        buf901 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf899, squeeze_373, buf900, buf901, 432, 28, grid=grid(432), stream=stream0)
        buf902 = reinterpret_tensor(buf892, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf892  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_123, buf895, convolution_230, unsqueeze_1718, buf900, squeeze_373, buf898, primals_486, buf902, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf895
        del convolution_230
        del primals_486
        del relu_123
        del squeeze_373
        del unsqueeze_1718
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf903 = aten.convolution_backward(buf902, convolution_229, primals_485, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf902
        del convolution_229
        del primals_485
        buf904 = buf903[0]
        buf905 = buf903[1]
        del buf903
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf906 = aten.convolution_backward(buf904, relu_118, primals_484, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del buf904
        del primals_484
        buf907 = buf906[0]
        buf908 = buf906[1]
        del buf906
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf913 = aten.convolution_backward(buf911, convolution_227, primals_481, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf911
        del convolution_227
        del primals_481
        buf914 = buf913[0]
        buf915 = buf913[1]
        del buf913
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf916 = aten.convolution_backward(buf914, relu_121, primals_480, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_480
        buf917 = buf916[0]
        buf918 = buf916[1]
        del buf916
        buf919 = reinterpret_tensor(buf899, (432, 28), (28, 1), 0); del buf899  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_121, buf917, buf919, 12096, 126, grid=grid(12096), stream=stream0)
        buf920 = buf900; del buf900  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf919, buf920, 432, 28, grid=grid(432), stream=stream0)
        buf921 = reinterpret_tensor(buf919, (432, 28), (1, 432), 0); del buf919  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_121, buf917, convolution_226, unsqueeze_1742, buf921, 12096, 126, grid=grid(12096), stream=stream0)
        buf922 = empty((432, ), device='cuda', dtype=torch.float32)
        buf923 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf921, squeeze_367, buf922, buf923, 432, 28, grid=grid(432), stream=stream0)
        buf924 = reinterpret_tensor(buf914, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf914  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_121, buf917, convolution_226, unsqueeze_1742, buf922, squeeze_367, buf920, primals_478, buf924, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf917
        del convolution_226
        del primals_478
        del relu_121
        del squeeze_367
        del unsqueeze_1742
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf925 = aten.convolution_backward(buf924, convolution_225, primals_477, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf924
        del convolution_225
        del primals_477
        buf926 = buf925[0]
        buf927 = buf925[1]
        del buf925
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf928 = aten.convolution_backward(buf926, relu_118, primals_476, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_476
        buf929 = buf928[0]
        buf930 = buf928[1]
        del buf928
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf931 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf837, (8, 432, 21, 21), (952560, 441, 21, 1), 190512), add_629, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_295)
        del add_629
        del getitem_295
        buf932 = buf931
        del buf931
        buf933 = buf922; del buf922  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_74.run(buf837, buf933, 432, 3528, grid=grid(432), stream=stream0)
        buf934 = reinterpret_tensor(buf921, (432, 28), (28, 1), 0); del buf921  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_75.run(buf837, convolution_224, unsqueeze_1754, buf934, 12096, 126, grid=grid(12096), stream=stream0)
        buf935 = empty((432, ), device='cuda', dtype=torch.float32)
        buf936 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf934, squeeze_364, buf935, buf936, 432, 28, grid=grid(432), stream=stream0)
        buf937 = reinterpret_tensor(buf926, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf926  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_76.run(buf837, convolution_224, unsqueeze_1754, buf935, squeeze_364, buf933, primals_474, buf937, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del convolution_224
        del primals_474
        del squeeze_364
        del unsqueeze_1754
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf938 = aten.convolution_backward(buf937, convolution_223, primals_473, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf937
        del convolution_223
        del primals_473
        buf939 = buf938[0]
        buf940 = buf938[1]
        del buf938
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf941 = aten.convolution_backward(buf939, relu_119, primals_472, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_472
        buf942 = buf941[0]
        buf943 = buf941[1]
        del buf941
        buf944 = buf934; del buf934  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_119, buf942, buf944, 12096, 126, grid=grid(12096), stream=stream0)
        buf945 = buf935; del buf935  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf944, buf945, 432, 28, grid=grid(432), stream=stream0)
        buf946 = reinterpret_tensor(buf944, (432, 28), (1, 432), 0); del buf944  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_119, buf942, convolution_222, unsqueeze_1766, buf946, 12096, 126, grid=grid(12096), stream=stream0)
        buf947 = empty((432, ), device='cuda', dtype=torch.float32)
        buf948 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf946, squeeze_361, buf947, buf948, 432, 28, grid=grid(432), stream=stream0)
        buf949 = reinterpret_tensor(buf939, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf939  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_119, buf942, convolution_222, unsqueeze_1766, buf947, squeeze_361, buf945, primals_470, buf949, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf942
        del convolution_222
        del primals_470
        del relu_119
        del squeeze_361
        del unsqueeze_1766
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf950 = aten.convolution_backward(buf949, convolution_221, primals_469, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf949
        del convolution_221
        del primals_469
        buf951 = buf950[0]
        buf952 = buf950[1]
        del buf950
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf953 = aten.convolution_backward(buf951, relu_118, primals_468, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 432, [True, True, False])
        del buf951
        del primals_468
        buf954 = buf953[0]
        buf955 = buf953[1]
        del buf953
        buf956 = buf907; del buf907  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_77.run(buf956, buf837, buf862, relu_118, buf929, buf932, buf954, 3456, 441, grid=grid(3456, 441), stream=stream0)
        del buf862
        del buf929
        del buf932
        del relu_118
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf957 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf837, (8, 432, 21, 21), (952560, 441, 21, 1), 0), add_624, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_289)
        del add_624
        del getitem_289
        buf958 = buf957
        del buf957
        buf959 = buf947; del buf947  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_78.run(buf837, buf959, 432, 3528, grid=grid(432), stream=stream0)
        buf960 = reinterpret_tensor(buf946, (432, 28), (28, 1), 0); del buf946  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_79.run(buf837, convolution_220, unsqueeze_1778, buf960, 12096, 126, grid=grid(12096), stream=stream0)
        buf961 = empty((432, ), device='cuda', dtype=torch.float32)
        buf962 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf960, squeeze_358, buf961, buf962, 432, 28, grid=grid(432), stream=stream0)
        buf963 = reinterpret_tensor(buf954, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf954  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_80.run(buf837, convolution_220, unsqueeze_1778, buf961, squeeze_358, buf959, primals_466, buf963, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf837
        del convolution_220
        del primals_466
        del squeeze_358
        del unsqueeze_1778
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf964 = aten.convolution_backward(buf963, convolution_219, primals_465, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf963
        del convolution_219
        del primals_465
        buf965 = buf964[0]
        buf966 = buf964[1]
        del buf964
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf967 = aten.convolution_backward(buf965, relu_117, primals_464, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_464
        buf968 = buf967[0]
        buf969 = buf967[1]
        del buf967
        buf970 = buf960; del buf960  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_117, buf968, buf970, 12096, 126, grid=grid(12096), stream=stream0)
        buf971 = buf961; del buf961  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf970, buf971, 432, 28, grid=grid(432), stream=stream0)
        buf972 = reinterpret_tensor(buf970, (432, 28), (1, 432), 0); del buf970  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_117, buf968, convolution_218, unsqueeze_1790, buf972, 12096, 126, grid=grid(12096), stream=stream0)
        buf973 = empty((432, ), device='cuda', dtype=torch.float32)
        buf974 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf972, squeeze_355, buf973, buf974, 432, 28, grid=grid(432), stream=stream0)
        buf975 = reinterpret_tensor(buf965, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf965  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_117, buf968, convolution_218, unsqueeze_1790, buf973, squeeze_355, buf971, primals_462, buf975, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf968
        del convolution_218
        del primals_462
        del relu_117
        del squeeze_355
        del unsqueeze_1790
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf976 = aten.convolution_backward(buf975, convolution_217, primals_461, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf975
        del convolution_217
        del primals_461
        buf977 = buf976[0]
        buf978 = buf976[1]
        del buf976
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf979 = aten.convolution_backward(buf977, relu_116, primals_460, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_460
        buf980 = buf979[0]
        buf981 = buf979[1]
        del buf979
        buf982 = buf973; del buf973  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_81.run(buf956, buf982, 432, 3528, grid=grid(432), stream=stream0)
        buf983 = reinterpret_tensor(buf972, (432, 28), (28, 1), 0); del buf972  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_82.run(buf956, convolution_216, unsqueeze_1802, buf983, 12096, 126, grid=grid(12096), stream=stream0)
        buf984 = empty((432, ), device='cuda', dtype=torch.float32)
        buf985 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf983, squeeze_352, buf984, buf985, 432, 28, grid=grid(432), stream=stream0)
        buf986 = reinterpret_tensor(buf977, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf977  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_83.run(buf956, convolution_216, unsqueeze_1802, buf984, squeeze_352, buf982, primals_458, buf986, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf956
        del convolution_216
        del primals_458
        del squeeze_352
        del unsqueeze_1802
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf987 = aten.convolution_backward(buf986, relu_115, primals_457, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf986
        del primals_457
        buf988 = buf987[0]
        buf989 = buf987[1]
        del buf987
        buf990 = buf983; del buf983  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_84.run(relu_116, buf859, buf958, buf980, buf990, 12096, 126, grid=grid(12096), stream=stream0)
        buf991 = buf984; del buf984  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf990, buf991, 432, 28, grid=grid(432), stream=stream0)
        buf992 = reinterpret_tensor(buf990, (432, 28), (1, 432), 0); del buf990  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_85.run(relu_116, buf859, buf958, buf980, convolution_215, unsqueeze_1814, buf992, 12096, 126, grid=grid(12096), stream=stream0)
        buf993 = empty((432, ), device='cuda', dtype=torch.float32)
        buf995 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf992, squeeze_349, buf993, buf995, 432, 28, grid=grid(432), stream=stream0)
        buf994 = buf958; del buf958  # reuse
        buf996 = buf994; del buf994  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_86.run(buf996, relu_116, buf859, buf980, convolution_215, unsqueeze_1814, buf993, squeeze_349, buf991, primals_455, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf859
        del buf980
        del convolution_215
        del primals_455
        del relu_116
        del squeeze_349
        del unsqueeze_1814
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf997 = aten.convolution_backward(buf996, relu_101, primals_454, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_454
        buf998 = buf997[0]
        buf999 = buf997[1]
        del buf997
        buf1000 = buf835; del buf835  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_87.run(buf1000, relu_115, buf988, 17280, 441, grid=grid(17280, 441), stream=stream0)
        del buf988
        del relu_115
        buf1001 = buf993; del buf993  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_59.run(buf1000, buf1001, 432, 3528, grid=grid(432), stream=stream0)
        buf1002 = reinterpret_tensor(buf992, (432, 28), (28, 1), 0); del buf992  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_60.run(buf1000, convolution_214, unsqueeze_1826, buf1002, 12096, 126, grid=grid(12096), stream=stream0)
        buf1003 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1004 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1002, squeeze_346, buf1003, buf1004, 432, 28, grid=grid(432), stream=stream0)
        buf1005 = buf996; del buf996  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_62.run(buf1000, convolution_214, unsqueeze_1826, buf1003, squeeze_346, buf1001, primals_452, buf1005, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del convolution_214
        del primals_452
        del squeeze_346
        del unsqueeze_1826
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1006 = aten.convolution_backward(buf1005, convolution_213, primals_451, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1005
        del convolution_213
        del primals_451
        buf1007 = buf1006[0]
        buf1008 = buf1006[1]
        del buf1006
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1009 = aten.convolution_backward(buf1007, relu_113, primals_450, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_450
        buf1010 = buf1009[0]
        buf1011 = buf1009[1]
        del buf1009
        buf1012 = buf1002; del buf1002  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_113, buf1010, buf1012, 12096, 126, grid=grid(12096), stream=stream0)
        buf1013 = buf1003; del buf1003  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1012, buf1013, 432, 28, grid=grid(432), stream=stream0)
        buf1014 = reinterpret_tensor(buf1012, (432, 28), (1, 432), 0); del buf1012  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_113, buf1010, convolution_212, unsqueeze_1838, buf1014, 12096, 126, grid=grid(12096), stream=stream0)
        buf1015 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1016 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1014, squeeze_343, buf1015, buf1016, 432, 28, grid=grid(432), stream=stream0)
        buf1017 = reinterpret_tensor(buf1007, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1007  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_113, buf1010, convolution_212, unsqueeze_1838, buf1015, squeeze_343, buf1013, primals_448, buf1017, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1010
        del convolution_212
        del primals_448
        del relu_113
        del squeeze_343
        del unsqueeze_1838
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1018 = aten.convolution_backward(buf1017, convolution_211, primals_447, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1017
        del convolution_211
        del primals_447
        buf1019 = buf1018[0]
        buf1020 = buf1018[1]
        del buf1018
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1021 = aten.convolution_backward(buf1019, relu_102, primals_446, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_446
        buf1022 = buf1021[0]
        buf1023 = buf1021[1]
        del buf1021
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1024 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1000, (8, 432, 21, 21), (952560, 441, 21, 1), 571536), add_554, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_261)
        buf1025 = buf1024
        del buf1024
        buf1026 = buf1015; del buf1015  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_68.run(buf1000, buf1026, 432, 3528, grid=grid(432), stream=stream0)
        buf1027 = reinterpret_tensor(buf1014, (432, 28), (28, 1), 0); del buf1014  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_69.run(buf1000, convolution_210, unsqueeze_1850, buf1027, 12096, 126, grid=grid(12096), stream=stream0)
        buf1028 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1029 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1027, squeeze_340, buf1028, buf1029, 432, 28, grid=grid(432), stream=stream0)
        buf1030 = reinterpret_tensor(buf1019, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1019  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_70.run(buf1000, convolution_210, unsqueeze_1850, buf1028, squeeze_340, buf1026, primals_444, buf1030, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del convolution_210
        del primals_444
        del squeeze_340
        del unsqueeze_1850
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1031 = aten.convolution_backward(buf1030, convolution_209, primals_443, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1030
        del convolution_209
        del primals_443
        buf1032 = buf1031[0]
        buf1033 = buf1031[1]
        del buf1031
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1034 = aten.convolution_backward(buf1032, relu_111, primals_442, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_442
        buf1035 = buf1034[0]
        buf1036 = buf1034[1]
        del buf1034
        buf1037 = buf1027; del buf1027  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_111, buf1035, buf1037, 12096, 126, grid=grid(12096), stream=stream0)
        buf1038 = buf1028; del buf1028  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1037, buf1038, 432, 28, grid=grid(432), stream=stream0)
        buf1039 = reinterpret_tensor(buf1037, (432, 28), (1, 432), 0); del buf1037  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_111, buf1035, convolution_208, unsqueeze_1862, buf1039, 12096, 126, grid=grid(12096), stream=stream0)
        buf1040 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1041 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1039, squeeze_337, buf1040, buf1041, 432, 28, grid=grid(432), stream=stream0)
        buf1042 = reinterpret_tensor(buf1032, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1032  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_111, buf1035, convolution_208, unsqueeze_1862, buf1040, squeeze_337, buf1038, primals_440, buf1042, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1035
        del convolution_208
        del primals_440
        del relu_111
        del squeeze_337
        del unsqueeze_1862
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1043 = aten.convolution_backward(buf1042, convolution_207, primals_439, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_207
        del primals_439
        buf1044 = buf1043[0]
        buf1045 = buf1043[1]
        del buf1043
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1046 = aten.convolution_backward(buf1044, relu_110, primals_438, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_438
        buf1047 = buf1046[0]
        buf1048 = buf1046[1]
        del buf1046
        buf1049 = buf1040; del buf1040  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_71.run(buf1000, relu_110, buf1047, buf1049, 432, 3528, grid=grid(432), stream=stream0)
        buf1050 = reinterpret_tensor(buf1039, (432, 28), (28, 1), 0); del buf1039  # reuse
        buf1072 = buf887; del buf887  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_72.run(buf1000, relu_110, buf1047, convolution_206, unsqueeze_1874, convolution_202, unsqueeze_1898, buf1050, buf1072, 12096, 126, grid=grid(12096), stream=stream0)
        buf1051 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1053 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1050, squeeze_334, buf1051, buf1053, 432, 28, grid=grid(432), stream=stream0)
        buf1073 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1075 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1072, squeeze_328, buf1073, buf1075, 432, 28, grid=grid(432), stream=stream0)
        buf1052 = reinterpret_tensor(buf1044, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1044  # reuse
        buf1074 = buf1042; del buf1042  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_73.run(buf1000, relu_110, buf1047, convolution_206, unsqueeze_1874, buf1051, squeeze_334, buf1049, primals_436, convolution_202, unsqueeze_1898, buf1073, squeeze_328, primals_428, buf1052, buf1074, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1047
        del convolution_202
        del convolution_206
        del primals_428
        del primals_436
        del relu_110
        del squeeze_328
        del squeeze_334
        del unsqueeze_1874
        del unsqueeze_1898
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1054 = aten.convolution_backward(buf1052, convolution_205, primals_435, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1052
        del convolution_205
        del primals_435
        buf1055 = buf1054[0]
        buf1056 = buf1054[1]
        del buf1054
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1057 = aten.convolution_backward(buf1055, relu_109, primals_434, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_434
        buf1058 = buf1057[0]
        buf1059 = buf1057[1]
        del buf1057
        buf1060 = buf1072; del buf1072  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_109, buf1058, buf1060, 12096, 126, grid=grid(12096), stream=stream0)
        buf1061 = buf1073; del buf1073  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1060, buf1061, 432, 28, grid=grid(432), stream=stream0)
        buf1062 = reinterpret_tensor(buf1060, (432, 28), (1, 432), 0); del buf1060  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_109, buf1058, convolution_204, unsqueeze_1886, buf1062, 12096, 126, grid=grid(12096), stream=stream0)
        buf1063 = buf1051; del buf1051  # reuse
        buf1064 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1062, squeeze_331, buf1063, buf1064, 432, 28, grid=grid(432), stream=stream0)
        buf1065 = reinterpret_tensor(buf1055, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1055  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_109, buf1058, convolution_204, unsqueeze_1886, buf1063, squeeze_331, buf1061, primals_432, buf1065, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1058
        del convolution_204
        del primals_432
        del relu_109
        del squeeze_331
        del unsqueeze_1886
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1066 = aten.convolution_backward(buf1065, convolution_203, primals_431, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1065
        del convolution_203
        del primals_431
        buf1067 = buf1066[0]
        buf1068 = buf1066[1]
        del buf1066
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1069 = aten.convolution_backward(buf1067, relu_104, primals_430, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del buf1067
        del primals_430
        buf1070 = buf1069[0]
        buf1071 = buf1069[1]
        del buf1069
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1076 = aten.convolution_backward(buf1074, convolution_201, primals_427, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1074
        del convolution_201
        del primals_427
        buf1077 = buf1076[0]
        buf1078 = buf1076[1]
        del buf1076
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1079 = aten.convolution_backward(buf1077, relu_107, primals_426, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_426
        buf1080 = buf1079[0]
        buf1081 = buf1079[1]
        del buf1079
        buf1082 = reinterpret_tensor(buf1062, (432, 28), (28, 1), 0); del buf1062  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_107, buf1080, buf1082, 12096, 126, grid=grid(12096), stream=stream0)
        buf1083 = buf1063; del buf1063  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1082, buf1083, 432, 28, grid=grid(432), stream=stream0)
        buf1084 = reinterpret_tensor(buf1082, (432, 28), (1, 432), 0); del buf1082  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_107, buf1080, convolution_200, unsqueeze_1910, buf1084, 12096, 126, grid=grid(12096), stream=stream0)
        buf1085 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1086 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1084, squeeze_325, buf1085, buf1086, 432, 28, grid=grid(432), stream=stream0)
        buf1087 = reinterpret_tensor(buf1077, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1077  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_107, buf1080, convolution_200, unsqueeze_1910, buf1085, squeeze_325, buf1083, primals_424, buf1087, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1080
        del convolution_200
        del primals_424
        del relu_107
        del squeeze_325
        del unsqueeze_1910
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1088 = aten.convolution_backward(buf1087, convolution_199, primals_423, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1087
        del convolution_199
        del primals_423
        buf1089 = buf1088[0]
        buf1090 = buf1088[1]
        del buf1088
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1091 = aten.convolution_backward(buf1089, relu_104, primals_422, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_422
        buf1092 = buf1091[0]
        buf1093 = buf1091[1]
        del buf1091
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1094 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1000, (8, 432, 21, 21), (952560, 441, 21, 1), 190512), add_554, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_261)
        del add_554
        del getitem_261
        buf1095 = buf1094
        del buf1094
        buf1096 = buf1085; del buf1085  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_74.run(buf1000, buf1096, 432, 3528, grid=grid(432), stream=stream0)
        buf1097 = reinterpret_tensor(buf1084, (432, 28), (28, 1), 0); del buf1084  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_75.run(buf1000, convolution_198, unsqueeze_1922, buf1097, 12096, 126, grid=grid(12096), stream=stream0)
        buf1098 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1099 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1097, squeeze_322, buf1098, buf1099, 432, 28, grid=grid(432), stream=stream0)
        buf1100 = reinterpret_tensor(buf1089, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1089  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_76.run(buf1000, convolution_198, unsqueeze_1922, buf1098, squeeze_322, buf1096, primals_420, buf1100, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del convolution_198
        del primals_420
        del squeeze_322
        del unsqueeze_1922
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1101 = aten.convolution_backward(buf1100, convolution_197, primals_419, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1100
        del convolution_197
        del primals_419
        buf1102 = buf1101[0]
        buf1103 = buf1101[1]
        del buf1101
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1104 = aten.convolution_backward(buf1102, relu_105, primals_418, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_418
        buf1105 = buf1104[0]
        buf1106 = buf1104[1]
        del buf1104
        buf1107 = buf1097; del buf1097  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_105, buf1105, buf1107, 12096, 126, grid=grid(12096), stream=stream0)
        buf1108 = buf1098; del buf1098  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1107, buf1108, 432, 28, grid=grid(432), stream=stream0)
        buf1109 = reinterpret_tensor(buf1107, (432, 28), (1, 432), 0); del buf1107  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_105, buf1105, convolution_196, unsqueeze_1934, buf1109, 12096, 126, grid=grid(12096), stream=stream0)
        buf1110 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1111 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1109, squeeze_319, buf1110, buf1111, 432, 28, grid=grid(432), stream=stream0)
        buf1112 = reinterpret_tensor(buf1102, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1102  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_105, buf1105, convolution_196, unsqueeze_1934, buf1110, squeeze_319, buf1108, primals_416, buf1112, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1105
        del convolution_196
        del primals_416
        del relu_105
        del squeeze_319
        del unsqueeze_1934
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1113 = aten.convolution_backward(buf1112, convolution_195, primals_415, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1112
        del convolution_195
        del primals_415
        buf1114 = buf1113[0]
        buf1115 = buf1113[1]
        del buf1113
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1116 = aten.convolution_backward(buf1114, relu_104, primals_414, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 432, [True, True, False])
        del buf1114
        del primals_414
        buf1117 = buf1116[0]
        buf1118 = buf1116[1]
        del buf1116
        buf1119 = buf1070; del buf1070  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_77.run(buf1119, buf1000, buf1025, relu_104, buf1092, buf1095, buf1117, 3456, 441, grid=grid(3456, 441), stream=stream0)
        del buf1025
        del buf1092
        del buf1095
        del relu_104
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1120 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1000, (8, 432, 21, 21), (952560, 441, 21, 1), 0), add_549, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_255)
        del add_549
        del getitem_255
        buf1121 = buf1120
        del buf1120
        buf1122 = buf1110; del buf1110  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_78.run(buf1000, buf1122, 432, 3528, grid=grid(432), stream=stream0)
        buf1123 = reinterpret_tensor(buf1109, (432, 28), (28, 1), 0); del buf1109  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_79.run(buf1000, convolution_194, unsqueeze_1946, buf1123, 12096, 126, grid=grid(12096), stream=stream0)
        buf1124 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1125 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1123, squeeze_316, buf1124, buf1125, 432, 28, grid=grid(432), stream=stream0)
        buf1126 = reinterpret_tensor(buf1117, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1117  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_80.run(buf1000, convolution_194, unsqueeze_1946, buf1124, squeeze_316, buf1122, primals_412, buf1126, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1000
        del convolution_194
        del primals_412
        del squeeze_316
        del unsqueeze_1946
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1127 = aten.convolution_backward(buf1126, convolution_193, primals_411, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1126
        del convolution_193
        del primals_411
        buf1128 = buf1127[0]
        buf1129 = buf1127[1]
        del buf1127
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1130 = aten.convolution_backward(buf1128, relu_103, primals_410, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_410
        buf1131 = buf1130[0]
        buf1132 = buf1130[1]
        del buf1130
        buf1133 = buf1123; del buf1123  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_103, buf1131, buf1133, 12096, 126, grid=grid(12096), stream=stream0)
        buf1134 = buf1124; del buf1124  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1133, buf1134, 432, 28, grid=grid(432), stream=stream0)
        buf1135 = reinterpret_tensor(buf1133, (432, 28), (1, 432), 0); del buf1133  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_103, buf1131, convolution_192, unsqueeze_1958, buf1135, 12096, 126, grid=grid(12096), stream=stream0)
        buf1136 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1137 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1135, squeeze_313, buf1136, buf1137, 432, 28, grid=grid(432), stream=stream0)
        buf1138 = reinterpret_tensor(buf1128, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1128  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_103, buf1131, convolution_192, unsqueeze_1958, buf1136, squeeze_313, buf1134, primals_408, buf1138, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1131
        del convolution_192
        del primals_408
        del relu_103
        del squeeze_313
        del unsqueeze_1958
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1139 = aten.convolution_backward(buf1138, convolution_191, primals_407, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1138
        del convolution_191
        del primals_407
        buf1140 = buf1139[0]
        buf1141 = buf1139[1]
        del buf1139
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1142 = aten.convolution_backward(buf1140, relu_102, primals_406, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_406
        buf1143 = buf1142[0]
        buf1144 = buf1142[1]
        del buf1142
        buf1145 = buf1136; del buf1136  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_81.run(buf1119, buf1145, 432, 3528, grid=grid(432), stream=stream0)
        buf1146 = reinterpret_tensor(buf1135, (432, 28), (28, 1), 0); del buf1135  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_82.run(buf1119, convolution_190, unsqueeze_1970, buf1146, 12096, 126, grid=grid(12096), stream=stream0)
        buf1147 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1148 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1146, squeeze_310, buf1147, buf1148, 432, 28, grid=grid(432), stream=stream0)
        buf1149 = reinterpret_tensor(buf1140, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1140  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_83.run(buf1119, convolution_190, unsqueeze_1970, buf1147, squeeze_310, buf1145, primals_404, buf1149, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1119
        del convolution_190
        del primals_404
        del squeeze_310
        del unsqueeze_1970
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1150 = aten.convolution_backward(buf1149, relu_101, primals_403, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1149
        del primals_403
        buf1151 = buf1150[0]
        buf1152 = buf1150[1]
        del buf1150
        buf1153 = buf1146; del buf1146  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_84.run(relu_102, buf1022, buf1121, buf1143, buf1153, 12096, 126, grid=grid(12096), stream=stream0)
        buf1154 = buf1147; del buf1147  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1153, buf1154, 432, 28, grid=grid(432), stream=stream0)
        buf1155 = reinterpret_tensor(buf1153, (432, 28), (1, 432), 0); del buf1153  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_85.run(relu_102, buf1022, buf1121, buf1143, cat_9, unsqueeze_1982, buf1155, 12096, 126, grid=grid(12096), stream=stream0)
        buf1156 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1158 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1155, squeeze_307, buf1156, buf1158, 432, 28, grid=grid(432), stream=stream0)
        buf1157 = buf1121; del buf1121  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_88.run(buf1157, relu_102, buf1022, buf1143, cat_9, unsqueeze_1982, buf1156, squeeze_307, buf1154, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1022
        del cat_9
        del relu_102
        del unsqueeze_1982
        buf1159 = empty_strided((8, 216, 21, 21), (95256, 1, 4536, 216), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        triton_poi_fused_convolution_backward_89.run(buf1157, squeeze_307, primals_401, buf1159, 762048, grid=grid(762048), stream=stream0)
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1160 = aten.convolution_backward(buf1159, avg_pool2d_5, primals_400, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del avg_pool2d_5
        del primals_400
        buf1161 = buf1160[0]
        buf1162 = buf1160[1]
        del buf1160
        buf1163 = buf1159; del buf1159  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        triton_poi_fused_convolution_backward_90.run(buf1157, squeeze_307, primals_401, buf1163, 762048, grid=grid(762048), stream=stream0)
        del primals_401
        del squeeze_307
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1164 = aten.convolution_backward(buf1163, avg_pool2d_4, primals_399, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del avg_pool2d_4
        del buf1163
        del primals_399
        buf1165 = buf1164[0]
        buf1166 = buf1164[1]
        del buf1164
        buf1168 = buf1151; del buf1151  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_91.run(buf1168, relu_101, buf998, 17280, 441, grid=grid(17280, 441), stream=stream0)
        del buf998
        del relu_101
        buf1169 = buf1156; del buf1156  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_59.run(buf1168, buf1169, 432, 3528, grid=grid(432), stream=stream0)
        buf1170 = reinterpret_tensor(buf1155, (432, 28), (28, 1), 0); del buf1155  # reuse
        buf1177 = buf1050; del buf1050  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_92.run(buf1168, convolution_187, unsqueeze_1994, convolution_186, unsqueeze_2006, buf1170, buf1177, 12096, 126, grid=grid(12096), stream=stream0)
        buf1171 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1172 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1170, squeeze_304, buf1171, buf1172, 432, 28, grid=grid(432), stream=stream0)
        buf1178 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1179 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1177, squeeze_301, buf1178, buf1179, 432, 28, grid=grid(432), stream=stream0)
        buf1173 = buf1157; del buf1157  # reuse
        buf1180 = reinterpret_tensor(buf1143, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1143  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_93.run(buf1168, convolution_187, unsqueeze_1994, buf1171, squeeze_304, buf1169, primals_397, convolution_186, unsqueeze_2006, buf1178, squeeze_301, primals_395, buf1173, buf1180, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del convolution_186
        del convolution_187
        del primals_395
        del primals_397
        del squeeze_301
        del squeeze_304
        del unsqueeze_1994
        del unsqueeze_2006
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1174 = aten.convolution_backward(buf1173, constant_pad_nd_28, primals_20, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del constant_pad_nd_28
        del primals_20
        buf1175 = buf1174[0]
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1199 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1168, (8, 432, 21, 21), (952560, 441, 21, 1), 571536), constant_pad_nd_23, [3, 3], [2, 2], [0, 0], [1, 1], False, getitem_225)
        buf1200 = buf1199
        del buf1199
        buf1201 = buf1178; del buf1178  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_68.run(buf1168, buf1201, 432, 3528, grid=grid(432), stream=stream0)
        buf1202 = buf1177; del buf1177  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_69.run(buf1168, convolution_182, unsqueeze_2030, buf1202, 12096, 126, grid=grid(12096), stream=stream0)
        buf1203 = buf1171; del buf1171  # reuse
        buf1204 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1202, squeeze_295, buf1203, buf1204, 432, 28, grid=grid(432), stream=stream0)
        buf1205 = buf1173; del buf1173  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_70.run(buf1168, convolution_182, unsqueeze_2030, buf1203, squeeze_295, buf1201, primals_388, buf1205, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del convolution_182
        del primals_388
        del squeeze_295
        del unsqueeze_2030
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1206 = aten.convolution_backward(buf1205, convolution_181, primals_387, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1205
        del convolution_181
        del primals_387
        buf1207 = buf1206[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1209 = aten.convolution_backward(buf1207, relu_96, primals_386, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_386
        buf1210 = buf1209[0]
        buf1212 = buf1202; del buf1202  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_96, buf1210, buf1212, 12096, 126, grid=grid(12096), stream=stream0)
        buf1213 = buf1203; del buf1203  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1212, buf1213, 432, 28, grid=grid(432), stream=stream0)
        buf1214 = reinterpret_tensor(buf1212, (432, 28), (1, 432), 0); del buf1212  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_96, buf1210, convolution_180, unsqueeze_2042, buf1214, 12096, 126, grid=grid(12096), stream=stream0)
        buf1215 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1216 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1214, squeeze_292, buf1215, buf1216, 432, 28, grid=grid(432), stream=stream0)
        buf1217 = reinterpret_tensor(buf1207, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1207  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_96, buf1210, convolution_180, unsqueeze_2042, buf1215, squeeze_292, buf1213, primals_384, buf1217, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1210
        del convolution_180
        del primals_384
        del relu_96
        del squeeze_292
        del unsqueeze_2042
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1218 = aten.convolution_backward(buf1217, convolution_179, primals_383, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_179
        del primals_383
        buf1219 = buf1218[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1221 = aten.convolution_backward(buf1219, relu_95, primals_382, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_382
        buf1222 = buf1221[0]
        buf1224 = buf1215; del buf1215  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_71.run(buf1168, relu_95, buf1222, buf1224, 432, 3528, grid=grid(432), stream=stream0)
        buf1225 = reinterpret_tensor(buf1214, (432, 28), (28, 1), 0); del buf1214  # reuse
        buf1247 = buf1170; del buf1170  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_72.run(buf1168, relu_95, buf1222, convolution_178, unsqueeze_2054, convolution_174, unsqueeze_2078, buf1225, buf1247, 12096, 126, grid=grid(12096), stream=stream0)
        buf1226 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1228 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1225, squeeze_289, buf1226, buf1228, 432, 28, grid=grid(432), stream=stream0)
        del buf1225
        buf1248 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1250 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1247, squeeze_283, buf1248, buf1250, 432, 28, grid=grid(432), stream=stream0)
        buf1227 = reinterpret_tensor(buf1219, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1219  # reuse
        buf1249 = buf1217; del buf1217  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_73.run(buf1168, relu_95, buf1222, convolution_178, unsqueeze_2054, buf1226, squeeze_289, buf1224, primals_380, convolution_174, unsqueeze_2078, buf1248, squeeze_283, primals_373, buf1227, buf1249, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1222
        del convolution_174
        del convolution_178
        del primals_373
        del primals_380
        del relu_95
        del squeeze_283
        del squeeze_289
        del unsqueeze_2054
        del unsqueeze_2078
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1229 = aten.convolution_backward(buf1227, convolution_177, primals_379, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1227
        del convolution_177
        del primals_379
        buf1230 = buf1229[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1232 = aten.convolution_backward(buf1230, relu_94, primals_378, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_378
        buf1233 = buf1232[0]
        buf1235 = buf1247; del buf1247  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_94, buf1233, buf1235, 12096, 126, grid=grid(12096), stream=stream0)
        buf1236 = buf1248; del buf1248  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1235, buf1236, 432, 28, grid=grid(432), stream=stream0)
        buf1237 = reinterpret_tensor(buf1235, (432, 28), (1, 432), 0); del buf1235  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_94, buf1233, convolution_176, unsqueeze_2066, buf1237, 12096, 126, grid=grid(12096), stream=stream0)
        buf1238 = buf1226; del buf1226  # reuse
        buf1239 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1237, squeeze_286, buf1238, buf1239, 432, 28, grid=grid(432), stream=stream0)
        buf1240 = reinterpret_tensor(buf1230, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1230  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_94, buf1233, convolution_176, unsqueeze_2066, buf1238, squeeze_286, buf1236, primals_376, buf1240, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1233
        del convolution_176
        del primals_376
        del relu_94
        del squeeze_286
        del unsqueeze_2066
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1241 = aten.convolution_backward(buf1240, convolution_175, primals_375, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1240
        del convolution_175
        del primals_375
        buf1242 = buf1241[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1244 = aten.convolution_backward(buf1242, constant_pad_nd_25, primals_18, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 432, [True, True, False])
        del buf1242
        del constant_pad_nd_25
        del primals_18
        buf1245 = buf1244[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1251 = aten.convolution_backward(buf1249, convolution_173, primals_372, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1249
        del convolution_173
        del primals_372
        buf1252 = buf1251[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1254 = aten.convolution_backward(buf1252, relu_92, primals_371, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_371
        buf1255 = buf1254[0]
        buf1257 = reinterpret_tensor(buf1237, (432, 28), (28, 1), 0); del buf1237  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_92, buf1255, buf1257, 12096, 126, grid=grid(12096), stream=stream0)
        buf1258 = buf1238; del buf1238  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1257, buf1258, 432, 28, grid=grid(432), stream=stream0)
        buf1259 = reinterpret_tensor(buf1257, (432, 28), (1, 432), 0); del buf1257  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_92, buf1255, convolution_172, unsqueeze_2090, buf1259, 12096, 126, grid=grid(12096), stream=stream0)
        buf1260 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1261 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1259, squeeze_280, buf1260, buf1261, 432, 28, grid=grid(432), stream=stream0)
        buf1262 = reinterpret_tensor(buf1252, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1252  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_92, buf1255, convolution_172, unsqueeze_2090, buf1260, squeeze_280, buf1258, primals_369, buf1262, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1255
        del convolution_172
        del primals_369
        del relu_92
        del squeeze_280
        del unsqueeze_2090
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1263 = aten.convolution_backward(buf1262, convolution_171, primals_368, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1262
        del convolution_171
        del primals_368
        buf1264 = buf1263[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1266 = aten.convolution_backward(buf1264, constant_pad_nd_24, primals_17, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 432, [True, True, False])
        del constant_pad_nd_24
        del primals_17
        buf1267 = buf1266[0]
        buf1269 = buf1175; del buf1175  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.threshold_backward]
        triton_poi_fused_add_constant_pad_nd_threshold_backward_94.run(buf1269, le_100, buf1200, buf1245, buf1267, 3456, 1764, grid=grid(3456, 1764), stream=stream0)
        del buf1200
        del buf1245
        del buf1267
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1270 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1168, (8, 432, 21, 21), (952560, 441, 21, 1), 190512), constant_pad_nd_23, [3, 3], [2, 2], [0, 0], [1, 1], False, getitem_225)
        del constant_pad_nd_23
        del getitem_225
        buf1271 = buf1270
        del buf1270
        buf1272 = buf1260; del buf1260  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_74.run(buf1168, buf1272, 432, 3528, grid=grid(432), stream=stream0)
        buf1273 = reinterpret_tensor(buf1259, (432, 28), (28, 1), 0); del buf1259  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_75.run(buf1168, convolution_170, unsqueeze_2102, buf1273, 12096, 126, grid=grid(12096), stream=stream0)
        buf1274 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1275 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1273, squeeze_277, buf1274, buf1275, 432, 28, grid=grid(432), stream=stream0)
        buf1276 = reinterpret_tensor(buf1264, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1264  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_76.run(buf1168, convolution_170, unsqueeze_2102, buf1274, squeeze_277, buf1272, primals_366, buf1276, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del convolution_170
        del primals_366
        del squeeze_277
        del unsqueeze_2102
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1277 = aten.convolution_backward(buf1276, convolution_169, primals_365, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1276
        del convolution_169
        del primals_365
        buf1278 = buf1277[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1280 = aten.convolution_backward(buf1278, relu_90, primals_364, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_364
        buf1281 = buf1280[0]
        buf1283 = buf1273; del buf1273  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_90, buf1281, buf1283, 12096, 126, grid=grid(12096), stream=stream0)
        buf1284 = buf1274; del buf1274  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1283, buf1284, 432, 28, grid=grid(432), stream=stream0)
        buf1285 = reinterpret_tensor(buf1283, (432, 28), (1, 432), 0); del buf1283  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_90, buf1281, convolution_168, unsqueeze_2114, buf1285, 12096, 126, grid=grid(12096), stream=stream0)
        buf1286 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1287 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1285, squeeze_274, buf1286, buf1287, 432, 28, grid=grid(432), stream=stream0)
        buf1288 = reinterpret_tensor(buf1278, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1278  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_90, buf1281, convolution_168, unsqueeze_2114, buf1286, squeeze_274, buf1284, primals_362, buf1288, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1281
        del convolution_168
        del primals_362
        del relu_90
        del squeeze_274
        del unsqueeze_2114
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1289 = aten.convolution_backward(buf1288, convolution_167, primals_361, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1288
        del convolution_167
        del primals_361
        buf1290 = buf1289[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1292 = aten.convolution_backward(buf1290, constant_pad_nd_22, primals_16, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 432, [True, True, False])
        del buf1290
        del constant_pad_nd_22
        del primals_16
        buf1293 = buf1292[0]
        buf1320 = empty((432, 111), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_95.run(buf1269, buf1271, le_100, buf1293, buf1320, 47952, 128, grid=grid(47952), stream=stream0)
        buf1321 = buf1286; del buf1286  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_96.run(buf1320, buf1321, 432, 111, grid=grid(432), stream=stream0)
        buf1322 = reinterpret_tensor(buf1320, (432, 111), (1, 432), 0); del buf1320  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_97.run(buf1269, buf1271, le_100, buf1293, convolution_162, unsqueeze_2150, buf1322, 47952, 128, grid=grid(47952), stream=stream0)
        buf1323 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1325 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_98.run(buf1322, squeeze_265, buf1323, buf1325, 432, 111, grid=grid(432), stream=stream0)
        buf1324 = empty_strided((8, 432, 42, 42), (762048, 1, 18144, 432), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_99.run(buf1269, buf1271, le_100, buf1293, convolution_162, unsqueeze_2150, buf1323, squeeze_265, buf1321, buf1324, 14112, 432, grid=grid(14112, 432), stream=stream0)
        del buf1269
        del buf1271
        del buf1293
        del convolution_162
        del le_100
        del unsqueeze_2150
        buf1326 = buf1324; del buf1324  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_100.run(buf1326, squeeze_265, primals_352, 3456, 1764, grid=grid(3456, 1764), stream=stream0)
        del primals_352
        del squeeze_265
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1327 = aten.convolution_backward(buf1326, relu_86, primals_351, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_351
        buf1328 = buf1327[0]
        buf1167 = empty((8, 1080, 42, 42), device='cuda', dtype=torch.float32)
        buf1340 = buf1167; del buf1167  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.avg_pool2d_backward, aten.constant_pad_nd, aten.threshold_backward]
        triton_poi_fused_add_avg_pool2d_backward_constant_pad_nd_threshold_backward_101.run(buf1340, buf1161, buf1165, relu_86, buf1328, 8640, 1764, grid=grid(8640, 1764), stream=stream0)
        del buf1161
        del buf1165
        del buf1328
        del relu_86
        buf1176 = buf1174[1]
        del buf1174
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1181 = aten.convolution_backward(buf1180, convolution_185, primals_394, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1180
        del convolution_185
        del primals_394
        buf1182 = buf1181[0]
        buf1183 = buf1181[1]
        del buf1181
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1184 = aten.convolution_backward(buf1182, relu_98, primals_393, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_393
        buf1185 = buf1184[0]
        buf1186 = buf1184[1]
        del buf1184
        buf1187 = reinterpret_tensor(buf1285, (432, 28), (28, 1), 0); del buf1285  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_98, buf1185, buf1187, 12096, 126, grid=grid(12096), stream=stream0)
        buf1188 = buf1323; del buf1323  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1187, buf1188, 432, 28, grid=grid(432), stream=stream0)
        buf1189 = reinterpret_tensor(buf1187, (432, 28), (1, 432), 0); del buf1187  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_98, buf1185, convolution_184, unsqueeze_2018, buf1189, 12096, 126, grid=grid(12096), stream=stream0)
        buf1190 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1191 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1189, squeeze_298, buf1190, buf1191, 432, 28, grid=grid(432), stream=stream0)
        buf1192 = reinterpret_tensor(buf1182, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1182  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_98, buf1185, convolution_184, unsqueeze_2018, buf1190, squeeze_298, buf1188, primals_391, buf1192, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1185
        del convolution_184
        del primals_391
        del relu_98
        del squeeze_298
        del unsqueeze_2018
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1193 = aten.convolution_backward(buf1192, convolution_183, primals_390, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1192
        del convolution_183
        del primals_390
        buf1194 = buf1193[0]
        buf1195 = buf1193[1]
        del buf1193
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1196 = aten.convolution_backward(buf1194, constant_pad_nd_27, primals_19, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 432, [True, True, False])
        del constant_pad_nd_27
        del primals_19
        buf1197 = buf1196[0]
        buf1198 = buf1196[1]
        del buf1196
        buf1208 = buf1206[1]
        del buf1206
        buf1211 = buf1209[1]
        del buf1209
        buf1220 = buf1218[1]
        del buf1218
        buf1223 = buf1221[1]
        del buf1221
        buf1231 = buf1229[1]
        del buf1229
        buf1234 = buf1232[1]
        del buf1232
        buf1243 = buf1241[1]
        del buf1241
        buf1246 = buf1244[1]
        del buf1244
        buf1253 = buf1251[1]
        del buf1251
        buf1256 = buf1254[1]
        del buf1254
        buf1265 = buf1263[1]
        del buf1263
        buf1268 = buf1266[1]
        del buf1266
        buf1279 = buf1277[1]
        del buf1277
        buf1282 = buf1280[1]
        del buf1280
        buf1291 = buf1289[1]
        del buf1289
        buf1294 = buf1292[1]
        del buf1292
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1295 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1168, (8, 432, 21, 21), (952560, 441, 21, 1), 0), constant_pad_nd_21, [3, 3], [2, 2], [0, 0], [1, 1], False, getitem_219)
        del constant_pad_nd_21
        del getitem_219
        buf1296 = buf1295
        del buf1295
        buf1297 = buf1190; del buf1190  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_78.run(buf1168, buf1297, 432, 3528, grid=grid(432), stream=stream0)
        buf1298 = reinterpret_tensor(buf1189, (432, 28), (28, 1), 0); del buf1189  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_79.run(buf1168, convolution_166, unsqueeze_2126, buf1298, 12096, 126, grid=grid(12096), stream=stream0)
        buf1299 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1300 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_61.run(buf1298, squeeze_271, buf1299, buf1300, 432, 28, grid=grid(432), stream=stream0)
        buf1301 = reinterpret_tensor(buf1194, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1194  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_80.run(buf1168, convolution_166, unsqueeze_2126, buf1299, squeeze_271, buf1297, primals_359, buf1301, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1168
        del convolution_166
        del primals_359
        del squeeze_271
        del unsqueeze_2126
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1302 = aten.convolution_backward(buf1301, convolution_165, primals_358, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1301
        del convolution_165
        del primals_358
        buf1303 = buf1302[0]
        buf1304 = buf1302[1]
        del buf1302
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1305 = aten.convolution_backward(buf1303, relu_88, primals_357, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 432, [True, True, False])
        del primals_357
        buf1306 = buf1305[0]
        buf1307 = buf1305[1]
        del buf1305
        buf1308 = buf1298; del buf1298  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(relu_88, buf1306, buf1308, 12096, 126, grid=grid(12096), stream=stream0)
        buf1309 = buf1299; del buf1299  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf1308, buf1309, 432, 28, grid=grid(432), stream=stream0)
        buf1310 = reinterpret_tensor(buf1308, (432, 28), (1, 432), 0); del buf1308  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_65.run(relu_88, buf1306, convolution_164, unsqueeze_2138, buf1310, 12096, 126, grid=grid(12096), stream=stream0)
        buf1311 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1312 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_66.run(buf1310, squeeze_268, buf1311, buf1312, 432, 28, grid=grid(432), stream=stream0)
        del buf1310
        buf1313 = reinterpret_tensor(buf1303, (8, 432, 21, 21), (190512, 1, 9072, 432), 0); del buf1303  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_67.run(relu_88, buf1306, convolution_164, unsqueeze_2138, buf1311, squeeze_268, buf1309, primals_355, buf1313, 3528, 432, grid=grid(3528, 432), stream=stream0)
        del buf1306
        del convolution_164
        del primals_355
        del relu_88
        del squeeze_268
        del unsqueeze_2138
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1314 = aten.convolution_backward(buf1313, convolution_163, primals_354, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_163
        del primals_354
        buf1315 = buf1314[0]
        buf1316 = buf1314[1]
        del buf1314
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1317 = aten.convolution_backward(buf1315, constant_pad_nd_20, primals_15, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 432, [True, True, False])
        del constant_pad_nd_20
        del primals_15
        buf1318 = buf1317[0]
        buf1319 = buf1317[1]
        del buf1317
        buf1329 = buf1327[1]
        del buf1327
        buf1330 = reinterpret_tensor(buf1322, (432, 111), (111, 1), 0); del buf1322  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_102.run(le_102, buf1197, buf1296, buf1318, buf1330, 47952, 128, grid=grid(47952), stream=stream0)
        buf1331 = buf1311; del buf1311  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_96.run(buf1330, buf1331, 432, 111, grid=grid(432), stream=stream0)
        buf1332 = reinterpret_tensor(buf1330, (432, 111), (1, 432), 0); del buf1330  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_103.run(le_102, buf1197, buf1296, buf1318, convolution_161, unsqueeze_2162, buf1332, 47952, 128, grid=grid(47952), stream=stream0)
        buf1333 = empty((432, ), device='cuda', dtype=torch.float32)
        buf1335 = empty((432, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_98.run(buf1332, squeeze_262, buf1333, buf1335, 432, 111, grid=grid(432), stream=stream0)
        del buf1332
        buf1334 = buf1326; del buf1326  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_104.run(le_102, buf1197, buf1296, buf1318, convolution_161, unsqueeze_2162, buf1333, squeeze_262, buf1334, 14112, 432, grid=grid(14112, 432), stream=stream0)
        del buf1197
        del buf1296
        del buf1318
        del convolution_161
        del le_102
        del unsqueeze_2162
        buf1336 = buf1334; del buf1334  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_105.run(buf1336, buf1331, squeeze_262, primals_349, 3456, 1764, grid=grid(3456, 1764), stream=stream0)
        del primals_349
        del squeeze_262
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1337 = aten.convolution_backward(buf1336, relu_72, primals_348, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1336
        del primals_348
        buf1338 = buf1337[0]
        buf1339 = buf1337[1]
        del buf1337
        buf1341 = reinterpret_tensor(buf1333, (216, 2), (1, 216), 0); del buf1333  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_106.run(buf1340, buf1341, 432, 7056, grid=grid(432), stream=stream0)
        buf1342 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1341, buf1342, 216, 2, grid=grid(216), stream=stream0)
        buf1343 = empty((216, 111), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_108.run(buf1340, convolution_160, unsqueeze_2174, buf1343, 23976, 128, grid=grid(23976), stream=stream0)
        buf1344 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1345 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1343, squeeze_259, buf1344, buf1345, 216, 111, grid=grid(216), stream=stream0)
        buf1346 = reinterpret_tensor(buf670, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf670  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_110.run(buf1340, convolution_160, unsqueeze_2174, buf1344, squeeze_259, buf1342, primals_346, buf1346, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del convolution_160
        del primals_346
        del squeeze_259
        del unsqueeze_2174
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1347 = aten.convolution_backward(buf1346, convolution_159, primals_345, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1346
        del convolution_159
        del primals_345
        buf1348 = buf1347[0]
        buf1349 = buf1347[1]
        del buf1347
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1350 = aten.convolution_backward(buf1348, relu_84, primals_344, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_344
        buf1351 = buf1350[0]
        buf1352 = buf1350[1]
        del buf1350
        buf1353 = buf1343; del buf1343  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_84, buf1351, buf1353, 23976, 128, grid=grid(23976), stream=stream0)
        buf1354 = buf1344; del buf1344  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1353, buf1354, 216, 111, grid=grid(216), stream=stream0)
        buf1355 = reinterpret_tensor(buf1353, (216, 111), (1, 216), 0); del buf1353  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_84, buf1351, convolution_158, unsqueeze_2186, buf1355, 23976, 128, grid=grid(23976), stream=stream0)
        buf1356 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1357 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1355, squeeze_256, buf1356, buf1357, 216, 111, grid=grid(216), stream=stream0)
        buf1358 = reinterpret_tensor(buf1348, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1348  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_84, buf1351, convolution_158, unsqueeze_2186, buf1356, squeeze_256, buf1354, primals_342, buf1358, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1351
        del convolution_158
        del primals_342
        del relu_84
        del squeeze_256
        del unsqueeze_2186
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1359 = aten.convolution_backward(buf1358, convolution_157, primals_341, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1358
        del convolution_157
        del primals_341
        buf1360 = buf1359[0]
        buf1361 = buf1359[1]
        del buf1359
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1362 = aten.convolution_backward(buf1360, relu_73, primals_340, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_340
        buf1363 = buf1362[0]
        buf1364 = buf1362[1]
        del buf1362
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1365 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1340, (8, 216, 42, 42), (1905120, 1764, 42, 1), 1143072), add_399, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_191)
        buf1366 = buf1365
        del buf1365
        buf1367 = buf1341; del buf1341  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_116.run(buf1340, buf1367, 432, 7056, grid=grid(432), stream=stream0)
        buf1368 = buf1356; del buf1356  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1367, buf1368, 216, 2, grid=grid(216), stream=stream0)
        buf1369 = reinterpret_tensor(buf1355, (216, 111), (111, 1), 0); del buf1355  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_117.run(buf1340, convolution_156, unsqueeze_2198, buf1369, 23976, 128, grid=grid(23976), stream=stream0)
        buf1370 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1371 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1369, squeeze_253, buf1370, buf1371, 216, 111, grid=grid(216), stream=stream0)
        buf1372 = reinterpret_tensor(buf1360, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1360  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_118.run(buf1340, convolution_156, unsqueeze_2198, buf1370, squeeze_253, buf1368, primals_338, buf1372, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del convolution_156
        del primals_338
        del squeeze_253
        del unsqueeze_2198
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1373 = aten.convolution_backward(buf1372, convolution_155, primals_337, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1372
        del convolution_155
        del primals_337
        buf1374 = buf1373[0]
        buf1375 = buf1373[1]
        del buf1373
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1376 = aten.convolution_backward(buf1374, relu_82, primals_336, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_336
        buf1377 = buf1376[0]
        buf1378 = buf1376[1]
        del buf1376
        buf1379 = buf1369; del buf1369  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_82, buf1377, buf1379, 23976, 128, grid=grid(23976), stream=stream0)
        buf1380 = buf1370; del buf1370  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1379, buf1380, 216, 111, grid=grid(216), stream=stream0)
        buf1381 = reinterpret_tensor(buf1379, (216, 111), (1, 216), 0); del buf1379  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_82, buf1377, convolution_154, unsqueeze_2210, buf1381, 23976, 128, grid=grid(23976), stream=stream0)
        buf1382 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1383 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1381, squeeze_250, buf1382, buf1383, 216, 111, grid=grid(216), stream=stream0)
        buf1384 = reinterpret_tensor(buf1374, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1374  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_82, buf1377, convolution_154, unsqueeze_2210, buf1382, squeeze_250, buf1380, primals_334, buf1384, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1377
        del convolution_154
        del primals_334
        del relu_82
        del squeeze_250
        del unsqueeze_2210
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1385 = aten.convolution_backward(buf1384, convolution_153, primals_333, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_153
        del primals_333
        buf1386 = buf1385[0]
        buf1387 = buf1385[1]
        del buf1385
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1388 = aten.convolution_backward(buf1386, relu_81, primals_332, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_332
        buf1389 = buf1388[0]
        buf1390 = buf1388[1]
        del buf1388
        buf1391 = buf1367; del buf1367  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_119.run(buf1340, relu_81, buf1389, buf1391, 432, 7056, grid=grid(432), stream=stream0)
        buf1392 = buf1382; del buf1382  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1391, buf1392, 216, 2, grid=grid(216), stream=stream0)
        buf1393 = reinterpret_tensor(buf1381, (216, 111), (111, 1), 0); del buf1381  # reuse
        buf1415 = empty((216, 111), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_120.run(buf1340, relu_81, buf1389, convolution_152, unsqueeze_2222, convolution_148, unsqueeze_2246, buf1393, buf1415, 23976, 128, grid=grid(23976), stream=stream0)
        buf1394 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1396 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1393, squeeze_247, buf1394, buf1396, 216, 111, grid=grid(216), stream=stream0)
        buf1416 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1418 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1415, squeeze_241, buf1416, buf1418, 216, 111, grid=grid(216), stream=stream0)
        buf1395 = reinterpret_tensor(buf1386, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1386  # reuse
        buf1417 = buf1384; del buf1384  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_121.run(buf1340, relu_81, buf1389, convolution_152, unsqueeze_2222, buf1394, squeeze_247, buf1392, primals_330, convolution_148, unsqueeze_2246, buf1416, squeeze_241, primals_322, buf1395, buf1417, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1389
        del convolution_148
        del convolution_152
        del primals_322
        del primals_330
        del relu_81
        del squeeze_241
        del squeeze_247
        del unsqueeze_2222
        del unsqueeze_2246
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1397 = aten.convolution_backward(buf1395, convolution_151, primals_329, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1395
        del convolution_151
        del primals_329
        buf1398 = buf1397[0]
        buf1399 = buf1397[1]
        del buf1397
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1400 = aten.convolution_backward(buf1398, relu_80, primals_328, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_328
        buf1401 = buf1400[0]
        buf1402 = buf1400[1]
        del buf1400
        buf1403 = buf1415; del buf1415  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_80, buf1401, buf1403, 23976, 128, grid=grid(23976), stream=stream0)
        buf1404 = buf1416; del buf1416  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1403, buf1404, 216, 111, grid=grid(216), stream=stream0)
        buf1405 = reinterpret_tensor(buf1403, (216, 111), (1, 216), 0); del buf1403  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_80, buf1401, convolution_150, unsqueeze_2234, buf1405, 23976, 128, grid=grid(23976), stream=stream0)
        buf1406 = buf1394; del buf1394  # reuse
        buf1407 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1405, squeeze_244, buf1406, buf1407, 216, 111, grid=grid(216), stream=stream0)
        buf1408 = reinterpret_tensor(buf1398, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1398  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_80, buf1401, convolution_150, unsqueeze_2234, buf1406, squeeze_244, buf1404, primals_326, buf1408, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1401
        del convolution_150
        del primals_326
        del relu_80
        del squeeze_244
        del unsqueeze_2234
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1409 = aten.convolution_backward(buf1408, convolution_149, primals_325, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1408
        del convolution_149
        del primals_325
        buf1410 = buf1409[0]
        buf1411 = buf1409[1]
        del buf1409
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1412 = aten.convolution_backward(buf1410, relu_75, primals_324, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del buf1410
        del primals_324
        buf1413 = buf1412[0]
        buf1414 = buf1412[1]
        del buf1412
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1419 = aten.convolution_backward(buf1417, convolution_147, primals_321, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1417
        del convolution_147
        del primals_321
        buf1420 = buf1419[0]
        buf1421 = buf1419[1]
        del buf1419
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1422 = aten.convolution_backward(buf1420, relu_78, primals_320, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_320
        buf1423 = buf1422[0]
        buf1424 = buf1422[1]
        del buf1422
        buf1425 = reinterpret_tensor(buf1405, (216, 111), (111, 1), 0); del buf1405  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_78, buf1423, buf1425, 23976, 128, grid=grid(23976), stream=stream0)
        buf1426 = buf1406; del buf1406  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1425, buf1426, 216, 111, grid=grid(216), stream=stream0)
        buf1427 = reinterpret_tensor(buf1425, (216, 111), (1, 216), 0); del buf1425  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_78, buf1423, convolution_146, unsqueeze_2258, buf1427, 23976, 128, grid=grid(23976), stream=stream0)
        buf1428 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1429 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1427, squeeze_238, buf1428, buf1429, 216, 111, grid=grid(216), stream=stream0)
        buf1430 = reinterpret_tensor(buf1420, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1420  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_78, buf1423, convolution_146, unsqueeze_2258, buf1428, squeeze_238, buf1426, primals_318, buf1430, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1423
        del convolution_146
        del primals_318
        del relu_78
        del squeeze_238
        del unsqueeze_2258
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1431 = aten.convolution_backward(buf1430, convolution_145, primals_317, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1430
        del convolution_145
        del primals_317
        buf1432 = buf1431[0]
        buf1433 = buf1431[1]
        del buf1431
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1434 = aten.convolution_backward(buf1432, relu_75, primals_316, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_316
        buf1435 = buf1434[0]
        buf1436 = buf1434[1]
        del buf1434
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1437 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1340, (8, 216, 42, 42), (1905120, 1764, 42, 1), 381024), add_399, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_191)
        del add_399
        del getitem_191
        buf1438 = buf1437
        del buf1437
        buf1439 = buf1391; del buf1391  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_122.run(buf1340, buf1439, 432, 7056, grid=grid(432), stream=stream0)
        buf1440 = buf1428; del buf1428  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1439, buf1440, 216, 2, grid=grid(216), stream=stream0)
        buf1441 = reinterpret_tensor(buf1427, (216, 111), (111, 1), 0); del buf1427  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_123.run(buf1340, convolution_144, unsqueeze_2270, buf1441, 23976, 128, grid=grid(23976), stream=stream0)
        buf1442 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1443 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1441, squeeze_235, buf1442, buf1443, 216, 111, grid=grid(216), stream=stream0)
        buf1444 = reinterpret_tensor(buf1432, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1432  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_124.run(buf1340, convolution_144, unsqueeze_2270, buf1442, squeeze_235, buf1440, primals_314, buf1444, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del convolution_144
        del primals_314
        del squeeze_235
        del unsqueeze_2270
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1445 = aten.convolution_backward(buf1444, convolution_143, primals_313, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1444
        del convolution_143
        del primals_313
        buf1446 = buf1445[0]
        buf1447 = buf1445[1]
        del buf1445
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1448 = aten.convolution_backward(buf1446, relu_76, primals_312, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_312
        buf1449 = buf1448[0]
        buf1450 = buf1448[1]
        del buf1448
        buf1451 = buf1441; del buf1441  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_76, buf1449, buf1451, 23976, 128, grid=grid(23976), stream=stream0)
        buf1452 = buf1442; del buf1442  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1451, buf1452, 216, 111, grid=grid(216), stream=stream0)
        buf1453 = reinterpret_tensor(buf1451, (216, 111), (1, 216), 0); del buf1451  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_76, buf1449, convolution_142, unsqueeze_2282, buf1453, 23976, 128, grid=grid(23976), stream=stream0)
        buf1454 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1455 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1453, squeeze_232, buf1454, buf1455, 216, 111, grid=grid(216), stream=stream0)
        buf1456 = reinterpret_tensor(buf1446, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1446  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_76, buf1449, convolution_142, unsqueeze_2282, buf1454, squeeze_232, buf1452, primals_310, buf1456, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1449
        del convolution_142
        del primals_310
        del relu_76
        del squeeze_232
        del unsqueeze_2282
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1457 = aten.convolution_backward(buf1456, convolution_141, primals_309, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1456
        del convolution_141
        del primals_309
        buf1458 = buf1457[0]
        buf1459 = buf1457[1]
        del buf1457
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1460 = aten.convolution_backward(buf1458, relu_75, primals_308, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 216, [True, True, False])
        del buf1458
        del primals_308
        buf1461 = buf1460[0]
        buf1462 = buf1460[1]
        del buf1460
        buf1463 = buf1413; del buf1413  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_125.run(buf1463, buf1340, buf1366, relu_75, buf1435, buf1438, buf1461, 1728, 1764, grid=grid(1728, 1764), stream=stream0)
        del buf1366
        del buf1435
        del buf1438
        del relu_75
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1464 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1340, (8, 216, 42, 42), (1905120, 1764, 42, 1), 0), add_394, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_185)
        del add_394
        del getitem_185
        buf1465 = buf1464
        del buf1464
        buf1466 = buf1439; del buf1439  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_126.run(buf1340, buf1466, 432, 7056, grid=grid(432), stream=stream0)
        buf1467 = buf1454; del buf1454  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1466, buf1467, 216, 2, grid=grid(216), stream=stream0)
        buf1468 = reinterpret_tensor(buf1453, (216, 111), (111, 1), 0); del buf1453  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_127.run(buf1340, convolution_140, unsqueeze_2294, buf1468, 23976, 128, grid=grid(23976), stream=stream0)
        buf1469 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1470 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1468, squeeze_229, buf1469, buf1470, 216, 111, grid=grid(216), stream=stream0)
        buf1471 = reinterpret_tensor(buf1461, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1461  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_128.run(buf1340, convolution_140, unsqueeze_2294, buf1469, squeeze_229, buf1467, primals_306, buf1471, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1340
        del convolution_140
        del primals_306
        del squeeze_229
        del unsqueeze_2294
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1472 = aten.convolution_backward(buf1471, convolution_139, primals_305, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1471
        del convolution_139
        del primals_305
        buf1473 = buf1472[0]
        buf1474 = buf1472[1]
        del buf1472
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1475 = aten.convolution_backward(buf1473, relu_74, primals_304, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_304
        buf1476 = buf1475[0]
        buf1477 = buf1475[1]
        del buf1475
        buf1478 = buf1468; del buf1468  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_74, buf1476, buf1478, 23976, 128, grid=grid(23976), stream=stream0)
        buf1479 = buf1469; del buf1469  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1478, buf1479, 216, 111, grid=grid(216), stream=stream0)
        buf1480 = reinterpret_tensor(buf1478, (216, 111), (1, 216), 0); del buf1478  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_74, buf1476, convolution_138, unsqueeze_2306, buf1480, 23976, 128, grid=grid(23976), stream=stream0)
        buf1481 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1482 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1480, squeeze_226, buf1481, buf1482, 216, 111, grid=grid(216), stream=stream0)
        buf1483 = reinterpret_tensor(buf1473, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1473  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_74, buf1476, convolution_138, unsqueeze_2306, buf1481, squeeze_226, buf1479, primals_302, buf1483, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1476
        del convolution_138
        del primals_302
        del relu_74
        del squeeze_226
        del unsqueeze_2306
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1484 = aten.convolution_backward(buf1483, convolution_137, primals_301, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1483
        del convolution_137
        del primals_301
        buf1485 = buf1484[0]
        buf1486 = buf1484[1]
        del buf1484
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1487 = aten.convolution_backward(buf1485, relu_73, primals_300, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_300
        buf1488 = buf1487[0]
        buf1489 = buf1487[1]
        del buf1487
        buf1490 = buf1466; del buf1466  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_129.run(buf1463, buf1490, 432, 7056, grid=grid(432), stream=stream0)
        buf1491 = buf1481; del buf1481  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1490, buf1491, 216, 2, grid=grid(216), stream=stream0)
        buf1492 = reinterpret_tensor(buf1480, (216, 111), (111, 1), 0); del buf1480  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_130.run(buf1463, convolution_136, unsqueeze_2318, buf1492, 23976, 128, grid=grid(23976), stream=stream0)
        buf1493 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1494 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1492, squeeze_223, buf1493, buf1494, 216, 111, grid=grid(216), stream=stream0)
        buf1495 = reinterpret_tensor(buf1485, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1485  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_131.run(buf1463, convolution_136, unsqueeze_2318, buf1493, squeeze_223, buf1491, primals_298, buf1495, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1463
        del convolution_136
        del primals_298
        del squeeze_223
        del unsqueeze_2318
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1496 = aten.convolution_backward(buf1495, relu_72, primals_297, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1495
        del primals_297
        buf1497 = buf1496[0]
        buf1498 = buf1496[1]
        del buf1496
        buf1499 = buf1492; del buf1492  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_132.run(relu_73, buf1363, buf1465, buf1488, buf1499, 23976, 128, grid=grid(23976), stream=stream0)
        buf1500 = buf1493; del buf1493  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1499, buf1500, 216, 111, grid=grid(216), stream=stream0)
        buf1501 = reinterpret_tensor(buf1499, (216, 111), (1, 216), 0); del buf1499  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_133.run(relu_73, buf1363, buf1465, buf1488, convolution_135, unsqueeze_2330, buf1501, 23976, 128, grid=grid(23976), stream=stream0)
        buf1502 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1504 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1501, squeeze_220, buf1502, buf1504, 216, 111, grid=grid(216), stream=stream0)
        buf1503 = buf1465; del buf1465  # reuse
        buf1505 = buf1503; del buf1503  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_134.run(buf1505, relu_73, buf1363, buf1488, convolution_135, unsqueeze_2330, buf1502, squeeze_220, buf1500, primals_295, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1363
        del buf1488
        del convolution_135
        del primals_295
        del relu_73
        del squeeze_220
        del unsqueeze_2330
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1506 = aten.convolution_backward(buf1505, relu_58, primals_294, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_294
        buf1507 = buf1506[0]
        buf1508 = buf1506[1]
        del buf1506
        buf1509 = buf1338; del buf1338  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_135.run(buf1509, relu_72, buf1497, 8640, 1764, grid=grid(8640, 1764), stream=stream0)
        del buf1497
        del relu_72
        buf1510 = buf1490; del buf1490  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_106.run(buf1509, buf1510, 432, 7056, grid=grid(432), stream=stream0)
        buf1511 = buf1502; del buf1502  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1510, buf1511, 216, 2, grid=grid(216), stream=stream0)
        buf1512 = reinterpret_tensor(buf1501, (216, 111), (111, 1), 0); del buf1501  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_108.run(buf1509, convolution_134, unsqueeze_2342, buf1512, 23976, 128, grid=grid(23976), stream=stream0)
        buf1513 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1514 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1512, squeeze_217, buf1513, buf1514, 216, 111, grid=grid(216), stream=stream0)
        buf1515 = buf1505; del buf1505  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_110.run(buf1509, convolution_134, unsqueeze_2342, buf1513, squeeze_217, buf1511, primals_292, buf1515, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del convolution_134
        del primals_292
        del squeeze_217
        del unsqueeze_2342
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1516 = aten.convolution_backward(buf1515, convolution_133, primals_291, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1515
        del convolution_133
        del primals_291
        buf1517 = buf1516[0]
        buf1518 = buf1516[1]
        del buf1516
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1519 = aten.convolution_backward(buf1517, relu_70, primals_290, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_290
        buf1520 = buf1519[0]
        buf1521 = buf1519[1]
        del buf1519
        buf1522 = buf1512; del buf1512  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_70, buf1520, buf1522, 23976, 128, grid=grid(23976), stream=stream0)
        buf1523 = buf1513; del buf1513  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1522, buf1523, 216, 111, grid=grid(216), stream=stream0)
        buf1524 = reinterpret_tensor(buf1522, (216, 111), (1, 216), 0); del buf1522  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_70, buf1520, convolution_132, unsqueeze_2354, buf1524, 23976, 128, grid=grid(23976), stream=stream0)
        buf1525 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1526 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1524, squeeze_214, buf1525, buf1526, 216, 111, grid=grid(216), stream=stream0)
        buf1527 = reinterpret_tensor(buf1517, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1517  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_70, buf1520, convolution_132, unsqueeze_2354, buf1525, squeeze_214, buf1523, primals_288, buf1527, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1520
        del convolution_132
        del primals_288
        del relu_70
        del squeeze_214
        del unsqueeze_2354
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1528 = aten.convolution_backward(buf1527, convolution_131, primals_287, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1527
        del convolution_131
        del primals_287
        buf1529 = buf1528[0]
        buf1530 = buf1528[1]
        del buf1528
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1531 = aten.convolution_backward(buf1529, relu_59, primals_286, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_286
        buf1532 = buf1531[0]
        buf1533 = buf1531[1]
        del buf1531
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1534 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1509, (8, 216, 42, 42), (1905120, 1764, 42, 1), 1143072), add_324, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_157)
        buf1535 = buf1534
        del buf1534
        buf1536 = buf1510; del buf1510  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_116.run(buf1509, buf1536, 432, 7056, grid=grid(432), stream=stream0)
        buf1537 = buf1525; del buf1525  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1536, buf1537, 216, 2, grid=grid(216), stream=stream0)
        buf1538 = reinterpret_tensor(buf1524, (216, 111), (111, 1), 0); del buf1524  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_117.run(buf1509, convolution_130, unsqueeze_2366, buf1538, 23976, 128, grid=grid(23976), stream=stream0)
        buf1539 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1540 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1538, squeeze_211, buf1539, buf1540, 216, 111, grid=grid(216), stream=stream0)
        buf1541 = reinterpret_tensor(buf1529, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1529  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_118.run(buf1509, convolution_130, unsqueeze_2366, buf1539, squeeze_211, buf1537, primals_284, buf1541, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del convolution_130
        del primals_284
        del squeeze_211
        del unsqueeze_2366
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1542 = aten.convolution_backward(buf1541, convolution_129, primals_283, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1541
        del convolution_129
        del primals_283
        buf1543 = buf1542[0]
        buf1544 = buf1542[1]
        del buf1542
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1545 = aten.convolution_backward(buf1543, relu_68, primals_282, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_282
        buf1546 = buf1545[0]
        buf1547 = buf1545[1]
        del buf1545
        buf1548 = buf1538; del buf1538  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_68, buf1546, buf1548, 23976, 128, grid=grid(23976), stream=stream0)
        buf1549 = buf1539; del buf1539  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1548, buf1549, 216, 111, grid=grid(216), stream=stream0)
        buf1550 = reinterpret_tensor(buf1548, (216, 111), (1, 216), 0); del buf1548  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_68, buf1546, convolution_128, unsqueeze_2378, buf1550, 23976, 128, grid=grid(23976), stream=stream0)
        buf1551 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1552 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1550, squeeze_208, buf1551, buf1552, 216, 111, grid=grid(216), stream=stream0)
        buf1553 = reinterpret_tensor(buf1543, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1543  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_68, buf1546, convolution_128, unsqueeze_2378, buf1551, squeeze_208, buf1549, primals_280, buf1553, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1546
        del convolution_128
        del primals_280
        del relu_68
        del squeeze_208
        del unsqueeze_2378
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1554 = aten.convolution_backward(buf1553, convolution_127, primals_279, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_127
        del primals_279
        buf1555 = buf1554[0]
        buf1556 = buf1554[1]
        del buf1554
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1557 = aten.convolution_backward(buf1555, relu_67, primals_278, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_278
        buf1558 = buf1557[0]
        buf1559 = buf1557[1]
        del buf1557
        buf1560 = buf1536; del buf1536  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_119.run(buf1509, relu_67, buf1558, buf1560, 432, 7056, grid=grid(432), stream=stream0)
        buf1561 = buf1551; del buf1551  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1560, buf1561, 216, 2, grid=grid(216), stream=stream0)
        buf1562 = reinterpret_tensor(buf1550, (216, 111), (111, 1), 0); del buf1550  # reuse
        buf1584 = buf1393; del buf1393  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_120.run(buf1509, relu_67, buf1558, convolution_126, unsqueeze_2390, convolution_122, unsqueeze_2414, buf1562, buf1584, 23976, 128, grid=grid(23976), stream=stream0)
        buf1563 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1565 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1562, squeeze_205, buf1563, buf1565, 216, 111, grid=grid(216), stream=stream0)
        buf1585 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1587 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1584, squeeze_199, buf1585, buf1587, 216, 111, grid=grid(216), stream=stream0)
        buf1564 = reinterpret_tensor(buf1555, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1555  # reuse
        buf1586 = buf1553; del buf1553  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_121.run(buf1509, relu_67, buf1558, convolution_126, unsqueeze_2390, buf1563, squeeze_205, buf1561, primals_276, convolution_122, unsqueeze_2414, buf1585, squeeze_199, primals_268, buf1564, buf1586, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1558
        del convolution_122
        del convolution_126
        del primals_268
        del primals_276
        del relu_67
        del squeeze_199
        del squeeze_205
        del unsqueeze_2390
        del unsqueeze_2414
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1566 = aten.convolution_backward(buf1564, convolution_125, primals_275, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1564
        del convolution_125
        del primals_275
        buf1567 = buf1566[0]
        buf1568 = buf1566[1]
        del buf1566
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1569 = aten.convolution_backward(buf1567, relu_66, primals_274, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_274
        buf1570 = buf1569[0]
        buf1571 = buf1569[1]
        del buf1569
        buf1572 = buf1584; del buf1584  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_66, buf1570, buf1572, 23976, 128, grid=grid(23976), stream=stream0)
        buf1573 = buf1585; del buf1585  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1572, buf1573, 216, 111, grid=grid(216), stream=stream0)
        buf1574 = reinterpret_tensor(buf1572, (216, 111), (1, 216), 0); del buf1572  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_66, buf1570, convolution_124, unsqueeze_2402, buf1574, 23976, 128, grid=grid(23976), stream=stream0)
        buf1575 = buf1563; del buf1563  # reuse
        buf1576 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1574, squeeze_202, buf1575, buf1576, 216, 111, grid=grid(216), stream=stream0)
        buf1577 = reinterpret_tensor(buf1567, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1567  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_66, buf1570, convolution_124, unsqueeze_2402, buf1575, squeeze_202, buf1573, primals_272, buf1577, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1570
        del convolution_124
        del primals_272
        del relu_66
        del squeeze_202
        del unsqueeze_2402
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1578 = aten.convolution_backward(buf1577, convolution_123, primals_271, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1577
        del convolution_123
        del primals_271
        buf1579 = buf1578[0]
        buf1580 = buf1578[1]
        del buf1578
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1581 = aten.convolution_backward(buf1579, relu_61, primals_270, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del buf1579
        del primals_270
        buf1582 = buf1581[0]
        buf1583 = buf1581[1]
        del buf1581
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1588 = aten.convolution_backward(buf1586, convolution_121, primals_267, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1586
        del convolution_121
        del primals_267
        buf1589 = buf1588[0]
        buf1590 = buf1588[1]
        del buf1588
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1591 = aten.convolution_backward(buf1589, relu_64, primals_266, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_266
        buf1592 = buf1591[0]
        buf1593 = buf1591[1]
        del buf1591
        buf1594 = reinterpret_tensor(buf1574, (216, 111), (111, 1), 0); del buf1574  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_64, buf1592, buf1594, 23976, 128, grid=grid(23976), stream=stream0)
        buf1595 = buf1575; del buf1575  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1594, buf1595, 216, 111, grid=grid(216), stream=stream0)
        buf1596 = reinterpret_tensor(buf1594, (216, 111), (1, 216), 0); del buf1594  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_64, buf1592, convolution_120, unsqueeze_2426, buf1596, 23976, 128, grid=grid(23976), stream=stream0)
        buf1597 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1598 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1596, squeeze_196, buf1597, buf1598, 216, 111, grid=grid(216), stream=stream0)
        buf1599 = reinterpret_tensor(buf1589, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1589  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_64, buf1592, convolution_120, unsqueeze_2426, buf1597, squeeze_196, buf1595, primals_264, buf1599, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1592
        del convolution_120
        del primals_264
        del relu_64
        del squeeze_196
        del unsqueeze_2426
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1600 = aten.convolution_backward(buf1599, convolution_119, primals_263, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1599
        del convolution_119
        del primals_263
        buf1601 = buf1600[0]
        buf1602 = buf1600[1]
        del buf1600
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1603 = aten.convolution_backward(buf1601, relu_61, primals_262, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_262
        buf1604 = buf1603[0]
        buf1605 = buf1603[1]
        del buf1603
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1606 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1509, (8, 216, 42, 42), (1905120, 1764, 42, 1), 381024), add_324, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_157)
        del add_324
        del getitem_157
        buf1607 = buf1606
        del buf1606
        buf1608 = buf1560; del buf1560  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_122.run(buf1509, buf1608, 432, 7056, grid=grid(432), stream=stream0)
        buf1609 = buf1597; del buf1597  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1608, buf1609, 216, 2, grid=grid(216), stream=stream0)
        buf1610 = reinterpret_tensor(buf1596, (216, 111), (111, 1), 0); del buf1596  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_123.run(buf1509, convolution_118, unsqueeze_2438, buf1610, 23976, 128, grid=grid(23976), stream=stream0)
        buf1611 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1612 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1610, squeeze_193, buf1611, buf1612, 216, 111, grid=grid(216), stream=stream0)
        buf1613 = reinterpret_tensor(buf1601, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1601  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_124.run(buf1509, convolution_118, unsqueeze_2438, buf1611, squeeze_193, buf1609, primals_260, buf1613, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del convolution_118
        del primals_260
        del squeeze_193
        del unsqueeze_2438
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1614 = aten.convolution_backward(buf1613, convolution_117, primals_259, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1613
        del convolution_117
        del primals_259
        buf1615 = buf1614[0]
        buf1616 = buf1614[1]
        del buf1614
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1617 = aten.convolution_backward(buf1615, relu_62, primals_258, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_258
        buf1618 = buf1617[0]
        buf1619 = buf1617[1]
        del buf1617
        buf1620 = buf1610; del buf1610  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_62, buf1618, buf1620, 23976, 128, grid=grid(23976), stream=stream0)
        buf1621 = buf1611; del buf1611  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1620, buf1621, 216, 111, grid=grid(216), stream=stream0)
        buf1622 = reinterpret_tensor(buf1620, (216, 111), (1, 216), 0); del buf1620  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_62, buf1618, convolution_116, unsqueeze_2450, buf1622, 23976, 128, grid=grid(23976), stream=stream0)
        buf1623 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1624 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1622, squeeze_190, buf1623, buf1624, 216, 111, grid=grid(216), stream=stream0)
        buf1625 = reinterpret_tensor(buf1615, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1615  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_62, buf1618, convolution_116, unsqueeze_2450, buf1623, squeeze_190, buf1621, primals_256, buf1625, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1618
        del convolution_116
        del primals_256
        del relu_62
        del squeeze_190
        del unsqueeze_2450
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1626 = aten.convolution_backward(buf1625, convolution_115, primals_255, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1625
        del convolution_115
        del primals_255
        buf1627 = buf1626[0]
        buf1628 = buf1626[1]
        del buf1626
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1629 = aten.convolution_backward(buf1627, relu_61, primals_254, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 216, [True, True, False])
        del buf1627
        del primals_254
        buf1630 = buf1629[0]
        buf1631 = buf1629[1]
        del buf1629
        buf1632 = buf1582; del buf1582  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_125.run(buf1632, buf1509, buf1535, relu_61, buf1604, buf1607, buf1630, 1728, 1764, grid=grid(1728, 1764), stream=stream0)
        del buf1535
        del buf1604
        del buf1607
        del relu_61
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1633 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1509, (8, 216, 42, 42), (1905120, 1764, 42, 1), 0), add_319, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_151)
        del add_319
        del getitem_151
        buf1634 = buf1633
        del buf1633
        buf1635 = buf1608; del buf1608  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_126.run(buf1509, buf1635, 432, 7056, grid=grid(432), stream=stream0)
        buf1636 = buf1623; del buf1623  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1635, buf1636, 216, 2, grid=grid(216), stream=stream0)
        buf1637 = reinterpret_tensor(buf1622, (216, 111), (111, 1), 0); del buf1622  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_127.run(buf1509, convolution_114, unsqueeze_2462, buf1637, 23976, 128, grid=grid(23976), stream=stream0)
        buf1638 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1639 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1637, squeeze_187, buf1638, buf1639, 216, 111, grid=grid(216), stream=stream0)
        buf1640 = reinterpret_tensor(buf1630, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1630  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_128.run(buf1509, convolution_114, unsqueeze_2462, buf1638, squeeze_187, buf1636, primals_252, buf1640, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1509
        del convolution_114
        del primals_252
        del squeeze_187
        del unsqueeze_2462
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1641 = aten.convolution_backward(buf1640, convolution_113, primals_251, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1640
        del convolution_113
        del primals_251
        buf1642 = buf1641[0]
        buf1643 = buf1641[1]
        del buf1641
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1644 = aten.convolution_backward(buf1642, relu_60, primals_250, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_250
        buf1645 = buf1644[0]
        buf1646 = buf1644[1]
        del buf1644
        buf1647 = buf1637; del buf1637  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_60, buf1645, buf1647, 23976, 128, grid=grid(23976), stream=stream0)
        buf1648 = buf1638; del buf1638  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1647, buf1648, 216, 111, grid=grid(216), stream=stream0)
        buf1649 = reinterpret_tensor(buf1647, (216, 111), (1, 216), 0); del buf1647  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_60, buf1645, convolution_112, unsqueeze_2474, buf1649, 23976, 128, grid=grid(23976), stream=stream0)
        buf1650 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1651 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1649, squeeze_184, buf1650, buf1651, 216, 111, grid=grid(216), stream=stream0)
        buf1652 = reinterpret_tensor(buf1642, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1642  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_60, buf1645, convolution_112, unsqueeze_2474, buf1650, squeeze_184, buf1648, primals_248, buf1652, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1645
        del convolution_112
        del primals_248
        del relu_60
        del squeeze_184
        del unsqueeze_2474
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1653 = aten.convolution_backward(buf1652, convolution_111, primals_247, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1652
        del convolution_111
        del primals_247
        buf1654 = buf1653[0]
        buf1655 = buf1653[1]
        del buf1653
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1656 = aten.convolution_backward(buf1654, relu_59, primals_246, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_246
        buf1657 = buf1656[0]
        buf1658 = buf1656[1]
        del buf1656
        buf1659 = buf1635; del buf1635  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_129.run(buf1632, buf1659, 432, 7056, grid=grid(432), stream=stream0)
        buf1660 = buf1650; del buf1650  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1659, buf1660, 216, 2, grid=grid(216), stream=stream0)
        buf1661 = reinterpret_tensor(buf1649, (216, 111), (111, 1), 0); del buf1649  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_130.run(buf1632, convolution_110, unsqueeze_2486, buf1661, 23976, 128, grid=grid(23976), stream=stream0)
        buf1662 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1663 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1661, squeeze_181, buf1662, buf1663, 216, 111, grid=grid(216), stream=stream0)
        buf1664 = reinterpret_tensor(buf1654, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1654  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_131.run(buf1632, convolution_110, unsqueeze_2486, buf1662, squeeze_181, buf1660, primals_244, buf1664, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1632
        del convolution_110
        del primals_244
        del squeeze_181
        del unsqueeze_2486
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1665 = aten.convolution_backward(buf1664, relu_58, primals_243, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1664
        del primals_243
        buf1666 = buf1665[0]
        buf1667 = buf1665[1]
        del buf1665
        buf1668 = buf1661; del buf1661  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_132.run(relu_59, buf1532, buf1634, buf1657, buf1668, 23976, 128, grid=grid(23976), stream=stream0)
        buf1669 = buf1662; del buf1662  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1668, buf1669, 216, 111, grid=grid(216), stream=stream0)
        buf1670 = reinterpret_tensor(buf1668, (216, 111), (1, 216), 0); del buf1668  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_133.run(relu_59, buf1532, buf1634, buf1657, convolution_109, unsqueeze_2498, buf1670, 23976, 128, grid=grid(23976), stream=stream0)
        buf1671 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1673 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1670, squeeze_178, buf1671, buf1673, 216, 111, grid=grid(216), stream=stream0)
        buf1672 = buf1634; del buf1634  # reuse
        buf1674 = buf1672; del buf1672  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_134.run(buf1674, relu_59, buf1532, buf1657, convolution_109, unsqueeze_2498, buf1671, squeeze_178, buf1669, primals_241, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1532
        del buf1657
        del convolution_109
        del primals_241
        del relu_59
        del squeeze_178
        del unsqueeze_2498
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1675 = aten.convolution_backward(buf1674, relu_44, primals_240, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_240
        buf1676 = buf1675[0]
        buf1677 = buf1675[1]
        del buf1675
        buf1678 = buf1507; del buf1507  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_135.run(buf1678, relu_58, buf1666, 8640, 1764, grid=grid(8640, 1764), stream=stream0)
        del buf1666
        del relu_58
        buf1679 = buf1659; del buf1659  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_106.run(buf1678, buf1679, 432, 7056, grid=grid(432), stream=stream0)
        buf1680 = buf1671; del buf1671  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1679, buf1680, 216, 2, grid=grid(216), stream=stream0)
        buf1681 = reinterpret_tensor(buf1670, (216, 111), (111, 1), 0); del buf1670  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_108.run(buf1678, convolution_108, unsqueeze_2510, buf1681, 23976, 128, grid=grid(23976), stream=stream0)
        buf1682 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1683 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1681, squeeze_175, buf1682, buf1683, 216, 111, grid=grid(216), stream=stream0)
        buf1684 = buf1674; del buf1674  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_110.run(buf1678, convolution_108, unsqueeze_2510, buf1682, squeeze_175, buf1680, primals_238, buf1684, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del convolution_108
        del primals_238
        del squeeze_175
        del unsqueeze_2510
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1685 = aten.convolution_backward(buf1684, convolution_107, primals_237, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1684
        del convolution_107
        del primals_237
        buf1686 = buf1685[0]
        buf1687 = buf1685[1]
        del buf1685
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1688 = aten.convolution_backward(buf1686, relu_56, primals_236, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_236
        buf1689 = buf1688[0]
        buf1690 = buf1688[1]
        del buf1688
        buf1691 = buf1681; del buf1681  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_56, buf1689, buf1691, 23976, 128, grid=grid(23976), stream=stream0)
        buf1692 = buf1682; del buf1682  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1691, buf1692, 216, 111, grid=grid(216), stream=stream0)
        buf1693 = reinterpret_tensor(buf1691, (216, 111), (1, 216), 0); del buf1691  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_56, buf1689, convolution_106, unsqueeze_2522, buf1693, 23976, 128, grid=grid(23976), stream=stream0)
        buf1694 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1695 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1693, squeeze_172, buf1694, buf1695, 216, 111, grid=grid(216), stream=stream0)
        buf1696 = reinterpret_tensor(buf1686, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1686  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_56, buf1689, convolution_106, unsqueeze_2522, buf1694, squeeze_172, buf1692, primals_234, buf1696, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1689
        del convolution_106
        del primals_234
        del relu_56
        del squeeze_172
        del unsqueeze_2522
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1697 = aten.convolution_backward(buf1696, convolution_105, primals_233, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1696
        del convolution_105
        del primals_233
        buf1698 = buf1697[0]
        buf1699 = buf1697[1]
        del buf1697
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1700 = aten.convolution_backward(buf1698, relu_45, primals_232, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_232
        buf1701 = buf1700[0]
        buf1702 = buf1700[1]
        del buf1700
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1703 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1678, (8, 216, 42, 42), (1905120, 1764, 42, 1), 1143072), add_249, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_123)
        buf1704 = buf1703
        del buf1703
        buf1705 = buf1679; del buf1679  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_116.run(buf1678, buf1705, 432, 7056, grid=grid(432), stream=stream0)
        buf1706 = buf1694; del buf1694  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1705, buf1706, 216, 2, grid=grid(216), stream=stream0)
        buf1707 = reinterpret_tensor(buf1693, (216, 111), (111, 1), 0); del buf1693  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_117.run(buf1678, convolution_104, unsqueeze_2534, buf1707, 23976, 128, grid=grid(23976), stream=stream0)
        buf1708 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1709 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1707, squeeze_169, buf1708, buf1709, 216, 111, grid=grid(216), stream=stream0)
        buf1710 = reinterpret_tensor(buf1698, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1698  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_118.run(buf1678, convolution_104, unsqueeze_2534, buf1708, squeeze_169, buf1706, primals_230, buf1710, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del convolution_104
        del primals_230
        del squeeze_169
        del unsqueeze_2534
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1711 = aten.convolution_backward(buf1710, convolution_103, primals_229, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1710
        del convolution_103
        del primals_229
        buf1712 = buf1711[0]
        buf1713 = buf1711[1]
        del buf1711
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1714 = aten.convolution_backward(buf1712, relu_54, primals_228, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_228
        buf1715 = buf1714[0]
        buf1716 = buf1714[1]
        del buf1714
        buf1717 = buf1707; del buf1707  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_54, buf1715, buf1717, 23976, 128, grid=grid(23976), stream=stream0)
        buf1718 = buf1708; del buf1708  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1717, buf1718, 216, 111, grid=grid(216), stream=stream0)
        buf1719 = reinterpret_tensor(buf1717, (216, 111), (1, 216), 0); del buf1717  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_54, buf1715, convolution_102, unsqueeze_2546, buf1719, 23976, 128, grid=grid(23976), stream=stream0)
        buf1720 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1721 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1719, squeeze_166, buf1720, buf1721, 216, 111, grid=grid(216), stream=stream0)
        buf1722 = reinterpret_tensor(buf1712, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1712  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_54, buf1715, convolution_102, unsqueeze_2546, buf1720, squeeze_166, buf1718, primals_226, buf1722, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1715
        del convolution_102
        del primals_226
        del relu_54
        del squeeze_166
        del unsqueeze_2546
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1723 = aten.convolution_backward(buf1722, convolution_101, primals_225, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_101
        del primals_225
        buf1724 = buf1723[0]
        buf1725 = buf1723[1]
        del buf1723
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1726 = aten.convolution_backward(buf1724, relu_53, primals_224, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_224
        buf1727 = buf1726[0]
        buf1728 = buf1726[1]
        del buf1726
        buf1729 = buf1705; del buf1705  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_119.run(buf1678, relu_53, buf1727, buf1729, 432, 7056, grid=grid(432), stream=stream0)
        buf1730 = buf1720; del buf1720  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1729, buf1730, 216, 2, grid=grid(216), stream=stream0)
        buf1731 = reinterpret_tensor(buf1719, (216, 111), (111, 1), 0); del buf1719  # reuse
        buf1753 = buf1562; del buf1562  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_120.run(buf1678, relu_53, buf1727, convolution_100, unsqueeze_2558, convolution_96, unsqueeze_2582, buf1731, buf1753, 23976, 128, grid=grid(23976), stream=stream0)
        buf1732 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1734 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1731, squeeze_163, buf1732, buf1734, 216, 111, grid=grid(216), stream=stream0)
        buf1754 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1756 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1753, squeeze_157, buf1754, buf1756, 216, 111, grid=grid(216), stream=stream0)
        buf1733 = reinterpret_tensor(buf1724, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1724  # reuse
        buf1755 = buf1722; del buf1722  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_121.run(buf1678, relu_53, buf1727, convolution_100, unsqueeze_2558, buf1732, squeeze_163, buf1730, primals_222, convolution_96, unsqueeze_2582, buf1754, squeeze_157, primals_214, buf1733, buf1755, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1727
        del convolution_100
        del convolution_96
        del primals_214
        del primals_222
        del relu_53
        del squeeze_157
        del squeeze_163
        del unsqueeze_2558
        del unsqueeze_2582
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1735 = aten.convolution_backward(buf1733, convolution_99, primals_221, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1733
        del convolution_99
        del primals_221
        buf1736 = buf1735[0]
        buf1737 = buf1735[1]
        del buf1735
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1738 = aten.convolution_backward(buf1736, relu_52, primals_220, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_220
        buf1739 = buf1738[0]
        buf1740 = buf1738[1]
        del buf1738
        buf1741 = buf1753; del buf1753  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_52, buf1739, buf1741, 23976, 128, grid=grid(23976), stream=stream0)
        buf1742 = buf1754; del buf1754  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1741, buf1742, 216, 111, grid=grid(216), stream=stream0)
        buf1743 = reinterpret_tensor(buf1741, (216, 111), (1, 216), 0); del buf1741  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_52, buf1739, convolution_98, unsqueeze_2570, buf1743, 23976, 128, grid=grid(23976), stream=stream0)
        buf1744 = buf1732; del buf1732  # reuse
        buf1745 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1743, squeeze_160, buf1744, buf1745, 216, 111, grid=grid(216), stream=stream0)
        buf1746 = reinterpret_tensor(buf1736, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1736  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_52, buf1739, convolution_98, unsqueeze_2570, buf1744, squeeze_160, buf1742, primals_218, buf1746, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1739
        del convolution_98
        del primals_218
        del relu_52
        del squeeze_160
        del unsqueeze_2570
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1747 = aten.convolution_backward(buf1746, convolution_97, primals_217, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1746
        del convolution_97
        del primals_217
        buf1748 = buf1747[0]
        buf1749 = buf1747[1]
        del buf1747
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1750 = aten.convolution_backward(buf1748, relu_47, primals_216, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del buf1748
        del primals_216
        buf1751 = buf1750[0]
        buf1752 = buf1750[1]
        del buf1750
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1757 = aten.convolution_backward(buf1755, convolution_95, primals_213, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1755
        del convolution_95
        del primals_213
        buf1758 = buf1757[0]
        buf1759 = buf1757[1]
        del buf1757
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1760 = aten.convolution_backward(buf1758, relu_50, primals_212, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_212
        buf1761 = buf1760[0]
        buf1762 = buf1760[1]
        del buf1760
        buf1763 = reinterpret_tensor(buf1743, (216, 111), (111, 1), 0); del buf1743  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_50, buf1761, buf1763, 23976, 128, grid=grid(23976), stream=stream0)
        buf1764 = buf1744; del buf1744  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1763, buf1764, 216, 111, grid=grid(216), stream=stream0)
        buf1765 = reinterpret_tensor(buf1763, (216, 111), (1, 216), 0); del buf1763  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_50, buf1761, convolution_94, unsqueeze_2594, buf1765, 23976, 128, grid=grid(23976), stream=stream0)
        buf1766 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1767 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1765, squeeze_154, buf1766, buf1767, 216, 111, grid=grid(216), stream=stream0)
        buf1768 = reinterpret_tensor(buf1758, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1758  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_50, buf1761, convolution_94, unsqueeze_2594, buf1766, squeeze_154, buf1764, primals_210, buf1768, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1761
        del convolution_94
        del primals_210
        del relu_50
        del squeeze_154
        del unsqueeze_2594
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1769 = aten.convolution_backward(buf1768, convolution_93, primals_209, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1768
        del convolution_93
        del primals_209
        buf1770 = buf1769[0]
        buf1771 = buf1769[1]
        del buf1769
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1772 = aten.convolution_backward(buf1770, relu_47, primals_208, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_208
        buf1773 = buf1772[0]
        buf1774 = buf1772[1]
        del buf1772
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1775 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1678, (8, 216, 42, 42), (1905120, 1764, 42, 1), 381024), add_249, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_123)
        del add_249
        del getitem_123
        buf1776 = buf1775
        del buf1775
        buf1777 = buf1729; del buf1729  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_122.run(buf1678, buf1777, 432, 7056, grid=grid(432), stream=stream0)
        buf1778 = buf1766; del buf1766  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1777, buf1778, 216, 2, grid=grid(216), stream=stream0)
        buf1779 = reinterpret_tensor(buf1765, (216, 111), (111, 1), 0); del buf1765  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_123.run(buf1678, convolution_92, unsqueeze_2606, buf1779, 23976, 128, grid=grid(23976), stream=stream0)
        buf1780 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1781 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1779, squeeze_151, buf1780, buf1781, 216, 111, grid=grid(216), stream=stream0)
        buf1782 = reinterpret_tensor(buf1770, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1770  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_124.run(buf1678, convolution_92, unsqueeze_2606, buf1780, squeeze_151, buf1778, primals_206, buf1782, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del convolution_92
        del primals_206
        del squeeze_151
        del unsqueeze_2606
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1783 = aten.convolution_backward(buf1782, convolution_91, primals_205, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1782
        del convolution_91
        del primals_205
        buf1784 = buf1783[0]
        buf1785 = buf1783[1]
        del buf1783
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1786 = aten.convolution_backward(buf1784, relu_48, primals_204, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_204
        buf1787 = buf1786[0]
        buf1788 = buf1786[1]
        del buf1786
        buf1789 = buf1779; del buf1779  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_48, buf1787, buf1789, 23976, 128, grid=grid(23976), stream=stream0)
        buf1790 = buf1780; del buf1780  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1789, buf1790, 216, 111, grid=grid(216), stream=stream0)
        buf1791 = reinterpret_tensor(buf1789, (216, 111), (1, 216), 0); del buf1789  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_48, buf1787, convolution_90, unsqueeze_2618, buf1791, 23976, 128, grid=grid(23976), stream=stream0)
        buf1792 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1793 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1791, squeeze_148, buf1792, buf1793, 216, 111, grid=grid(216), stream=stream0)
        buf1794 = reinterpret_tensor(buf1784, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1784  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_48, buf1787, convolution_90, unsqueeze_2618, buf1792, squeeze_148, buf1790, primals_202, buf1794, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1787
        del convolution_90
        del primals_202
        del relu_48
        del squeeze_148
        del unsqueeze_2618
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1795 = aten.convolution_backward(buf1794, convolution_89, primals_201, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1794
        del convolution_89
        del primals_201
        buf1796 = buf1795[0]
        buf1797 = buf1795[1]
        del buf1795
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1798 = aten.convolution_backward(buf1796, relu_47, primals_200, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 216, [True, True, False])
        del buf1796
        del primals_200
        buf1799 = buf1798[0]
        buf1800 = buf1798[1]
        del buf1798
        buf1801 = buf1751; del buf1751  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_125.run(buf1801, buf1678, buf1704, relu_47, buf1773, buf1776, buf1799, 1728, 1764, grid=grid(1728, 1764), stream=stream0)
        del buf1704
        del buf1773
        del buf1776
        del relu_47
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1802 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1678, (8, 216, 42, 42), (1905120, 1764, 42, 1), 0), add_244, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_117)
        del add_244
        del getitem_117
        buf1803 = buf1802
        del buf1802
        buf1804 = buf1777; del buf1777  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_126.run(buf1678, buf1804, 432, 7056, grid=grid(432), stream=stream0)
        buf1805 = buf1792; del buf1792  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1804, buf1805, 216, 2, grid=grid(216), stream=stream0)
        buf1806 = reinterpret_tensor(buf1791, (216, 111), (111, 1), 0); del buf1791  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_127.run(buf1678, convolution_88, unsqueeze_2630, buf1806, 23976, 128, grid=grid(23976), stream=stream0)
        buf1807 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1808 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1806, squeeze_145, buf1807, buf1808, 216, 111, grid=grid(216), stream=stream0)
        buf1809 = reinterpret_tensor(buf1799, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1799  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_128.run(buf1678, convolution_88, unsqueeze_2630, buf1807, squeeze_145, buf1805, primals_198, buf1809, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1678
        del convolution_88
        del primals_198
        del squeeze_145
        del unsqueeze_2630
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1810 = aten.convolution_backward(buf1809, convolution_87, primals_197, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1809
        del convolution_87
        del primals_197
        buf1811 = buf1810[0]
        buf1812 = buf1810[1]
        del buf1810
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1813 = aten.convolution_backward(buf1811, relu_46, primals_196, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_196
        buf1814 = buf1813[0]
        buf1815 = buf1813[1]
        del buf1813
        buf1816 = buf1806; del buf1806  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_46, buf1814, buf1816, 23976, 128, grid=grid(23976), stream=stream0)
        buf1817 = buf1807; del buf1807  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1816, buf1817, 216, 111, grid=grid(216), stream=stream0)
        buf1818 = reinterpret_tensor(buf1816, (216, 111), (1, 216), 0); del buf1816  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_46, buf1814, convolution_86, unsqueeze_2642, buf1818, 23976, 128, grid=grid(23976), stream=stream0)
        buf1819 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1820 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1818, squeeze_142, buf1819, buf1820, 216, 111, grid=grid(216), stream=stream0)
        buf1821 = reinterpret_tensor(buf1811, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1811  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_46, buf1814, convolution_86, unsqueeze_2642, buf1819, squeeze_142, buf1817, primals_194, buf1821, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1814
        del convolution_86
        del primals_194
        del relu_46
        del squeeze_142
        del unsqueeze_2642
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1822 = aten.convolution_backward(buf1821, convolution_85, primals_193, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1821
        del convolution_85
        del primals_193
        buf1823 = buf1822[0]
        buf1824 = buf1822[1]
        del buf1822
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1825 = aten.convolution_backward(buf1823, relu_45, primals_192, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_192
        buf1826 = buf1825[0]
        buf1827 = buf1825[1]
        del buf1825
        buf1828 = buf1804; del buf1804  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_129.run(buf1801, buf1828, 432, 7056, grid=grid(432), stream=stream0)
        buf1829 = buf1819; del buf1819  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1828, buf1829, 216, 2, grid=grid(216), stream=stream0)
        buf1830 = reinterpret_tensor(buf1818, (216, 111), (111, 1), 0); del buf1818  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_130.run(buf1801, convolution_84, unsqueeze_2654, buf1830, 23976, 128, grid=grid(23976), stream=stream0)
        buf1831 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1832 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1830, squeeze_139, buf1831, buf1832, 216, 111, grid=grid(216), stream=stream0)
        buf1833 = reinterpret_tensor(buf1823, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1823  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_131.run(buf1801, convolution_84, unsqueeze_2654, buf1831, squeeze_139, buf1829, primals_190, buf1833, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1801
        del convolution_84
        del primals_190
        del squeeze_139
        del unsqueeze_2654
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1834 = aten.convolution_backward(buf1833, relu_44, primals_189, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1833
        del primals_189
        buf1835 = buf1834[0]
        buf1836 = buf1834[1]
        del buf1834
        buf1837 = buf1830; del buf1830  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_132.run(relu_45, buf1701, buf1803, buf1826, buf1837, 23976, 128, grid=grid(23976), stream=stream0)
        buf1838 = buf1831; del buf1831  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1837, buf1838, 216, 111, grid=grid(216), stream=stream0)
        buf1839 = reinterpret_tensor(buf1837, (216, 111), (1, 216), 0); del buf1837  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_133.run(relu_45, buf1701, buf1803, buf1826, convolution_83, unsqueeze_2666, buf1839, 23976, 128, grid=grid(23976), stream=stream0)
        buf1840 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1842 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1839, squeeze_136, buf1840, buf1842, 216, 111, grid=grid(216), stream=stream0)
        buf1841 = buf1803; del buf1803  # reuse
        buf1843 = buf1841; del buf1841  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_convolution_backward_native_batch_norm_backward_threshold_backward_134.run(buf1843, relu_45, buf1701, buf1826, convolution_83, unsqueeze_2666, buf1840, squeeze_136, buf1838, primals_187, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1701
        del buf1826
        del convolution_83
        del primals_187
        del relu_45
        del squeeze_136
        del unsqueeze_2666
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1844 = aten.convolution_backward(buf1843, relu_30, primals_186, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_186
        buf1845 = buf1844[0]
        buf1846 = buf1844[1]
        del buf1844
        buf1847 = buf1676; del buf1676  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_135.run(buf1847, relu_44, buf1835, 8640, 1764, grid=grid(8640, 1764), stream=stream0)
        del buf1835
        del relu_44
        buf1848 = buf1828; del buf1828  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_106.run(buf1847, buf1848, 432, 7056, grid=grid(432), stream=stream0)
        buf1849 = buf1840; del buf1840  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1848, buf1849, 216, 2, grid=grid(216), stream=stream0)
        buf1850 = reinterpret_tensor(buf1839, (216, 111), (111, 1), 0); del buf1839  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_108.run(buf1847, convolution_82, unsqueeze_2678, buf1850, 23976, 128, grid=grid(23976), stream=stream0)
        buf1851 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1852 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1850, squeeze_133, buf1851, buf1852, 216, 111, grid=grid(216), stream=stream0)
        buf1853 = buf1843; del buf1843  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_110.run(buf1847, convolution_82, unsqueeze_2678, buf1851, squeeze_133, buf1849, primals_184, buf1853, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del convolution_82
        del primals_184
        del squeeze_133
        del unsqueeze_2678
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1854 = aten.convolution_backward(buf1853, convolution_81, primals_183, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1853
        del convolution_81
        del primals_183
        buf1855 = buf1854[0]
        buf1856 = buf1854[1]
        del buf1854
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1857 = aten.convolution_backward(buf1855, relu_42, primals_182, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_182
        buf1858 = buf1857[0]
        buf1859 = buf1857[1]
        del buf1857
        buf1860 = buf1850; del buf1850  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_42, buf1858, buf1860, 23976, 128, grid=grid(23976), stream=stream0)
        buf1861 = buf1851; del buf1851  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1860, buf1861, 216, 111, grid=grid(216), stream=stream0)
        buf1862 = reinterpret_tensor(buf1860, (216, 111), (1, 216), 0); del buf1860  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_42, buf1858, convolution_80, unsqueeze_2690, buf1862, 23976, 128, grid=grid(23976), stream=stream0)
        buf1863 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1864 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1862, squeeze_130, buf1863, buf1864, 216, 111, grid=grid(216), stream=stream0)
        buf1865 = reinterpret_tensor(buf1855, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1855  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_42, buf1858, convolution_80, unsqueeze_2690, buf1863, squeeze_130, buf1861, primals_180, buf1865, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1858
        del convolution_80
        del primals_180
        del relu_42
        del squeeze_130
        del unsqueeze_2690
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1866 = aten.convolution_backward(buf1865, convolution_79, primals_179, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1865
        del convolution_79
        del primals_179
        buf1867 = buf1866[0]
        buf1868 = buf1866[1]
        del buf1866
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1869 = aten.convolution_backward(buf1867, relu_31, primals_178, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_178
        buf1870 = buf1869[0]
        buf1871 = buf1869[1]
        del buf1869
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1872 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1847, (8, 216, 42, 42), (1905120, 1764, 42, 1), 1143072), add_174, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_89)
        buf1873 = buf1872
        del buf1872
        buf1874 = buf1848; del buf1848  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_116.run(buf1847, buf1874, 432, 7056, grid=grid(432), stream=stream0)
        buf1875 = buf1863; del buf1863  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1874, buf1875, 216, 2, grid=grid(216), stream=stream0)
        buf1876 = reinterpret_tensor(buf1862, (216, 111), (111, 1), 0); del buf1862  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_117.run(buf1847, convolution_78, unsqueeze_2702, buf1876, 23976, 128, grid=grid(23976), stream=stream0)
        buf1877 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1878 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1876, squeeze_127, buf1877, buf1878, 216, 111, grid=grid(216), stream=stream0)
        buf1879 = reinterpret_tensor(buf1867, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1867  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_118.run(buf1847, convolution_78, unsqueeze_2702, buf1877, squeeze_127, buf1875, primals_176, buf1879, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del convolution_78
        del primals_176
        del squeeze_127
        del unsqueeze_2702
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1880 = aten.convolution_backward(buf1879, convolution_77, primals_175, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1879
        del convolution_77
        del primals_175
        buf1881 = buf1880[0]
        buf1882 = buf1880[1]
        del buf1880
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1883 = aten.convolution_backward(buf1881, relu_40, primals_174, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_174
        buf1884 = buf1883[0]
        buf1885 = buf1883[1]
        del buf1883
        buf1886 = buf1876; del buf1876  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_40, buf1884, buf1886, 23976, 128, grid=grid(23976), stream=stream0)
        buf1887 = buf1877; del buf1877  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1886, buf1887, 216, 111, grid=grid(216), stream=stream0)
        buf1888 = reinterpret_tensor(buf1886, (216, 111), (1, 216), 0); del buf1886  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_40, buf1884, convolution_76, unsqueeze_2714, buf1888, 23976, 128, grid=grid(23976), stream=stream0)
        buf1889 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1890 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1888, squeeze_124, buf1889, buf1890, 216, 111, grid=grid(216), stream=stream0)
        buf1891 = reinterpret_tensor(buf1881, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1881  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_40, buf1884, convolution_76, unsqueeze_2714, buf1889, squeeze_124, buf1887, primals_172, buf1891, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1884
        del convolution_76
        del primals_172
        del relu_40
        del squeeze_124
        del unsqueeze_2714
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1892 = aten.convolution_backward(buf1891, convolution_75, primals_171, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_75
        del primals_171
        buf1893 = buf1892[0]
        buf1894 = buf1892[1]
        del buf1892
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1895 = aten.convolution_backward(buf1893, relu_39, primals_170, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_170
        buf1896 = buf1895[0]
        buf1897 = buf1895[1]
        del buf1895
        buf1898 = buf1874; del buf1874  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_119.run(buf1847, relu_39, buf1896, buf1898, 432, 7056, grid=grid(432), stream=stream0)
        buf1899 = buf1889; del buf1889  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1898, buf1899, 216, 2, grid=grid(216), stream=stream0)
        buf1900 = reinterpret_tensor(buf1888, (216, 111), (111, 1), 0); del buf1888  # reuse
        buf1922 = buf1731; del buf1731  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_120.run(buf1847, relu_39, buf1896, convolution_74, unsqueeze_2726, convolution_70, unsqueeze_2750, buf1900, buf1922, 23976, 128, grid=grid(23976), stream=stream0)
        buf1901 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1903 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1900, squeeze_121, buf1901, buf1903, 216, 111, grid=grid(216), stream=stream0)
        del buf1900
        buf1923 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1925 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1922, squeeze_115, buf1923, buf1925, 216, 111, grid=grid(216), stream=stream0)
        buf1902 = reinterpret_tensor(buf1893, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1893  # reuse
        buf1924 = buf1891; del buf1891  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_121.run(buf1847, relu_39, buf1896, convolution_74, unsqueeze_2726, buf1901, squeeze_121, buf1899, primals_168, convolution_70, unsqueeze_2750, buf1923, squeeze_115, primals_160, buf1902, buf1924, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1896
        del convolution_70
        del convolution_74
        del primals_160
        del primals_168
        del relu_39
        del squeeze_115
        del squeeze_121
        del unsqueeze_2726
        del unsqueeze_2750
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1904 = aten.convolution_backward(buf1902, convolution_73, primals_167, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1902
        del convolution_73
        del primals_167
        buf1905 = buf1904[0]
        buf1906 = buf1904[1]
        del buf1904
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1907 = aten.convolution_backward(buf1905, relu_38, primals_166, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_166
        buf1908 = buf1907[0]
        buf1909 = buf1907[1]
        del buf1907
        buf1910 = buf1922; del buf1922  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_38, buf1908, buf1910, 23976, 128, grid=grid(23976), stream=stream0)
        buf1911 = buf1923; del buf1923  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1910, buf1911, 216, 111, grid=grid(216), stream=stream0)
        buf1912 = reinterpret_tensor(buf1910, (216, 111), (1, 216), 0); del buf1910  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_38, buf1908, convolution_72, unsqueeze_2738, buf1912, 23976, 128, grid=grid(23976), stream=stream0)
        buf1913 = buf1901; del buf1901  # reuse
        buf1914 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1912, squeeze_118, buf1913, buf1914, 216, 111, grid=grid(216), stream=stream0)
        buf1915 = reinterpret_tensor(buf1905, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1905  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_38, buf1908, convolution_72, unsqueeze_2738, buf1913, squeeze_118, buf1911, primals_164, buf1915, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1908
        del convolution_72
        del primals_164
        del relu_38
        del squeeze_118
        del unsqueeze_2738
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1916 = aten.convolution_backward(buf1915, convolution_71, primals_163, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1915
        del convolution_71
        del primals_163
        buf1917 = buf1916[0]
        buf1918 = buf1916[1]
        del buf1916
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1919 = aten.convolution_backward(buf1917, relu_33, primals_162, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 216, [True, True, False])
        del buf1917
        del primals_162
        buf1920 = buf1919[0]
        buf1921 = buf1919[1]
        del buf1919
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1926 = aten.convolution_backward(buf1924, convolution_69, primals_159, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1924
        del convolution_69
        del primals_159
        buf1927 = buf1926[0]
        buf1928 = buf1926[1]
        del buf1926
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1929 = aten.convolution_backward(buf1927, relu_36, primals_158, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_158
        buf1930 = buf1929[0]
        buf1931 = buf1929[1]
        del buf1929
        buf1932 = reinterpret_tensor(buf1912, (216, 111), (111, 1), 0); del buf1912  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_36, buf1930, buf1932, 23976, 128, grid=grid(23976), stream=stream0)
        buf1933 = buf1913; del buf1913  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1932, buf1933, 216, 111, grid=grid(216), stream=stream0)
        buf1934 = reinterpret_tensor(buf1932, (216, 111), (1, 216), 0); del buf1932  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_36, buf1930, convolution_68, unsqueeze_2762, buf1934, 23976, 128, grid=grid(23976), stream=stream0)
        buf1935 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1936 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1934, squeeze_112, buf1935, buf1936, 216, 111, grid=grid(216), stream=stream0)
        buf1937 = reinterpret_tensor(buf1927, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1927  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_36, buf1930, convolution_68, unsqueeze_2762, buf1935, squeeze_112, buf1933, primals_156, buf1937, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1930
        del convolution_68
        del primals_156
        del relu_36
        del squeeze_112
        del unsqueeze_2762
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1938 = aten.convolution_backward(buf1937, convolution_67, primals_155, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1937
        del convolution_67
        del primals_155
        buf1939 = buf1938[0]
        buf1940 = buf1938[1]
        del buf1938
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1941 = aten.convolution_backward(buf1939, relu_33, primals_154, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_154
        buf1942 = buf1941[0]
        buf1943 = buf1941[1]
        del buf1941
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1944 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1847, (8, 216, 42, 42), (1905120, 1764, 42, 1), 381024), add_174, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_89)
        del add_174
        del getitem_89
        buf1945 = buf1944
        del buf1944
        buf1946 = buf1898; del buf1898  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_122.run(buf1847, buf1946, 432, 7056, grid=grid(432), stream=stream0)
        buf1947 = buf1935; del buf1935  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1946, buf1947, 216, 2, grid=grid(216), stream=stream0)
        buf1948 = reinterpret_tensor(buf1934, (216, 111), (111, 1), 0); del buf1934  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_123.run(buf1847, convolution_66, unsqueeze_2774, buf1948, 23976, 128, grid=grid(23976), stream=stream0)
        buf1949 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1950 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1948, squeeze_109, buf1949, buf1950, 216, 111, grid=grid(216), stream=stream0)
        buf1951 = reinterpret_tensor(buf1939, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1939  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_124.run(buf1847, convolution_66, unsqueeze_2774, buf1949, squeeze_109, buf1947, primals_152, buf1951, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del convolution_66
        del primals_152
        del squeeze_109
        del unsqueeze_2774
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1952 = aten.convolution_backward(buf1951, convolution_65, primals_151, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1951
        del convolution_65
        del primals_151
        buf1953 = buf1952[0]
        buf1954 = buf1952[1]
        del buf1952
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1955 = aten.convolution_backward(buf1953, relu_34, primals_150, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_150
        buf1956 = buf1955[0]
        buf1957 = buf1955[1]
        del buf1955
        buf1958 = buf1948; del buf1948  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_34, buf1956, buf1958, 23976, 128, grid=grid(23976), stream=stream0)
        buf1959 = buf1949; del buf1949  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1958, buf1959, 216, 111, grid=grid(216), stream=stream0)
        buf1960 = reinterpret_tensor(buf1958, (216, 111), (1, 216), 0); del buf1958  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_34, buf1956, convolution_64, unsqueeze_2786, buf1960, 23976, 128, grid=grid(23976), stream=stream0)
        buf1961 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1962 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1960, squeeze_106, buf1961, buf1962, 216, 111, grid=grid(216), stream=stream0)
        buf1963 = reinterpret_tensor(buf1953, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1953  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_34, buf1956, convolution_64, unsqueeze_2786, buf1961, squeeze_106, buf1959, primals_148, buf1963, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1956
        del convolution_64
        del primals_148
        del relu_34
        del squeeze_106
        del unsqueeze_2786
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1964 = aten.convolution_backward(buf1963, convolution_63, primals_147, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1963
        del convolution_63
        del primals_147
        buf1965 = buf1964[0]
        buf1966 = buf1964[1]
        del buf1964
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1967 = aten.convolution_backward(buf1965, relu_33, primals_146, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 216, [True, True, False])
        del buf1965
        del primals_146
        buf1968 = buf1967[0]
        buf1969 = buf1967[1]
        del buf1967
        buf1970 = buf1920; del buf1920  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_125.run(buf1970, buf1847, buf1873, relu_33, buf1942, buf1945, buf1968, 1728, 1764, grid=grid(1728, 1764), stream=stream0)
        del buf1873
        del buf1942
        del buf1945
        del relu_33
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf1971 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf1847, (8, 216, 42, 42), (1905120, 1764, 42, 1), 0), add_169, [3, 3], [1, 1], [1, 1], [1, 1], False, getitem_83)
        del add_169
        del getitem_83
        buf1972 = buf1971
        del buf1971
        buf1973 = buf1946; del buf1946  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_126.run(buf1847, buf1973, 432, 7056, grid=grid(432), stream=stream0)
        buf1974 = buf1961; del buf1961  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1973, buf1974, 216, 2, grid=grid(216), stream=stream0)
        buf1975 = reinterpret_tensor(buf1960, (216, 111), (111, 1), 0); del buf1960  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_127.run(buf1847, convolution_62, unsqueeze_2798, buf1975, 23976, 128, grid=grid(23976), stream=stream0)
        buf1976 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1977 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1975, squeeze_103, buf1976, buf1977, 216, 111, grid=grid(216), stream=stream0)
        buf1978 = reinterpret_tensor(buf1968, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1968  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_128.run(buf1847, convolution_62, unsqueeze_2798, buf1976, squeeze_103, buf1974, primals_144, buf1978, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1847
        del convolution_62
        del primals_144
        del squeeze_103
        del unsqueeze_2798
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1979 = aten.convolution_backward(buf1978, convolution_61, primals_143, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1978
        del convolution_61
        del primals_143
        buf1980 = buf1979[0]
        buf1981 = buf1979[1]
        del buf1979
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1982 = aten.convolution_backward(buf1980, relu_32, primals_142, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_142
        buf1983 = buf1982[0]
        buf1984 = buf1982[1]
        del buf1982
        buf1985 = buf1975; del buf1975  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(relu_32, buf1983, buf1985, 23976, 128, grid=grid(23976), stream=stream0)
        buf1986 = buf1976; del buf1976  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf1985, buf1986, 216, 111, grid=grid(216), stream=stream0)
        buf1987 = reinterpret_tensor(buf1985, (216, 111), (1, 216), 0); del buf1985  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_113.run(relu_32, buf1983, convolution_60, unsqueeze_2810, buf1987, 23976, 128, grid=grid(23976), stream=stream0)
        buf1988 = empty((216, ), device='cuda', dtype=torch.float32)
        buf1989 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf1987, squeeze_100, buf1988, buf1989, 216, 111, grid=grid(216), stream=stream0)
        buf1990 = reinterpret_tensor(buf1980, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1980  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_115.run(relu_32, buf1983, convolution_60, unsqueeze_2810, buf1988, squeeze_100, buf1986, primals_140, buf1990, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1983
        del convolution_60
        del primals_140
        del relu_32
        del squeeze_100
        del unsqueeze_2810
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1991 = aten.convolution_backward(buf1990, convolution_59, primals_139, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1990
        del convolution_59
        del primals_139
        buf1992 = buf1991[0]
        buf1993 = buf1991[1]
        del buf1991
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1994 = aten.convolution_backward(buf1992, relu_31, primals_138, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 216, [True, True, False])
        del primals_138
        buf1995 = buf1994[0]
        buf1996 = buf1994[1]
        del buf1994
        buf1997 = buf1973; del buf1973  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_129.run(buf1970, buf1997, 432, 7056, grid=grid(432), stream=stream0)
        buf1998 = buf1988; del buf1988  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_107.run(buf1997, buf1998, 216, 2, grid=grid(216), stream=stream0)
        del buf1997
        buf1999 = reinterpret_tensor(buf1987, (216, 111), (111, 1), 0); del buf1987  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_130.run(buf1970, convolution_58, unsqueeze_2822, buf1999, 23976, 128, grid=grid(23976), stream=stream0)
        buf2000 = empty((216, ), device='cuda', dtype=torch.float32)
        buf2001 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_109.run(buf1999, squeeze_97, buf2000, buf2001, 216, 111, grid=grid(216), stream=stream0)
        buf2002 = reinterpret_tensor(buf1992, (8, 216, 42, 42), (381024, 1, 9072, 216), 0); del buf1992  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_131.run(buf1970, convolution_58, unsqueeze_2822, buf2000, squeeze_97, buf1998, primals_136, buf2002, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1970
        del convolution_58
        del primals_136
        del squeeze_97
        del unsqueeze_2822
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2003 = aten.convolution_backward(buf2002, relu_30, primals_135, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2002
        del primals_135
        buf2004 = buf2003[0]
        buf2005 = buf2003[1]
        del buf2003
        buf2006 = buf1999; del buf1999  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_132.run(relu_31, buf1870, buf1972, buf1995, buf2006, 23976, 128, grid=grid(23976), stream=stream0)
        buf2007 = buf2000; del buf2000  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_112.run(buf2006, buf2007, 216, 111, grid=grid(216), stream=stream0)
        buf2008 = reinterpret_tensor(buf2006, (216, 111), (1, 216), 0); del buf2006  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_133.run(relu_31, buf1870, buf1972, buf1995, cat_3, unsqueeze_2834, buf2008, 23976, 128, grid=grid(23976), stream=stream0)
        buf2009 = empty((216, ), device='cuda', dtype=torch.float32)
        buf2011 = empty((216, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(buf2008, squeeze_94, buf2009, buf2011, 216, 111, grid=grid(216), stream=stream0)
        del buf2008
        buf2010 = buf1972; del buf1972  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_136.run(buf2010, relu_31, buf1870, buf1995, cat_3, unsqueeze_2834, buf2009, squeeze_94, buf2007, 14112, 216, grid=grid(14112, 216), stream=stream0)
        del buf1870
        del buf1995
        del cat_3
        del relu_31
        del unsqueeze_2834
        buf2012 = reinterpret_tensor(buf1315, (8, 108, 42, 42), (190512, 1, 4536, 108), 0); del buf1315  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        triton_poi_fused_convolution_backward_137.run(buf2010, squeeze_94, primals_133, buf2012, 1524096, grid=grid(1524096), stream=stream0)
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2013 = aten.convolution_backward(buf2012, avg_pool2d_3, primals_132, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del avg_pool2d_3
        del primals_132
        buf2014 = buf2013[0]
        buf2015 = buf2013[1]
        del buf2013
        buf2016 = buf2012; del buf2012  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        triton_poi_fused_convolution_backward_138.run(buf2010, squeeze_94, primals_133, buf2016, 1524096, grid=grid(1524096), stream=stream0)
        del buf2010
        del primals_133
        del squeeze_94
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2017 = aten.convolution_backward(buf2016, avg_pool2d_2, primals_131, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del avg_pool2d_2
        del primals_131
        buf2018 = buf2017[0]
        buf2019 = buf2017[1]
        del buf2017
        buf2021 = buf1845; del buf1845  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_139.run(buf2021, relu_30, buf2004, 4320, 1764, grid=grid(4320, 1764), stream=stream0)
        del buf2004
        del relu_30
        buf2022 = reinterpret_tensor(buf2009, (108, 2), (1, 108), 0); del buf2009  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_140.run(buf2021, buf2022, 216, 7056, grid=grid(216), stream=stream0)
        buf2023 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_141.run(buf2022, buf2023, 108, 2, grid=grid(108), stream=stream0)
        buf2024 = empty((108, 111), device='cuda', dtype=torch.float32)
        buf2031 = empty((108, 111), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_142.run(buf2021, convolution_55, unsqueeze_2846, convolution_54, unsqueeze_2858, buf2024, buf2031, 11988, 128, grid=grid(11988), stream=stream0)
        buf2025 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2026 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_143.run(buf2024, squeeze_91, buf2025, buf2026, 108, 111, grid=grid(108), stream=stream0)
        buf2032 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2033 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_143.run(buf2031, squeeze_88, buf2032, buf2033, 108, 111, grid=grid(108), stream=stream0)
        buf2027 = buf2016; del buf2016  # reuse
        buf2034 = reinterpret_tensor(buf1313, (8, 108, 42, 42), (190512, 1, 4536, 108), 0); del buf1313  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_144.run(buf2021, convolution_55, unsqueeze_2846, buf2025, squeeze_91, buf2023, primals_129, convolution_54, unsqueeze_2858, buf2032, squeeze_88, primals_127, buf2027, buf2034, 14112, 108, grid=grid(14112, 108), stream=stream0)
        del convolution_54
        del convolution_55
        del primals_127
        del primals_129
        del squeeze_88
        del squeeze_91
        del unsqueeze_2846
        del unsqueeze_2858
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2028 = aten.convolution_backward(buf2027, constant_pad_nd_18, primals_14, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del constant_pad_nd_18
        del primals_14
        buf2029 = buf2028[0]
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf2053 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf2021, (8, 108, 42, 42), (952560, 1764, 42, 1), 571536), constant_pad_nd_13, [3, 3], [2, 2], [0, 0], [1, 1], False, getitem_53)
        buf2054 = buf2053
        del buf2053
        buf2055 = buf2022; del buf2022  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_145.run(buf2021, buf2055, 216, 7056, grid=grid(216), stream=stream0)
        buf2056 = buf2032; del buf2032  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_141.run(buf2055, buf2056, 108, 2, grid=grid(108), stream=stream0)
        buf2057 = buf2031; del buf2031  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_146.run(buf2021, convolution_50, unsqueeze_2882, buf2057, 11988, 128, grid=grid(11988), stream=stream0)
        buf2058 = buf2025; del buf2025  # reuse
        buf2059 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_143.run(buf2057, squeeze_82, buf2058, buf2059, 108, 111, grid=grid(108), stream=stream0)
        buf2060 = buf2027; del buf2027  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_147.run(buf2021, convolution_50, unsqueeze_2882, buf2058, squeeze_82, buf2056, primals_120, buf2060, 14112, 108, grid=grid(14112, 108), stream=stream0)
        del convolution_50
        del primals_120
        del squeeze_82
        del unsqueeze_2882
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2061 = aten.convolution_backward(buf2060, convolution_49, primals_119, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2060
        del convolution_49
        del primals_119
        buf2062 = buf2061[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2064 = aten.convolution_backward(buf2062, relu_25, primals_118, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 108, [True, True, False])
        del primals_118
        buf2065 = buf2064[0]
        buf2067 = buf2057; del buf2057  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_148.run(relu_25, buf2065, buf2067, 11988, 128, grid=grid(11988), stream=stream0)
        buf2068 = buf2058; del buf2058  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_149.run(buf2067, buf2068, 108, 111, grid=grid(108), stream=stream0)
        buf2069 = reinterpret_tensor(buf2067, (108, 111), (1, 108), 0); del buf2067  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_150.run(relu_25, buf2065, convolution_48, unsqueeze_2894, buf2069, 11988, 128, grid=grid(11988), stream=stream0)
        buf2070 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2071 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_151.run(buf2069, squeeze_79, buf2070, buf2071, 108, 111, grid=grid(108), stream=stream0)
        buf2072 = reinterpret_tensor(buf2062, (8, 108, 42, 42), (190512, 1, 4536, 108), 0); del buf2062  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_152.run(relu_25, buf2065, convolution_48, unsqueeze_2894, buf2070, squeeze_79, buf2068, primals_116, buf2072, 14112, 108, grid=grid(14112, 108), stream=stream0)
        del buf2065
        del convolution_48
        del primals_116
        del relu_25
        del squeeze_79
        del unsqueeze_2894
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf2073 = aten.convolution_backward(buf2072, convolution_47, primals_115, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_47
        del primals_115
        buf2074 = buf2073[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2076 = aten.convolution_backward(buf2074, relu_24, primals_114, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 108, [True, True, False])
        del primals_114
        buf2077 = buf2076[0]
        buf2079 = buf2055; del buf2055  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_153.run(buf2021, relu_24, buf2077, buf2079, 216, 7056, grid=grid(216), stream=stream0)
        buf2080 = buf2070; del buf2070  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_141.run(buf2079, buf2080, 108, 2, grid=grid(108), stream=stream0)
        buf2081 = reinterpret_tensor(buf2069, (108, 111), (111, 1), 0); del buf2069  # reuse
        buf2103 = buf2024; del buf2024  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_154.run(buf2021, relu_24, buf2077, convolution_46, unsqueeze_2906, convolution_42, unsqueeze_2930, buf2081, buf2103, 11988, 128, grid=grid(11988), stream=stream0)
        buf2082 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2084 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_143.run(buf2081, squeeze_76, buf2082, buf2084, 108, 111, grid=grid(108), stream=stream0)
        del buf2081
        buf2104 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2106 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_143.run(buf2103, squeeze_70, buf2104, buf2106, 108, 111, grid=grid(108), stream=stream0)
        buf2083 = reinterpret_tensor(buf2074, (8, 108, 42, 42), (190512, 1, 4536, 108), 0); del buf2074  # reuse
        buf2105 = buf2072; del buf2072  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_155.run(buf2021, relu_24, buf2077, convolution_46, unsqueeze_2906, buf2082, squeeze_76, buf2080, primals_112, convolution_42, unsqueeze_2930, buf2104, squeeze_70, primals_105, buf2083, buf2105, 14112, 108, grid=grid(14112, 108), stream=stream0)
        del buf2077
        del convolution_42
        del convolution_46
        del primals_105
        del primals_112
        del relu_24
        del squeeze_70
        del squeeze_76
        del unsqueeze_2906
        del unsqueeze_2930
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2085 = aten.convolution_backward(buf2083, convolution_45, primals_111, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2083
        del convolution_45
        del primals_111
        buf2086 = buf2085[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2088 = aten.convolution_backward(buf2086, relu_23, primals_110, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 108, [True, True, False])
        del primals_110
        buf2089 = buf2088[0]
        buf2091 = buf2103; del buf2103  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_148.run(relu_23, buf2089, buf2091, 11988, 128, grid=grid(11988), stream=stream0)
        buf2092 = buf2104; del buf2104  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_149.run(buf2091, buf2092, 108, 111, grid=grid(108), stream=stream0)
        buf2093 = reinterpret_tensor(buf2091, (108, 111), (1, 108), 0); del buf2091  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_150.run(relu_23, buf2089, convolution_44, unsqueeze_2918, buf2093, 11988, 128, grid=grid(11988), stream=stream0)
        buf2094 = buf2082; del buf2082  # reuse
        buf2095 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_151.run(buf2093, squeeze_73, buf2094, buf2095, 108, 111, grid=grid(108), stream=stream0)
        buf2096 = reinterpret_tensor(buf2086, (8, 108, 42, 42), (190512, 1, 4536, 108), 0); del buf2086  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_152.run(relu_23, buf2089, convolution_44, unsqueeze_2918, buf2094, squeeze_73, buf2092, primals_108, buf2096, 14112, 108, grid=grid(14112, 108), stream=stream0)
        del buf2089
        del convolution_44
        del primals_108
        del relu_23
        del squeeze_73
        del unsqueeze_2918
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf2097 = aten.convolution_backward(buf2096, convolution_43, primals_107, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2096
        del convolution_43
        del primals_107
        buf2098 = buf2097[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2100 = aten.convolution_backward(buf2098, constant_pad_nd_15, primals_12, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 108, [True, True, False])
        del buf2098
        del constant_pad_nd_15
        del primals_12
        buf2101 = buf2100[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2107 = aten.convolution_backward(buf2105, convolution_41, primals_104, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2105
        del convolution_41
        del primals_104
        buf2108 = buf2107[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2110 = aten.convolution_backward(buf2108, relu_21, primals_103, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 108, [True, True, False])
        del primals_103
        buf2111 = buf2110[0]
        buf2113 = reinterpret_tensor(buf2093, (108, 111), (111, 1), 0); del buf2093  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_148.run(relu_21, buf2111, buf2113, 11988, 128, grid=grid(11988), stream=stream0)
        buf2114 = buf2094; del buf2094  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_149.run(buf2113, buf2114, 108, 111, grid=grid(108), stream=stream0)
        buf2115 = reinterpret_tensor(buf2113, (108, 111), (1, 108), 0); del buf2113  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_150.run(relu_21, buf2111, convolution_40, unsqueeze_2942, buf2115, 11988, 128, grid=grid(11988), stream=stream0)
        buf2116 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2117 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_151.run(buf2115, squeeze_67, buf2116, buf2117, 108, 111, grid=grid(108), stream=stream0)
        buf2118 = reinterpret_tensor(buf2108, (8, 108, 42, 42), (190512, 1, 4536, 108), 0); del buf2108  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_152.run(relu_21, buf2111, convolution_40, unsqueeze_2942, buf2116, squeeze_67, buf2114, primals_101, buf2118, 14112, 108, grid=grid(14112, 108), stream=stream0)
        del buf2111
        del convolution_40
        del primals_101
        del relu_21
        del squeeze_67
        del unsqueeze_2942
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf2119 = aten.convolution_backward(buf2118, convolution_39, primals_100, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2118
        del convolution_39
        del primals_100
        buf2120 = buf2119[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2122 = aten.convolution_backward(buf2120, constant_pad_nd_14, primals_11, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 108, [True, True, False])
        del constant_pad_nd_14
        del primals_11
        buf2123 = buf2122[0]
        buf2125 = buf2029; del buf2029  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.threshold_backward]
        triton_poi_fused_add_constant_pad_nd_threshold_backward_156.run(buf2125, le_171, buf2054, buf2101, buf2123, 864, 6889, grid=grid(864, 6889), stream=stream0)
        del buf2054
        del buf2101
        del buf2123
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf2126 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf2021, (8, 108, 42, 42), (952560, 1764, 42, 1), 190512), constant_pad_nd_13, [3, 3], [2, 2], [0, 0], [1, 1], False, getitem_53)
        del constant_pad_nd_13
        del getitem_53
        buf2127 = buf2126
        del buf2126
        buf2128 = buf2079; del buf2079  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_157.run(buf2021, buf2128, 216, 7056, grid=grid(216), stream=stream0)
        buf2129 = buf2116; del buf2116  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_141.run(buf2128, buf2129, 108, 2, grid=grid(108), stream=stream0)
        buf2130 = reinterpret_tensor(buf2115, (108, 111), (111, 1), 0); del buf2115  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_158.run(buf2021, convolution_38, unsqueeze_2954, buf2130, 11988, 128, grid=grid(11988), stream=stream0)
        buf2131 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2132 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_143.run(buf2130, squeeze_64, buf2131, buf2132, 108, 111, grid=grid(108), stream=stream0)
        buf2133 = reinterpret_tensor(buf2120, (8, 108, 42, 42), (190512, 1, 4536, 108), 0); del buf2120  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_159.run(buf2021, convolution_38, unsqueeze_2954, buf2131, squeeze_64, buf2129, primals_98, buf2133, 14112, 108, grid=grid(14112, 108), stream=stream0)
        del convolution_38
        del primals_98
        del squeeze_64
        del unsqueeze_2954
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2134 = aten.convolution_backward(buf2133, convolution_37, primals_97, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2133
        del convolution_37
        del primals_97
        buf2135 = buf2134[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2137 = aten.convolution_backward(buf2135, relu_19, primals_96, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 108, [True, True, False])
        del primals_96
        buf2138 = buf2137[0]
        buf2140 = buf2130; del buf2130  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_148.run(relu_19, buf2138, buf2140, 11988, 128, grid=grid(11988), stream=stream0)
        buf2141 = buf2131; del buf2131  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_149.run(buf2140, buf2141, 108, 111, grid=grid(108), stream=stream0)
        buf2142 = reinterpret_tensor(buf2140, (108, 111), (1, 108), 0); del buf2140  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_150.run(relu_19, buf2138, convolution_36, unsqueeze_2966, buf2142, 11988, 128, grid=grid(11988), stream=stream0)
        buf2143 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2144 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_151.run(buf2142, squeeze_61, buf2143, buf2144, 108, 111, grid=grid(108), stream=stream0)
        buf2145 = reinterpret_tensor(buf2135, (8, 108, 42, 42), (190512, 1, 4536, 108), 0); del buf2135  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_152.run(relu_19, buf2138, convolution_36, unsqueeze_2966, buf2143, squeeze_61, buf2141, primals_94, buf2145, 14112, 108, grid=grid(14112, 108), stream=stream0)
        del buf2138
        del convolution_36
        del primals_94
        del relu_19
        del squeeze_61
        del unsqueeze_2966
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf2146 = aten.convolution_backward(buf2145, convolution_35, primals_93, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2145
        del convolution_35
        del primals_93
        buf2147 = buf2146[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2149 = aten.convolution_backward(buf2147, constant_pad_nd_12, primals_10, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 108, [True, True, False])
        del buf2147
        del constant_pad_nd_12
        del primals_10
        buf2150 = buf2149[0]
        buf2179 = empty((108, 431), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_160.run(buf2125, buf2127, le_171, buf2150, buf2179, 46548, 128, grid=grid(46548), stream=stream0)
        buf2180 = buf2143; del buf2143  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_161.run(buf2179, buf2180, 108, 431, grid=grid(108), stream=stream0)
        buf2181 = reinterpret_tensor(buf2179, (108, 431), (1, 108), 0); del buf2179  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_162.run(buf2125, buf2127, le_171, buf2150, convolution_30, unsqueeze_3002, buf2181, 46548, 128, grid=grid(46548), stream=stream0)
        buf2182 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2184 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_163.run(buf2181, squeeze_52, buf2182, buf2184, 108, 431, grid=grid(108), stream=stream0)
        buf2183 = empty_strided((8, 108, 83, 83), (744012, 1, 8964, 108), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_164.run(buf2125, buf2127, le_171, buf2150, convolution_30, unsqueeze_3002, buf2182, squeeze_52, buf2183, 55112, 108, grid=grid(55112, 108), stream=stream0)
        del buf2125
        del buf2127
        del buf2150
        del convolution_30
        del le_171
        del unsqueeze_3002
        buf2185 = buf2183; del buf2183  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_165.run(buf2185, buf2180, squeeze_52, primals_84, 864, 6889, grid=grid(864, 6889), stream=stream0)
        del primals_84
        del squeeze_52
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2186 = aten.convolution_backward(buf2185, relu_15, primals_83, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_83
        buf2187 = buf2186[0]
        buf2020 = empty((8, 270, 83, 83), device='cuda', dtype=torch.float32)
        buf2204 = buf2020; del buf2020  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.avg_pool2d_backward, aten.constant_pad_nd, aten.threshold_backward]
        triton_poi_fused_add_avg_pool2d_backward_constant_pad_nd_threshold_backward_166.run(buf2204, buf2014, buf2018, relu_15, buf2187, 2160, 6889, grid=grid(2160, 6889), stream=stream0)
        del buf2014
        del buf2018
        del buf2187
        del relu_15
        buf2030 = buf2028[1]
        del buf2028
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2035 = aten.convolution_backward(buf2034, convolution_53, primals_126, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2034
        del convolution_53
        del primals_126
        buf2036 = buf2035[0]
        buf2037 = buf2035[1]
        del buf2035
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2038 = aten.convolution_backward(buf2036, relu_27, primals_125, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 108, [True, True, False])
        del primals_125
        buf2039 = buf2038[0]
        buf2040 = buf2038[1]
        del buf2038
        buf2041 = reinterpret_tensor(buf2142, (108, 111), (111, 1), 0); del buf2142  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_148.run(relu_27, buf2039, buf2041, 11988, 128, grid=grid(11988), stream=stream0)
        buf2042 = buf2182; del buf2182  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_149.run(buf2041, buf2042, 108, 111, grid=grid(108), stream=stream0)
        buf2043 = reinterpret_tensor(buf2041, (108, 111), (1, 108), 0); del buf2041  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_150.run(relu_27, buf2039, convolution_52, unsqueeze_2870, buf2043, 11988, 128, grid=grid(11988), stream=stream0)
        buf2044 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2045 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_151.run(buf2043, squeeze_85, buf2044, buf2045, 108, 111, grid=grid(108), stream=stream0)
        buf2046 = reinterpret_tensor(buf2036, (8, 108, 42, 42), (190512, 1, 4536, 108), 0); del buf2036  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_152.run(relu_27, buf2039, convolution_52, unsqueeze_2870, buf2044, squeeze_85, buf2042, primals_123, buf2046, 14112, 108, grid=grid(14112, 108), stream=stream0)
        del buf2039
        del convolution_52
        del primals_123
        del relu_27
        del squeeze_85
        del unsqueeze_2870
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf2047 = aten.convolution_backward(buf2046, convolution_51, primals_122, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2046
        del convolution_51
        del primals_122
        buf2048 = buf2047[0]
        buf2049 = buf2047[1]
        del buf2047
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2050 = aten.convolution_backward(buf2048, constant_pad_nd_17, primals_13, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 108, [True, True, False])
        del constant_pad_nd_17
        del primals_13
        buf2051 = buf2050[0]
        buf2052 = buf2050[1]
        del buf2050
        buf2063 = buf2061[1]
        del buf2061
        buf2066 = buf2064[1]
        del buf2064
        buf2075 = buf2073[1]
        del buf2073
        buf2078 = buf2076[1]
        del buf2076
        buf2087 = buf2085[1]
        del buf2085
        buf2090 = buf2088[1]
        del buf2088
        buf2099 = buf2097[1]
        del buf2097
        buf2102 = buf2100[1]
        del buf2100
        buf2109 = buf2107[1]
        del buf2107
        buf2112 = buf2110[1]
        del buf2110
        buf2121 = buf2119[1]
        del buf2119
        buf2124 = buf2122[1]
        del buf2122
        buf2136 = buf2134[1]
        del buf2134
        buf2139 = buf2137[1]
        del buf2137
        buf2148 = buf2146[1]
        del buf2146
        buf2151 = buf2149[1]
        del buf2149
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf2152 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf2021, (8, 108, 42, 42), (952560, 1764, 42, 1), 0), constant_pad_nd_11, [3, 3], [2, 2], [0, 0], [1, 1], False, getitem_47)
        del constant_pad_nd_11
        del getitem_47
        buf2153 = buf2152
        del buf2152
        buf2154 = buf2128; del buf2128  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_167.run(buf2021, buf2154, 216, 7056, grid=grid(216), stream=stream0)
        buf2155 = buf2044; del buf2044  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_141.run(buf2154, buf2155, 108, 2, grid=grid(108), stream=stream0)
        del buf2154
        buf2156 = reinterpret_tensor(buf2043, (108, 111), (111, 1), 0); del buf2043  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_168.run(buf2021, convolution_34, unsqueeze_2978, buf2156, 11988, 128, grid=grid(11988), stream=stream0)
        buf2157 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2158 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_143.run(buf2156, squeeze_58, buf2157, buf2158, 108, 111, grid=grid(108), stream=stream0)
        buf2159 = reinterpret_tensor(buf2048, (8, 108, 42, 42), (190512, 1, 4536, 108), 0); del buf2048  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_169.run(buf2021, convolution_34, unsqueeze_2978, buf2157, squeeze_58, buf2155, primals_91, buf2159, 14112, 108, grid=grid(14112, 108), stream=stream0)
        del buf2021
        del convolution_34
        del primals_91
        del squeeze_58
        del unsqueeze_2978
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2160 = aten.convolution_backward(buf2159, convolution_33, primals_90, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2159
        del convolution_33
        del primals_90
        buf2161 = buf2160[0]
        buf2162 = buf2160[1]
        del buf2160
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2163 = aten.convolution_backward(buf2161, relu_17, primals_89, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 108, [True, True, False])
        del primals_89
        buf2164 = buf2163[0]
        buf2165 = buf2163[1]
        del buf2163
        buf2166 = buf2156; del buf2156  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_148.run(relu_17, buf2164, buf2166, 11988, 128, grid=grid(11988), stream=stream0)
        buf2167 = buf2157; del buf2157  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_149.run(buf2166, buf2167, 108, 111, grid=grid(108), stream=stream0)
        buf2168 = reinterpret_tensor(buf2166, (108, 111), (1, 108), 0); del buf2166  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_150.run(relu_17, buf2164, convolution_32, unsqueeze_2990, buf2168, 11988, 128, grid=grid(11988), stream=stream0)
        buf2169 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2170 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_151.run(buf2168, squeeze_55, buf2169, buf2170, 108, 111, grid=grid(108), stream=stream0)
        del buf2168
        buf2171 = reinterpret_tensor(buf2161, (8, 108, 42, 42), (190512, 1, 4536, 108), 0); del buf2161  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_152.run(relu_17, buf2164, convolution_32, unsqueeze_2990, buf2169, squeeze_55, buf2167, primals_87, buf2171, 14112, 108, grid=grid(14112, 108), stream=stream0)
        del buf2164
        del convolution_32
        del primals_87
        del relu_17
        del squeeze_55
        del unsqueeze_2990
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf2172 = aten.convolution_backward(buf2171, convolution_31, primals_86, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2171
        del convolution_31
        del primals_86
        buf2173 = buf2172[0]
        buf2174 = buf2172[1]
        del buf2172
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2175 = aten.convolution_backward(buf2173, constant_pad_nd_10, primals_9, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 108, [True, True, False])
        del buf2173
        del constant_pad_nd_10
        del primals_9
        buf2176 = buf2175[0]
        buf2177 = buf2175[1]
        del buf2175
        buf2178 = reinterpret_tensor(buf2185, (8, 108, 83, 83), (744012, 6889, 83, 1), 0); del buf2185  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.threshold_backward]
        triton_poi_fused_add_constant_pad_nd_threshold_backward_170.run(le_173, buf2051, buf2153, buf2176, buf2178, 864, 6889, grid=grid(864, 6889), stream=stream0)
        del buf2051
        del buf2153
        del buf2176
        del le_173
        buf2188 = buf2186[1]
        del buf2186
        buf2189 = empty_strided((108, 7), (1, 108), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_171.run(buf2178, buf2189, 756, 7874, grid=grid(756), stream=stream0)
        buf2190 = buf2169; del buf2169  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_172.run(buf2189, buf2190, 108, 7, grid=grid(108), stream=stream0)
        del buf2189
        buf2191 = reinterpret_tensor(buf2181, (108, 431), (431, 1), 0); del buf2181  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_173.run(buf2178, cat_1, unsqueeze_3014, buf2191, 46548, 128, grid=grid(46548), stream=stream0)
        buf2192 = empty((108, ), device='cuda', dtype=torch.float32)
        buf2194 = empty((108, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_174.run(buf2191, squeeze_49, buf2192, buf2194, 108, 431, grid=grid(108), stream=stream0)
        del buf2191
        buf2193 = buf2178; del buf2178  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_poi_fused_native_batch_norm_backward_175.run(buf2193, cat_1, unsqueeze_3014, buf2192, squeeze_49, buf2190, primals_81, 864, 6889, grid=grid(864, 6889), stream=stream0)
        del buf2192
        del cat_1
        del primals_81
        del squeeze_49
        del unsqueeze_3014
        buf2195 = empty_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        triton_poi_fused_convolution_backward_176.run(buf2193, buf2195, 432, 6889, grid=grid(432, 6889), stream=stream0)
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2196 = aten.convolution_backward(buf2195, avg_pool2d_1, primals_80, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del avg_pool2d_1
        del primals_80
        buf2197 = buf2196[0]
        buf2198 = buf2196[1]
        del buf2196
        buf2199 = buf2195; del buf2195  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        triton_poi_fused_convolution_backward_177.run(buf2193, buf2199, 432, 6889, grid=grid(432, 6889), stream=stream0)
        del buf2193
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2200 = aten.convolution_backward(buf2199, avg_pool2d, primals_79, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del avg_pool2d
        del primals_79
        buf2201 = buf2200[0]
        buf2202 = buf2200[1]
        del buf2200
        buf2205 = empty_strided((54, 7), (1, 54), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_178.run(buf2204, buf2205, 378, 7874, grid=grid(378), stream=stream0)
        buf2206 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_179.run(buf2205, buf2206, 54, 7, grid=grid(54), stream=stream0)
        buf2207 = empty((54, 431), device='cuda', dtype=torch.float32)
        buf2214 = empty((54, 431), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_180.run(buf2204, convolution_27, unsqueeze_3026, convolution_26, unsqueeze_3038, buf2207, buf2214, 23274, 128, grid=grid(23274), stream=stream0)
        buf2208 = empty((54, ), device='cuda', dtype=torch.float32)
        buf2209 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_181.run(buf2207, squeeze_46, buf2208, buf2209, 54, 431, grid=grid(54), stream=stream0)
        buf2215 = empty((54, ), device='cuda', dtype=torch.float32)
        buf2216 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_181.run(buf2214, squeeze_43, buf2215, buf2216, 54, 431, grid=grid(54), stream=stream0)
        buf2210 = buf2199; del buf2199  # reuse
        buf2217 = empty_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_182.run(buf2204, convolution_27, unsqueeze_3026, buf2208, squeeze_46, buf2206, primals_77, convolution_26, unsqueeze_3038, buf2215, squeeze_43, primals_75, buf2210, buf2217, 55112, 54, grid=grid(55112, 54), stream=stream0)
        del convolution_26
        del convolution_27
        del primals_75
        del primals_77
        del squeeze_43
        del squeeze_46
        del unsqueeze_3026
        del unsqueeze_3038
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2218 = aten.convolution_backward(buf2217, convolution_25, primals_74, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2217
        del convolution_25
        del primals_74
        buf2219 = buf2218[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2221 = aten.convolution_backward(buf2219, relu_12, primals_73, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 54, [True, True, False])
        del primals_73
        buf2222 = buf2221[0]
        buf2224 = buf2214; del buf2214  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_183.run(relu_12, buf2222, buf2224, 23274, 128, grid=grid(23274), stream=stream0)
        buf2225 = buf2215; del buf2215  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_184.run(buf2224, buf2225, 54, 431, grid=grid(54), stream=stream0)
        buf2226 = reinterpret_tensor(buf2224, (54, 431), (1, 54), 0); del buf2224  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_185.run(relu_12, buf2222, convolution_24, unsqueeze_3050, buf2226, 23274, 128, grid=grid(23274), stream=stream0)
        buf2227 = buf2208; del buf2208  # reuse
        buf2228 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_186.run(buf2226, squeeze_40, buf2227, buf2228, 54, 431, grid=grid(54), stream=stream0)
        buf2229 = reinterpret_tensor(buf2219, (8, 54, 83, 83), (372006, 1, 4482, 54), 0); del buf2219  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_187.run(relu_12, buf2222, convolution_24, unsqueeze_3050, buf2227, squeeze_40, buf2225, primals_71, buf2229, 55112, 54, grid=grid(55112, 54), stream=stream0)
        del convolution_24
        del primals_71
        del relu_12
        del squeeze_40
        del unsqueeze_3050
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf2230 = aten.convolution_backward(buf2229, convolution_23, primals_70, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_23
        del primals_70
        buf2231 = buf2230[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2233 = aten.convolution_backward(buf2231, constant_pad_nd_7, primals_7, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 96, [True, True, False])
        del buf2231
        del constant_pad_nd_7
        del primals_7
        buf2234 = buf2233[0]
        buf2335 = buf2205; del buf2205  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_188.run(buf2204, buf2335, 378, 7874, grid=grid(378), stream=stream0)
        buf2336 = buf2227; del buf2227  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_179.run(buf2335, buf2336, 54, 7, grid=grid(54), stream=stream0)
        buf2337 = reinterpret_tensor(buf2226, (54, 431), (431, 1), 0); del buf2226  # reuse
        buf2346 = buf2207; del buf2207  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_189.run(buf2204, convolution_6, unsqueeze_3158, convolution_5, unsqueeze_3170, buf2337, buf2346, 23274, 128, grid=grid(23274), stream=stream0)
        buf2338 = empty((54, ), device='cuda', dtype=torch.float32)
        buf2339 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_181.run(buf2337, squeeze_13, buf2338, buf2339, 54, 431, grid=grid(54), stream=stream0)
        buf2347 = empty((54, ), device='cuda', dtype=torch.float32)
        buf2348 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_181.run(buf2346, squeeze_10, buf2347, buf2348, 54, 431, grid=grid(54), stream=stream0)
        buf2340 = buf2229; del buf2229  # reuse
        buf2349 = reinterpret_tensor(buf2222, (8, 54, 83, 83), (372006, 1, 4482, 54), 0); del buf2222  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_190.run(buf2204, convolution_6, unsqueeze_3158, buf2338, squeeze_13, buf2336, primals_39, convolution_5, unsqueeze_3170, buf2347, squeeze_10, primals_36, buf2340, buf2349, 55112, 54, grid=grid(55112, 54), stream=stream0)
        del convolution_5
        del convolution_6
        del primals_36
        del primals_39
        del squeeze_10
        del squeeze_13
        del unsqueeze_3158
        del unsqueeze_3170
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2341 = aten.convolution_backward(buf2340, getitem_8, primals_38, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2340
        del getitem_8
        del primals_38
        buf2342 = buf2341[0]
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf2344 = aten.max_pool2d_with_indices_backward(buf2342, constant_pad_nd_1, [3, 3], [2, 2], [0, 0], [1, 1], False, getitem_9)
        del buf2342
        del constant_pad_nd_1
        del getitem_9
        buf2345 = buf2344
        del buf2344
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2350 = aten.convolution_backward(buf2349, convolution_4, primals_35, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2349
        del convolution_4
        del primals_35
        buf2351 = buf2350[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2353 = aten.convolution_backward(buf2351, relu_2, primals_34, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 54, [True, True, False])
        del primals_34
        buf2354 = buf2353[0]
        buf2356 = buf2346; del buf2346  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_183.run(relu_2, buf2354, buf2356, 23274, 128, grid=grid(23274), stream=stream0)
        buf2357 = buf2347; del buf2347  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_184.run(buf2356, buf2357, 54, 431, grid=grid(54), stream=stream0)
        buf2358 = reinterpret_tensor(buf2356, (54, 431), (1, 54), 0); del buf2356  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_185.run(relu_2, buf2354, convolution_3, unsqueeze_3182, buf2358, 23274, 128, grid=grid(23274), stream=stream0)
        buf2359 = buf2338; del buf2338  # reuse
        buf2360 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_186.run(buf2358, squeeze_7, buf2359, buf2360, 54, 431, grid=grid(54), stream=stream0)
        buf2361 = reinterpret_tensor(buf2351, (8, 54, 83, 83), (372006, 1, 4482, 54), 0); del buf2351  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_187.run(relu_2, buf2354, convolution_3, unsqueeze_3182, buf2359, squeeze_7, buf2357, primals_32, buf2361, 55112, 54, grid=grid(55112, 54), stream=stream0)
        del buf2354
        del convolution_3
        del primals_32
        del relu_2
        del squeeze_7
        del unsqueeze_3182
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf2362 = aten.convolution_backward(buf2361, convolution_2, primals_31, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2361
        del convolution_2
        del primals_31
        buf2363 = buf2362[0]
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2365 = aten.convolution_backward(buf2363, constant_pad_nd, primals_3, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 96, [True, True, False])
        del buf2363
        del constant_pad_nd
        del primals_3
        buf2366 = buf2365[0]
        buf2203 = empty((8, 96, 165, 165), device='cuda', dtype=torch.float32)
        buf2368 = buf2203; del buf2203  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.avg_pool2d_backward, aten.constant_pad_nd, aten.threshold_backward]
        triton_poi_fused_add_avg_pool2d_backward_constant_pad_nd_threshold_backward_191.run(buf2368, buf2197, buf2201, relu, buf2234, buf2345, buf2366, 768, 27225, grid=grid(768, 27225), stream=stream0)
        del buf2197
        del buf2201
        del buf2234
        del buf2345
        del buf2366
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2211 = aten.convolution_backward(buf2210, constant_pad_nd_8, primals_8, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del constant_pad_nd_8
        del primals_8
        buf2212 = buf2211[0]
        buf2213 = buf2211[1]
        del buf2211
        buf2220 = buf2218[1]
        del buf2218
        buf2223 = buf2221[1]
        del buf2221
        buf2232 = buf2230[1]
        del buf2230
        buf2235 = buf2233[1]
        del buf2233
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf2236 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf2204, (8, 54, 83, 83), (1860030, 6889, 83, 1), 1116018), constant_pad_nd_3, [3, 3], [2, 2], [0, 0], [1, 1], False, getitem_17)
        buf2237 = buf2236
        del buf2236
        buf2238 = buf2335; del buf2335  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_192.run(buf2204, buf2238, 378, 7874, grid=grid(378), stream=stream0)
        buf2239 = buf2359; del buf2359  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_179.run(buf2238, buf2239, 54, 7, grid=grid(54), stream=stream0)
        buf2240 = reinterpret_tensor(buf2358, (54, 431), (431, 1), 0); del buf2358  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_193.run(buf2204, convolution_22, unsqueeze_3062, buf2240, 23274, 128, grid=grid(23274), stream=stream0)
        buf2241 = empty((54, ), device='cuda', dtype=torch.float32)
        buf2242 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_181.run(buf2240, squeeze_37, buf2241, buf2242, 54, 431, grid=grid(54), stream=stream0)
        buf2243 = buf2210; del buf2210  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_194.run(buf2204, convolution_22, unsqueeze_3062, buf2241, squeeze_37, buf2239, primals_68, buf2243, 55112, 54, grid=grid(55112, 54), stream=stream0)
        del convolution_22
        del primals_68
        del squeeze_37
        del unsqueeze_3062
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2244 = aten.convolution_backward(buf2243, convolution_21, primals_67, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2243
        del convolution_21
        del primals_67
        buf2245 = buf2244[0]
        buf2246 = buf2244[1]
        del buf2244
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2247 = aten.convolution_backward(buf2245, relu_10, primals_66, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 54, [True, True, False])
        del primals_66
        buf2248 = buf2247[0]
        buf2249 = buf2247[1]
        del buf2247
        buf2250 = buf2240; del buf2240  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_183.run(relu_10, buf2248, buf2250, 23274, 128, grid=grid(23274), stream=stream0)
        buf2251 = buf2241; del buf2241  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_184.run(buf2250, buf2251, 54, 431, grid=grid(54), stream=stream0)
        buf2252 = reinterpret_tensor(buf2250, (54, 431), (1, 54), 0); del buf2250  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_185.run(relu_10, buf2248, convolution_20, unsqueeze_3074, buf2252, 23274, 128, grid=grid(23274), stream=stream0)
        buf2253 = empty((54, ), device='cuda', dtype=torch.float32)
        buf2254 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_186.run(buf2252, squeeze_34, buf2253, buf2254, 54, 431, grid=grid(54), stream=stream0)
        buf2255 = reinterpret_tensor(buf2245, (8, 54, 83, 83), (372006, 1, 4482, 54), 0); del buf2245  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_187.run(relu_10, buf2248, convolution_20, unsqueeze_3074, buf2253, squeeze_34, buf2251, primals_64, buf2255, 55112, 54, grid=grid(55112, 54), stream=stream0)
        del buf2248
        del convolution_20
        del primals_64
        del relu_10
        del squeeze_34
        del unsqueeze_3074
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf2256 = aten.convolution_backward(buf2255, convolution_19, primals_63, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del convolution_19
        del primals_63
        buf2257 = buf2256[0]
        buf2258 = buf2256[1]
        del buf2256
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2259 = aten.convolution_backward(buf2257, relu_9, primals_62, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 54, [True, True, False])
        del primals_62
        buf2260 = buf2259[0]
        buf2261 = buf2259[1]
        del buf2259
        buf2262 = buf2238; del buf2238  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_195.run(buf2204, relu_9, buf2260, buf2262, 378, 7874, grid=grid(378), stream=stream0)
        buf2263 = buf2253; del buf2253  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_179.run(buf2262, buf2263, 54, 7, grid=grid(54), stream=stream0)
        buf2264 = reinterpret_tensor(buf2252, (54, 431), (431, 1), 0); del buf2252  # reuse
        buf2286 = buf2337; del buf2337  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_196.run(buf2204, relu_9, buf2260, convolution_18, unsqueeze_3086, convolution_14, unsqueeze_3110, buf2264, buf2286, 23274, 128, grid=grid(23274), stream=stream0)
        buf2265 = empty((54, ), device='cuda', dtype=torch.float32)
        buf2267 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_181.run(buf2264, squeeze_31, buf2265, buf2267, 54, 431, grid=grid(54), stream=stream0)
        del buf2264
        buf2287 = empty((54, ), device='cuda', dtype=torch.float32)
        buf2289 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_181.run(buf2286, squeeze_25, buf2287, buf2289, 54, 431, grid=grid(54), stream=stream0)
        buf2266 = reinterpret_tensor(buf2257, (8, 54, 83, 83), (372006, 1, 4482, 54), 0); del buf2257  # reuse
        buf2288 = buf2255; del buf2255  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_197.run(buf2204, relu_9, buf2260, convolution_18, unsqueeze_3086, buf2265, squeeze_31, buf2263, primals_60, convolution_14, unsqueeze_3110, buf2287, squeeze_25, primals_53, buf2266, buf2288, 55112, 54, grid=grid(55112, 54), stream=stream0)
        del buf2260
        del convolution_14
        del convolution_18
        del primals_53
        del primals_60
        del relu_9
        del squeeze_25
        del squeeze_31
        del unsqueeze_3086
        del unsqueeze_3110
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2268 = aten.convolution_backward(buf2266, convolution_17, primals_59, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2266
        del convolution_17
        del primals_59
        buf2269 = buf2268[0]
        buf2270 = buf2268[1]
        del buf2268
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2271 = aten.convolution_backward(buf2269, relu_8, primals_58, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 54, [True, True, False])
        del primals_58
        buf2272 = buf2271[0]
        buf2273 = buf2271[1]
        del buf2271
        buf2274 = buf2286; del buf2286  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_183.run(relu_8, buf2272, buf2274, 23274, 128, grid=grid(23274), stream=stream0)
        buf2275 = buf2287; del buf2287  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_184.run(buf2274, buf2275, 54, 431, grid=grid(54), stream=stream0)
        buf2276 = reinterpret_tensor(buf2274, (54, 431), (1, 54), 0); del buf2274  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_185.run(relu_8, buf2272, convolution_16, unsqueeze_3098, buf2276, 23274, 128, grid=grid(23274), stream=stream0)
        buf2277 = buf2265; del buf2265  # reuse
        buf2278 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_186.run(buf2276, squeeze_28, buf2277, buf2278, 54, 431, grid=grid(54), stream=stream0)
        buf2279 = reinterpret_tensor(buf2269, (8, 54, 83, 83), (372006, 1, 4482, 54), 0); del buf2269  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_187.run(relu_8, buf2272, convolution_16, unsqueeze_3098, buf2277, squeeze_28, buf2275, primals_56, buf2279, 55112, 54, grid=grid(55112, 54), stream=stream0)
        del buf2272
        del convolution_16
        del primals_56
        del relu_8
        del squeeze_28
        del unsqueeze_3098
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf2280 = aten.convolution_backward(buf2279, convolution_15, primals_55, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2279
        del convolution_15
        del primals_55
        buf2281 = buf2280[0]
        buf2282 = buf2280[1]
        del buf2280
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2283 = aten.convolution_backward(buf2281, constant_pad_nd_5, primals_6, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 54, [True, True, False])
        del buf2281
        del constant_pad_nd_5
        del primals_6
        buf2284 = buf2283[0]
        buf2285 = buf2283[1]
        del buf2283
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2290 = aten.convolution_backward(buf2288, convolution_13, primals_52, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2288
        del convolution_13
        del primals_52
        buf2291 = buf2290[0]
        buf2292 = buf2290[1]
        del buf2290
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2293 = aten.convolution_backward(buf2291, relu_6, primals_51, [0], [1, 1], [2, 2], [1, 1], False, [0, 0], 54, [True, True, False])
        del primals_51
        buf2294 = buf2293[0]
        buf2295 = buf2293[1]
        del buf2293
        buf2296 = reinterpret_tensor(buf2276, (54, 431), (431, 1), 0); del buf2276  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_183.run(relu_6, buf2294, buf2296, 23274, 128, grid=grid(23274), stream=stream0)
        buf2297 = buf2277; del buf2277  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_184.run(buf2296, buf2297, 54, 431, grid=grid(54), stream=stream0)
        buf2298 = reinterpret_tensor(buf2296, (54, 431), (1, 54), 0); del buf2296  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_185.run(relu_6, buf2294, convolution_12, unsqueeze_3122, buf2298, 23274, 128, grid=grid(23274), stream=stream0)
        buf2299 = empty((54, ), device='cuda', dtype=torch.float32)
        buf2300 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_186.run(buf2298, squeeze_22, buf2299, buf2300, 54, 431, grid=grid(54), stream=stream0)
        buf2301 = reinterpret_tensor(buf2291, (8, 54, 83, 83), (372006, 1, 4482, 54), 0); del buf2291  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_187.run(relu_6, buf2294, convolution_12, unsqueeze_3122, buf2299, squeeze_22, buf2297, primals_49, buf2301, 55112, 54, grid=grid(55112, 54), stream=stream0)
        del buf2294
        del convolution_12
        del primals_49
        del relu_6
        del squeeze_22
        del unsqueeze_3122
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf2302 = aten.convolution_backward(buf2301, convolution_11, primals_48, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2301
        del convolution_11
        del primals_48
        buf2303 = buf2302[0]
        buf2304 = buf2302[1]
        del buf2302
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2305 = aten.convolution_backward(buf2303, constant_pad_nd_4, primals_5, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 54, [True, True, False])
        del constant_pad_nd_4
        del primals_5
        buf2306 = buf2305[0]
        buf2307 = buf2305[1]
        del buf2305
        buf2308 = buf2212; del buf2212  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.threshold_backward]
        triton_poi_fused_add_constant_pad_nd_threshold_backward_198.run(buf2308, le_186, buf2237, buf2284, buf2306, 432, 27225, grid=grid(432, 27225), stream=stream0)
        del buf2237
        del buf2284
        del buf2306
        # Source Nodes: [], Original ATen: [aten.max_pool2d_with_indices_backward]
        buf2309 = aten.max_pool2d_with_indices_backward(reinterpret_tensor(buf2204, (8, 54, 83, 83), (1860030, 6889, 83, 1), 372006), constant_pad_nd_3, [3, 3], [2, 2], [0, 0], [1, 1], False, getitem_17)
        del constant_pad_nd_3
        del getitem_17
        buf2310 = buf2309
        del buf2309
        buf2311 = buf2262; del buf2262  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_199.run(buf2204, buf2311, 378, 7874, grid=grid(378), stream=stream0)
        buf2312 = buf2299; del buf2299  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_179.run(buf2311, buf2312, 54, 7, grid=grid(54), stream=stream0)
        del buf2311
        buf2313 = reinterpret_tensor(buf2298, (54, 431), (431, 1), 0); del buf2298  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_200.run(buf2204, convolution_10, unsqueeze_3134, buf2313, 23274, 128, grid=grid(23274), stream=stream0)
        buf2314 = empty((54, ), device='cuda', dtype=torch.float32)
        buf2315 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_native_batch_norm_backward_181.run(buf2313, squeeze_19, buf2314, buf2315, 54, 431, grid=grid(54), stream=stream0)
        buf2316 = reinterpret_tensor(buf2303, (8, 54, 83, 83), (372006, 1, 4482, 54), 0); del buf2303  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_201.run(buf2204, convolution_10, unsqueeze_3134, buf2314, squeeze_19, buf2312, primals_46, buf2316, 55112, 54, grid=grid(55112, 54), stream=stream0)
        del buf2204
        del convolution_10
        del primals_46
        del squeeze_19
        del unsqueeze_3134
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2317 = aten.convolution_backward(buf2316, convolution_9, primals_45, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2316
        del convolution_9
        del primals_45
        buf2318 = buf2317[0]
        buf2319 = buf2317[1]
        del buf2317
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2320 = aten.convolution_backward(buf2318, relu_4, primals_44, [0], [1, 1], [3, 3], [1, 1], False, [0, 0], 54, [True, True, False])
        del primals_44
        buf2321 = buf2320[0]
        buf2322 = buf2320[1]
        del buf2320
        buf2323 = buf2313; del buf2313  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_183.run(relu_4, buf2321, buf2323, 23274, 128, grid=grid(23274), stream=stream0)
        buf2324 = buf2314; del buf2314  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_184.run(buf2323, buf2324, 54, 431, grid=grid(54), stream=stream0)
        buf2325 = reinterpret_tensor(buf2323, (54, 431), (1, 54), 0); del buf2323  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_185.run(relu_4, buf2321, convolution_8, unsqueeze_3146, buf2325, 23274, 128, grid=grid(23274), stream=stream0)
        buf2326 = empty((54, ), device='cuda', dtype=torch.float32)
        buf2327 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_186.run(buf2325, squeeze_16, buf2326, buf2327, 54, 431, grid=grid(54), stream=stream0)
        del buf2325
        buf2328 = reinterpret_tensor(buf2318, (8, 54, 83, 83), (372006, 1, 4482, 54), 0); del buf2318  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_187.run(relu_4, buf2321, convolution_8, unsqueeze_3146, buf2326, squeeze_16, buf2324, primals_42, buf2328, 55112, 54, grid=grid(55112, 54), stream=stream0)
        del buf2321
        del convolution_8
        del primals_42
        del relu_4
        del squeeze_16
        del unsqueeze_3146
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf2329 = aten.convolution_backward(buf2328, convolution_7, primals_41, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2328
        del convolution_7
        del primals_41
        buf2330 = buf2329[0]
        buf2331 = buf2329[1]
        del buf2329
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2332 = aten.convolution_backward(buf2330, constant_pad_nd_2, primals_4, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 54, [True, True, False])
        del buf2330
        del constant_pad_nd_2
        del primals_4
        buf2333 = buf2332[0]
        buf2334 = buf2332[1]
        del buf2332
        buf2343 = buf2341[1]
        del buf2341
        buf2352 = buf2350[1]
        del buf2350
        buf2355 = buf2353[1]
        del buf2353
        buf2364 = buf2362[1]
        del buf2362
        buf2367 = buf2365[1]
        del buf2365
        buf2369 = empty((54, 990), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_202.run(buf2308, buf2310, le_186, buf2333, buf2369, 53460, 220, grid=grid(53460), stream=stream0)
        buf2370 = buf2326; del buf2326  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_203.run(buf2369, buf2370, 54, 990, grid=grid(54), stream=stream0)
        buf2371 = reinterpret_tensor(buf2369, (54, 990), (1, 54), 0); del buf2369  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_204.run(buf2308, buf2310, le_186, buf2333, convolution_1, unsqueeze_3194, buf2371, 53460, 220, grid=grid(53460), stream=stream0)
        buf2372 = empty((54, ), device='cuda', dtype=torch.float32)
        buf2374 = empty((54, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_205.run(buf2371, squeeze_4, buf2372, buf2374, 54, 990, grid=grid(54), stream=stream0)
        del buf2371
        buf2373 = empty_strided((8, 54, 165, 165), (1470150, 1, 8910, 54), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.constant_pad_nd, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_constant_pad_nd_native_batch_norm_backward_threshold_backward_206.run(buf2308, buf2310, le_186, buf2333, convolution_1, unsqueeze_3194, buf2372, squeeze_4, buf2373, 217800, 54, grid=grid(217800, 54), stream=stream0)
        del buf2308
        del buf2310
        del buf2333
        del buf2372
        del convolution_1
        del le_186
        del unsqueeze_3194
        buf2375 = buf2373; del buf2373  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_207.run(buf2375, buf2370, squeeze_4, primals_29, 432, 27225, grid=grid(432, 27225), stream=stream0)
        del primals_29
        del squeeze_4
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf2376 = aten.convolution_backward(buf2375, relu, primals_28, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf2375
        del primals_28
        buf2377 = buf2376[0]
        buf2378 = buf2376[1]
        del buf2376
        buf2379 = empty_strided((96, 6), (1, 96), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_208.run(buf2368, relu, buf2377, buf2379, 576, 36300, grid=grid(576), stream=stream0)
        buf2380 = empty((96, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_209.run(buf2379, buf2380, 96, 6, grid=grid(96), stream=stream0)
        del buf2379
        buf2381 = empty((96, 990), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_210.run(buf2368, relu, buf2377, convolution, unsqueeze_3206, buf2381, 95040, 220, grid=grid(95040), stream=stream0)
        buf2382 = empty((96, ), device='cuda', dtype=torch.float32)
        buf2384 = empty((96, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_211.run(buf2381, squeeze_1, buf2382, buf2384, 96, 990, grid=grid(96), stream=stream0)
        del buf2381
        buf2383 = empty_strided((8, 96, 165, 165), (2613600, 1, 15840, 96), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_212.run(buf2368, relu, buf2377, convolution, unsqueeze_3206, buf2382, squeeze_1, buf2380, primals_1, buf2383, 217800, 96, grid=grid(217800, 96), stream=stream0)
        del buf2368
        del buf2377
        del buf2382
        del convolution
        del primals_1
        del relu
        del squeeze_1
        del unsqueeze_3206
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf2385 = aten.convolution_backward(buf2383, primals_1381, primals_27, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [False, True, False])
        del buf2383
        del primals_1381
        del primals_27
        buf2386 = buf2385[1]
        return (buf2384, buf2380, buf2367, buf2334, buf2307, buf2285, buf2235, buf2213, buf2177, buf2151, buf2124, buf2102, buf2052, buf2030, buf1319, buf1294, buf1268, buf1246, buf1198, buf1176, buf654, buf629, buf603, buf581, buf533, buf511, buf2386, buf2378, buf2374, buf2370, buf2364, buf2360, buf2357, buf2355, buf2352, buf2348, buf2336, buf2343, buf2339, buf2336, buf2331, buf2327, buf2324, buf2322, buf2319, buf2315, buf2312, buf2304, buf2300, buf2297, buf2295, buf2292, buf2289, buf2263, buf2282, buf2278, buf2275, buf2273, buf2270, buf2267, buf2263, buf2261, buf2258, buf2254, buf2251, buf2249, buf2246, buf2242, buf2239, buf2232, buf2228, buf2225, buf2223, buf2220, buf2216, buf2206, buf2209, buf2206, buf2202, buf2198, buf2194, buf2190, buf2188, buf2184, buf2180, buf2174, buf2170, buf2167, buf2165, buf2162, buf2158, buf2155, buf2148, buf2144, buf2141, buf2139, buf2136, buf2132, buf2129, buf2121, buf2117, buf2114, buf2112, buf2109, buf2106, buf2080, buf2099, buf2095, buf2092, buf2090, buf2087, buf2084, buf2080, buf2078, buf2075, buf2071, buf2068, buf2066, buf2063, buf2059, buf2056, buf2049, buf2045, buf2042, buf2040, buf2037, buf2033, buf2023, buf2026, buf2023, buf2019, buf2015, buf2011, buf2007, buf2005, buf2001, buf1998, buf1996, buf1993, buf1989, buf1986, buf1984, buf1981, buf1977, buf1974, buf1969, buf1966, buf1962, buf1959, buf1957, buf1954, buf1950, buf1947, buf1943, buf1940, buf1936, buf1933, buf1931, buf1928, buf1925, buf1899, buf1921, buf1918, buf1914, buf1911, buf1909, buf1906, buf1903, buf1899, buf1897, buf1894, buf1890, buf1887, buf1885, buf1882, buf1878, buf1875, buf1871, buf1868, buf1864, buf1861, buf1859, buf1856, buf1852, buf1849, buf1846, buf1842, buf1838, buf1836, buf1832, buf1829, buf1827, buf1824, buf1820, buf1817, buf1815, buf1812, buf1808, buf1805, buf1800, buf1797, buf1793, buf1790, buf1788, buf1785, buf1781, buf1778, buf1774, buf1771, buf1767, buf1764, buf1762, buf1759, buf1756, buf1730, buf1752, buf1749, buf1745, buf1742, buf1740, buf1737, buf1734, buf1730, buf1728, buf1725, buf1721, buf1718, buf1716, buf1713, buf1709, buf1706, buf1702, buf1699, buf1695, buf1692, buf1690, buf1687, buf1683, buf1680, buf1677, buf1673, buf1669, buf1667, buf1663, buf1660, buf1658, buf1655, buf1651, buf1648, buf1646, buf1643, buf1639, buf1636, buf1631, buf1628, buf1624, buf1621, buf1619, buf1616, buf1612, buf1609, buf1605, buf1602, buf1598, buf1595, buf1593, buf1590, buf1587, buf1561, buf1583, buf1580, buf1576, buf1573, buf1571, buf1568, buf1565, buf1561, buf1559, buf1556, buf1552, buf1549, buf1547, buf1544, buf1540, buf1537, buf1533, buf1530, buf1526, buf1523, buf1521, buf1518, buf1514, buf1511, buf1508, buf1504, buf1500, buf1498, buf1494, buf1491, buf1489, buf1486, buf1482, buf1479, buf1477, buf1474, buf1470, buf1467, buf1462, buf1459, buf1455, buf1452, buf1450, buf1447, buf1443, buf1440, buf1436, buf1433, buf1429, buf1426, buf1424, buf1421, buf1418, buf1392, buf1414, buf1411, buf1407, buf1404, buf1402, buf1399, buf1396, buf1392, buf1390, buf1387, buf1383, buf1380, buf1378, buf1375, buf1371, buf1368, buf1364, buf1361, buf1357, buf1354, buf1352, buf1349, buf1345, buf1342, buf1339, buf1335, buf1331, buf1329, buf1325, buf1321, buf1316, buf1312, buf1309, buf1307, buf1304, buf1300, buf1297, buf1291, buf1287, buf1284, buf1282, buf1279, buf1275, buf1272, buf1265, buf1261, buf1258, buf1256, buf1253, buf1250, buf1224, buf1243, buf1239, buf1236, buf1234, buf1231, buf1228, buf1224, buf1223, buf1220, buf1216, buf1213, buf1211, buf1208, buf1204, buf1201, buf1195, buf1191, buf1188, buf1186, buf1183, buf1179, buf1169, buf1172, buf1169, buf1166, buf1162, buf1158, buf1154, buf1152, buf1148, buf1145, buf1144, buf1141, buf1137, buf1134, buf1132, buf1129, buf1125, buf1122, buf1118, buf1115, buf1111, buf1108, buf1106, buf1103, buf1099, buf1096, buf1093, buf1090, buf1086, buf1083, buf1081, buf1078, buf1075, buf1049, buf1071, buf1068, buf1064, buf1061, buf1059, buf1056, buf1053, buf1049, buf1048, buf1045, buf1041, buf1038, buf1036, buf1033, buf1029, buf1026, buf1023, buf1020, buf1016, buf1013, buf1011, buf1008, buf1004, buf1001, buf999, buf995, buf991, buf989, buf985, buf982, buf981, buf978, buf974, buf971, buf969, buf966, buf962, buf959, buf955, buf952, buf948, buf945, buf943, buf940, buf936, buf933, buf930, buf927, buf923, buf920, buf918, buf915, buf912, buf886, buf908, buf905, buf901, buf898, buf896, buf893, buf890, buf886, buf885, buf882, buf878, buf875, buf873, buf870, buf866, buf863, buf860, buf857, buf853, buf850, buf848, buf845, buf841, buf838, buf836, buf832, buf828, buf826, buf822, buf819, buf818, buf815, buf811, buf808, buf806, buf803, buf799, buf796, buf792, buf789, buf785, buf782, buf780, buf777, buf773, buf770, buf767, buf764, buf760, buf757, buf755, buf752, buf749, buf723, buf745, buf742, buf738, buf735, buf733, buf730, buf727, buf723, buf722, buf719, buf715, buf712, buf710, buf707, buf703, buf700, buf697, buf694, buf690, buf687, buf685, buf682, buf678, buf675, buf673, buf669, buf666, buf665, buf661, buf657, buf651, buf647, buf644, buf642, buf639, buf635, buf632, buf626, buf622, buf619, buf617, buf614, buf610, buf607, buf600, buf596, buf593, buf591, buf588, buf585, buf559, buf578, buf574, buf571, buf569, buf566, buf563, buf559, buf558, buf555, buf551, buf548, buf546, buf543, buf539, buf536, buf530, buf526, buf523, buf521, buf518, buf514, buf504, buf507, buf504, buf501, buf497, buf493, buf489, buf487, buf483, buf480, buf479, buf476, buf472, buf469, buf467, buf464, buf460, buf457, buf453, buf450, buf446, buf443, buf441, buf438, buf434, buf431, buf428, buf425, buf421, buf418, buf416, buf413, buf410, buf384, buf406, buf403, buf399, buf396, buf394, buf391, buf388, buf384, buf383, buf380, buf376, buf373, buf371, buf368, buf364, buf361, buf358, buf355, buf351, buf348, buf346, buf343, buf339, buf336, buf334, buf330, buf326, buf324, buf320, buf317, buf316, buf313, buf309, buf306, buf304, buf301, buf297, buf294, buf290, buf287, buf283, buf280, buf278, buf275, buf271, buf268, buf265, buf262, buf258, buf255, buf253, buf250, buf247, buf221, buf243, buf240, buf236, buf233, buf231, buf228, buf225, buf221, buf220, buf217, buf213, buf210, buf208, buf205, buf201, buf198, buf195, buf192, buf188, buf185, buf183, buf180, buf176, buf173, buf171, buf167, buf163, buf161, buf157, buf154, buf152, buf149, buf145, buf142, buf140, buf137, buf133, buf130, buf125, buf122, buf118, buf115, buf113, buf110, buf106, buf103, buf99, buf96, buf92, buf89, buf87, buf84, buf81, buf55, buf77, buf74, buf70, buf67, buf65, buf62, buf59, buf55, buf53, buf50, buf46, buf43, buf41, buf38, buf34, buf31, buf27, buf24, buf20, buf17, buf15, buf12, buf8, buf5, reinterpret_tensor(buf1, (1000, 4320), (4320, 1), 0), reinterpret_tensor(buf2, (1000, ), (1, ), 0), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((96, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_3 = rand_strided((96, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_4 = rand_strided((54, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_5 = rand_strided((54, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_6 = rand_strided((54, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_7 = rand_strided((96, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_8 = rand_strided((54, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_9 = rand_strided((108, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_10 = rand_strided((108, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_11 = rand_strided((108, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_12 = rand_strided((108, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_13 = rand_strided((108, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_14 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_15 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_16 = rand_strided((432, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_17 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_18 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_19 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_20 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_21 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_22 = rand_strided((864, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_23 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_24 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_25 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_26 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_27 = rand_strided((96, 3, 3, 3), (27, 1, 9, 3), device='cuda:0', dtype=torch.float32)
    primals_28 = rand_strided((54, 96, 1, 1), (96, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_29 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_31 = rand_strided((54, 96, 1, 1), (96, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_32 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_34 = rand_strided((54, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_35 = rand_strided((54, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_36 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_38 = rand_strided((54, 96, 1, 1), (96, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_39 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_41 = rand_strided((54, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_42 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_44 = rand_strided((54, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_45 = rand_strided((54, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_46 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_48 = rand_strided((54, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_49 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_51 = rand_strided((54, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_52 = rand_strided((54, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_53 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_55 = rand_strided((54, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_56 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_58 = rand_strided((54, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_59 = rand_strided((54, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_60 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_62 = rand_strided((54, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_63 = rand_strided((54, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_64 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_66 = rand_strided((54, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_67 = rand_strided((54, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_68 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_70 = rand_strided((54, 96, 1, 1), (96, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_71 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_73 = rand_strided((54, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_74 = rand_strided((54, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_75 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_77 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_79 = rand_strided((54, 96, 1, 1), (96, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_80 = rand_strided((54, 96, 1, 1), (96, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_81 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_83 = rand_strided((108, 270, 1, 1), (270, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_84 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_86 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_87 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_89 = rand_strided((108, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_90 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_91 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_93 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_94 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_96 = rand_strided((108, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_97 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_98 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_100 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_101 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_103 = rand_strided((108, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_104 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_105 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_107 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_108 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_110 = rand_strided((108, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_111 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_112 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_114 = rand_strided((108, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_115 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_116 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_118 = rand_strided((108, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_119 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_120 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_122 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_123 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_125 = rand_strided((108, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_126 = rand_strided((108, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_127 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_129 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_131 = rand_strided((108, 270, 1, 1), (270, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_132 = rand_strided((108, 270, 1, 1), (270, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_133 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_135 = rand_strided((216, 540, 1, 1), (540, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_136 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_138 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_139 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_140 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_142 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_143 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_144 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_146 = rand_strided((216, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_147 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_148 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_150 = rand_strided((216, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_151 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_152 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_154 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_155 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_156 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_158 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_159 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_160 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_162 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_163 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_164 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_166 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_167 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_168 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_170 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_171 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_172 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_174 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_175 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_176 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_178 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_179 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_180 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_182 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_183 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_184 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_186 = rand_strided((216, 540, 1, 1), (540, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_187 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_189 = rand_strided((216, 1080, 1, 1), (1080, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_190 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_192 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_193 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_194 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_196 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_197 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_198 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_200 = rand_strided((216, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_201 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_202 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_204 = rand_strided((216, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_205 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_206 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_208 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_209 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_210 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_212 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_213 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_214 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_216 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_217 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_218 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_220 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_221 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_222 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_224 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_225 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_226 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_228 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_229 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_230 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_232 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_233 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_234 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_236 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_237 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_238 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_240 = rand_strided((216, 1080, 1, 1), (1080, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_241 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_243 = rand_strided((216, 1080, 1, 1), (1080, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_244 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_246 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_247 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_248 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_250 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_251 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_252 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_254 = rand_strided((216, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_255 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_256 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_258 = rand_strided((216, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_259 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_260 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_262 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_263 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_264 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_266 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_267 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_268 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_270 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_271 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_272 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_274 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_275 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_276 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_278 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_279 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_280 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_282 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_283 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_284 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_286 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_287 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_288 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_290 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_291 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_292 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_294 = rand_strided((216, 1080, 1, 1), (1080, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_295 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_297 = rand_strided((216, 1080, 1, 1), (1080, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_298 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_300 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_301 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_302 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_304 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_305 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_306 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_308 = rand_strided((216, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_309 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_310 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_312 = rand_strided((216, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_313 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_314 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_316 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_317 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_318 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_320 = rand_strided((216, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_321 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_322 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_324 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_325 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_326 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_328 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_329 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_330 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_332 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_333 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_334 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_336 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_337 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_338 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_340 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_341 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_342 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_344 = rand_strided((216, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_345 = rand_strided((216, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_346 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_348 = rand_strided((432, 1080, 1, 1), (1080, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_349 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_351 = rand_strided((432, 1080, 1, 1), (1080, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_352 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_354 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_355 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_357 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_358 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_359 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_361 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_362 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_364 = rand_strided((432, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_365 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_366 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_368 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_369 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_371 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_372 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_373 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_375 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_376 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_378 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_379 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_380 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_382 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_383 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_384 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_386 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_387 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_388 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_390 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_391 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_393 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_394 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_395 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_397 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_399 = rand_strided((216, 1080, 1, 1), (1080, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_400 = rand_strided((216, 1080, 1, 1), (1080, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_401 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_403 = rand_strided((432, 2160, 1, 1), (2160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_404 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_406 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_407 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_408 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_410 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_411 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_412 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_414 = rand_strided((432, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_415 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_416 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_418 = rand_strided((432, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_419 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_420 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_422 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_423 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_424 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_426 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_427 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_428 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_430 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_431 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_432 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_434 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_435 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_436 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_438 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_439 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_440 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_442 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_443 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_444 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_446 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_447 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_448 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_450 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_451 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_452 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_454 = rand_strided((432, 2160, 1, 1), (2160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_455 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_457 = rand_strided((432, 2160, 1, 1), (2160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_458 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_460 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_461 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_462 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_464 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_465 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_466 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_468 = rand_strided((432, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_469 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_470 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_472 = rand_strided((432, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_473 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_474 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_476 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_477 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_478 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_480 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_481 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_482 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_484 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_485 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_486 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_488 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_489 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_490 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_492 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_493 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_494 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_496 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_497 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_498 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_500 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_501 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_502 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_504 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_505 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_506 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_508 = rand_strided((432, 2160, 1, 1), (2160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_509 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_511 = rand_strided((432, 2160, 1, 1), (2160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_512 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_514 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_515 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_516 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_518 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_519 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_520 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_522 = rand_strided((432, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_523 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_524 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_526 = rand_strided((432, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_527 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_528 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_530 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_531 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_532 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_534 = rand_strided((432, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_535 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_536 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_538 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_539 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_540 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_542 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_543 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_544 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_546 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_547 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_548 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_550 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_551 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_552 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_554 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_555 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_556 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_558 = rand_strided((432, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_559 = rand_strided((432, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_560 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_562 = rand_strided((864, 2160, 1, 1), (2160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_563 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_565 = rand_strided((864, 2160, 1, 1), (2160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_566 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_568 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_569 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_571 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_572 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_573 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_575 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_576 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_578 = rand_strided((864, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_579 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_580 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_582 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_583 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_585 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_586 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_587 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_589 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_590 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_592 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_593 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_594 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_596 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_597 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_598 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_600 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_601 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_602 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_604 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_605 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_607 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_608 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_609 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_611 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_613 = rand_strided((432, 2160, 1, 1), (2160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_614 = rand_strided((432, 2160, 1, 1), (2160, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_615 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_617 = rand_strided((864, 4320, 1, 1), (4320, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_618 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_620 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_621 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_622 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_624 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_625 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_626 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_628 = rand_strided((864, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_629 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_630 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_632 = rand_strided((864, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_633 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_634 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_636 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_637 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_638 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_640 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_641 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_642 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_644 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_645 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_646 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_648 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_649 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_650 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_652 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_653 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_654 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_656 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_657 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_658 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_660 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_661 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_662 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_664 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_665 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_666 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_668 = rand_strided((864, 4320, 1, 1), (4320, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_669 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_671 = rand_strided((864, 4320, 1, 1), (4320, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_672 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_674 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_675 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_676 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_678 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_679 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_680 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_682 = rand_strided((864, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_683 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_684 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_686 = rand_strided((864, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_687 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_688 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_690 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_691 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_692 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_694 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_695 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_696 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_698 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_699 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_700 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_702 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_703 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_704 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_706 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_707 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_708 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_710 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_711 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_712 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_714 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_715 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_716 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_718 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_719 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_720 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_722 = rand_strided((864, 4320, 1, 1), (4320, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_723 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_725 = rand_strided((864, 4320, 1, 1), (4320, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_726 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_728 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_729 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_730 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_732 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_733 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_734 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_736 = rand_strided((864, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_737 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_738 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_740 = rand_strided((864, 1, 7, 7), (49, 49, 7, 1), device='cuda:0', dtype=torch.float32)
    primals_741 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_742 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_744 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_745 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_746 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_748 = rand_strided((864, 1, 5, 5), (25, 25, 5, 1), device='cuda:0', dtype=torch.float32)
    primals_749 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_750 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_752 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_753 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_754 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_756 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_757 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_758 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_760 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_761 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_762 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_764 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_765 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_766 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_768 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_769 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_770 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_772 = rand_strided((864, 1, 3, 3), (9, 9, 3, 1), device='cuda:0', dtype=torch.float32)
    primals_773 = rand_strided((864, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_774 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_1381 = rand_strided((8, 3, 331, 331), (328683, 1, 993, 3), device='cuda:0', dtype=torch.float32)
    convolution = rand_strided((8, 96, 165, 165), (2613600, 1, 15840, 96), device='cuda:0', dtype=torch.float32)
    squeeze_1 = rand_strided((96, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu = rand_strided((8, 96, 165, 165), (2613600, 1, 15840, 96), device='cuda:0', dtype=torch.float32)
    convolution_1 = rand_strided((8, 54, 165, 165), (1470150, 1, 8910, 54), device='cuda:0', dtype=torch.float32)
    squeeze_4 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd = rand_strided((8, 96, 169, 169), (2741856, 1, 16224, 96), device='cuda:0', dtype=torch.float32)
    convolution_2 = rand_strided((8, 96, 83, 83), (661344, 1, 7968, 96), device='cuda:0', dtype=torch.float32)
    convolution_3 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_7 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_2 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_4 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_5 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_10 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_1 = rand_strided((8, 96, 167, 167), (2677344, 1, 16032, 96), device='cuda:0', dtype=torch.float32)
    getitem_8 = rand_strided((8, 96, 83, 83), (661344, 1, 7968, 96), device='cuda:0', dtype=torch.float32)
    getitem_9 = rand_strided((8, 96, 83, 83), (661344, 1, 7968, 96), device='cuda:0', dtype=torch.int64)
    convolution_6 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_13 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_2 = rand_strided((8, 54, 171, 171), (1579014, 1, 9234, 54), device='cuda:0', dtype=torch.float32)
    convolution_7 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_8 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_16 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_4 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_9 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_10 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_19 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_3 = rand_strided((8, 54, 167, 167), (1506006, 1, 9018, 54), device='cuda:0', dtype=torch.float32)
    getitem_17 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.int64)
    constant_pad_nd_4 = rand_strided((8, 54, 169, 169), (1542294, 1, 9126, 54), device='cuda:0', dtype=torch.float32)
    convolution_11 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_12 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_22 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_6 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_13 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_14 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_25 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_5 = rand_strided((8, 54, 167, 167), (1506006, 1, 9018, 54), device='cuda:0', dtype=torch.float32)
    convolution_15 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_16 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_28 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_8 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_17 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_18 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_31 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_9 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_19 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_20 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_34 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_10 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_21 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_22 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_37 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_7 = rand_strided((8, 96, 167, 167), (2677344, 1, 16032, 96), device='cuda:0', dtype=torch.float32)
    convolution_23 = rand_strided((8, 96, 83, 83), (661344, 1, 7968, 96), device='cuda:0', dtype=torch.float32)
    convolution_24 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_40 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_12 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_25 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    convolution_26 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_43 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_8 = rand_strided((8, 54, 165, 165), (1470150, 1, 8910, 54), device='cuda:0', dtype=torch.float32)
    convolution_27 = rand_strided((8, 54, 83, 83), (372006, 1, 4482, 54), device='cuda:0', dtype=torch.float32)
    squeeze_46 = rand_strided((54, ), (1, ), device='cuda:0', dtype=torch.float32)
    avg_pool2d = rand_strided((8, 96, 83, 83), (661344, 1, 7968, 96), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_9 = rand_strided((8, 96, 165, 165), (2613600, 1, 15840, 96), device='cuda:0', dtype=torch.float32)
    avg_pool2d_1 = rand_strided((8, 96, 83, 83), (661344, 1, 7968, 96), device='cuda:0', dtype=torch.float32)
    cat_1 = rand_strided((8, 108, 83, 83), (744012, 1, 8964, 108), device='cuda:0', dtype=torch.float32)
    squeeze_49 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_15 = rand_strided((8, 270, 83, 83), (1860030, 1, 22410, 270), device='cuda:0', dtype=torch.float32)
    convolution_30 = rand_strided((8, 108, 83, 83), (744012, 1, 8964, 108), device='cuda:0', dtype=torch.float32)
    squeeze_52 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_10 = rand_strided((8, 108, 87, 87), (817452, 1, 9396, 108), device='cuda:0', dtype=torch.float32)
    convolution_31 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_32 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_55 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_17 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_33 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_34 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_58 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_11 = rand_strided((8, 108, 85, 85), (780300, 1, 9180, 108), device='cuda:0', dtype=torch.float32)
    getitem_47 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.int64)
    constant_pad_nd_12 = rand_strided((8, 108, 89, 89), (855468, 1, 9612, 108), device='cuda:0', dtype=torch.float32)
    convolution_35 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_36 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_61 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_19 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_37 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_38 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_64 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_13 = rand_strided((8, 108, 85, 85), (780300, 1, 9180, 108), device='cuda:0', dtype=torch.float32)
    getitem_53 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.int64)
    constant_pad_nd_14 = rand_strided((8, 108, 87, 87), (817452, 1, 9396, 108), device='cuda:0', dtype=torch.float32)
    convolution_39 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_40 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_67 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_21 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_41 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_42 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_70 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_15 = rand_strided((8, 108, 85, 85), (780300, 1, 9180, 108), device='cuda:0', dtype=torch.float32)
    convolution_43 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_44 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_73 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_23 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_45 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_46 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_76 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_24 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_47 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_48 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_79 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_25 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_49 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_50 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_82 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_17 = rand_strided((8, 108, 85, 85), (780300, 1, 9180, 108), device='cuda:0', dtype=torch.float32)
    convolution_51 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_52 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_85 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_27 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_53 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    convolution_54 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_88 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_18 = rand_strided((8, 108, 83, 83), (744012, 1, 8964, 108), device='cuda:0', dtype=torch.float32)
    convolution_55 = rand_strided((8, 108, 42, 42), (190512, 1, 4536, 108), device='cuda:0', dtype=torch.float32)
    squeeze_91 = rand_strided((108, ), (1, ), device='cuda:0', dtype=torch.float32)
    avg_pool2d_2 = rand_strided((8, 270, 42, 42), (476280, 1, 11340, 270), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_19 = rand_strided((8, 270, 83, 83), (1860030, 1, 22410, 270), device='cuda:0', dtype=torch.float32)
    avg_pool2d_3 = rand_strided((8, 270, 42, 42), (476280, 1, 11340, 270), device='cuda:0', dtype=torch.float32)
    cat_3 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_94 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_169 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    relu_30 = rand_strided((8, 540, 42, 42), (952560, 1, 22680, 540), device='cuda:0', dtype=torch.float32)
    convolution_58 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_97 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_174 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    relu_31 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_59 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_60 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_100 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_32 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_61 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_62 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_103 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_83 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.int64)
    relu_33 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_63 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_64 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_106 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_34 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_65 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_66 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_109 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_89 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.int64)
    convolution_67 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_68 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_112 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_36 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_69 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_70 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_115 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_71 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_72 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_118 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_38 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_73 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_74 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_121 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_39 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_75 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_76 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_124 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_40 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_77 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_78 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_127 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_79 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_80 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_130 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_42 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_81 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_82 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_133 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_83 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_136 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_244 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    relu_44 = rand_strided((8, 1080, 42, 42), (1905120, 1, 45360, 1080), device='cuda:0', dtype=torch.float32)
    convolution_84 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_139 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_249 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    relu_45 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_85 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_86 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_142 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_46 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_87 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_88 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_145 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_117 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.int64)
    relu_47 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_89 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_90 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_148 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_48 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_91 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_92 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_151 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_123 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.int64)
    convolution_93 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_94 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_154 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_50 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_95 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_96 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_157 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_97 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_98 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_160 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_52 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_99 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_100 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_163 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_53 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_101 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_102 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_166 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_54 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_103 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_104 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_169 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_105 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_106 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_172 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_56 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_107 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_108 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_175 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_109 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_178 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_319 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    relu_58 = rand_strided((8, 1080, 42, 42), (1905120, 1, 45360, 1080), device='cuda:0', dtype=torch.float32)
    convolution_110 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_181 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_324 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    relu_59 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_111 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_112 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_184 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_60 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_113 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_114 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_187 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_151 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.int64)
    relu_61 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_115 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_116 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_190 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_62 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_117 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_118 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_193 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_157 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.int64)
    convolution_119 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_120 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_196 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_64 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_121 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_122 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_199 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_123 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_124 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_202 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_66 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_125 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_126 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_205 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_67 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_127 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_128 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_208 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_68 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_129 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_130 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_211 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_131 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_132 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_214 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_70 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_133 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_134 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_217 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_135 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_220 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_394 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    relu_72 = rand_strided((8, 1080, 42, 42), (1905120, 1, 45360, 1080), device='cuda:0', dtype=torch.float32)
    convolution_136 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_223 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_399 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    relu_73 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_137 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_138 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_226 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_74 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_139 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_140 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_229 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_185 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.int64)
    relu_75 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_141 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_142 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_232 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_76 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_143 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_144 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_235 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_191 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.int64)
    convolution_145 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_146 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_238 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_78 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_147 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_148 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_241 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_149 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_150 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_244 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_80 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_151 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_152 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_247 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_81 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_153 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_154 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_250 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_82 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_155 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_156 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_253 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_157 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_158 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_256 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_84 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_159 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    convolution_160 = rand_strided((8, 216, 42, 42), (381024, 1, 9072, 216), device='cuda:0', dtype=torch.float32)
    squeeze_259 = rand_strided((216, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_161 = rand_strided((8, 432, 42, 42), (762048, 1, 18144, 432), device='cuda:0', dtype=torch.float32)
    squeeze_262 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_86 = rand_strided((8, 1080, 42, 42), (1905120, 1, 45360, 1080), device='cuda:0', dtype=torch.float32)
    convolution_162 = rand_strided((8, 432, 42, 42), (762048, 1, 18144, 432), device='cuda:0', dtype=torch.float32)
    squeeze_265 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_20 = rand_strided((8, 432, 45, 45), (874800, 1, 19440, 432), device='cuda:0', dtype=torch.float32)
    convolution_163 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_164 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_268 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_88 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_165 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_166 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_271 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_21 = rand_strided((8, 432, 43, 43), (798768, 1, 18576, 432), device='cuda:0', dtype=torch.float32)
    getitem_219 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.int64)
    constant_pad_nd_22 = rand_strided((8, 432, 47, 47), (954288, 1, 20304, 432), device='cuda:0', dtype=torch.float32)
    convolution_167 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_168 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_274 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_90 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_169 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_170 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_277 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_23 = rand_strided((8, 432, 43, 43), (798768, 1, 18576, 432), device='cuda:0', dtype=torch.float32)
    getitem_225 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.int64)
    constant_pad_nd_24 = rand_strided((8, 432, 45, 45), (874800, 1, 19440, 432), device='cuda:0', dtype=torch.float32)
    convolution_171 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_172 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_280 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_92 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_173 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_174 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_283 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_25 = rand_strided((8, 432, 43, 43), (798768, 1, 18576, 432), device='cuda:0', dtype=torch.float32)
    convolution_175 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_176 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_286 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_94 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_177 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_178 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_289 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_95 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_179 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_180 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_292 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_96 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_181 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_182 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_295 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_27 = rand_strided((8, 432, 43, 43), (798768, 1, 18576, 432), device='cuda:0', dtype=torch.float32)
    convolution_183 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_184 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_298 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_98 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_185 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_186 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_301 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_28 = rand_strided((8, 432, 42, 42), (762048, 1, 18144, 432), device='cuda:0', dtype=torch.float32)
    convolution_187 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_304 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    avg_pool2d_4 = rand_strided((8, 1080, 21, 21), (476280, 1, 22680, 1080), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_29 = rand_strided((8, 1080, 42, 42), (1905120, 1, 45360, 1080), device='cuda:0', dtype=torch.float32)
    avg_pool2d_5 = rand_strided((8, 1080, 21, 21), (476280, 1, 22680, 1080), device='cuda:0', dtype=torch.float32)
    cat_9 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_307 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_549 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    relu_101 = rand_strided((8, 2160, 21, 21), (952560, 1, 45360, 2160), device='cuda:0', dtype=torch.float32)
    convolution_190 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_310 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_554 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    relu_102 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_191 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_192 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_313 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_103 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_193 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_194 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_316 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_255 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.int64)
    relu_104 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_195 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_196 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_319 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_105 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_197 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_198 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_322 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_261 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.int64)
    convolution_199 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_200 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_325 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_107 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_201 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_202 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_328 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_203 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_204 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_331 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_109 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_205 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_206 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_334 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_110 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_207 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_208 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_337 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_111 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_209 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_210 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_340 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_211 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_212 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_343 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_113 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_213 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_214 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_346 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_215 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_349 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_624 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    relu_115 = rand_strided((8, 2160, 21, 21), (952560, 1, 45360, 2160), device='cuda:0', dtype=torch.float32)
    convolution_216 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_352 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_629 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    relu_116 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_217 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_218 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_355 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_117 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_219 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_220 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_358 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_289 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.int64)
    relu_118 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_221 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_222 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_361 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_119 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_223 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_224 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_364 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_295 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.int64)
    convolution_225 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_226 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_367 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_121 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_227 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_228 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_370 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_229 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_230 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_373 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_123 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_231 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_232 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_376 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_124 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_233 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_234 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_379 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_125 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_235 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_236 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_382 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_237 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_238 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_385 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_127 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_239 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_240 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_388 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_241 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_391 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_699 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    relu_129 = rand_strided((8, 2160, 21, 21), (952560, 1, 45360, 2160), device='cuda:0', dtype=torch.float32)
    convolution_242 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_394 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_704 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    relu_130 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_243 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_244 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_397 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_131 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_245 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_246 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_400 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_323 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.int64)
    relu_132 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_247 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_248 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_403 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_133 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_249 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_250 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_406 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_329 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.int64)
    convolution_251 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_252 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_409 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_135 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_253 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_254 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_412 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_255 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_256 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_415 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_137 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_257 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_258 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_418 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_138 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_259 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_260 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_421 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_139 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_261 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_262 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_424 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_263 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_264 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_427 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_141 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_265 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    convolution_266 = rand_strided((8, 432, 21, 21), (190512, 1, 9072, 432), device='cuda:0', dtype=torch.float32)
    squeeze_430 = rand_strided((432, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_267 = rand_strided((8, 864, 21, 21), (381024, 1, 18144, 864), device='cuda:0', dtype=torch.float32)
    squeeze_433 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_143 = rand_strided((8, 2160, 21, 21), (952560, 1, 45360, 2160), device='cuda:0', dtype=torch.float32)
    convolution_268 = rand_strided((8, 864, 21, 21), (381024, 1, 18144, 864), device='cuda:0', dtype=torch.float32)
    squeeze_436 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_30 = rand_strided((8, 864, 25, 25), (540000, 1, 21600, 864), device='cuda:0', dtype=torch.float32)
    convolution_269 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_270 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_439 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_145 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_271 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_272 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_442 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_31 = rand_strided((8, 864, 23, 23), (457056, 1, 19872, 864), device='cuda:0', dtype=torch.float32)
    getitem_357 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.int64)
    constant_pad_nd_32 = rand_strided((8, 864, 27, 27), (629856, 1, 23328, 864), device='cuda:0', dtype=torch.float32)
    convolution_273 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_274 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_445 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_147 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_275 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_276 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_448 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_33 = rand_strided((8, 864, 23, 23), (457056, 1, 19872, 864), device='cuda:0', dtype=torch.float32)
    getitem_363 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.int64)
    constant_pad_nd_34 = rand_strided((8, 864, 25, 25), (540000, 1, 21600, 864), device='cuda:0', dtype=torch.float32)
    convolution_277 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_278 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_451 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_149 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_279 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_280 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_454 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_35 = rand_strided((8, 864, 23, 23), (457056, 1, 19872, 864), device='cuda:0', dtype=torch.float32)
    convolution_281 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_282 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_457 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_151 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_283 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_284 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_460 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_152 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_285 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_286 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_463 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_153 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_287 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_288 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_466 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_37 = rand_strided((8, 864, 23, 23), (457056, 1, 19872, 864), device='cuda:0', dtype=torch.float32)
    convolution_289 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_290 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_469 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_155 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_291 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_292 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_472 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_38 = rand_strided((8, 864, 21, 21), (381024, 1, 18144, 864), device='cuda:0', dtype=torch.float32)
    convolution_293 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_475 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    avg_pool2d_6 = rand_strided((8, 2160, 11, 11), (261360, 1, 23760, 2160), device='cuda:0', dtype=torch.float32)
    constant_pad_nd_39 = rand_strided((8, 2160, 21, 21), (952560, 1, 45360, 2160), device='cuda:0', dtype=torch.float32)
    avg_pool2d_7 = rand_strided((8, 2160, 11, 11), (261360, 1, 23760, 2160), device='cuda:0', dtype=torch.float32)
    cat_14 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_478 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_854 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    relu_158 = rand_strided((8, 4320, 11, 11), (522720, 1, 47520, 4320), device='cuda:0', dtype=torch.float32)
    convolution_296 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_481 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_859 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    relu_159 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_297 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_298 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_484 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_160 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_299 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_300 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_487 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_393 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.int64)
    relu_161 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_301 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_302 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_490 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_162 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_303 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_304 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_493 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_399 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.int64)
    convolution_305 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_306 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_496 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_164 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_307 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_308 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_499 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_309 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_310 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_502 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_166 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_311 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_312 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_505 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_167 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_313 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_314 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_508 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_168 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_315 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_316 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_511 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_317 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_318 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_514 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_170 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_319 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_320 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_517 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_321 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_520 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_929 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    relu_172 = rand_strided((8, 4320, 11, 11), (522720, 1, 47520, 4320), device='cuda:0', dtype=torch.float32)
    convolution_322 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_523 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_934 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    relu_173 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_323 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_324 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_526 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_174 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_325 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_326 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_529 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_427 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.int64)
    relu_175 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_327 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_328 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_532 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_176 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_329 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_330 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_535 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_433 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.int64)
    convolution_331 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_332 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_538 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_178 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_333 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_334 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_541 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_335 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_336 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_544 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_180 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_337 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_338 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_547 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_181 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_339 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_340 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_550 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_182 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_341 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_342 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_553 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_343 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_344 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_556 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_184 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_345 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_346 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_559 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_347 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_562 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_1004 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    relu_186 = rand_strided((8, 4320, 11, 11), (522720, 1, 47520, 4320), device='cuda:0', dtype=torch.float32)
    convolution_348 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_565 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_1009 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    relu_187 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_349 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_350 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_568 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_188 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_351 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_352 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_571 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_461 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.int64)
    relu_189 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_353 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_354 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_574 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_190 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_355 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_356 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_577 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_467 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.int64)
    convolution_357 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_358 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_580 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_192 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_359 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_360 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_583 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_361 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_362 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_586 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_194 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_363 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_364 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_589 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_195 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_365 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_366 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_592 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_196 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_367 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_368 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_595 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_369 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_370 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_598 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_198 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_371 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    convolution_372 = rand_strided((8, 864, 11, 11), (104544, 1, 9504, 864), device='cuda:0', dtype=torch.float32)
    squeeze_601 = rand_strided((864, ), (1, ), device='cuda:0', dtype=torch.float32)
    clone = rand_strided((8, 4320), (4320, 1), device='cuda:0', dtype=torch.float32)
    permute_1 = rand_strided((1000, 4320), (4320, 1), device='cuda:0', dtype=torch.float32)
    le = rand_strided((8, 4320, 11, 11), (522720, 1, 47520, 4320), device='cuda:0', dtype=torch.bool)
    unsqueeze_806 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_818 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_830 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_842 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_854 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_866 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_878 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_890 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_902 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_914 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_926 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_938 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_950 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_962 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_974 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_986 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_998 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1010 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1022 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1034 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1046 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1058 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1070 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1082 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1094 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1106 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1118 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1130 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1142 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1154 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1166 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1178 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1190 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1202 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1214 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1226 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1238 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1250 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1262 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1274 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1286 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1298 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1310 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_43 = rand_strided((8, 864, 21, 21), (381024, 1, 18144, 864), device='cuda:0', dtype=torch.bool)
    unsqueeze_1322 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1334 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_45 = rand_strided((8, 864, 21, 21), (381024, 1, 18144, 864), device='cuda:0', dtype=torch.bool)
    unsqueeze_1346 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1358 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1370 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1382 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1394 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1406 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1418 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1430 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1442 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1454 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1466 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1478 = rand_strided((1, 864, 1, 1), (864, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1490 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1502 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1514 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1526 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1538 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1550 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1562 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1574 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1586 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1598 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1610 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1622 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1634 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1646 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1658 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1670 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1682 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1694 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1706 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1718 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1730 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1742 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1754 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1766 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1778 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1790 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1802 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1814 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1826 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1838 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1850 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1862 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1874 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1886 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1898 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1910 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1922 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1934 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1946 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1958 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1970 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1982 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1994 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_100 = rand_strided((8, 432, 42, 42), (762048, 1, 18144, 432), device='cuda:0', dtype=torch.bool)
    unsqueeze_2006 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2018 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_102 = rand_strided((8, 432, 42, 42), (762048, 1, 18144, 432), device='cuda:0', dtype=torch.bool)
    unsqueeze_2030 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2042 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2054 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2066 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2078 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2090 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2102 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2114 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2126 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2138 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2150 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2162 = rand_strided((1, 432, 1, 1), (432, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2174 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2186 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2198 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2210 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2222 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2234 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2246 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2258 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2270 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2282 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2294 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2306 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2318 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2330 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2342 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2354 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2366 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2378 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2390 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2402 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2414 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2426 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2438 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2450 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2462 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2474 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2486 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2498 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2510 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2522 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2534 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2546 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2558 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2570 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2582 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2594 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2606 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2618 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2630 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2642 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2654 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2666 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2678 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2690 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2702 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2714 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2726 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2738 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2750 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2762 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2774 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2786 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2798 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2810 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2822 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2834 = rand_strided((1, 216, 1, 1), (216, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2846 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_171 = rand_strided((8, 108, 83, 83), (744012, 1, 8964, 108), device='cuda:0', dtype=torch.bool)
    unsqueeze_2858 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2870 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_173 = rand_strided((8, 108, 83, 83), (744012, 1, 8964, 108), device='cuda:0', dtype=torch.bool)
    unsqueeze_2882 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2894 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2906 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2918 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2930 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2942 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2954 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2966 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2978 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2990 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3002 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3014 = rand_strided((1, 108, 1, 1), (108, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3026 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_186 = rand_strided((8, 54, 165, 165), (1470150, 1, 8910, 54), device='cuda:0', dtype=torch.bool)
    unsqueeze_3038 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3050 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3062 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3074 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3086 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3098 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3110 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3122 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3134 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3146 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3158 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3170 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3182 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3194 = rand_strided((1, 54, 1, 1), (54, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_3206 = rand_strided((1, 96, 1, 1), (96, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    tangents_1 = rand_strided((8, 1000), (1000, 1), device='cuda:0', dtype=torch.float32)
    return print_performance(lambda: call([primals_1, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_31, primals_32, primals_34, primals_35, primals_36, primals_38, primals_39, primals_41, primals_42, primals_44, primals_45, primals_46, primals_48, primals_49, primals_51, primals_52, primals_53, primals_55, primals_56, primals_58, primals_59, primals_60, primals_62, primals_63, primals_64, primals_66, primals_67, primals_68, primals_70, primals_71, primals_73, primals_74, primals_75, primals_77, primals_79, primals_80, primals_81, primals_83, primals_84, primals_86, primals_87, primals_89, primals_90, primals_91, primals_93, primals_94, primals_96, primals_97, primals_98, primals_100, primals_101, primals_103, primals_104, primals_105, primals_107, primals_108, primals_110, primals_111, primals_112, primals_114, primals_115, primals_116, primals_118, primals_119, primals_120, primals_122, primals_123, primals_125, primals_126, primals_127, primals_129, primals_131, primals_132, primals_133, primals_135, primals_136, primals_138, primals_139, primals_140, primals_142, primals_143, primals_144, primals_146, primals_147, primals_148, primals_150, primals_151, primals_152, primals_154, primals_155, primals_156, primals_158, primals_159, primals_160, primals_162, primals_163, primals_164, primals_166, primals_167, primals_168, primals_170, primals_171, primals_172, primals_174, primals_175, primals_176, primals_178, primals_179, primals_180, primals_182, primals_183, primals_184, primals_186, primals_187, primals_189, primals_190, primals_192, primals_193, primals_194, primals_196, primals_197, primals_198, primals_200, primals_201, primals_202, primals_204, primals_205, primals_206, primals_208, primals_209, primals_210, primals_212, primals_213, primals_214, primals_216, primals_217, primals_218, primals_220, primals_221, primals_222, primals_224, primals_225, primals_226, primals_228, primals_229, primals_230, primals_232, primals_233, primals_234, primals_236, primals_237, primals_238, primals_240, primals_241, primals_243, primals_244, primals_246, primals_247, primals_248, primals_250, primals_251, primals_252, primals_254, primals_255, primals_256, primals_258, primals_259, primals_260, primals_262, primals_263, primals_264, primals_266, primals_267, primals_268, primals_270, primals_271, primals_272, primals_274, primals_275, primals_276, primals_278, primals_279, primals_280, primals_282, primals_283, primals_284, primals_286, primals_287, primals_288, primals_290, primals_291, primals_292, primals_294, primals_295, primals_297, primals_298, primals_300, primals_301, primals_302, primals_304, primals_305, primals_306, primals_308, primals_309, primals_310, primals_312, primals_313, primals_314, primals_316, primals_317, primals_318, primals_320, primals_321, primals_322, primals_324, primals_325, primals_326, primals_328, primals_329, primals_330, primals_332, primals_333, primals_334, primals_336, primals_337, primals_338, primals_340, primals_341, primals_342, primals_344, primals_345, primals_346, primals_348, primals_349, primals_351, primals_352, primals_354, primals_355, primals_357, primals_358, primals_359, primals_361, primals_362, primals_364, primals_365, primals_366, primals_368, primals_369, primals_371, primals_372, primals_373, primals_375, primals_376, primals_378, primals_379, primals_380, primals_382, primals_383, primals_384, primals_386, primals_387, primals_388, primals_390, primals_391, primals_393, primals_394, primals_395, primals_397, primals_399, primals_400, primals_401, primals_403, primals_404, primals_406, primals_407, primals_408, primals_410, primals_411, primals_412, primals_414, primals_415, primals_416, primals_418, primals_419, primals_420, primals_422, primals_423, primals_424, primals_426, primals_427, primals_428, primals_430, primals_431, primals_432, primals_434, primals_435, primals_436, primals_438, primals_439, primals_440, primals_442, primals_443, primals_444, primals_446, primals_447, primals_448, primals_450, primals_451, primals_452, primals_454, primals_455, primals_457, primals_458, primals_460, primals_461, primals_462, primals_464, primals_465, primals_466, primals_468, primals_469, primals_470, primals_472, primals_473, primals_474, primals_476, primals_477, primals_478, primals_480, primals_481, primals_482, primals_484, primals_485, primals_486, primals_488, primals_489, primals_490, primals_492, primals_493, primals_494, primals_496, primals_497, primals_498, primals_500, primals_501, primals_502, primals_504, primals_505, primals_506, primals_508, primals_509, primals_511, primals_512, primals_514, primals_515, primals_516, primals_518, primals_519, primals_520, primals_522, primals_523, primals_524, primals_526, primals_527, primals_528, primals_530, primals_531, primals_532, primals_534, primals_535, primals_536, primals_538, primals_539, primals_540, primals_542, primals_543, primals_544, primals_546, primals_547, primals_548, primals_550, primals_551, primals_552, primals_554, primals_555, primals_556, primals_558, primals_559, primals_560, primals_562, primals_563, primals_565, primals_566, primals_568, primals_569, primals_571, primals_572, primals_573, primals_575, primals_576, primals_578, primals_579, primals_580, primals_582, primals_583, primals_585, primals_586, primals_587, primals_589, primals_590, primals_592, primals_593, primals_594, primals_596, primals_597, primals_598, primals_600, primals_601, primals_602, primals_604, primals_605, primals_607, primals_608, primals_609, primals_611, primals_613, primals_614, primals_615, primals_617, primals_618, primals_620, primals_621, primals_622, primals_624, primals_625, primals_626, primals_628, primals_629, primals_630, primals_632, primals_633, primals_634, primals_636, primals_637, primals_638, primals_640, primals_641, primals_642, primals_644, primals_645, primals_646, primals_648, primals_649, primals_650, primals_652, primals_653, primals_654, primals_656, primals_657, primals_658, primals_660, primals_661, primals_662, primals_664, primals_665, primals_666, primals_668, primals_669, primals_671, primals_672, primals_674, primals_675, primals_676, primals_678, primals_679, primals_680, primals_682, primals_683, primals_684, primals_686, primals_687, primals_688, primals_690, primals_691, primals_692, primals_694, primals_695, primals_696, primals_698, primals_699, primals_700, primals_702, primals_703, primals_704, primals_706, primals_707, primals_708, primals_710, primals_711, primals_712, primals_714, primals_715, primals_716, primals_718, primals_719, primals_720, primals_722, primals_723, primals_725, primals_726, primals_728, primals_729, primals_730, primals_732, primals_733, primals_734, primals_736, primals_737, primals_738, primals_740, primals_741, primals_742, primals_744, primals_745, primals_746, primals_748, primals_749, primals_750, primals_752, primals_753, primals_754, primals_756, primals_757, primals_758, primals_760, primals_761, primals_762, primals_764, primals_765, primals_766, primals_768, primals_769, primals_770, primals_772, primals_773, primals_774, primals_1381, convolution, squeeze_1, relu, convolution_1, squeeze_4, constant_pad_nd, convolution_2, convolution_3, squeeze_7, relu_2, convolution_4, convolution_5, squeeze_10, constant_pad_nd_1, getitem_8, getitem_9, convolution_6, squeeze_13, constant_pad_nd_2, convolution_7, convolution_8, squeeze_16, relu_4, convolution_9, convolution_10, squeeze_19, constant_pad_nd_3, getitem_17, constant_pad_nd_4, convolution_11, convolution_12, squeeze_22, relu_6, convolution_13, convolution_14, squeeze_25, constant_pad_nd_5, convolution_15, convolution_16, squeeze_28, relu_8, convolution_17, convolution_18, squeeze_31, relu_9, convolution_19, convolution_20, squeeze_34, relu_10, convolution_21, convolution_22, squeeze_37, constant_pad_nd_7, convolution_23, convolution_24, squeeze_40, relu_12, convolution_25, convolution_26, squeeze_43, constant_pad_nd_8, convolution_27, squeeze_46, avg_pool2d, constant_pad_nd_9, avg_pool2d_1, cat_1, squeeze_49, relu_15, convolution_30, squeeze_52, constant_pad_nd_10, convolution_31, convolution_32, squeeze_55, relu_17, convolution_33, convolution_34, squeeze_58, constant_pad_nd_11, getitem_47, constant_pad_nd_12, convolution_35, convolution_36, squeeze_61, relu_19, convolution_37, convolution_38, squeeze_64, constant_pad_nd_13, getitem_53, constant_pad_nd_14, convolution_39, convolution_40, squeeze_67, relu_21, convolution_41, convolution_42, squeeze_70, constant_pad_nd_15, convolution_43, convolution_44, squeeze_73, relu_23, convolution_45, convolution_46, squeeze_76, relu_24, convolution_47, convolution_48, squeeze_79, relu_25, convolution_49, convolution_50, squeeze_82, constant_pad_nd_17, convolution_51, convolution_52, squeeze_85, relu_27, convolution_53, convolution_54, squeeze_88, constant_pad_nd_18, convolution_55, squeeze_91, avg_pool2d_2, constant_pad_nd_19, avg_pool2d_3, cat_3, squeeze_94, add_169, relu_30, convolution_58, squeeze_97, add_174, relu_31, convolution_59, convolution_60, squeeze_100, relu_32, convolution_61, convolution_62, squeeze_103, getitem_83, relu_33, convolution_63, convolution_64, squeeze_106, relu_34, convolution_65, convolution_66, squeeze_109, getitem_89, convolution_67, convolution_68, squeeze_112, relu_36, convolution_69, convolution_70, squeeze_115, convolution_71, convolution_72, squeeze_118, relu_38, convolution_73, convolution_74, squeeze_121, relu_39, convolution_75, convolution_76, squeeze_124, relu_40, convolution_77, convolution_78, squeeze_127, convolution_79, convolution_80, squeeze_130, relu_42, convolution_81, convolution_82, squeeze_133, convolution_83, squeeze_136, add_244, relu_44, convolution_84, squeeze_139, add_249, relu_45, convolution_85, convolution_86, squeeze_142, relu_46, convolution_87, convolution_88, squeeze_145, getitem_117, relu_47, convolution_89, convolution_90, squeeze_148, relu_48, convolution_91, convolution_92, squeeze_151, getitem_123, convolution_93, convolution_94, squeeze_154, relu_50, convolution_95, convolution_96, squeeze_157, convolution_97, convolution_98, squeeze_160, relu_52, convolution_99, convolution_100, squeeze_163, relu_53, convolution_101, convolution_102, squeeze_166, relu_54, convolution_103, convolution_104, squeeze_169, convolution_105, convolution_106, squeeze_172, relu_56, convolution_107, convolution_108, squeeze_175, convolution_109, squeeze_178, add_319, relu_58, convolution_110, squeeze_181, add_324, relu_59, convolution_111, convolution_112, squeeze_184, relu_60, convolution_113, convolution_114, squeeze_187, getitem_151, relu_61, convolution_115, convolution_116, squeeze_190, relu_62, convolution_117, convolution_118, squeeze_193, getitem_157, convolution_119, convolution_120, squeeze_196, relu_64, convolution_121, convolution_122, squeeze_199, convolution_123, convolution_124, squeeze_202, relu_66, convolution_125, convolution_126, squeeze_205, relu_67, convolution_127, convolution_128, squeeze_208, relu_68, convolution_129, convolution_130, squeeze_211, convolution_131, convolution_132, squeeze_214, relu_70, convolution_133, convolution_134, squeeze_217, convolution_135, squeeze_220, add_394, relu_72, convolution_136, squeeze_223, add_399, relu_73, convolution_137, convolution_138, squeeze_226, relu_74, convolution_139, convolution_140, squeeze_229, getitem_185, relu_75, convolution_141, convolution_142, squeeze_232, relu_76, convolution_143, convolution_144, squeeze_235, getitem_191, convolution_145, convolution_146, squeeze_238, relu_78, convolution_147, convolution_148, squeeze_241, convolution_149, convolution_150, squeeze_244, relu_80, convolution_151, convolution_152, squeeze_247, relu_81, convolution_153, convolution_154, squeeze_250, relu_82, convolution_155, convolution_156, squeeze_253, convolution_157, convolution_158, squeeze_256, relu_84, convolution_159, convolution_160, squeeze_259, convolution_161, squeeze_262, relu_86, convolution_162, squeeze_265, constant_pad_nd_20, convolution_163, convolution_164, squeeze_268, relu_88, convolution_165, convolution_166, squeeze_271, constant_pad_nd_21, getitem_219, constant_pad_nd_22, convolution_167, convolution_168, squeeze_274, relu_90, convolution_169, convolution_170, squeeze_277, constant_pad_nd_23, getitem_225, constant_pad_nd_24, convolution_171, convolution_172, squeeze_280, relu_92, convolution_173, convolution_174, squeeze_283, constant_pad_nd_25, convolution_175, convolution_176, squeeze_286, relu_94, convolution_177, convolution_178, squeeze_289, relu_95, convolution_179, convolution_180, squeeze_292, relu_96, convolution_181, convolution_182, squeeze_295, constant_pad_nd_27, convolution_183, convolution_184, squeeze_298, relu_98, convolution_185, convolution_186, squeeze_301, constant_pad_nd_28, convolution_187, squeeze_304, avg_pool2d_4, constant_pad_nd_29, avg_pool2d_5, cat_9, squeeze_307, add_549, relu_101, convolution_190, squeeze_310, add_554, relu_102, convolution_191, convolution_192, squeeze_313, relu_103, convolution_193, convolution_194, squeeze_316, getitem_255, relu_104, convolution_195, convolution_196, squeeze_319, relu_105, convolution_197, convolution_198, squeeze_322, getitem_261, convolution_199, convolution_200, squeeze_325, relu_107, convolution_201, convolution_202, squeeze_328, convolution_203, convolution_204, squeeze_331, relu_109, convolution_205, convolution_206, squeeze_334, relu_110, convolution_207, convolution_208, squeeze_337, relu_111, convolution_209, convolution_210, squeeze_340, convolution_211, convolution_212, squeeze_343, relu_113, convolution_213, convolution_214, squeeze_346, convolution_215, squeeze_349, add_624, relu_115, convolution_216, squeeze_352, add_629, relu_116, convolution_217, convolution_218, squeeze_355, relu_117, convolution_219, convolution_220, squeeze_358, getitem_289, relu_118, convolution_221, convolution_222, squeeze_361, relu_119, convolution_223, convolution_224, squeeze_364, getitem_295, convolution_225, convolution_226, squeeze_367, relu_121, convolution_227, convolution_228, squeeze_370, convolution_229, convolution_230, squeeze_373, relu_123, convolution_231, convolution_232, squeeze_376, relu_124, convolution_233, convolution_234, squeeze_379, relu_125, convolution_235, convolution_236, squeeze_382, convolution_237, convolution_238, squeeze_385, relu_127, convolution_239, convolution_240, squeeze_388, convolution_241, squeeze_391, add_699, relu_129, convolution_242, squeeze_394, add_704, relu_130, convolution_243, convolution_244, squeeze_397, relu_131, convolution_245, convolution_246, squeeze_400, getitem_323, relu_132, convolution_247, convolution_248, squeeze_403, relu_133, convolution_249, convolution_250, squeeze_406, getitem_329, convolution_251, convolution_252, squeeze_409, relu_135, convolution_253, convolution_254, squeeze_412, convolution_255, convolution_256, squeeze_415, relu_137, convolution_257, convolution_258, squeeze_418, relu_138, convolution_259, convolution_260, squeeze_421, relu_139, convolution_261, convolution_262, squeeze_424, convolution_263, convolution_264, squeeze_427, relu_141, convolution_265, convolution_266, squeeze_430, convolution_267, squeeze_433, relu_143, convolution_268, squeeze_436, constant_pad_nd_30, convolution_269, convolution_270, squeeze_439, relu_145, convolution_271, convolution_272, squeeze_442, constant_pad_nd_31, getitem_357, constant_pad_nd_32, convolution_273, convolution_274, squeeze_445, relu_147, convolution_275, convolution_276, squeeze_448, constant_pad_nd_33, getitem_363, constant_pad_nd_34, convolution_277, convolution_278, squeeze_451, relu_149, convolution_279, convolution_280, squeeze_454, constant_pad_nd_35, convolution_281, convolution_282, squeeze_457, relu_151, convolution_283, convolution_284, squeeze_460, relu_152, convolution_285, convolution_286, squeeze_463, relu_153, convolution_287, convolution_288, squeeze_466, constant_pad_nd_37, convolution_289, convolution_290, squeeze_469, relu_155, convolution_291, convolution_292, squeeze_472, constant_pad_nd_38, convolution_293, squeeze_475, avg_pool2d_6, constant_pad_nd_39, avg_pool2d_7, cat_14, squeeze_478, add_854, relu_158, convolution_296, squeeze_481, add_859, relu_159, convolution_297, convolution_298, squeeze_484, relu_160, convolution_299, convolution_300, squeeze_487, getitem_393, relu_161, convolution_301, convolution_302, squeeze_490, relu_162, convolution_303, convolution_304, squeeze_493, getitem_399, convolution_305, convolution_306, squeeze_496, relu_164, convolution_307, convolution_308, squeeze_499, convolution_309, convolution_310, squeeze_502, relu_166, convolution_311, convolution_312, squeeze_505, relu_167, convolution_313, convolution_314, squeeze_508, relu_168, convolution_315, convolution_316, squeeze_511, convolution_317, convolution_318, squeeze_514, relu_170, convolution_319, convolution_320, squeeze_517, convolution_321, squeeze_520, add_929, relu_172, convolution_322, squeeze_523, add_934, relu_173, convolution_323, convolution_324, squeeze_526, relu_174, convolution_325, convolution_326, squeeze_529, getitem_427, relu_175, convolution_327, convolution_328, squeeze_532, relu_176, convolution_329, convolution_330, squeeze_535, getitem_433, convolution_331, convolution_332, squeeze_538, relu_178, convolution_333, convolution_334, squeeze_541, convolution_335, convolution_336, squeeze_544, relu_180, convolution_337, convolution_338, squeeze_547, relu_181, convolution_339, convolution_340, squeeze_550, relu_182, convolution_341, convolution_342, squeeze_553, convolution_343, convolution_344, squeeze_556, relu_184, convolution_345, convolution_346, squeeze_559, convolution_347, squeeze_562, add_1004, relu_186, convolution_348, squeeze_565, add_1009, relu_187, convolution_349, convolution_350, squeeze_568, relu_188, convolution_351, convolution_352, squeeze_571, getitem_461, relu_189, convolution_353, convolution_354, squeeze_574, relu_190, convolution_355, convolution_356, squeeze_577, getitem_467, convolution_357, convolution_358, squeeze_580, relu_192, convolution_359, convolution_360, squeeze_583, convolution_361, convolution_362, squeeze_586, relu_194, convolution_363, convolution_364, squeeze_589, relu_195, convolution_365, convolution_366, squeeze_592, relu_196, convolution_367, convolution_368, squeeze_595, convolution_369, convolution_370, squeeze_598, relu_198, convolution_371, convolution_372, squeeze_601, clone, permute_1, le, unsqueeze_806, unsqueeze_818, unsqueeze_830, unsqueeze_842, unsqueeze_854, unsqueeze_866, unsqueeze_878, unsqueeze_890, unsqueeze_902, unsqueeze_914, unsqueeze_926, unsqueeze_938, unsqueeze_950, unsqueeze_962, unsqueeze_974, unsqueeze_986, unsqueeze_998, unsqueeze_1010, unsqueeze_1022, unsqueeze_1034, unsqueeze_1046, unsqueeze_1058, unsqueeze_1070, unsqueeze_1082, unsqueeze_1094, unsqueeze_1106, unsqueeze_1118, unsqueeze_1130, unsqueeze_1142, unsqueeze_1154, unsqueeze_1166, unsqueeze_1178, unsqueeze_1190, unsqueeze_1202, unsqueeze_1214, unsqueeze_1226, unsqueeze_1238, unsqueeze_1250, unsqueeze_1262, unsqueeze_1274, unsqueeze_1286, unsqueeze_1298, unsqueeze_1310, le_43, unsqueeze_1322, unsqueeze_1334, le_45, unsqueeze_1346, unsqueeze_1358, unsqueeze_1370, unsqueeze_1382, unsqueeze_1394, unsqueeze_1406, unsqueeze_1418, unsqueeze_1430, unsqueeze_1442, unsqueeze_1454, unsqueeze_1466, unsqueeze_1478, unsqueeze_1490, unsqueeze_1502, unsqueeze_1514, unsqueeze_1526, unsqueeze_1538, unsqueeze_1550, unsqueeze_1562, unsqueeze_1574, unsqueeze_1586, unsqueeze_1598, unsqueeze_1610, unsqueeze_1622, unsqueeze_1634, unsqueeze_1646, unsqueeze_1658, unsqueeze_1670, unsqueeze_1682, unsqueeze_1694, unsqueeze_1706, unsqueeze_1718, unsqueeze_1730, unsqueeze_1742, unsqueeze_1754, unsqueeze_1766, unsqueeze_1778, unsqueeze_1790, unsqueeze_1802, unsqueeze_1814, unsqueeze_1826, unsqueeze_1838, unsqueeze_1850, unsqueeze_1862, unsqueeze_1874, unsqueeze_1886, unsqueeze_1898, unsqueeze_1910, unsqueeze_1922, unsqueeze_1934, unsqueeze_1946, unsqueeze_1958, unsqueeze_1970, unsqueeze_1982, unsqueeze_1994, le_100, unsqueeze_2006, unsqueeze_2018, le_102, unsqueeze_2030, unsqueeze_2042, unsqueeze_2054, unsqueeze_2066, unsqueeze_2078, unsqueeze_2090, unsqueeze_2102, unsqueeze_2114, unsqueeze_2126, unsqueeze_2138, unsqueeze_2150, unsqueeze_2162, unsqueeze_2174, unsqueeze_2186, unsqueeze_2198, unsqueeze_2210, unsqueeze_2222, unsqueeze_2234, unsqueeze_2246, unsqueeze_2258, unsqueeze_2270, unsqueeze_2282, unsqueeze_2294, unsqueeze_2306, unsqueeze_2318, unsqueeze_2330, unsqueeze_2342, unsqueeze_2354, unsqueeze_2366, unsqueeze_2378, unsqueeze_2390, unsqueeze_2402, unsqueeze_2414, unsqueeze_2426, unsqueeze_2438, unsqueeze_2450, unsqueeze_2462, unsqueeze_2474, unsqueeze_2486, unsqueeze_2498, unsqueeze_2510, unsqueeze_2522, unsqueeze_2534, unsqueeze_2546, unsqueeze_2558, unsqueeze_2570, unsqueeze_2582, unsqueeze_2594, unsqueeze_2606, unsqueeze_2618, unsqueeze_2630, unsqueeze_2642, unsqueeze_2654, unsqueeze_2666, unsqueeze_2678, unsqueeze_2690, unsqueeze_2702, unsqueeze_2714, unsqueeze_2726, unsqueeze_2738, unsqueeze_2750, unsqueeze_2762, unsqueeze_2774, unsqueeze_2786, unsqueeze_2798, unsqueeze_2810, unsqueeze_2822, unsqueeze_2834, unsqueeze_2846, le_171, unsqueeze_2858, unsqueeze_2870, le_173, unsqueeze_2882, unsqueeze_2894, unsqueeze_2906, unsqueeze_2918, unsqueeze_2930, unsqueeze_2942, unsqueeze_2954, unsqueeze_2966, unsqueeze_2978, unsqueeze_2990, unsqueeze_3002, unsqueeze_3014, unsqueeze_3026, le_186, unsqueeze_3038, unsqueeze_3050, unsqueeze_3062, unsqueeze_3074, unsqueeze_3086, unsqueeze_3098, unsqueeze_3110, unsqueeze_3122, unsqueeze_3134, unsqueeze_3146, unsqueeze_3158, unsqueeze_3170, unsqueeze_3182, unsqueeze_3194, unsqueeze_3206, tangents_1]), times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('pnasnet5large', benchmark_compiled_module)
