
from ctypes import c_void_p, c_long
import torch
import math
import random
import os
import tempfile
from math import inf, nan
from torch._inductor.hooks import run_intermediate_hooks
from torch._inductor.utils import maybe_profile
from torch._inductor.codegen.memory_planning import _align as align

from torch import device, empty, empty_strided
from torch._inductor.codecache import AsyncCompile
from torch._inductor.select_algorithm import extern_kernels

aten = torch.ops.aten
inductor_ops = torch.ops.inductor
assert_size_stride = torch._C._dynamo.guards.assert_size_stride
alloc_from_pool = torch.ops.inductor._alloc_from_pool
reinterpret_tensor = torch.ops.inductor._reinterpret_tensor
async_compile = AsyncCompile()


# kernel path: /tmp/torchinductor_youkaichao/qt/cqtcbt4g57hrviadj7jcv3toxxh4xcnjikjw4nylbesjf4s5onol.py
# Source Nodes: [], Original ATen: [aten.sum]

triton_per_fused_sum_0 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 8],
    reduction_hint=ReductionHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_sum_0', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 1000
    rnumel = 8
    RBLOCK: tl.constexpr = 8
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (1000*r1)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')

import triton
import triton.language as tl
from torch._inductor.triton_heuristics import grid, start_graph, end_graph
from torch._C import _cuda_getCurrentRawStream as get_cuda_stream


# kernel path: /tmp/torchinductor_youkaichao/ue/cuea7xzgrpl7q7q4un2mkpv5rilgsblwvi4xsjcmezhsr2rajrkw.py
# Source Nodes: [], Original ATen: [aten.div, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_div_native_batch_norm_backward_threshold_backward_1 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_div_native_batch_norm_backward_threshold_backward_1', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 8192
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 2048
    x1 = (xindex // 2048)
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp10 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    _tmp14 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (2048*r2) + (200704*x1)), rmask, eviction_policy='evict_first').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (x0 + (2048*(r2 // 49)) + (4096*x1)), rmask, eviction_policy='evict_first', other=0.0)
        tmp9 = tl.load(in_ptr2 + (x0 + (2048*r2) + (200704*x1)), rmask, eviction_policy='evict_first', other=0.0)
        tmp2 = 49.0
        tmp3 = tmp1 / tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask, tmp8, _tmp7)
        tmp11 = tmp9 - tmp10
        tmp12 = tmp5 * tmp11
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(rmask, tmp15, _tmp14)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, None)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp14, None)
''')


# kernel path: /tmp/torchinductor_youkaichao/mf/cmf5t4e6xvm27nrgloapcstyjquqh3s4cbjtoiat2vpmbe6wdqsr.py
# Source Nodes: [], Original ATen: [aten.div, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_div_native_batch_norm_backward_threshold_backward_2 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[2048, 4],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_div_native_batch_norm_backward_threshold_backward_2', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 2048
    rnumel = 4
    RBLOCK: tl.constexpr = 4
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (2048*r1)), rmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, None)
''')


# kernel path: /tmp/torchinductor_youkaichao/k5/ck5xfolx5trg253cosfgkr7lq2zxghfuag74lhbxh4xes2wmutnz.py
# Source Nodes: [], Original ATen: [aten.div, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_div_native_batch_norm_backward_threshold_backward_3 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[2048, 4],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_div_native_batch_norm_backward_threshold_backward_3', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 2048
    rnumel = 4
    RBLOCK: tl.constexpr = 4
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (2048*r1)), rmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, None)
    tl.store(out_ptr0 + (x0), tmp4, None)
''')


# kernel path: /tmp/torchinductor_youkaichao/ex/cex74hvhpmvea2l3hcxqmbclqrk5qz4mytdyhqp7lvzenxmdvqir.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.div, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_div_native_batch_norm_backward_threshold_backward_4 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1048576], 
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_div_native_batch_norm_backward_threshold_backward_4', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 802816
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x3 = xindex
    x0 = xindex % 2048
    x2 = (xindex // 100352)
    tmp0 = tl.load(in_ptr0 + (x3), None).to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (x0 + (2048*x2)), None, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x3), None)
    tmp7 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr7 + (x0), None, eviction_policy='evict_last')
    tmp2 = 49.0
    tmp3 = tmp1 / tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.002551020408163265
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x3), tmp22, None)
''')


# kernel path: /tmp/torchinductor_youkaichao/47/c47d3dcnoaq4g7ht2ovob5ibnop32iq6xj6e3g6pwmqmfa4iu3dh.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_5 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_5', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp8 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (32928 + (49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp7 = tl.load(in_ptr2 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp3 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp12, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/77/c77f6xju6srybobrst6sxg3dhk4dgm6cjl46p4sc4a33kzp643vz.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_6 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 4],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_6', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 112
    rnumel = 4
    RBLOCK: tl.constexpr = 4
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (112*r1)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/33/c33chqev2klsftynulg2zrsrpgberzbak7bdwvfyzy66ewa6gyqw.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_7 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 4],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_7', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 112
    rnumel = 4
    RBLOCK: tl.constexpr = 4
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (112*r1)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ih/cihfg24cn2klzundf55mcuxax24xnaowbdhk6zdkew352otrg3h5.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_8 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_8', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (32928 + y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.002551020408163265
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/yb/cyb4vk5kdnjtspzcuvposuulr3jfnmmud56n5xfpmaorxgn6vjhi.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_9 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 512],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_9', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
    xnumel = 112
    XBLOCK: tl.constexpr = 1
    rnumel = 392
    RBLOCK: tl.constexpr = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r3 = rindex
    x0 = xindex
    r1 = rindex % 49
    r2 = (rindex // 49)
    tmp0 = tl.load(in_ptr0 + (x0 + (112*r3)), rmask & xmask).to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (27440 + r1 + (49*x0) + (43904*r2)), rmask & xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (r1 + (49*x0) + (5488*r2)), rmask & xmask, other=0.0)
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp6 = tl.broadcast_to(tmp5, [RBLOCK])
    tmp8 = tl.where(rmask & xmask, tmp6, 0)
    tmp9 = triton_helpers.promote_to_tensor(tl.sum(tmp8, 0))
    tl.store(out_ptr0 + (x0), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/3t/c3th43tnawoqytyjub4bc7doiy435uxklcgfszr7zlrddf3olzub.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_10 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_10', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    tmp7 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (27440 + (49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((49*x0) + (5488*(r2 // 49)) + (10976*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/cu/ccuqswk56bpu3iy4h6xks2nmt6qkayjiw46dexks5jfxcnymvck3.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_11 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_11', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (27440 + y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (49*x2) + (5488*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.002551020408163265
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zg/czgzcq7cklh7frojoo324funvw2rxpe22zvbfvwaqzoryd7mxvgx.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_12 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 512],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_12', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
    xnumel = 112
    XBLOCK: tl.constexpr = 1
    rnumel = 392
    RBLOCK: tl.constexpr = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r3 = rindex
    x0 = xindex
    r1 = rindex % 49
    r2 = (rindex // 49)
    tmp0 = tl.load(in_ptr0 + (x0 + (112*r3)), rmask & xmask).to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (21952 + r1 + (49*x0) + (43904*r2)), rmask & xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (r1 + (49*x0) + (5488*r2)), rmask & xmask, other=0.0)
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp6 = tl.broadcast_to(tmp5, [RBLOCK])
    tmp8 = tl.where(rmask & xmask, tmp6, 0)
    tmp9 = triton_helpers.promote_to_tensor(tl.sum(tmp8, 0))
    tl.store(out_ptr0 + (x0), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/c7/cc7gbx674wb5tpq7ava6cii7trbuhqatvhfvkcbkozyutf6sm5o5.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_13 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_13', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    tmp7 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (21952 + (49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((49*x0) + (5488*(r2 // 49)) + (10976*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7u/c7ufcxyigms2mtlpnfdh2tyodk5neqgqvfv4icehxfgilwl2lyxs.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_14 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_14', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (21952 + y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (49*x2) + (5488*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.002551020408163265
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rk/crkog5ml6wdhciwswjotc4yuaxpbjtumodve2oxbrisx6jzuiynm.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_15 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 512],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_15', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
    xnumel = 112
    XBLOCK: tl.constexpr = 1
    rnumel = 392
    RBLOCK: tl.constexpr = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r3 = rindex
    x0 = xindex
    r1 = rindex % 49
    r2 = (rindex // 49)
    tmp0 = tl.load(in_ptr0 + (x0 + (112*r3)), rmask & xmask).to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (16464 + r1 + (49*x0) + (43904*r2)), rmask & xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (r1 + (49*x0) + (5488*r2)), rmask & xmask, other=0.0)
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp6 = tl.broadcast_to(tmp5, [RBLOCK])
    tmp8 = tl.where(rmask & xmask, tmp6, 0)
    tmp9 = triton_helpers.promote_to_tensor(tl.sum(tmp8, 0))
    tl.store(out_ptr0 + (x0), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/pv/cpvavs6tidwcsra5kgo6qkcha4yaap6sftvlsw5yqnzq66tvxpyi.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_16 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_16', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    tmp7 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (16464 + (49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((49*x0) + (5488*(r2 // 49)) + (10976*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/mm/cmmutuqazpi3xvuiawle3r4pz7tk4kh2e4yoqgc6i56co7d4hc2q.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_17 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_17', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (16464 + y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (49*x2) + (5488*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.002551020408163265
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ey/ceyswccsvwpdp6xxvq7zsddsmdkjs6osycpyarxehhpbtk7pjke7.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_18 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 512],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_18', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
    xnumel = 112
    XBLOCK: tl.constexpr = 1
    rnumel = 392
    RBLOCK: tl.constexpr = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r3 = rindex
    x0 = xindex
    r1 = rindex % 49
    r2 = (rindex // 49)
    tmp0 = tl.load(in_ptr0 + (x0 + (112*r3)), rmask & xmask).to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (10976 + r1 + (49*x0) + (43904*r2)), rmask & xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (r1 + (49*x0) + (5488*r2)), rmask & xmask, other=0.0)
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp6 = tl.broadcast_to(tmp5, [RBLOCK])
    tmp8 = tl.where(rmask & xmask, tmp6, 0)
    tmp9 = triton_helpers.promote_to_tensor(tl.sum(tmp8, 0))
    tl.store(out_ptr0 + (x0), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/nh/cnhrtfxo5r2nbakd4spmha6fcncqrcnqg4suru6uvcsopxfkgouw.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_19 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_19', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    tmp7 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (10976 + (49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((49*x0) + (5488*(r2 // 49)) + (10976*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/sd/csd5oj3t4ckpnlfn62onamulqsx5qay5fepv3wqbsaieqkbvzy7t.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_20 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_20', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (10976 + y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (49*x2) + (5488*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.002551020408163265
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ah/cahq4t2wjoevghh5safcbi3adhb2unjcwjykjtus4kt7f7bvuelb.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_21 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 512],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_21', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
    xnumel = 112
    XBLOCK: tl.constexpr = 1
    rnumel = 392
    RBLOCK: tl.constexpr = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r3 = rindex
    x0 = xindex
    r1 = rindex % 49
    r2 = (rindex // 49)
    tmp0 = tl.load(in_ptr0 + (x0 + (112*r3)), rmask & xmask).to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (5488 + r1 + (49*x0) + (43904*r2)), rmask & xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (r1 + (49*x0) + (5488*r2)), rmask & xmask, other=0.0)
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp6 = tl.broadcast_to(tmp5, [RBLOCK])
    tmp8 = tl.where(rmask & xmask, tmp6, 0)
    tmp9 = triton_helpers.promote_to_tensor(tl.sum(tmp8, 0))
    tl.store(out_ptr0 + (x0), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/sy/csywtvkev7sjrfai2nfs7kuud5hjddmcer6gpiymwo52agdoy2rl.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_22 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_22', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    tmp7 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (5488 + (49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((49*x0) + (5488*(r2 // 49)) + (10976*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7k/c7kb4jownd52w7ucc5rnohe2j2dvcg3ecidorffglrhdyfinmfme.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_23 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_23', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (5488 + y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (49*x2) + (5488*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.002551020408163265
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/g4/cg4byb2mdvbjk5gvqskxb74whfw26turqcxk267ymrxgauahtdup.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_24 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 512],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_24', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel):
    xnumel = 112
    XBLOCK: tl.constexpr = 1
    rnumel = 392
    RBLOCK: tl.constexpr = 512
    xoffset = tl.program_id(0) * XBLOCK
    xindex = tl.full([1], xoffset, tl.int32)
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[:]
    rmask = rindex < rnumel
    r3 = rindex
    x0 = xindex
    r1 = rindex % 49
    r2 = (rindex // 49)
    tmp0 = tl.load(in_ptr0 + (x0 + (112*r3)), rmask & xmask).to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (r1 + (49*x0) + (43904*r2)), rmask & xmask, other=0.0)
    tmp2 = tl.load(in_ptr2 + (r1 + (49*x0) + (5488*r2)), rmask & xmask, other=0.0)
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp6 = tl.broadcast_to(tmp5, [RBLOCK])
    tmp8 = tl.where(rmask & xmask, tmp6, 0)
    tmp9 = triton_helpers.promote_to_tensor(tl.sum(tmp8, 0))
    tl.store(out_ptr0 + (x0), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/73/c73kqhkdop2xdxmybmxcog6pog2pi4ar6snyuyjdgl2wgre4ar45.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_25 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_25', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    tmp7 = tl.load(in_ptr4 + (x0), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((49*x0) + (5488*(r2 // 49)) + (10976*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/52/c522kfa5zs5nsxb5q4h74mzc6tw4dfxwkym7vxnsy3hxaasg75bp.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_26 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_26', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (49*x2) + (5488*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.002551020408163265
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/m7/cm7b7gbclxtzc333fe2fozac22tm4ws5acyeh5swzw53dex4srpn.py
# Source Nodes: [], Original ATen: [aten.cat]

triton_poi_fused_cat_27 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[65536], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_27', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 43904
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = xindex % 5488
    x1 = (xindex // 5488)
    tmp0 = tl.load(in_ptr0 + (x2), xmask)
    tl.store(out_ptr0 + (x0 + (43904*x1)), tmp0, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ke/cketply45lis5wpcrhbos2stcufdlotwsupqbmxstl3niz4lp5qx.py
# Source Nodes: [], Original ATen: [aten.cat]

triton_poi_fused_cat_28 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[65536], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_28', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 43904
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 5488
    x1 = (xindex // 5488)
    tmp0 = tl.load(in_ptr0 + (38416 + x0 + (43904*x1)), xmask)
    tl.store(out_ptr0 + (x0 + (43904*x1)), tmp0, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ex/cexopm77nhmgrclr4pogfbjnqjf6bu7gy42xjstlwhtgllpj2nwb.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_29 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_29', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 3584
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 896
    x1 = (xindex // 896)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp8 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (896*r2) + (87808*x1)), rmask & xmask, eviction_policy='evict_first').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp7 = tl.load(in_ptr2 + (x0 + (896*r2) + (87808*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp3 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp12, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/fu/cfueddzt6liigizhuqcgbcobutptgiprg7hpryfskcbfnthwjurl.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_30 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 4],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_30', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 896
    rnumel = 4
    RBLOCK: tl.constexpr = 4
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (896*r1)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/dl/cdl7wpgsrreh6eziv4rerta3g2ig37rrzlz5akcdrnccrntlo44m.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_31 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 4],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_31', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 896
    rnumel = 4
    RBLOCK: tl.constexpr = 4
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (896*r1)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7o/c7ot5n7l6xj6qgidzisnhdmfina4bclqnvsl7vjbfocj6tcv4qrt.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_32 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_32', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 896
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (896*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (896*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.002551020408163265
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (896*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/nq/cnqre4uq6esfsjatl4whulu53zex5ji6uoznycaafoejl4bdqadl.py
# Source Nodes: [], Original ATen: [aten.add, aten.div, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_div_native_batch_norm_backward_threshold_backward_33 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*i1', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_div_native_batch_norm_backward_threshold_backward_33', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 8192
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 2048
    x1 = (xindex // 2048)
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp15 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    _tmp19 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (2048*r2) + (200704*x1)), rmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.load(in_ptr1 + (x0 + (2048*r2) + (200704*x1)), rmask, eviction_policy='evict_first').to(tl.int1)
        tmp4 = tl.load(in_ptr2 + (x0 + (2048*(r2 // 49)) + (4096*x1)), rmask, eviction_policy='evict_first', other=0.0)
        tmp8 = tl.load(in_ptr3 + ((49*x0) + (100352*(r2 // 49)) + (200704*x1) + (r2 % 49)), rmask, eviction_policy='evict_first', other=0.0)
        tmp14 = tl.load(in_ptr4 + (x0 + (2048*r2) + (200704*x1)), rmask, eviction_policy='evict_first', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp5 = 49.0
        tmp6 = tmp4 / tmp5
        tmp7 = tl.where(tmp3, tmp1, tmp6)
        tmp9 = tmp7 + tmp8
        tmp10 = tl.where(tmp2, tmp1, tmp9)
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask, tmp13, _tmp12)
        tmp16 = tmp14 - tmp15
        tmp17 = tmp10 * tmp16
        tmp18 = tl.broadcast_to(tmp17, [XBLOCK, RBLOCK])
        tmp20 = _tmp19 + tmp18
        _tmp19 = tl.where(rmask, tmp20, _tmp19)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp12, None)
    tmp19 = tl.sum(_tmp19, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp19, None)
''')


# kernel path: /tmp/torchinductor_youkaichao/ur/curcx5wb67jc7jcehhwtbrrcezaxqmpznhmox6tosfup4nnvcygi.py
# Source Nodes: [], Original ATen: [aten.add, aten.convolution_backward, aten.div, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_convolution_backward_div_native_batch_norm_backward_threshold_backward_34 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 2048], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: 'i32', 12: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(11, 12))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_convolution_backward_div_native_batch_norm_backward_threshold_backward_34', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 2048
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = (yindex // 49)
    y0 = yindex % 49
    tmp0 = tl.load(in_ptr0 + (x2 + (2048*y3)), ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (x2 + (2048*y3)), ymask, eviction_policy='evict_last').to(tl.int1)
    tmp4 = tl.load(in_ptr2 + (x2 + (2048*y1)), ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr3 + (y0 + (49*x2) + (100352*y1)), ymask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr4 + (x2 + (2048*y3)), ymask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), None, eviction_policy='evict_last')
    tmp14 = tl.load(in_ptr6 + (x2), None, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), None, eviction_policy='evict_last')
    tmp22 = tl.load(in_ptr8 + (x2), None, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr9 + (x2), None, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp5 = 49.0
    tmp6 = tmp4 / tmp5
    tmp7 = tl.where(tmp3, tmp1, tmp6)
    tmp9 = tmp7 + tmp8
    tmp10 = tl.where(tmp2, tmp1, tmp9)
    tmp13 = tmp11 - tmp12
    tmp15 = 0.002551020408163265
    tmp16 = tmp14 * tmp15
    tmp18 = tmp17 * tmp17
    tmp19 = tmp16 * tmp18
    tmp20 = tmp13 * tmp19
    tmp21 = tmp10 - tmp20
    tmp23 = tmp22 * tmp15
    tmp24 = tmp21 - tmp23
    tmp26 = tmp17 * tmp25
    tmp27 = tmp24 * tmp26
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (2048*y3)), tmp27, ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/bm/cbmooo2lypn36xjy4z5gzr6xg4su5fczjicqx6wabsswxr2jpvvq.py
# Source Nodes: [], Original ATen: [aten.add, aten.div, aten.threshold_backward]

triton_poi_fused_add_div_threshold_backward_35 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 2048], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*i1', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7, 8))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_div_threshold_backward_35', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 2048
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y1 = (yindex // 49)
    y0 = yindex % 49
    tmp0 = tl.load(in_ptr0 + (x2 + (2048*y3)), ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (x2 + (2048*y3)), ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr2 + (x2 + (2048*y3)), ymask, eviction_policy='evict_last').to(tl.int1)
    tmp6 = tl.load(in_ptr3 + (x2 + (2048*y1)), ymask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr4 + (y0 + (49*x2) + (100352*y1)), ymask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr5 + (y0 + (49*x2) + (100352*y1)), ymask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tmp3 <= tmp1
    tmp7 = 49.0
    tmp8 = tmp6 / tmp7
    tmp9 = tl.where(tmp5, tmp1, tmp8)
    tmp11 = tmp9 + tmp10
    tmp12 = tl.where(tmp4, tmp1, tmp11)
    tmp14 = tmp12 + tmp13
    tmp15 = tl.where(tmp2, tmp1, tmp14)
    tl.store(out_ptr0 + (x2 + (2048*y3)), tmp15, ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/s3/cs3wiarwkbxyjki5w66ptpyxwkpzf6w4cavxsdjntmwcivdlqohl.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_36 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_36', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, out_ptr2, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 8192
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 2048
    x1 = (xindex // 2048)
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp5 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    tmp12 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    _tmp16 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (2048*r2) + (200704*x1)), rmask, eviction_policy='evict_first', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x0 + (2048*r2) + (200704*x1)), rmask, eviction_policy='evict_first', other=0.0)
        tmp11 = tl.load(in_ptr3 + (x0 + (2048*r2) + (200704*x1)), rmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask, tmp3, _tmp2)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp0 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask, tmp10, _tmp9)
        tmp13 = tmp11 - tmp12
        tmp14 = tmp0 * tmp13
        tmp15 = tl.broadcast_to(tmp14, [XBLOCK, RBLOCK])
        tmp17 = _tmp16 + tmp15
        _tmp16 = tl.where(rmask, tmp17, _tmp16)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp2, None)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp9, None)
    tmp16 = tl.sum(_tmp16, 1)[:, None]
    tl.store(out_ptr2 + (x3), tmp16, None)
''')


# kernel path: /tmp/torchinductor_youkaichao/i3/ci32bhhy3avkv4gfpbgrvuvaxqqd74jff7zpxu3cmcdzcup6fprf.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_37 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1048576], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(14,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_37', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, out_ptr0, out_ptr1, xnumel, XBLOCK : tl.constexpr):
    xnumel = 802816
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = xindex % 2048
    tmp0 = tl.load(in_ptr0 + (x2), None)
    tmp1 = tl.load(in_ptr1 + (x2), None)
    tmp2 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), None)
    tmp19 = tl.load(in_ptr8 + (x0), None, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr9 + (x0), None, eviction_policy='evict_last')
    tmp23 = tl.load(in_ptr10 + (x0), None, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr11 + (x0), None, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.002551020408163265
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tmp20 = tmp18 - tmp19
    tmp22 = tmp21 * tmp5
    tmp24 = tmp23 * tmp23
    tmp25 = tmp22 * tmp24
    tmp26 = tmp20 * tmp25
    tmp27 = tmp0 - tmp26
    tmp28 = tmp27 - tmp13
    tmp30 = tmp23 * tmp29
    tmp31 = tmp28 * tmp30
    tl.store(out_ptr0 + (x2), tmp17, None)
    tl.store(out_ptr1 + (x2), tmp31, None)
''')


# kernel path: /tmp/torchinductor_youkaichao/3b/c3bijxznvg3xpc7j75dm5misccf4625ysk7fwelmxmubibkntsjv.py
# Source Nodes: [], Original ATen: [aten.avg_pool2d_backward]

triton_poi_fused_avg_pool2d_backward_38 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[262144], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_avg_pool2d_backward_38', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 175616
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 14
    x1 = (xindex // 14) % 14
    x2 = (xindex // 196) % 112
    x3 = (xindex // 21952)
    x6 = xindex % 21952
    tmp0 = tl.load(in_ptr0 + (38416 + (7*(tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(7, 1 + ((1 + x1) // 2)))))) + (7*(tl.where((tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(7, 1 + ((1 + x1) // 2))))) >= 0, 0, 7))) + (49*x2) + (43904*x3) + (tl.math.min(tl.math.max(0, (x0 // 2)), (-1) + (tl.math.min(7, 1 + ((1 + x0) // 2))))) + (tl.where((tl.math.min(tl.math.max(0, (x0 // 2)), (-1) + (tl.math.min(7, 1 + ((1 + x0) // 2))))) >= 0, 0, 7))), xmask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr0 + (38416 + (7*(tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(7, 1 + ((1 + x1) // 2)))))) + (7*(tl.where((tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(7, 1 + ((1 + x1) // 2))))) >= 0, 0, 7))) + (49*x2) + (43904*x3) + (tl.math.min(1 + (tl.math.max(0, (x0 // 2))), (-1) + (tl.math.min(7, 1 + ((1 + x0) // 2))))) + (tl.where((tl.math.min(1 + (tl.math.max(0, (x0 // 2))), (-1) + (tl.math.min(7, 1 + ((1 + x0) // 2))))) >= 0, 0, 7))), xmask)
    tmp18 = tl.load(in_ptr0 + (38416 + (7*(tl.math.min(1 + (tl.math.max(0, (x1 // 2))), (-1) + (tl.math.min(7, 1 + ((1 + x1) // 2)))))) + (7*(tl.where((tl.math.min(1 + (tl.math.max(0, (x1 // 2))), (-1) + (tl.math.min(7, 1 + ((1 + x1) // 2))))) >= 0, 0, 7))) + (49*x2) + (43904*x3) + (tl.math.min(tl.math.max(0, (x0 // 2)), (-1) + (tl.math.min(7, 1 + ((1 + x0) // 2))))) + (tl.where((tl.math.min(tl.math.max(0, (x0 // 2)), (-1) + (tl.math.min(7, 1 + ((1 + x0) // 2))))) >= 0, 0, 7))), xmask, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr0 + (38416 + (7*(tl.math.min(1 + (tl.math.max(0, (x1 // 2))), (-1) + (tl.math.min(7, 1 + ((1 + x1) // 2)))))) + (7*(tl.where((tl.math.min(1 + (tl.math.max(0, (x1 // 2))), (-1) + (tl.math.min(7, 1 + ((1 + x1) // 2))))) >= 0, 0, 7))) + (49*x2) + (43904*x3) + (tl.math.min(1 + (tl.math.max(0, (x0 // 2))), (-1) + (tl.math.min(7, 1 + ((1 + x0) // 2))))) + (tl.where((tl.math.min(1 + (tl.math.max(0, (x0 // 2))), (-1) + (tl.math.min(7, 1 + ((1 + x0) // 2))))) >= 0, 0, 7))), xmask)
    tmp1 = tmp0 / 9
    tmp2 = tl.math.max(0, (x1 // 2))
    tmp3 = tl.math.min(7, 1 + ((1 + x1) // 2))
    tmp4 = tmp2 < tmp3
    tmp5 = tl.math.max(0, (x0 // 2))
    tmp6 = tl.math.min(7, 1 + ((1 + x0) // 2))
    tmp7 = tmp5 < tmp6
    tmp8 = tmp4 & tmp7
    tmp9 = 0.0
    tmp10 = tl.where(tmp8, tmp1, tmp9)
    tmp12 = tmp11 / 9
    tmp13 = 1 + (tl.math.max(0, (x0 // 2)))
    tmp14 = tmp13 < tmp6
    tmp15 = tmp4 & tmp14
    tmp16 = tmp10 + tmp12
    tmp17 = tl.where(tmp15, tmp16, tmp10)
    tmp19 = tmp18 / 9
    tmp20 = 1 + (tl.math.max(0, (x1 // 2)))
    tmp21 = tmp20 < tmp3
    tmp22 = tmp21 & tmp7
    tmp23 = tmp17 + tmp19
    tmp24 = tl.where(tmp22, tmp23, tmp17)
    tmp26 = tmp25 / 9
    tmp27 = tmp21 & tmp14
    tmp28 = tmp24 + tmp26
    tmp29 = tl.where(tmp27, tmp28, tmp24)
    tl.store(out_ptr0 + (x6 + (175616*x3)), tmp29, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/pi/cpidqow7wucb5e3zghc7vlij63uf4assk5zd6i6vtah4dsufqn32.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_39 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_39', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp8 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (27440 + (49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp7 = tl.load(in_ptr2 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp3 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp12, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/s3/cs32aco2m6ibhxkphjdkzsmancjqzd2npu7er7ywyjb2svblql5l.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_40 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_40', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (27440 + y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.002551020408163265
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/k5/ck5dchgtknvgswb4dxfo6ha6d5z7cfyx3ix2wg2dpxms3rr3y3kg.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_41 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_41', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp8 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (21952 + (49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp7 = tl.load(in_ptr2 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp3 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp12, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/m2/cm2x5g5o2d44asuvmybfmjkxlkrf5ahghk4zybo7wnnar5woztn5.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_42 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_42', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (21952 + y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.002551020408163265
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ff/cff2i765skwvnwp65wgyko6lexesl5uu5m4pysxozccgxqr2rooz.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_43 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_43', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp8 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (16464 + (49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp7 = tl.load(in_ptr2 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp3 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp12, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/xm/cxmgn7sgdaqwz2xq7outp6y4duqzizdsgm33xmbpp2mmnahxhtsp.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_44 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_44', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (16464 + y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.002551020408163265
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/z3/cz3phpnut6clmew42rnyffkw5mkhmjdfnngiyy4nwqjjj3kgus2b.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_45 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_45', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp8 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (10976 + (49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp7 = tl.load(in_ptr2 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp3 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp12, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/fh/cfhrrioud3me2nxz66eoo4vv7kutzwcapux4rjwfpzmqwrzsyuew.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_46 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_46', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (10976 + y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.002551020408163265
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ox/coxmuw4rgbqjpjc4gccjmggaxxdwtskm2yu2c6fype6mokpwzu7i.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_47 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_47', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp8 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (5488 + (49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp7 = tl.load(in_ptr2 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp3 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp12, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/hr/chrzx2v4i2l7t27gbcsehc6jmxkeme36x64vzfo3zy3mdzk2xglc.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_48 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_48', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (5488 + y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.002551020408163265
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/tl/ctlujmb43fmvknqcncqzrndyrwjlw3kby2fcvi34bjb6ksiup5tn.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_49 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_49', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 98
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp8 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((49*x0) + (43904*(r2 // 49)) + (87808*x1) + (r2 % 49)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp7 = tl.load(in_ptr2 + (x0 + (112*r2) + (10976*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp3 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp12, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/co/ccogxofu6gty2gdcbgb2otxyd5ktqra2re6y2fyxl3437kbsndhk.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_50 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[512, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_50', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 392
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 49
    y1 = (yindex // 49)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (49*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.002551020408163265
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/6e/c6etx4k2tlvllc3aiew4em35zcx5n4ejncpobogs233gyzjd4l2k.py
# Source Nodes: [], Original ATen: [aten.cat]

triton_poi_fused_cat_51 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[262144], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_51', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 175616
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = xindex % 21952
    x1 = (xindex // 21952)
    tmp0 = tl.load(in_ptr0 + (x2), xmask)
    tl.store(out_ptr0 + (x0 + (175616*x1)), tmp0, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/cv/ccvde6tn2gp4m53pclofpkhl2waqxman4iydyidart5jqirmxaeh.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_52 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_52', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 11648
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp10 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (896*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + ((196*x1) + (175616*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.full(tmp6.shape, 0, tmp6.dtype)
        tmp8 = tl.where(tmp2, tmp6, tmp7)
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, RBLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(rmask & xmask, tmp11, _tmp10)
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp10, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/xe/cxeztjgvyf6tlk3rmhizpj2oqhjohlshni6ynkoks73vznvpfqvy.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_53 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 16],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_53', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 896
    rnumel = 13
    RBLOCK: tl.constexpr = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (13*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zi/czipljbqikq675wwdivdoupcgsjamwd77kihyydnxbupvrozar3a.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_54 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_54', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 11648
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 896)
    x0 = xindex % 896
    _tmp14 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x1)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (896*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + ((196*x0) + (175616*(((r2 + (121*x1)) // 196) % 8)) + ((r2 + (121*x1)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.load(in_ptr2 + (x0 + (896*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.full(tmp10.shape, 0, tmp10.dtype)
        tmp12 = tl.where(tmp2, tmp10, tmp11)
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(rmask & xmask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp14, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/cg/ccgbrqpccfwy3ntussjfa3qo5eaik7i3g2g6ogslr44az2t75t25.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_55 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 16],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_55', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 896
    rnumel = 13
    RBLOCK: tl.constexpr = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (896*r1)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/wz/cwzuomc5jno6qzvkwhdywcjcwlpm3eo6ftbwvvdvicbsefpna7w3.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_56 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_56', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 896
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (896*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (196*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (896*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.0006377551020408163
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (896*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/nv/cnvzpwexbvxv3oedl7hmbax7dabboj55olihofxvg3z6ia4cs7hp.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_57 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 2048],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_57', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1024
    rnumel = 1568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp8 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 196
        r2 = (rindex // 196)
        tmp0 = tl.load(in_ptr0 + (x0 + (1024*r3)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr1 + (r1 + (196*x0) + (200704*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (r1 + (196*x0) + (200704*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp5 = tmp3 + tmp4
        tmp6 = tl.where(tmp2, tmp1, tmp5)
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(rmask & xmask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp8, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/tf/ctfnldfkygnonmhdbvtbzduw5zb46nqcsdwbpyy6yvow5qxwib6b.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_58 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_58', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 13312
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp17 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (1024*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = 0.0
        tmp5 = tmp3 <= tmp4
        tmp6 = tl.load(in_ptr1 + ((196*x1) + (200704*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.load(in_ptr2 + ((196*x1) + (200704*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tmp6 + tmp7
        tmp9 = tl.where(tmp5, tmp4, tmp8)
        tmp10 = tl.load(in_ptr3 + (x1 + (1024*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp11 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp12 = tmp10 - tmp11
        tmp13 = tmp9 * tmp12
        tmp14 = tl.full(tmp13.shape, 0, tmp13.dtype)
        tmp15 = tl.where(tmp2, tmp13, tmp14)
        tmp16 = tl.broadcast_to(tmp15, [XBLOCK, RBLOCK])
        tmp18 = _tmp17 + tmp16
        _tmp17 = tl.where(rmask & xmask, tmp18, _tmp17)
    tmp17 = tl.sum(_tmp17, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp17, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/tp/ctpq5cvnvwnuabqsv66n3q5vcbzz2phicclmq4c63zwkkpia266a.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_59 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[1024, 16],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_59', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 1024
    rnumel = 13
    RBLOCK: tl.constexpr = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (13*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/gl/cglz45cmnh5flx5upcjcz4aqawi6ugepfky5vzwhhigvxlqm3a2z.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_60 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_60', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 1024
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (1024*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (196*x2) + (200704*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0 + (196*x2) + (200704*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (x2 + (1024*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp5 = tmp3 + tmp4
    tmp6 = tl.where(tmp2, tmp1, tmp5)
    tmp9 = tmp7 - tmp8
    tmp11 = 0.0006377551020408163
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp6 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tl.store(out_ptr0 + (x2 + (1024*y3)), tmp23, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/vl/cvlm43432i4acjc64yetzf4bajuzdcvaz6uf2y43cbumkks2ktu7.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_61 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_61', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp10 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (65856 + (196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.full(tmp6.shape, 0, tmp6.dtype)
        tmp8 = tl.where(tmp2, tmp6, tmp7)
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, RBLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(rmask & xmask, tmp11, _tmp10)
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp10, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ts/cts5qgr54vjeq4phtt2zppogdubjefvgg54tko6imjp5q4atvyef.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_62 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[64, 16],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_62', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 13
    RBLOCK: tl.constexpr = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (13*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ov/covuyqpdjm2yxo4fhed3fvr2gjcpwk32au6h7anmhdfi4f3lsqxc.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_63 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_63', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 56)
    x0 = xindex % 56
    _tmp14 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x1)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (65856 + (196*x0) + (87808*(((r2 + (121*x1)) // 196) % 8)) + ((r2 + (121*x1)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.load(in_ptr2 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.full(tmp10.shape, 0, tmp10.dtype)
        tmp12 = tl.where(tmp2, tmp10, tmp11)
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(rmask & xmask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp14, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ei/ceio5z3wm4rga7las6sweyuqgsgitv6c7t6b4nqehc647gacp457.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_64 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[64, 16],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_64', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 13
    RBLOCK: tl.constexpr = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (56*r1)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rr/crrr3wdtf73rm43l5y5fmucf4txrr2omjlmnkqmnabtpqfkopsqa.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_65 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_65', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (65856 + y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.0006377551020408163
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/wd/cwdjjvdwyavzbmk3csiceu44ku5u6zga3n4hdyhoieijbwp5hj4r.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_66 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 2048],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_66', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 1568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 196
        r2 = (rindex // 196)
        tmp0 = tl.load(in_ptr0 + (x0 + (56*r3)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (54880 + r1 + (196*x0) + (87808*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (196*x0) + (10976*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ln/clnxp7fejzwm3puzsxvro3vzlvxgmmceztwioopl5mt7w76ocgsg.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_67 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_67', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp16 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (54880 + (196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + ((196*x1) + (10976*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 + tmp5
        tmp7 = 0.0
        tmp8 = tl.where(tmp3, tmp7, tmp6)
        tmp9 = tl.load(in_ptr3 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp11 = tmp9 - tmp10
        tmp12 = tmp8 * tmp11
        tmp13 = tl.full(tmp12.shape, 0, tmp12.dtype)
        tmp14 = tl.where(tmp2, tmp12, tmp13)
        tmp15 = tl.broadcast_to(tmp14, [XBLOCK, RBLOCK])
        tmp17 = _tmp16 + tmp15
        _tmp16 = tl.where(rmask & xmask, tmp17, _tmp16)
    tmp16 = tl.sum(_tmp16, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp16, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/cn/ccn5nacryctpaoca7vclb5s7rcpfvx7wtqzcmyy3jpevcpiovrnc.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_68 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[64, 16],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_68', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 13
    RBLOCK: tl.constexpr = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (13*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/25/c25tmvegdgh7k4rumwa7pkfb6d3xmbkv6k2mjdracsaay2gzh3vq.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_69 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_69', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (54880 + y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (196*x2) + (10976*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.0006377551020408163
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/2f/c2fpwezz42dh5ewolpofacm32q7op543hc6sh2n7yc4lbzcw7asv.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_70 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 2048],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_70', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 1568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 196
        r2 = (rindex // 196)
        tmp0 = tl.load(in_ptr0 + (x0 + (56*r3)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (43904 + r1 + (196*x0) + (87808*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (196*x0) + (10976*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/s7/cs7rp6fxoa7icf5hndv2dkvdi5qepk43rkkvboadtg4v7hr4w6gz.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_71 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_71', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp16 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (43904 + (196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + ((196*x1) + (10976*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 + tmp5
        tmp7 = 0.0
        tmp8 = tl.where(tmp3, tmp7, tmp6)
        tmp9 = tl.load(in_ptr3 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp11 = tmp9 - tmp10
        tmp12 = tmp8 * tmp11
        tmp13 = tl.full(tmp12.shape, 0, tmp12.dtype)
        tmp14 = tl.where(tmp2, tmp12, tmp13)
        tmp15 = tl.broadcast_to(tmp14, [XBLOCK, RBLOCK])
        tmp17 = _tmp16 + tmp15
        _tmp16 = tl.where(rmask & xmask, tmp17, _tmp16)
    tmp16 = tl.sum(_tmp16, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp16, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/vw/cvwzyzf5tkhpdeolezmqamaviktesweehq7jvqptjgsfp6qkrs7x.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_72 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_72', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (43904 + y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (196*x2) + (10976*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.0006377551020408163
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/mt/cmtm4ayhm6tgrb5ury6flw2tarxaquvcmmmfsolfaee6muwcuqda.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_73 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 2048],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_73', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 1568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 196
        r2 = (rindex // 196)
        tmp0 = tl.load(in_ptr0 + (x0 + (56*r3)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (32928 + r1 + (196*x0) + (87808*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (196*x0) + (10976*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/z4/cz42w24hputomq24qv5ijctfvim4oakodwpmbxgtiydk4vsp7wgd.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_74 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_74', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp16 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (32928 + (196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + ((196*x1) + (10976*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 + tmp5
        tmp7 = 0.0
        tmp8 = tl.where(tmp3, tmp7, tmp6)
        tmp9 = tl.load(in_ptr3 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp11 = tmp9 - tmp10
        tmp12 = tmp8 * tmp11
        tmp13 = tl.full(tmp12.shape, 0, tmp12.dtype)
        tmp14 = tl.where(tmp2, tmp12, tmp13)
        tmp15 = tl.broadcast_to(tmp14, [XBLOCK, RBLOCK])
        tmp17 = _tmp16 + tmp15
        _tmp16 = tl.where(rmask & xmask, tmp17, _tmp16)
    tmp16 = tl.sum(_tmp16, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp16, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zp/czpxrbnxikhzm6yl6ckqexswioqqotgqkc5ig2cg4xd2bfzm3umi.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_75 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_75', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (32928 + y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (196*x2) + (10976*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.0006377551020408163
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/4v/c4vnc3uasbpm2twgcorx2yygw6wmxmcfnjmch3eo5yvc47ev2wbw.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_76 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 2048],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_76', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 1568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 196
        r2 = (rindex // 196)
        tmp0 = tl.load(in_ptr0 + (x0 + (56*r3)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (21952 + r1 + (196*x0) + (87808*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (196*x0) + (10976*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/m2/cm2z5miembi34dr3z442r4p4jbl266bdlttog36o367xdkamwfan.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_77 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_77', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp16 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (21952 + (196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + ((196*x1) + (10976*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 + tmp5
        tmp7 = 0.0
        tmp8 = tl.where(tmp3, tmp7, tmp6)
        tmp9 = tl.load(in_ptr3 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp11 = tmp9 - tmp10
        tmp12 = tmp8 * tmp11
        tmp13 = tl.full(tmp12.shape, 0, tmp12.dtype)
        tmp14 = tl.where(tmp2, tmp12, tmp13)
        tmp15 = tl.broadcast_to(tmp14, [XBLOCK, RBLOCK])
        tmp17 = _tmp16 + tmp15
        _tmp16 = tl.where(rmask & xmask, tmp17, _tmp16)
    tmp16 = tl.sum(_tmp16, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp16, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/er/cer62fazb654cjwsptbnx7qy5z35fhcxvf6sbo6rkh4eggr62lrr.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_78 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_78', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (21952 + y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (196*x2) + (10976*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.0006377551020408163
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/n5/cn5anaxf4s5b7mx3aclq6lahkrwhfnhi3qw7t2av4jbb432mnd3s.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_79 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 2048],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_79', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 1568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 196
        r2 = (rindex // 196)
        tmp0 = tl.load(in_ptr0 + (x0 + (56*r3)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (10976 + r1 + (196*x0) + (87808*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (196*x0) + (10976*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ls/clsnedzeaynaxrc5wjs7zrvzmrxwfizbflktzcdvgxg6iww5bjvl.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_80 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_80', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp16 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (10976 + (196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + ((196*x1) + (10976*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 + tmp5
        tmp7 = 0.0
        tmp8 = tl.where(tmp3, tmp7, tmp6)
        tmp9 = tl.load(in_ptr3 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp11 = tmp9 - tmp10
        tmp12 = tmp8 * tmp11
        tmp13 = tl.full(tmp12.shape, 0, tmp12.dtype)
        tmp14 = tl.where(tmp2, tmp12, tmp13)
        tmp15 = tl.broadcast_to(tmp14, [XBLOCK, RBLOCK])
        tmp17 = _tmp16 + tmp15
        _tmp16 = tl.where(rmask & xmask, tmp17, _tmp16)
    tmp16 = tl.sum(_tmp16, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp16, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/bi/cbizrjauiygn3pvf75ungmdploms3ffh2ly6yfozcq6wtkbej6qu.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_81 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_81', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (10976 + y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (196*x2) + (10976*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.0006377551020408163
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/e4/ce4bq6pfd5jvunpdexfwvxrh2xn2gjzoxt3vaixywehpkbv7i2ik.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_82 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 2048],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_82', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 1568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 196
        r2 = (rindex // 196)
        tmp0 = tl.load(in_ptr0 + (x0 + (56*r3)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (r1 + (196*x0) + (87808*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (196*x0) + (10976*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/pw/cpw27sngxay2nz5s522gdvegreaq3gpue5of2qziqloejklicatu.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_83 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_83', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp16 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + ((196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + ((196*x1) + (10976*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 + tmp5
        tmp7 = 0.0
        tmp8 = tl.where(tmp3, tmp7, tmp6)
        tmp9 = tl.load(in_ptr3 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp10 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp11 = tmp9 - tmp10
        tmp12 = tmp8 * tmp11
        tmp13 = tl.full(tmp12.shape, 0, tmp12.dtype)
        tmp14 = tl.where(tmp2, tmp12, tmp13)
        tmp15 = tl.broadcast_to(tmp14, [XBLOCK, RBLOCK])
        tmp17 = _tmp16 + tmp15
        _tmp16 = tl.where(rmask & xmask, tmp17, _tmp16)
    tmp16 = tl.sum(_tmp16, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp16, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rj/crjwwihxqmni6akn7lpl4qzil5rwywz3bbho7nyolyahmf34snzs.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_84 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_84', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (196*x2) + (10976*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.0006377551020408163
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/b4/cb4yxbdm7az4tjlx5bdyp3ca252b7lx46d2ld4lkbkf4cpz2h5uo.py
# Source Nodes: [], Original ATen: [aten.cat]

triton_poi_fused_cat_85 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[131072], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_85', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 87808
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = xindex % 10976
    x1 = (xindex // 10976)
    tmp0 = tl.load(in_ptr0 + (x2), xmask)
    tl.store(out_ptr0 + (x0 + (87808*x1)), tmp0, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/pt/cpt55zkleli3t7sohrl35znsu2tdhdlln2hp6yw3puhaesvkryaj.py
# Source Nodes: [], Original ATen: [aten.cat]

triton_poi_fused_cat_86 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[131072], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_86', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 87808
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 10976
    x1 = (xindex // 10976)
    tmp0 = tl.load(in_ptr0 + (76832 + x0 + (87808*x1)), xmask)
    tl.store(out_ptr0 + (x0 + (87808*x1)), tmp0, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rb/crb46exbal7tsolqdmhhnocfxc5d5jtpikcubwdo3y7g35nsoxr3.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_87 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_87', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 5824
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp10 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (448*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + ((196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.full(tmp6.shape, 0, tmp6.dtype)
        tmp8 = tl.where(tmp2, tmp6, tmp7)
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, RBLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(rmask & xmask, tmp11, _tmp10)
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp10, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/gi/cginqhubcsi6cws6a2t7sfu4eksblq2npyhvztxs5vkx64akrrdt.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_88 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[512, 16],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_88', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 13
    RBLOCK: tl.constexpr = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (13*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/m4/cm4lukrrakmtosf647flcq2snuvuvg42jwhelamteyxl4l7fsuzi.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_89 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[8192, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_89', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 5824
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 448)
    x0 = xindex % 448
    _tmp14 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x1)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (448*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + ((196*x0) + (87808*(((r2 + (121*x1)) // 196) % 8)) + ((r2 + (121*x1)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.load(in_ptr2 + (x0 + (448*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.full(tmp10.shape, 0, tmp10.dtype)
        tmp12 = tl.where(tmp2, tmp10, tmp11)
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(rmask & xmask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp14, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ue/cuevgisqz6nm3syuwfehvjdvz32qhya7dxiratndvf7a46aov7zp.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_90 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[512, 16],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_90', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 13
    RBLOCK: tl.constexpr = 16
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (448*r1)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/p2/cp2tp2ch7t73bhhj4glvojzcegctfwwlfkyyuhldwdgioyi55xs6.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_91 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_91', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 448
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (448*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (448*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.0006377551020408163
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (448*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/dr/cdrm64gm6yr7hq4bcdldu7iavezlhuakq37cn3b3pxqfxgqoejev.py
# Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]

triton_poi_fused_add_threshold_backward_92 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_threshold_backward_92', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 8192
    xnumel = 196
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 1024
    y1 = (yindex // 1024)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (1024*x2) + (200704*y1)), xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (1024*x2) + (200704*y1)), xmask, eviction_policy='evict_last')
    tmp5 = tl.load(in_out_ptr0 + (x2 + (196*y3)), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x2 + (196*y3)), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x2 + (196*y3)), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tmp3 <= tmp1
    tmp7 = tmp5 + tmp6
    tmp8 = tl.where(tmp4, tmp1, tmp7)
    tmp10 = tmp8 + tmp9
    tmp11 = tl.where(tmp2, tmp1, tmp10)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (196*y3)), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/do/cdobh5tnjoaged3mdgqxr5zdjxh4lae3zdz6zps4lcig4vjo7t4x.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_93 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 2048],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_93', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1024
    rnumel = 1568
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex % 196
        r2 = (rindex // 196)
        tmp0 = tl.load(in_ptr0 + (r1 + (196*x0) + (200704*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/v3/cv3foczffieonzhdblrbuvrd5qnueaw6o63kzwzqgayfesfvsy62.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_94 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_94', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 13312
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((196*x1) + (200704*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (1024*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5h/c5hkchhig2wj4mj7kiu5ms52jz6bzmwmnuiypfnolzbq45gfix4o.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_95 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_95', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 1024
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (196*x2) + (200704*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (1024*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0006377551020408163
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (1024*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ew/cewcjgsepdlmktul52q7vuxjdsu6rz2xfpjx5f63eydpp5tttuu6.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_96 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_96', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 13312
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    _tmp20 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + ((196*x1) + (200704*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr1 + (x1 + (1024*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = tl.load(in_ptr2 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.full(tmp7.shape, 0, tmp7.dtype)
        tmp9 = tl.where(tmp2, tmp7, tmp8)
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
        tmp13 = tl.load(in_ptr3 + (x1 + (1024*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp14 = tl.load(in_ptr4 + (tl.broadcast_to(x1, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp15 = tmp13 - tmp14
        tmp16 = tmp3 * tmp15
        tmp17 = tl.full(tmp16.shape, 0, tmp16.dtype)
        tmp18 = tl.where(tmp2, tmp16, tmp17)
        tmp19 = tl.broadcast_to(tmp18, [XBLOCK, RBLOCK])
        tmp21 = _tmp20 + tmp19
        _tmp20 = tl.where(rmask & xmask, tmp21, _tmp20)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
    tmp20 = tl.sum(_tmp20, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp20, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/wx/cwxuttkwcmue3v7zqyvd6y26jpckrz4axpzj4bzxj27n2qpatou5.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_97 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: 'i32', 15: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(14, 15))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_97', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 1024
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (196*x2) + (200704*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (1024*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2 + (1024*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr9 + (x2), xmask, eviction_policy='evict_last')
    tmp23 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.0006377551020408163
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tmp20 = tmp18 - tmp19
    tmp22 = tmp21 * tmp5
    tmp24 = tmp23 * tmp23
    tmp25 = tmp22 * tmp24
    tmp26 = tmp20 * tmp25
    tmp27 = tmp0 - tmp26
    tmp28 = tmp27 - tmp13
    tmp30 = tmp23 * tmp29
    tmp31 = tmp28 * tmp30
    tl.store(out_ptr0 + (x2 + (1024*y3)), tmp17, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (1024*y3)), tmp31, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/4x/c4xbchxiz2jmvuxdkhsjruhxhij5gggcldqtuadpp3o2wi5g5rm3.py
# Source Nodes: [], Original ATen: [aten.avg_pool2d_backward]

triton_poi_fused_avg_pool2d_backward_98 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[524288], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_avg_pool2d_backward_98', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 351232
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 28
    x1 = (xindex // 28) % 28
    x2 = (xindex // 784) % 56
    x3 = (xindex // 43904)
    x6 = xindex % 43904
    tmp0 = tl.load(in_ptr0 + (76832 + (14*(tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(14, 1 + ((1 + x1) // 2)))))) + (14*(tl.where((tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(14, 1 + ((1 + x1) // 2))))) >= 0, 0, 14))) + (196*x2) + (87808*x3) + (tl.math.min(tl.math.max(0, (x0 // 2)), (-1) + (tl.math.min(14, 1 + ((1 + x0) // 2))))) + (tl.where((tl.math.min(tl.math.max(0, (x0 // 2)), (-1) + (tl.math.min(14, 1 + ((1 + x0) // 2))))) >= 0, 0, 14))), xmask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr0 + (76832 + (14*(tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(14, 1 + ((1 + x1) // 2)))))) + (14*(tl.where((tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(14, 1 + ((1 + x1) // 2))))) >= 0, 0, 14))) + (196*x2) + (87808*x3) + (tl.math.min(1 + (tl.math.max(0, (x0 // 2))), (-1) + (tl.math.min(14, 1 + ((1 + x0) // 2))))) + (tl.where((tl.math.min(1 + (tl.math.max(0, (x0 // 2))), (-1) + (tl.math.min(14, 1 + ((1 + x0) // 2))))) >= 0, 0, 14))), xmask)
    tmp18 = tl.load(in_ptr0 + (76832 + (14*(tl.math.min(1 + (tl.math.max(0, (x1 // 2))), (-1) + (tl.math.min(14, 1 + ((1 + x1) // 2)))))) + (14*(tl.where((tl.math.min(1 + (tl.math.max(0, (x1 // 2))), (-1) + (tl.math.min(14, 1 + ((1 + x1) // 2))))) >= 0, 0, 14))) + (196*x2) + (87808*x3) + (tl.math.min(tl.math.max(0, (x0 // 2)), (-1) + (tl.math.min(14, 1 + ((1 + x0) // 2))))) + (tl.where((tl.math.min(tl.math.max(0, (x0 // 2)), (-1) + (tl.math.min(14, 1 + ((1 + x0) // 2))))) >= 0, 0, 14))), xmask, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr0 + (76832 + (14*(tl.math.min(1 + (tl.math.max(0, (x1 // 2))), (-1) + (tl.math.min(14, 1 + ((1 + x1) // 2)))))) + (14*(tl.where((tl.math.min(1 + (tl.math.max(0, (x1 // 2))), (-1) + (tl.math.min(14, 1 + ((1 + x1) // 2))))) >= 0, 0, 14))) + (196*x2) + (87808*x3) + (tl.math.min(1 + (tl.math.max(0, (x0 // 2))), (-1) + (tl.math.min(14, 1 + ((1 + x0) // 2))))) + (tl.where((tl.math.min(1 + (tl.math.max(0, (x0 // 2))), (-1) + (tl.math.min(14, 1 + ((1 + x0) // 2))))) >= 0, 0, 14))), xmask)
    tmp1 = tmp0 / 9
    tmp2 = tl.math.max(0, (x1 // 2))
    tmp3 = tl.math.min(14, 1 + ((1 + x1) // 2))
    tmp4 = tmp2 < tmp3
    tmp5 = tl.math.max(0, (x0 // 2))
    tmp6 = tl.math.min(14, 1 + ((1 + x0) // 2))
    tmp7 = tmp5 < tmp6
    tmp8 = tmp4 & tmp7
    tmp9 = 0.0
    tmp10 = tl.where(tmp8, tmp1, tmp9)
    tmp12 = tmp11 / 9
    tmp13 = 1 + (tl.math.max(0, (x0 // 2)))
    tmp14 = tmp13 < tmp6
    tmp15 = tmp4 & tmp14
    tmp16 = tmp10 + tmp12
    tmp17 = tl.where(tmp15, tmp16, tmp10)
    tmp19 = tmp18 / 9
    tmp20 = 1 + (tl.math.max(0, (x1 // 2)))
    tmp21 = tmp20 < tmp3
    tmp22 = tmp21 & tmp7
    tmp23 = tmp17 + tmp19
    tmp24 = tl.where(tmp22, tmp23, tmp17)
    tmp26 = tmp25 / 9
    tmp27 = tmp21 & tmp14
    tmp28 = tmp24 + tmp26
    tmp29 = tl.where(tmp27, tmp28, tmp24)
    tl.store(out_ptr0 + (x6 + (351232*x3)), tmp29, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/uy/cuydnvj6bzrxroisj33qjmhllpfoswsrjjf455qfxrwcwsrmlbsz.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_99 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_99', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp10 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (54880 + (196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.full(tmp6.shape, 0, tmp6.dtype)
        tmp8 = tl.where(tmp2, tmp6, tmp7)
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, RBLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(rmask & xmask, tmp11, _tmp10)
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp10, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/uy/cuywbaonuqyb4vjft6buhilzbs4o3aha45cirwhbdfz2xise7egs.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_100 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_100', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 56)
    x0 = xindex % 56
    _tmp14 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x1)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (54880 + (196*x0) + (87808*(((r2 + (121*x1)) // 196) % 8)) + ((r2 + (121*x1)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.load(in_ptr2 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.full(tmp10.shape, 0, tmp10.dtype)
        tmp12 = tl.where(tmp2, tmp10, tmp11)
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(rmask & xmask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp14, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/r4/cr4x76tbrssnmr3wxcsck5vgfbqmnpmb6roqjoru2t6wzuouyahl.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_101 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_101', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (54880 + y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.0006377551020408163
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ot/cot6ow7kmdxnkw24fxds5m7ia7fi232g5a26evnlk3rmn3bheomu.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_102 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_102', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp10 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (43904 + (196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.full(tmp6.shape, 0, tmp6.dtype)
        tmp8 = tl.where(tmp2, tmp6, tmp7)
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, RBLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(rmask & xmask, tmp11, _tmp10)
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp10, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ut/cut5ocsdk6bdz247db7yzyodfddbezkrzk7hhbincyc34htovlez.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_103 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_103', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 56)
    x0 = xindex % 56
    _tmp14 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x1)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (43904 + (196*x0) + (87808*(((r2 + (121*x1)) // 196) % 8)) + ((r2 + (121*x1)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.load(in_ptr2 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.full(tmp10.shape, 0, tmp10.dtype)
        tmp12 = tl.where(tmp2, tmp10, tmp11)
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(rmask & xmask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp14, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/gv/cgvlqftkgzfblcz5oldcly7fccnxdrierxoiyd5faz74qzudhl2q.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_104 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_104', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (43904 + y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.0006377551020408163
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/3k/c3kf5e5rd6wq4gabrivuirfw3lms26t3s2o2mtw6mfculqcv5hia.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_105 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_105', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp10 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (32928 + (196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.full(tmp6.shape, 0, tmp6.dtype)
        tmp8 = tl.where(tmp2, tmp6, tmp7)
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, RBLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(rmask & xmask, tmp11, _tmp10)
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp10, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5u/c5uuqd2lr35ht3mjrs5cubi5dev2fgh6qkmz3mvptyrkwmjkjcd4.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_106 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_106', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 56)
    x0 = xindex % 56
    _tmp14 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x1)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (32928 + (196*x0) + (87808*(((r2 + (121*x1)) // 196) % 8)) + ((r2 + (121*x1)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.load(in_ptr2 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.full(tmp10.shape, 0, tmp10.dtype)
        tmp12 = tl.where(tmp2, tmp10, tmp11)
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(rmask & xmask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp14, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/e4/ce4hpob24ksrk3wjnod3lsxt5w4qwtr5unu4lt52mlmziw276fnv.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_107 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_107', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (32928 + y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.0006377551020408163
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/gr/cgrd54f4mzkekeluguyppwv2ci36iogobmvrcw2df4ounp3eobnh.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_108 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_108', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp10 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (21952 + (196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.full(tmp6.shape, 0, tmp6.dtype)
        tmp8 = tl.where(tmp2, tmp6, tmp7)
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, RBLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(rmask & xmask, tmp11, _tmp10)
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp10, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ba/cbaesjki46jwoljbm5i7joea4sy5rxspo55nsirxkslt4va4adkd.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_109 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_109', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 56)
    x0 = xindex % 56
    _tmp14 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x1)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (21952 + (196*x0) + (87808*(((r2 + (121*x1)) // 196) % 8)) + ((r2 + (121*x1)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.load(in_ptr2 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.full(tmp10.shape, 0, tmp10.dtype)
        tmp12 = tl.where(tmp2, tmp10, tmp11)
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(rmask & xmask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp14, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ob/cob6mk5m6g6i5xlt4reyi22oicjpjwemz6zq6kqhhukcoidxpuz2.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_110 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_110', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (21952 + y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.0006377551020408163
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ig/cig6ujdplfrz2bnjtpq6efghc3bz6geg6na2o3uzirct452waoct.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_111 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_111', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp10 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (10976 + (196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.full(tmp6.shape, 0, tmp6.dtype)
        tmp8 = tl.where(tmp2, tmp6, tmp7)
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, RBLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(rmask & xmask, tmp11, _tmp10)
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp10, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/6l/c6l2w7mhy4tr5eanavmywuuuoioth4il3q3zbebpdayyagctb7yx.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_112 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_112', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 56)
    x0 = xindex % 56
    _tmp14 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x1)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + (10976 + (196*x0) + (87808*(((r2 + (121*x1)) // 196) % 8)) + ((r2 + (121*x1)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.load(in_ptr2 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.full(tmp10.shape, 0, tmp10.dtype)
        tmp12 = tl.where(tmp2, tmp10, tmp11)
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(rmask & xmask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp14, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/me/cmeynbjrklpaisyo2hkjwfouzwisgroz623cpwwvs7kg6weifrta.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_113 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_113', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (10976 + y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.0006377551020408163
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/cv/ccvovf52ztii6roe3tyajbcq4o6w6hlavuicifdexfvo3otftpml.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_114 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_114', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 13
    x1 = (xindex // 13)
    _tmp10 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x0)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x1 + (56*((r2 + (121*x0)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + ((196*x1) + (87808*(((r2 + (121*x0)) // 196) % 8)) + ((r2 + (121*x0)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.full(tmp6.shape, 0, tmp6.dtype)
        tmp8 = tl.where(tmp2, tmp6, tmp7)
        tmp9 = tl.broadcast_to(tmp8, [XBLOCK, RBLOCK])
        tmp11 = _tmp10 + tmp9
        _tmp10 = tl.where(rmask & xmask, tmp11, _tmp10)
    tmp10 = tl.sum(_tmp10, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp10, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5y/c5y5lwjhubro5qn7ool5gvcahbsg36e6utf64zsa2pbjuupbyrar.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_115 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[1024, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_115', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 728
    rnumel = 121
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x1 = (xindex // 56)
    x0 = xindex % 56
    _tmp14 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = r2 + (121*x1)
        tmp1 = tl.full([1, 1], 1568, tl.int32)
        tmp2 = tmp0 < tmp1
        tmp3 = tl.load(in_ptr0 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp4 = tl.load(in_ptr1 + ((196*x0) + (87808*(((r2 + (121*x1)) // 196) % 8)) + ((r2 + (121*x1)) % 196)), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp5 = 0.0
        tmp6 = tl.where(tmp3, tmp5, tmp4)
        tmp7 = tl.load(in_ptr2 + (x0 + (56*((r2 + (121*x1)) % 1568))), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.load(in_ptr3 + (tl.broadcast_to(x0, [XBLOCK, RBLOCK])), rmask & tmp2 & xmask, eviction_policy='evict_last', other=0.0)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.full(tmp10.shape, 0, tmp10.dtype)
        tmp12 = tl.where(tmp2, tmp10, tmp11)
        tmp13 = tl.broadcast_to(tmp12, [XBLOCK, RBLOCK])
        tmp15 = _tmp14 + tmp13
        _tmp14 = tl.where(rmask & xmask, tmp15, _tmp14)
    tmp14 = tl.sum(_tmp14, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp14, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/xh/cxhbqi2yfohcuwmtx654zkkiuuqhnj4qm3blxx2eirglbxd6umc7.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_116 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 64], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_116', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 1568
    xnumel = 56
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 196
    y1 = (yindex // 196)
    tmp0 = tl.load(in_ptr0 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (196*x2) + (87808*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (56*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.0006377551020408163
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (56*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/6i/c6ikqstnca74ycgi2ukqfh6qvesfdpxtng3tanc7dsattbhc4nwg.py
# Source Nodes: [], Original ATen: [aten.cat]

triton_poi_fused_cat_117 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[524288], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_117', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 351232
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = xindex % 43904
    x1 = (xindex // 43904)
    tmp0 = tl.load(in_ptr0 + (x2), xmask)
    tl.store(out_ptr0 + (x0 + (351232*x1)), tmp0, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/gk/cgknues765wlku6jxszxju4purz6rwb3kxso6rt36c7opsrauyur.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_118 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3, 4))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_118', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 21952
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (448*r2) + (57344*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((784*x1) + (351232*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/wl/cwlkhfhh5p2tevgvqekpds4h5deu2gjzeqbi7ojzcxnx7b6tp7d7.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_119 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[512, 64],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_119', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 49
    RBLOCK: tl.constexpr = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (49*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ez/cezcp7hnmxbn62leakcgfcuh5dpc7qzy3kvivs55siwczuopbewm.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_120 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_120', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 21952
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 448
    x1 = (xindex // 448)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (448*r2) + (57344*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((784*x0) + (351232*((r2 + (128*x1)) // 784)) + ((r2 + (128*x1)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (448*r2) + (57344*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/iu/ciuupczbevzltekc7w5dlbxz73hmsqk4ln5vi6mmfso4carg7u7x.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_121 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[512, 64],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_121', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 448
    rnumel = 49
    RBLOCK: tl.constexpr = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (448*r1)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/bp/cbpenufyk6g53lqko5rpsobu4yqtytksr4dxy5juczohv725pq3w.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_122 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_122', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 448
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (448*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (784*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (448*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.00015943877551020407
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (448*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/sj/csj2eapnk5pbx32r6oamjifecaj6bufx5g5iwxy3ukjihaccxhhm.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_123 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_123', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 512
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp8 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 784
        r2 = (rindex // 784)
        tmp0 = tl.load(in_ptr0 + (x0 + (512*r3)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr1 + (r1 + (784*x0) + (401408*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (r1 + (784*x0) + (401408*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp5 = tmp3 + tmp4
        tmp6 = tl.where(tmp2, tmp1, tmp5)
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(rmask & xmask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp8, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/q7/cq7vxb7fvpsashqikikcgppvv5ce34qciqgje4gsnu6ppovdccl3.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_124 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6, 7))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_124', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 25088
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (512*r2) + (65536*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr1 + ((784*x1) + (401408*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + ((784*x1) + (401408*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.load(in_ptr3 + (x1 + (512*r2) + (65536*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp5 = tmp3 + tmp4
        tmp6 = tl.where(tmp2, tmp1, tmp5)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp12, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/2u/c2u6zvly63orhp7xp7gzojjxgcs2x7nwxrxzt5qv4c4uulb2zk5i.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_125 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[512, 64],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_125', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 512
    rnumel = 49
    RBLOCK: tl.constexpr = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (49*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/q2/cq23z7ovfrcbft3dzmpptns6a2etxvb5u6fzoucd5rvqgp5bannz.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_126 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_126', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 512
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (512*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (784*x2) + (401408*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0 + (784*x2) + (401408*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (x2 + (512*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp5 = tmp3 + tmp4
    tmp6 = tl.where(tmp2, tmp1, tmp5)
    tmp9 = tmp7 - tmp8
    tmp11 = 0.00015943877551020407
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp6 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tl.store(out_ptr0 + (x2 + (512*y3)), tmp23, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/hj/chjg67j6vxa7aa6j45a6e2gdqaqev6tecdi3t73ltnmfcoa7xko6.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_127 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_127', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (131712 + (784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/6d/c6da342fc6t5g7g2i43jhuhk7rkpq2ipskvcss7ovschtdwbajst.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_128 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[32, 64],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_128', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 28
    rnumel = 49
    RBLOCK: tl.constexpr = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (49*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/bw/cbw4xgcka4oygcih4lbzlyukrhj4litcvdgowdrf5bjxg6jkhq7q.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_129 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_129', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (131712 + (784*x0) + (175616*((r2 + (128*x1)) // 784)) + ((r2 + (128*x1)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zj/czjqyayvy4ciudet6yu5whd445w55fbq25itwnypiuqyghsc53zg.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_130 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[32, 64],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_130', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 28
    rnumel = 49
    RBLOCK: tl.constexpr = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (28*r1)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ph/cphwaymdnjajgiemmtfrxdiihulpw5foxm4xkt77kmqqw4egjg3b.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_131 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_131', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (131712 + y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.00015943877551020407
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/kd/ckduhc37ffuhkn36rpzsrx4rxmknz7d3cjr7mggvcbsre2odex3f.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_132 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_132', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 28
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 784
        r2 = (rindex // 784)
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r3)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (109760 + r1 + (784*x0) + (175616*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (784*x0) + (21952*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/d5/cd5ng2qygwgl27wounsjfbf3jh3jstxdm46f76zqto5sgeqlu3x5.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_133 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_133', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (109760 + (784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((784*x1) + (21952*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/in/cinrwyxpkd77jkpa3gbrioqmhiw2qwitfd2ubz2gyf7cql3mcdz5.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_134 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[32, 64],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_134', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 28
    rnumel = 49
    RBLOCK: tl.constexpr = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (49*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/uo/cuoz75yi6douckrohdfc7f5uy4enrnktguwol6aup6pljxulagca.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_135 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_135', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (109760 + y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (784*x2) + (21952*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.00015943877551020407
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/c2/cc2nbrenhqmkxdvxs62gpjmciajs5bnnxouramysjues2zqhajpv.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_136 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_136', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 28
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 784
        r2 = (rindex // 784)
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r3)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (87808 + r1 + (784*x0) + (175616*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (784*x0) + (21952*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7h/c7hiq5liya4tmk6cvyjr6von65ra4ku52nzha3xseqpwokru3nyn.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_137 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_137', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (87808 + (784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((784*x1) + (21952*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/p3/cp3s3wi6bvey3sscrpemgececafrya5fth7bkl5rtc6mwdjuzje3.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_138 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_138', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (87808 + y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (784*x2) + (21952*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.00015943877551020407
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/tn/ctnqv4bwbhcwu5azszpukvliivm7tqeej4lqrqkefnsnv6utylyk.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_139 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_139', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 28
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 784
        r2 = (rindex // 784)
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r3)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (65856 + r1 + (784*x0) + (175616*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (784*x0) + (21952*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/jd/cjdqymg2fr3fawkbcsmf322obifnlts2svnxnvxhhic4noghzb5z.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_140 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_140', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (65856 + (784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((784*x1) + (21952*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/qm/cqmd7pdtwaxyxkw3wtdksyukms7fzrh2ia5zrlcygkuelh26ili3.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_141 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_141', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (65856 + y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (784*x2) + (21952*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.00015943877551020407
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/vb/cvbw2jhx6zzj5mamrduvhlext2jro4rxlugjgvori2mmq2gnztgq.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_142 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_142', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 28
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 784
        r2 = (rindex // 784)
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r3)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (43904 + r1 + (784*x0) + (175616*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (784*x0) + (21952*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/a7/ca7ounr3cif5pz4b7w72cnh7l3ljdmi5rcofaolsgddbxhm77qtq.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_143 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_143', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (43904 + (784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((784*x1) + (21952*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ea/ceawanbfbrm7fdscte6pqywuxlviyd265x3mojpicazkfynapaxr.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_144 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_144', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (43904 + y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (784*x2) + (21952*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.00015943877551020407
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/pj/cpj3lanjk7gr3wrkp5bofkfubm7uzcikmbgfr7wmndlszhkegqka.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_145 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_145', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 28
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 784
        r2 = (rindex // 784)
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r3)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (21952 + r1 + (784*x0) + (175616*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (784*x0) + (21952*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/uh/cuh6undlttfpum6n6dvpmt2lpmpyc6j2ej3lg3yo53yxy66hnxgw.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_146 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_146', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (21952 + (784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((784*x1) + (21952*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/eu/ceu7jssfld46zzgfdavb5uunmlsollz6k4w5ppozuxuxkzhuiars.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_147 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_147', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (21952 + y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (784*x2) + (21952*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.00015943877551020407
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/kb/ckb66gyv7zkfs3hchogr6bmlvpdtdegc5z77ptrw46qo3cm6vzdz.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_148 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_148', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 28
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 784
        r2 = (rindex // 784)
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r3)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (r1 + (784*x0) + (175616*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + (r1 + (784*x0) + (21952*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/oe/coee32etic66fuc6v6ny42uv6ywju34hqgho6ya2ewlccbko2vb6.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_149 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_149', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((784*x1) + (21952*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/xh/cxhsv2ppgeogmgnif4bufhex55iyt4ndk7y6oo3f6slk4nv755bc.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_150 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_150', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (784*x2) + (21952*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 0.00015943877551020407
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/e6/ce6bc77zi2h22q7tlu42zo5kdqzkzjf452xnbiryvo7gi2emcxjn.py
# Source Nodes: [], Original ATen: [aten.cat]

triton_poi_fused_cat_151 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[262144], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_151', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 175616
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 21952
    x1 = (xindex // 21952)
    tmp0 = tl.load(in_ptr0 + (153664 + x0 + (175616*x1)), xmask)
    tl.store(out_ptr0 + (x0 + (175616*x1)), tmp0, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/wh/cwhgr4qzwy26ye6syi6z43xvyj7sw2gbxineb4il2swz2o7fhadb.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_152 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3, 4))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_152', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 10976
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (224*r2) + (28672*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/lr/clr2upk5krlv7rqvypufgz7ii7dr5ovw63zsegjnzvdxgmwsuztb.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_153 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[256, 64],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_153', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 224
    rnumel = 49
    RBLOCK: tl.constexpr = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (49*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/eo/ceocjdatqq5iwr6o33rhpridy7nhq2base5tv2lwofbre4rvf3xd.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_154 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16384, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_154', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 10976
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 224
    x1 = (xindex // 224)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (224*r2) + (28672*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((784*x0) + (175616*((r2 + (128*x1)) // 784)) + ((r2 + (128*x1)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (224*r2) + (28672*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/gm/cgmj2fkyypfybiternh6gjq44kp5sxbpb73ydqwqie3nrp3viixf.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_155 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[256, 64],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_155', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 224
    rnumel = 49
    RBLOCK: tl.constexpr = 64
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (224*r1)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/v2/cv2koknfjjklxcmwemxc4mcaj2m5tks3lj4htq4npbynkh2gdmvj.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_156 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_156', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 224
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (224*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (224*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.00015943877551020407
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (224*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7q/c7qyhm2p4suaepbkwcfsjjweubwelgnxsmsgqzk5p2hvz4tko4ob.py
# Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]

triton_poi_fused_add_threshold_backward_157 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_threshold_backward_157', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 4096
    xnumel = 784
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 512
    y1 = (yindex // 512)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (512*x2) + (401408*y1)), xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (512*x2) + (401408*y1)), xmask, eviction_policy='evict_last')
    tmp5 = tl.load(in_out_ptr0 + (x2 + (784*y3)), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x2 + (784*y3)), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x2 + (784*y3)), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tmp3 <= tmp1
    tmp7 = tmp5 + tmp6
    tmp8 = tl.where(tmp4, tmp1, tmp7)
    tmp10 = tmp8 + tmp9
    tmp11 = tl.where(tmp2, tmp1, tmp10)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (784*y3)), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/dy/cdy44hp4hecwug4rdwjckkcdbhfe7amh4tlrtu32vk4ti3hw6jst.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_158 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[512, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_158', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 512
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex % 784
        r2 = (rindex // 784)
        tmp0 = tl.load(in_ptr0 + (r1 + (784*x0) + (401408*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/42/c42dga4lsmnrn4ipgeexsiuqlwy5rs7cqbdlfmin6mxanegp6mym.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_159 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_159', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 25088
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    tmp2 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((784*x1) + (401408*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x1 + (512*r2) + (65536*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/l5/cl53gzmoyrathomhlulx3ky3pmygyjarolmmtzf6dmtcelq4iku2.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_160 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_160', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 512
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (784*x2) + (401408*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (512*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.00015943877551020407
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (512*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rh/crhido7urzyvxglqdtfynroergmtnyeow23w35iw3ozf6yh4uxka.py
# Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]

triton_poi_fused_add_threshold_backward_161 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[4096, 1024], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_threshold_backward_161', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 4096
    xnumel = 784
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 512
    y1 = (yindex // 512)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (512*x2) + (401408*y1)), xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (512*x2) + (401408*y1)), xmask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr2 + (x2 + (784*y3)), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (784*y3)), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_out_ptr0 + (x2 + (784*y3)), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tmp3 <= tmp1
    tmp7 = tmp5 + tmp6
    tmp8 = tl.where(tmp4, tmp1, tmp7)
    tmp10 = tmp8 + tmp9
    tmp11 = tl.where(tmp2, tmp1, tmp10)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (784*y3)), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ag/cagtafmlmtd7ffteqok7alxs3crjeknidhnubd5el45jeeskjrlc.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_162 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: 'i32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(7, 8))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_162', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 25088
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    tmp2 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp9 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp13 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((784*x1) + (401408*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x1 + (512*r2) + (65536*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp8 = tl.load(in_ptr3 + (x1 + (512*r2) + (65536*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
        tmp10 = tmp8 - tmp9
        tmp11 = tmp0 * tmp10
        tmp12 = tl.broadcast_to(tmp11, [XBLOCK, RBLOCK])
        tmp14 = _tmp13 + tmp12
        _tmp13 = tl.where(rmask & xmask, tmp14, _tmp13)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tmp13 = tl.sum(_tmp13, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp13, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/4o/c4oqfpldtg4m2yddbmf6a5s6lbiyjpskhj233lmhv6e7nz6ba5yl.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_163 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 512], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: 'i32', 15: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(14, 15))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_163', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 512
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (784*x2) + (401408*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (512*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2 + (512*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr9 + (x2), xmask, eviction_policy='evict_last')
    tmp23 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 0.00015943877551020407
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tmp20 = tmp18 - tmp19
    tmp22 = tmp21 * tmp5
    tmp24 = tmp23 * tmp23
    tmp25 = tmp22 * tmp24
    tmp26 = tmp20 * tmp25
    tmp27 = tmp0 - tmp26
    tmp28 = tmp27 - tmp13
    tmp30 = tmp23 * tmp29
    tmp31 = tmp28 * tmp30
    tl.store(out_ptr0 + (x2 + (512*y3)), tmp17, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (512*y3)), tmp31, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/os/cosqpjlylaqfzjknzrcp7gxb5p6oshqnvxiio3ecmu22jotc2n6g.py
# Source Nodes: [], Original ATen: [aten.avg_pool2d_backward]

triton_poi_fused_avg_pool2d_backward_164 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1048576], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_avg_pool2d_backward_164', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 702464
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 56
    x1 = (xindex // 56) % 56
    x2 = (xindex // 3136) % 28
    x3 = (xindex // 87808)
    x6 = xindex % 87808
    tmp0 = tl.load(in_ptr0 + (153664 + (28*(tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(28, 1 + ((1 + x1) // 2)))))) + (28*(tl.where((tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(28, 1 + ((1 + x1) // 2))))) >= 0, 0, 28))) + (784*x2) + (175616*x3) + (tl.math.min(tl.math.max(0, (x0 // 2)), (-1) + (tl.math.min(28, 1 + ((1 + x0) // 2))))) + (tl.where((tl.math.min(tl.math.max(0, (x0 // 2)), (-1) + (tl.math.min(28, 1 + ((1 + x0) // 2))))) >= 0, 0, 28))), None, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr0 + (153664 + (28*(tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(28, 1 + ((1 + x1) // 2)))))) + (28*(tl.where((tl.math.min(tl.math.max(0, (x1 // 2)), (-1) + (tl.math.min(28, 1 + ((1 + x1) // 2))))) >= 0, 0, 28))) + (784*x2) + (175616*x3) + (tl.math.min(1 + (tl.math.max(0, (x0 // 2))), (-1) + (tl.math.min(28, 1 + ((1 + x0) // 2))))) + (tl.where((tl.math.min(1 + (tl.math.max(0, (x0 // 2))), (-1) + (tl.math.min(28, 1 + ((1 + x0) // 2))))) >= 0, 0, 28))), None)
    tmp18 = tl.load(in_ptr0 + (153664 + (28*(tl.math.min(1 + (tl.math.max(0, (x1 // 2))), (-1) + (tl.math.min(28, 1 + ((1 + x1) // 2)))))) + (28*(tl.where((tl.math.min(1 + (tl.math.max(0, (x1 // 2))), (-1) + (tl.math.min(28, 1 + ((1 + x1) // 2))))) >= 0, 0, 28))) + (784*x2) + (175616*x3) + (tl.math.min(tl.math.max(0, (x0 // 2)), (-1) + (tl.math.min(28, 1 + ((1 + x0) // 2))))) + (tl.where((tl.math.min(tl.math.max(0, (x0 // 2)), (-1) + (tl.math.min(28, 1 + ((1 + x0) // 2))))) >= 0, 0, 28))), None, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr0 + (153664 + (28*(tl.math.min(1 + (tl.math.max(0, (x1 // 2))), (-1) + (tl.math.min(28, 1 + ((1 + x1) // 2)))))) + (28*(tl.where((tl.math.min(1 + (tl.math.max(0, (x1 // 2))), (-1) + (tl.math.min(28, 1 + ((1 + x1) // 2))))) >= 0, 0, 28))) + (784*x2) + (175616*x3) + (tl.math.min(1 + (tl.math.max(0, (x0 // 2))), (-1) + (tl.math.min(28, 1 + ((1 + x0) // 2))))) + (tl.where((tl.math.min(1 + (tl.math.max(0, (x0 // 2))), (-1) + (tl.math.min(28, 1 + ((1 + x0) // 2))))) >= 0, 0, 28))), None)
    tmp1 = tmp0 / 9
    tmp2 = tl.math.max(0, (x1 // 2))
    tmp3 = tl.math.min(28, 1 + ((1 + x1) // 2))
    tmp4 = tmp2 < tmp3
    tmp5 = tl.math.max(0, (x0 // 2))
    tmp6 = tl.math.min(28, 1 + ((1 + x0) // 2))
    tmp7 = tmp5 < tmp6
    tmp8 = tmp4 & tmp7
    tmp9 = 0.0
    tmp10 = tl.where(tmp8, tmp1, tmp9)
    tmp12 = tmp11 / 9
    tmp13 = 1 + (tl.math.max(0, (x0 // 2)))
    tmp14 = tmp13 < tmp6
    tmp15 = tmp4 & tmp14
    tmp16 = tmp10 + tmp12
    tmp17 = tl.where(tmp15, tmp16, tmp10)
    tmp19 = tmp18 / 9
    tmp20 = 1 + (tl.math.max(0, (x1 // 2)))
    tmp21 = tmp20 < tmp3
    tmp22 = tmp21 & tmp7
    tmp23 = tmp17 + tmp19
    tmp24 = tl.where(tmp22, tmp23, tmp17)
    tmp26 = tmp25 / 9
    tmp27 = tmp21 & tmp14
    tmp28 = tmp24 + tmp26
    tmp29 = tl.where(tmp27, tmp28, tmp24)
    tl.store(out_ptr0 + (x6 + (702464*x3)), tmp29, None)
''')


# kernel path: /tmp/torchinductor_youkaichao/r5/cr5l3zsyzgptqxxz24o5t7utrvxdxdotjhlugvgys2a2pjcuhi4x.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_165 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_165', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (109760 + (784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/n6/cn6ctgmzc7cyd55ea3vl6wra3iuvecueuro25kd6bfw6gmbkz2sy.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_166 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_166', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (109760 + (784*x0) + (175616*((r2 + (128*x1)) // 784)) + ((r2 + (128*x1)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/mg/cmgj565m7ufse4nrtsd2ye6mndjaoumbrnevk7z44shy4zb5zaa4.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_167 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_167', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (109760 + y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.00015943877551020407
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/yf/cyf5okbgdqehjq5ffua5xpjmbr7mtuujmoks7az26ovvyc5dfsbt.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_168 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_168', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (87808 + (784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/om/comcqwui3fdb7fh4oi6rbcqx5b2se777khq5eoiszscow7umz4mb.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_169 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_169', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (87808 + (784*x0) + (175616*((r2 + (128*x1)) // 784)) + ((r2 + (128*x1)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/cl/cclhtrlg5g2iwrmfskeuxbofgk2d5drbv2zuaxoo7qgitdbzjnrw.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_170 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_170', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (87808 + y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.00015943877551020407
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/fc/cfc2wh5vsgwcijxoljeqzo6pt6l7yubg53zbk3mjradw3rfrcvi7.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_171 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_171', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (65856 + (784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/qi/cqiqjlksxderng6wnurflqox76wj7vr2d6xz4mgwkbesa5gay4ea.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_172 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_172', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (65856 + (784*x0) + (175616*((r2 + (128*x1)) // 784)) + ((r2 + (128*x1)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/fs/cfseiqv4gvcu26utohboymf2poridfqsn4kkti45bgwfqu2uiffe.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_173 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_173', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (65856 + y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.00015943877551020407
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/pm/cpmuzifjjbay5hzub6b4wlawwcfs72mhzjhaowuzrgasui555sli.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_174 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_174', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (43904 + (784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ps/cpsqxs6nicfz3elvl3rpg364xtacpu5x6uyweg4hpdr4dgkb36dp.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_175 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_175', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (43904 + (784*x0) + (175616*((r2 + (128*x1)) // 784)) + ((r2 + (128*x1)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/tu/cturzor5c77oqzt5hin7maa6swwnpznqgqnrjfp23ar5rjdviuyw.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_176 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_176', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (43904 + y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.00015943877551020407
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/vu/cvu6gdekr5mpsib4pdjb3hhx2g2xkxaa7cckz6uwcg755hy4qzak.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_177 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_177', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (21952 + (784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/cw/ccwplk632p4oydlpbnanpw7zjyg5xmxftrj5ttg6tv265fbufhcz.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_178 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_178', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (21952 + (784*x0) + (175616*((r2 + (128*x1)) // 784)) + ((r2 + (128*x1)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/m6/cm6ryhqw6aujvu4kfzmhudzlzvsh43wgepa5bfwaiqa4t4bjvrfg.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_179 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_179', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (21952 + y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.00015943877551020407
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ii/ciicttxsgrvqyhb6tknhwj7ejit3n4mcyikvfv2lump36ilzhio2.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_180 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_180', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 49
    x1 = (xindex // 49)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (28*r2) + (3584*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((784*x1) + (175616*((r2 + (128*x0)) // 784)) + ((r2 + (128*x0)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/a2/ca224cgf3by6djgolmghkevbrht5ajmjdhhmbbcdiik54owj2zoq.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_181 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[2048, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_181', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 1372
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 28
    x1 = (xindex // 28)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((784*x0) + (175616*((r2 + (128*x1)) // 784)) + ((r2 + (128*x1)) % 784)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (28*r2) + (3584*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/jn/cjnfbimnf5fhpv4wf664ht3umxa2j57gncdbsro5ujzjiadujlha.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_182 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8192, 32], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_182', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 6272
    xnumel = 28
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 784
    y1 = (yindex // 784)
    tmp0 = tl.load(in_ptr0 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (784*x2) + (175616*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (28*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 0.00015943877551020407
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (28*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/do/cdopgdrd2bwekayu7x3oyj6km3lgrnvkzrv7xogbc5zlshbo5s43.py
# Source Nodes: [], Original ATen: [aten.cat]

triton_poi_fused_cat_183 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[1048576], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_183', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 702464
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = xindex % 87808
    x1 = (xindex // 87808)
    tmp0 = tl.load(in_ptr0 + (x2), None)
    tl.store(out_ptr0 + (x0 + (702464*x1)), tmp0, None)
''')


# kernel path: /tmp/torchinductor_youkaichao/vu/cvuo7nvjv7qj6f4li6xongly6vsvubk6thaj7deaau7oolguniwk.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_184 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3, 4))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_184', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 43904
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (224*r2) + (28672*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((3136*x1) + (702464*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/fm/cfml5cd2smilzhhfqwhu42crebhpaapolkkv6k6yeew52ljisgjh.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_185 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[256, 256],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_185', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 224
    rnumel = 196
    RBLOCK: tl.constexpr = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (196*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/du/cduicimeg437dfdipgfoezmrgftin4iz6cqvn2233u4pexd63dfb.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_186 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_186', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 43904
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 224
    x1 = (xindex // 224)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (224*r2) + (28672*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((3136*x0) + (702464*((r2 + (128*x1)) // 3136)) + ((r2 + (128*x1)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (224*r2) + (28672*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7k/c7kupekhlo6ude3cxfch6v4oxey2s3wcospdejj2ygre53qpenae.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_187 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[256, 256],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_187', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 224
    rnumel = 196
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (224*r1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/hn/chnowubn7weah56baw4jjbeh5h7n2h7u27elcrt6unul7tdpcl3k.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_188 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_188', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 224
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (224*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (3136*x2) + (702464*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (224*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 3.985969387755102e-05
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (224*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5j/c5jfd5d65gm4kk7ozyn2swf3lqv75qyagn2ngiazna667dvdsjc4.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_189 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[256, 32768],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_189', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 256
    rnumel = 25088
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp8 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r3 = rindex
        r1 = rindex % 3136
        r2 = (rindex // 3136)
        tmp0 = tl.load(in_ptr0 + (x0 + (256*r3)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr1 + (r1 + (3136*x0) + (802816*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (r1 + (3136*x0) + (802816*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp5 = tmp3 + tmp4
        tmp6 = tl.where(tmp2, tmp1, tmp5)
        tmp7 = tl.broadcast_to(tmp6, [XBLOCK, RBLOCK])
        tmp9 = _tmp8 + tmp7
        _tmp8 = tl.where(rmask & xmask, tmp9, _tmp8)
    tmp8 = tl.sum(_tmp8, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp8, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ru/crurruzxjxl2mvk5qrboswpy76ndxbanb5fzg5rtar3r3lk6r5iv.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_190 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6, 7))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_190', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 50176
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (256*r2) + (32768*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr1 + ((3136*x1) + (802816*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + ((3136*x1) + (802816*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.load(in_ptr3 + (x1 + (256*r2) + (32768*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp5 = tmp3 + tmp4
        tmp6 = tl.where(tmp2, tmp1, tmp5)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp12, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/p7/cp72mpwkbfo4hnwkov3zsfzl3nb4eqyzga7tp4d7566e6mk4roic.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_191 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[256, 256],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_191', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 256
    rnumel = 196
    RBLOCK: tl.constexpr = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (196*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/7v/c7vnwmnxcafwryylqwsircq5z5v3tzte3dtiezowekzg7zjq76rr.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_192 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10, 11))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_192', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 256
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (256*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (3136*x2) + (802816*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0 + (3136*x2) + (802816*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (x2 + (256*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp5 = tmp3 + tmp4
    tmp6 = tl.where(tmp2, tmp1, tmp5)
    tmp9 = tmp7 - tmp8
    tmp11 = 3.985969387755102e-05
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp6 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tl.store(out_ptr0 + (x2 + (256*y3)), tmp23, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/3h/c3he4xjvvrclhdfad75fynwfxec5mxcxfn2wp4kncuwn7zfxlner.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_193 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3, 4))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_193', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (263424 + (3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5h/c5he6be5orrdl76ue5kkizf5djaztxcwrr5r2yy5i7xnt7zyzber.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_194 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[16, 256],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_194', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 14
    rnumel = 196
    RBLOCK: tl.constexpr = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (196*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ts/cts6jlvoer4yfua52pqe322iatqdybazjjrwul4wdftm72ywcapm.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_195 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_195', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (263424 + (3136*x0) + (351232*((r2 + (128*x1)) // 3136)) + ((r2 + (128*x1)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/mu/cmu4k6jsdo2mlvsz6324jzs3dvtc7gj63alb7z3fzeo7evcyopvb.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_196 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[16, 256],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_196', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 14
    rnumel = 196
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5d/c5dfhh2tj4iv3lj45bw4ohurizydqoxdhxkk3tmmftsyfgrjlkbf.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_197 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_197', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (263424 + y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 3.985969387755102e-05
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/op/cops26upfpyxayuhexv6j2chef5aisp4atj7rsp6yvle7fgie3ax.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_198 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_198', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (87808*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (219520 + (3136*x0) + (351232*(r2 // 3136)) + (702464*x1) + (r2 % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((3136*x0) + (43904*(r2 // 3136)) + (87808*x1) + (r2 % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/27/c27avlo2xdca37bbihoq7mwtnmkf2u5cfuwnht5zk6ccqjiuhrkk.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_199 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[16, 4],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_199', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 14
    rnumel = 4
    RBLOCK: tl.constexpr = 4
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (x0 + (14*r1)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rc/crc6bsgpejthlzc6pdebdralojw473yw4vqwsqneajlwnox6rdza.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_200 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6, 7))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_200', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (219520 + (3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((3136*x1) + (43904*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/6g/c6g23gfmsyrjtwsyqi4pbpvvhrjyp3cyzzowydobskoaksx5tnvy.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_add_native_batch_norm_backward_threshold_backward_201 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[16, 256],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=())]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_add_native_batch_norm_backward_threshold_backward_201', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 14
    rnumel = 196
    RBLOCK: tl.constexpr = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (196*x0)), rmask & xmask, other=0.0)
    tmp5 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tmp6 = tmp4 * tmp5
    tl.store(out_ptr1 + (x0), tmp6, xmask)
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ql/cqltmhe6w35f56frip4ywrq3w34nver7vxnhrdreeupuvdqsu5eg.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_202 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_202', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (219520 + y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (3136*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 3.985969387755102e-05
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/nx/cnxdrs75zvqjw4angus7apjuxmtomphlk4hx4uveuyg4hka4b3ea.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_203 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_203', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (87808*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (175616 + (3136*x0) + (351232*(r2 // 3136)) + (702464*x1) + (r2 % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((3136*x0) + (43904*(r2 // 3136)) + (87808*x1) + (r2 % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/zv/czv4x7jm5v5v3zc7agxvr4vfeoacfw4e5aqwmjcsqyz6ukcggk6s.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_204 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6, 7))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_204', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (175616 + (3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((3136*x1) + (43904*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/o2/co2nbfovm4574hy6ls6ehdsa3bfyfajeh2csymikdfbgbn3y54qg.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_205 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_205', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (175616 + y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (3136*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 3.985969387755102e-05
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/6c/c6cbnd5hftnguwpgshei3xffji3uusqow72qpgtqynvngiatei62.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_206 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_206', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (87808*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (131712 + (3136*x0) + (351232*(r2 // 3136)) + (702464*x1) + (r2 % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((3136*x0) + (43904*(r2 // 3136)) + (87808*x1) + (r2 % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ii/ciim7mrimnwiuxldul7konlumcbzv7ck3x6eu2f353dik5ayqdo3.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_207 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6, 7))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_207', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (131712 + (3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((3136*x1) + (43904*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/lr/clrfva5fmzlfbq3xoyth4djybojp4pdi4kooq427inuohyebwwzv.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_208 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_208', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (131712 + y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (3136*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 3.985969387755102e-05
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/s2/cs2roqxmbvubnvrdjg4yi5vt557z3nshcubkfavlvze6xpvqzurn.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_209 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_209', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (87808*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (87808 + (3136*x0) + (351232*(r2 // 3136)) + (702464*x1) + (r2 % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((3136*x0) + (43904*(r2 // 3136)) + (87808*x1) + (r2 % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ej/cejlymg5fdukupww7cgdf2ey2eizgxqmlvnv43frepsv2snhhgzy.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_210 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6, 7))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_210', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (87808 + (3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((3136*x1) + (43904*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/2g/c2gy3uk254j4hhhttiek7rymrzg53qth6dne66se4qqbxxiw36mc.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_211 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_211', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (87808 + y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (3136*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 3.985969387755102e-05
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/u2/cu2vykegkiq4wv6avcgwelrduwl47pigi2qaq32jkkecdvoprkrn.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_212 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_212', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (87808*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (43904 + (3136*x0) + (351232*(r2 // 3136)) + (702464*x1) + (r2 % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((3136*x0) + (43904*(r2 // 3136)) + (87808*x1) + (r2 % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ve/cve7sxdtfytu4gbbe3hlardzkxxknt4ad4zlnxlnn6uakys3epc3.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_213 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6, 7))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_213', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (43904 + (3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((3136*x1) + (43904*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5g/c5gncerxk42pgn2r4fgius54vfotueda2cflipa23yrk3rduw63n.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_214 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_214', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (43904 + y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (3136*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 3.985969387755102e-05
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/jl/cjl47sjvdm3ae5xf5655yp44my3jvkhxud7xxylzudcl2qsv7b5z.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_215 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 8192],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_215', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 56
    rnumel = 6272
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    _tmp7 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (87808*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((3136*x0) + (351232*(r2 // 3136)) + (702464*x1) + (r2 % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((3136*x0) + (43904*(r2 // 3136)) + (87808*x1) + (r2 % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp6 = tl.broadcast_to(tmp5, [XBLOCK, RBLOCK])
        tmp8 = _tmp7 + tmp6
        _tmp7 = tl.where(rmask & xmask, tmp8, _tmp7)
    tmp7 = tl.sum(_tmp7, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp7, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/to/ctomregrdbjpyrkedypp4cgb5w4ao334v76kfwtw5l2h7yq6qraf.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_216 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6, 7))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_216', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    tmp7 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp11 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = tl.load(in_ptr2 + ((3136*x1) + (43904*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp6 = tl.load(in_ptr3 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 + tmp2
        tmp4 = 0.0
        tmp5 = tl.where(tmp0, tmp4, tmp3)
        tmp8 = tmp6 - tmp7
        tmp9 = tmp5 * tmp8
        tmp10 = tl.broadcast_to(tmp9, [XBLOCK, RBLOCK])
        tmp12 = _tmp11 + tmp10
        _tmp11 = tl.where(rmask & xmask, tmp12, _tmp11)
    tmp11 = tl.sum(_tmp11, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ih/cihxo5nxnu22hrohebnkycayrz5v46qtues5q3jeljb52phz7ddp.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_217 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: 'i32', 11: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(10,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_217', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (y0 + (3136*x2) + (43904*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr3 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp17 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp20 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 + tmp2
    tmp4 = 0.0
    tmp5 = tl.where(tmp0, tmp4, tmp3)
    tmp8 = tmp6 - tmp7
    tmp10 = 3.985969387755102e-05
    tmp11 = tmp9 * tmp10
    tmp13 = tmp12 * tmp12
    tmp14 = tmp11 * tmp13
    tmp15 = tmp8 * tmp14
    tmp16 = tmp5 - tmp15
    tmp18 = tmp17 * tmp10
    tmp19 = tmp16 - tmp18
    tmp21 = tmp12 * tmp20
    tmp22 = tmp19 * tmp21
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp22, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/va/cvamxca7dx3iwyhtvxl6peodco2qjwmxjdcqoecoxssiibp7x4ji.py
# Source Nodes: [], Original ATen: [aten.cat]

triton_poi_fused_cat_218 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[524288], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_cat_218', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 351232
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 43904
    x1 = (xindex // 43904)
    tmp0 = tl.load(in_ptr0 + (307328 + x0 + (351232*x1)), xmask)
    tl.store(out_ptr0 + (x0 + (351232*x1)), tmp0, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/n7/cn7alhy2iu5m6ucqgr5ffuyvd67jgk7scgimnnnd2uvld46rlwd2.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_219 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3, 4))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_219', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 21952
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (112*r2) + (14336*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5c/c5cuszbex4z26dd7uoox2ziawu2vd3v2eexw4kuzbeqvdisx3tzn.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_per_fused_native_batch_norm_backward_threshold_backward_220 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, persistent_reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@persistent_reduction(
    size_hints=[128, 256],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_per_fused_native_batch_norm_backward_threshold_backward_220', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr):
    xnumel = 112
    rnumel = 196
    RBLOCK: tl.constexpr = 256
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rindex = tl.arange(0, RBLOCK)[None, :]
    rmask = rindex < rnumel
    r1 = rindex
    x0 = xindex
    tmp0 = tl.load(in_ptr0 + (r1 + (196*x0)), rmask & xmask, other=0.0)
    tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
    tmp3 = tl.where(rmask & xmask, tmp1, 0)
    tmp4 = tl.sum(tmp3, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp4, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/57/c57b3c665igtmr25ehsgphsy7sfqi42cawnhfxujhbiufcfr2bm7.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_221 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[32768, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_221', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 21952
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 112
    x1 = (xindex // 112)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r2) + (14336*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((3136*x0) + (351232*((r2 + (128*x1)) // 3136)) + ((r2 + (128*x1)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (112*r2) + (14336*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/4y/c4yf23tzdm7jyzckaz7jtoswxttl3cmcajmnf6j2ylu5jgn2vrjg.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_222 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[128, 256],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_222', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 112
    rnumel = 196
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (112*r1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/gi/cgiho4rlvcnpkmok6wt4rldtofxdfftiwucyu6hhpap76grwizwu.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_223 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 128], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_223', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 112
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (112*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 3.985969387755102e-05
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (112*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/bf/cbfooz2jnxwac3vcryrtyzffw7onxxuysig66vwqg7df3lwiizfz.py
# Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]

triton_poi_fused_add_threshold_backward_224 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2048, 4096], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_threshold_backward_224', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 2048
    xnumel = 3136
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 256
    y1 = (yindex // 256)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (256*x2) + (802816*y1)), xmask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (256*x2) + (802816*y1)), xmask, eviction_policy='evict_last')
    tmp5 = tl.load(in_out_ptr0 + (x2 + (3136*y3)), xmask, eviction_policy='evict_last')
    tmp6 = tl.load(in_ptr2 + (x2 + (3136*y3)), xmask, eviction_policy='evict_last')
    tmp9 = tl.load(in_ptr3 + (x2 + (3136*y3)), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tmp3 <= tmp1
    tmp7 = tmp5 + tmp6
    tmp8 = tl.where(tmp4, tmp1, tmp7)
    tmp10 = tmp8 + tmp9
    tmp11 = tl.where(tmp2, tmp1, tmp10)
    tl.debug_barrier()
    tl.store(in_out_ptr0 + (x2 + (3136*y3)), tmp11, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rf/crfozwazwxs2ykkdyt4brseymopt7njggigx5xzuey2f6bupqpbx.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_225 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[256, 32768],
    reduction_hint=ReductionHint.INNER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_225', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 256
    rnumel = 25088
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex % 3136
        r2 = (rindex // 3136)
        tmp0 = tl.load(in_ptr0 + (r1 + (3136*x0) + (802816*r2)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ul/cul24aq6mkdp2yop53fbi3lkj3qsa4yz667dwuzp7iz553y7mfgn.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]

triton_red_fused_native_batch_norm_backward_226 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_226', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 50176
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    tmp2 = tl.load(in_ptr2 + (x1), xmask, eviction_policy='evict_last')
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + ((3136*x1) + (802816*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = tl.load(in_ptr1 + (x1 + (256*r2) + (32768*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tmp1 - tmp2
        tmp4 = tmp0 * tmp3
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/3b/c3bcndla2noirfii3djgawgazpcdtsdu27aw27766hxx5y4oh42r.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_227 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32', 9: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8, 9))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_227', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 256
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    y3 = yindex
    tmp0 = tl.load(in_ptr0 + (y0 + (3136*x2) + (802816*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp1 = tl.load(in_ptr1 + (x2 + (256*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp2 = tl.load(in_ptr2 + (x2), xmask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp12 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp3 = tmp1 - tmp2
    tmp5 = 3.985969387755102e-05
    tmp6 = tmp4 * tmp5
    tmp8 = tmp7 * tmp7
    tmp9 = tmp6 * tmp8
    tmp10 = tmp3 * tmp9
    tmp11 = tmp0 - tmp10
    tmp13 = tmp12 * tmp5
    tmp14 = tmp11 - tmp13
    tmp16 = tmp7 * tmp15
    tmp17 = tmp14 * tmp16
    tl.store(out_ptr0 + (x2 + (256*y3)), tmp17, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/6o/c6oc7wyujki2xmsvo5556tseorryx4y72rat5lksmjogejngwf3d.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_add_native_batch_norm_backward_threshold_backward_228 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9, 10))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_add_native_batch_norm_backward_threshold_backward_228', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 50176
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    tmp8 = tl.load(in_ptr4 + (x1), xmask, eviction_policy='evict_last')
    _tmp12 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp15 = tl.load(in_ptr6 + (x1), xmask, eviction_policy='evict_last')
    _tmp19 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (256*r2) + (32768*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp3 = tl.load(in_ptr1 + ((3136*x1) + (802816*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + ((3136*x1) + (802816*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp7 = tl.load(in_ptr3 + (x1 + (256*r2) + (32768*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp14 = tl.load(in_ptr5 + (x1 + (256*r2) + (32768*x0)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp5 = tmp3 + tmp4
        tmp6 = tl.where(tmp2, tmp1, tmp5)
        tmp9 = tmp7 - tmp8
        tmp10 = tmp6 * tmp9
        tmp11 = tl.broadcast_to(tmp10, [XBLOCK, RBLOCK])
        tmp13 = _tmp12 + tmp11
        _tmp12 = tl.where(rmask & xmask, tmp13, _tmp12)
        tmp16 = tmp14 - tmp15
        tmp17 = tmp6 * tmp16
        tmp18 = tl.broadcast_to(tmp17, [XBLOCK, RBLOCK])
        tmp20 = _tmp19 + tmp18
        _tmp19 = tl.where(rmask & xmask, tmp20, _tmp19)
    tmp12 = tl.sum(_tmp12, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp12, xmask)
    tmp19 = tl.sum(_tmp19, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp19, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/cq/ccqj7utstieex6yc6ax7f35eysbgnucga62mv7hhg7ly72guq3hi.py
# Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_add_native_batch_norm_backward_threshold_backward_229 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 256], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: '*fp32', 10: '*fp32', 11: '*fp32', 12: '*fp32', 13: '*fp32', 14: '*fp32', 15: '*fp32', 16: 'i32', 17: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(16, 17))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_native_batch_norm_backward_threshold_backward_229', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, in_ptr8, in_ptr9, in_ptr10, in_ptr11, in_ptr12, in_ptr13, out_ptr0, out_ptr1, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 256
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (256*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp3 = tl.load(in_ptr1 + (y0 + (3136*x2) + (802816*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (y0 + (3136*x2) + (802816*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr3 + (x2 + (256*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp13 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp21 = tl.load(in_ptr8 + (x2), xmask, eviction_policy='evict_last')
    tmp24 = tl.load(in_ptr9 + (x2 + (256*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp25 = tl.load(in_ptr10 + (x2), xmask, eviction_policy='evict_last')
    tmp27 = tl.load(in_ptr11 + (x2), xmask, eviction_policy='evict_last')
    tmp29 = tl.load(in_ptr12 + (x2), xmask, eviction_policy='evict_last')
    tmp35 = tl.load(in_ptr13 + (x2), xmask, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp5 = tmp3 + tmp4
    tmp6 = tl.where(tmp2, tmp1, tmp5)
    tmp9 = tmp7 - tmp8
    tmp11 = 3.985969387755102e-05
    tmp12 = tmp10 * tmp11
    tmp14 = tmp13 * tmp13
    tmp15 = tmp12 * tmp14
    tmp16 = tmp9 * tmp15
    tmp17 = tmp6 - tmp16
    tmp19 = tmp18 * tmp11
    tmp20 = tmp17 - tmp19
    tmp22 = tmp13 * tmp21
    tmp23 = tmp20 * tmp22
    tmp26 = tmp24 - tmp25
    tmp28 = tmp27 * tmp11
    tmp30 = tmp29 * tmp29
    tmp31 = tmp28 * tmp30
    tmp32 = tmp26 * tmp31
    tmp33 = tmp6 - tmp32
    tmp34 = tmp33 - tmp19
    tmp36 = tmp29 * tmp35
    tmp37 = tmp34 * tmp36
    tl.store(out_ptr0 + (x2 + (256*y3)), tmp23, xmask & ymask)
    tl.store(out_ptr1 + (x2 + (256*y3)), tmp37, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/a4/ca4akf54xka4wvtccj572qgkcz2z66exfc5ko4dint7llozmq4r6.py
# Source Nodes: [], Original ATen: [aten.avg_pool2d_backward]

triton_poi_fused_avg_pool2d_backward_230 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[524288], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_avg_pool2d_backward_230', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 351232
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex % 56
    x1 = (xindex // 56) % 56
    x2 = (xindex // 3136) % 14
    x3 = (xindex // 43904)
    x6 = xindex % 43904
    tmp0 = tl.load(in_ptr0 + (307328 + (56*(tl.math.min(tl.math.max(0, (-1) + x1), (-1) + (tl.math.min(56, 2 + x1))))) + (3136*x2) + (351232*x3) + (tl.math.min(tl.math.max(0, (-1) + x0), (-1) + (tl.math.min(56, 2 + x0))))), xmask, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr0 + (307328 + (56*(tl.math.min(tl.math.max(0, (-1) + x1), (-1) + (tl.math.min(56, 2 + x1))))) + (3136*x2) + (351232*x3) + (tl.math.min(1 + (tl.math.max(0, (-1) + x0)), (-1) + (tl.math.min(56, 2 + x0))))), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr0 + (307328 + (56*(tl.math.min(tl.math.max(0, (-1) + x1), (-1) + (tl.math.min(56, 2 + x1))))) + (3136*x2) + (351232*x3) + (tl.math.min(2 + (tl.math.max(0, (-1) + x0)), (-1) + (tl.math.min(56, 2 + x0))))), xmask)
    tmp25 = tl.load(in_ptr0 + (307328 + (56*(tl.math.min(1 + (tl.math.max(0, (-1) + x1)), (-1) + (tl.math.min(56, 2 + x1))))) + (3136*x2) + (351232*x3) + (tl.math.min(tl.math.max(0, (-1) + x0), (-1) + (tl.math.min(56, 2 + x0))))), xmask, eviction_policy='evict_last')
    tmp32 = tl.load(in_ptr0 + (307328 + (56*(tl.math.min(1 + (tl.math.max(0, (-1) + x1)), (-1) + (tl.math.min(56, 2 + x1))))) + (3136*x2) + (351232*x3) + (tl.math.min(1 + (tl.math.max(0, (-1) + x0)), (-1) + (tl.math.min(56, 2 + x0))))), xmask, eviction_policy='evict_last')
    tmp37 = tl.load(in_ptr0 + (307328 + (56*(tl.math.min(1 + (tl.math.max(0, (-1) + x1)), (-1) + (tl.math.min(56, 2 + x1))))) + (3136*x2) + (351232*x3) + (tl.math.min(2 + (tl.math.max(0, (-1) + x0)), (-1) + (tl.math.min(56, 2 + x0))))), xmask)
    tmp42 = tl.load(in_ptr0 + (307328 + (56*(tl.math.min(2 + (tl.math.max(0, (-1) + x1)), (-1) + (tl.math.min(56, 2 + x1))))) + (3136*x2) + (351232*x3) + (tl.math.min(tl.math.max(0, (-1) + x0), (-1) + (tl.math.min(56, 2 + x0))))), xmask, eviction_policy='evict_last')
    tmp49 = tl.load(in_ptr0 + (307328 + (56*(tl.math.min(2 + (tl.math.max(0, (-1) + x1)), (-1) + (tl.math.min(56, 2 + x1))))) + (3136*x2) + (351232*x3) + (tl.math.min(1 + (tl.math.max(0, (-1) + x0)), (-1) + (tl.math.min(56, 2 + x0))))), xmask, eviction_policy='evict_last')
    tmp54 = tl.load(in_ptr0 + (307328 + (56*(tl.math.min(2 + (tl.math.max(0, (-1) + x1)), (-1) + (tl.math.min(56, 2 + x1))))) + (3136*x2) + (351232*x3) + (tl.math.min(2 + (tl.math.max(0, (-1) + x0)), (-1) + (tl.math.min(56, 2 + x0))))), xmask)
    tmp1 = tmp0 / 9
    tmp2 = tl.math.max(0, (-1) + x1)
    tmp3 = tl.math.min(56, 2 + x1)
    tmp4 = tmp2 < tmp3
    tmp5 = tl.math.max(0, (-1) + x0)
    tmp6 = tl.math.min(56, 2 + x0)
    tmp7 = tmp5 < tmp6
    tmp8 = tmp4 & tmp7
    tmp9 = 0.0
    tmp10 = tl.where(tmp8, tmp1, tmp9)
    tmp12 = tmp11 / 9
    tmp13 = 1 + (tl.math.max(0, (-1) + x0))
    tmp14 = tmp13 < tmp6
    tmp15 = tmp4 & tmp14
    tmp16 = tmp10 + tmp12
    tmp17 = tl.where(tmp15, tmp16, tmp10)
    tmp19 = tmp18 / 9
    tmp20 = 2 + (tl.math.max(0, (-1) + x0))
    tmp21 = tmp20 < tmp6
    tmp22 = tmp4 & tmp21
    tmp23 = tmp17 + tmp19
    tmp24 = tl.where(tmp22, tmp23, tmp17)
    tmp26 = tmp25 / 9
    tmp27 = 1 + (tl.math.max(0, (-1) + x1))
    tmp28 = tmp27 < tmp3
    tmp29 = tmp28 & tmp7
    tmp30 = tmp24 + tmp26
    tmp31 = tl.where(tmp29, tmp30, tmp24)
    tmp33 = tmp32 / 9
    tmp34 = tmp28 & tmp14
    tmp35 = tmp31 + tmp33
    tmp36 = tl.where(tmp34, tmp35, tmp31)
    tmp38 = tmp37 / 9
    tmp39 = tmp28 & tmp21
    tmp40 = tmp36 + tmp38
    tmp41 = tl.where(tmp39, tmp40, tmp36)
    tmp43 = tmp42 / 9
    tmp44 = 2 + (tl.math.max(0, (-1) + x1))
    tmp45 = tmp44 < tmp3
    tmp46 = tmp45 & tmp7
    tmp47 = tmp41 + tmp43
    tmp48 = tl.where(tmp46, tmp47, tmp41)
    tmp50 = tmp49 / 9
    tmp51 = tmp45 & tmp14
    tmp52 = tmp48 + tmp50
    tmp53 = tl.where(tmp51, tmp52, tmp48)
    tmp55 = tmp54 / 9
    tmp56 = tmp45 & tmp21
    tmp57 = tmp53 + tmp55
    tmp58 = tl.where(tmp56, tmp57, tmp53)
    tl.store(out_ptr0 + (x6 + (351232*x3)), tmp58, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/mu/cmueim64526evffxcad4a4uc5jgsewwf6efoi5rxdvew24gv4xin.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_231 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3, 4))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_231', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (219520 + (3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/dz/cdzt2rpaztyonr4gtq52545kjq5o6l4c7ae6gx6dbjajopw6mfxq.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_232 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_232', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (219520 + (3136*x0) + (351232*((r2 + (128*x1)) // 3136)) + ((r2 + (128*x1)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/ar/car52d5pvkelytsg5bhm27ialunfg6ue6p3ltbrdih7pgn3tpr55.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_233 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_233', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (219520 + y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 3.985969387755102e-05
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/nl/cnlhaw6hv5zw2mv4hlrqeelzyyn6pakpj627chikhq2wmive5edd.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_234 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3, 4))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_234', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (175616 + (3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/u7/cu77vckasywhkbzialq65azmntmejtgfwcpd5ysqizmnus7jylip.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_235 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_235', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (175616 + (3136*x0) + (351232*((r2 + (128*x1)) // 3136)) + ((r2 + (128*x1)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/65/c65xq37fkamn6j2mvrczn7ixjdsjhqdofymlt4p26p3g4xd26idz.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_236 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_236', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (175616 + y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 3.985969387755102e-05
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/w3/cw3ugkekxgzhlnify6arjwjdkzv4tq6qyuezp6gxndibbpvqcna5.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_237 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3, 4))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_237', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (131712 + (3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/75/c75h3me3n4jyk2upwu6fid6vlzwj6xcqjk5fby4bcdcqgxvdpdes.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_238 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_238', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (131712 + (3136*x0) + (351232*((r2 + (128*x1)) // 3136)) + ((r2 + (128*x1)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/5d/c5d6fj4qps5g43vqe5lblnmei5xkal24cdyrtkla3hw523ni2xqm.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_239 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_239', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (131712 + y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 3.985969387755102e-05
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/4w/c4wdxbry7t4f4c3kvkjyixujfmziameasvrlzamv3hugex7xfj6g.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_240 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3, 4))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_240', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (87808 + (3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/gh/cghlyes22klokbkybn4vjivi6wgi2txgarflkpzy6zx7dbb6kwb3.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_241 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_241', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (87808 + (3136*x0) + (351232*((r2 + (128*x1)) // 3136)) + ((r2 + (128*x1)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/nw/cnwv4trprqbb7iereen3pfwmhg4hzczccnhme6kbkqovy6gk6zt6.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_242 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_242', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (87808 + y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 3.985969387755102e-05
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/qa/cqa7if7o3pedq4535yrmfljuahvvusres7wbf6uqzuccenowzrxz.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_243 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3, 4))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_243', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (43904 + (3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/x5/cx5qxr43htgdt6a7eaexc66bw56ktwrn6geloxxd3x3ysa3pfb5a.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_244 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_244', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + (43904 + (3136*x0) + (351232*((r2 + (128*x1)) // 3136)) + ((r2 + (128*x1)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/42/c42drjbkkpvllojzjnug4idntftpzg7ye2hb7pnl7wrzbncaqfmr.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_245 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_245', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (43904 + y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 3.985969387755102e-05
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/3v/c3vi4iwwudpcy2wmktxe6duxdgxtc5qr7ysnn3pirvv5ztubdaef.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_246 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: 'i32', 4: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 4), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(3, 4))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_246', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 196
    x1 = (xindex // 196)
    _tmp5 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x1 + (14*r2) + (1792*x0)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((3136*x1) + (351232*((r2 + (128*x0)) // 3136)) + ((r2 + (128*x0)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp4 = tl.broadcast_to(tmp3, [XBLOCK, RBLOCK])
        tmp6 = _tmp5 + tmp4
        _tmp5 = tl.where(rmask & xmask, tmp6, _tmp5)
    tmp5 = tl.sum(_tmp5, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/al/calnd66ovwqbkmrcgae5etl2dc7j6pdd6dqod2q2sb6ikikza4kh.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_247 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[4096, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: 'i32', 6: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 6), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(5, 6))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_247', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 2744
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 14
    x1 = (xindex // 14)
    tmp5 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp9 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last').to(tl.int1)
        tmp1 = tl.load(in_ptr1 + ((3136*x0) + (351232*((r2 + (128*x1)) // 3136)) + ((r2 + (128*x1)) % 3136)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp4 = tl.load(in_ptr2 + (x0 + (14*r2) + (1792*x1)), rmask & xmask, eviction_policy='evict_last', other=0.0)
        tmp2 = 0.0
        tmp3 = tl.where(tmp0, tmp2, tmp1)
        tmp6 = tmp4 - tmp5
        tmp7 = tmp3 * tmp6
        tmp8 = tl.broadcast_to(tmp7, [XBLOCK, RBLOCK])
        tmp10 = _tmp9 + tmp8
        _tmp9 = tl.where(rmask & xmask, tmp10, _tmp9)
    tmp9 = tl.sum(_tmp9, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp9, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/el/celguntoh5qkf4bjo6vtalp44anhib6i72ymfwjxzfa45pq57dt2.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_248 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[32768, 16], tile_hint=TileHint.DEFAULT,
    filename=__file__,
    triton_meta={'signature': {0: '*i1', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: '*fp32', 9: 'i32', 10: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(9,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_248', 'mutated_arg_names': []},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, in_ptr7, out_ptr0, ynumel, xnumel, YBLOCK : tl.constexpr, XBLOCK : tl.constexpr):
    ynumel = 25088
    xnumel = 14
    yoffset = tl.program_id(1) * YBLOCK
    yindex = yoffset + tl.arange(0, YBLOCK)[None, :]
    ymask = yindex < ynumel
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    x2 = xindex
    y3 = yindex
    y0 = yindex % 3136
    y1 = (yindex // 3136)
    tmp0 = tl.load(in_ptr0 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last').to(tl.int1)
    tmp1 = tl.load(in_ptr1 + (y0 + (3136*x2) + (351232*y1)), xmask & ymask, eviction_policy='evict_last')
    tmp4 = tl.load(in_ptr2 + (x2 + (14*y3)), xmask & ymask, eviction_policy='evict_last')
    tmp5 = tl.load(in_ptr3 + (x2), xmask, eviction_policy='evict_last')
    tmp7 = tl.load(in_ptr4 + (x2), xmask, eviction_policy='evict_last')
    tmp10 = tl.load(in_ptr5 + (x2), xmask, eviction_policy='evict_last')
    tmp15 = tl.load(in_ptr6 + (x2), xmask, eviction_policy='evict_last')
    tmp18 = tl.load(in_ptr7 + (x2), xmask, eviction_policy='evict_last')
    tmp2 = 0.0
    tmp3 = tl.where(tmp0, tmp2, tmp1)
    tmp6 = tmp4 - tmp5
    tmp8 = 3.985969387755102e-05
    tmp9 = tmp7 * tmp8
    tmp11 = tmp10 * tmp10
    tmp12 = tmp9 * tmp11
    tmp13 = tmp6 * tmp12
    tmp14 = tmp3 - tmp13
    tmp16 = tmp15 * tmp8
    tmp17 = tmp14 - tmp16
    tmp19 = tmp10 * tmp18
    tmp20 = tmp17 * tmp19
    tl.store(out_ptr0 + (x2 + (14*y3)), tmp20, xmask & ymask)
''')


# kernel path: /tmp/torchinductor_youkaichao/75/c75sm437nth6e7qpkvdkl34fe7r453lmvttg366zr74dmszsh3mf.py
# Source Nodes: [], Original ATen: [aten.add]

triton_poi_fused_add_249 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[2097152], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_add_249', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, xnumel, XBLOCK : tl.constexpr):
    xnumel = 1605632
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x0 = xindex
    tmp0 = tl.load(in_out_ptr0 + (x0), None)
    tmp1 = tl.load(in_ptr0 + (x0), None)
    tmp2 = tmp0 + tmp1
    tl.store(in_out_ptr0 + (x0), tmp2, None)
''')


# kernel path: /tmp/torchinductor_youkaichao/mn/cmnacut42kffvlijhvbcfw752jwkf3ugid6lia3jvnhgzsoqybkc.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_250 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[65536, 128],
    reduction_hint=ReductionHint.OUTER,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: 'i32', 7: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(6, 7))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_250', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, in_ptr2, in_ptr3, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 50176
    rnumel = 128
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex % 64
    x1 = (xindex // 64)
    _tmp6 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    x3 = xindex
    tmp9 = tl.load(in_ptr3 + (x0), xmask, eviction_policy='evict_last')
    _tmp13 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r2 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (64*r2) + (8192*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp3 = tl.load(in_ptr1 + (x0 + (64*r2) + (8192*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp8 = tl.load(in_ptr2 + (x0 + (64*r2) + (8192*x1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = 0.0
        tmp2 = tmp0 <= tmp1
        tmp4 = tl.where(tmp2, tmp1, tmp3)
        tmp5 = tl.broadcast_to(tmp4, [XBLOCK, RBLOCK])
        tmp7 = _tmp6 + tmp5
        _tmp6 = tl.where(rmask & xmask, tmp7, _tmp6)
        tmp10 = tmp8 - tmp9
        tmp11 = tmp4 * tmp10
        tmp12 = tl.broadcast_to(tmp11, [XBLOCK, RBLOCK])
        tmp14 = _tmp13 + tmp12
        _tmp13 = tl.where(rmask & xmask, tmp14, _tmp13)
    tmp6 = tl.sum(_tmp6, 1)[:, None]
    tl.store(out_ptr0 + (x3), tmp6, xmask)
    tmp13 = tl.sum(_tmp13, 1)[:, None]
    tl.store(out_ptr1 + (x3), tmp13, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/rb/crbiup3tl5ym6dvlogg43xhmxw6eqmvikc7qavu5pxewst5kmny5.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_251 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 1024],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: 'i32', 3: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(2, 3))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_251', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, out_ptr0, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 64
    rnumel = 784
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (64*r1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/pq/cpqjcg5jd3mvkl2yhil66r3skyh5cbdf5eecquc34of256kqtphv.py
# Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]

triton_red_fused_native_batch_norm_backward_threshold_backward_252 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, reduction
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@reduction(
    size_hints=[64, 1024],
    reduction_hint=ReductionHint.OUTER_TINY,
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: 'i32', 5: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(4, 5))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_red_fused_native_batch_norm_backward_threshold_backward_252', 'mutated_arg_names': []}
)
@triton.jit
def triton_(in_ptr0, in_ptr1, out_ptr0, out_ptr1, xnumel, rnumel, XBLOCK : tl.constexpr, RBLOCK : tl.constexpr):
    xnumel = 64
    rnumel = 784
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:, None]
    xmask = xindex < xnumel
    rbase = tl.arange(0, RBLOCK)[None, :]
    x0 = xindex
    _tmp2 = tl.full([XBLOCK, RBLOCK], 0, tl.float32)
    for roffset in range(0, rnumel, RBLOCK):
        rindex = roffset + rbase
        rmask = rindex < rnumel
        r1 = rindex
        tmp0 = tl.load(in_ptr0 + (x0 + (64*r1)), rmask & xmask, eviction_policy='evict_first', other=0.0)
        tmp1 = tl.broadcast_to(tmp0, [XBLOCK, RBLOCK])
        tmp3 = _tmp2 + tmp1
        _tmp2 = tl.where(rmask & xmask, tmp3, _tmp2)
    tmp2 = tl.sum(_tmp2, 1)[:, None]
    tl.store(out_ptr0 + (x0), tmp2, xmask)
    tmp4 = tl.load(in_ptr1 + (x0), xmask, eviction_policy='evict_last')
    tmp5 = tmp2 * tmp4
    tl.store(out_ptr1 + (x0), tmp5, xmask)
''')


# kernel path: /tmp/torchinductor_youkaichao/oo/coogm246mrrlblhsjak27u2nuj6i4kwhbrfb6wfclmthlb7lm7cp.py
# Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]

triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_253 = async_compile.triton('triton_', '''
import triton
import triton.language as tl
from torch._inductor.ir import ReductionHint
from torch._inductor.ir import TileHint
from torch._inductor.triton_heuristics import AutotuneHint, pointwise
from torch._inductor.utils import instance_descriptor
from torch._inductor import triton_helpers

@pointwise(
    size_hints=[8388608], 
    filename=__file__,
    triton_meta={'signature': {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: '*fp32', 4: '*fp32', 5: '*fp32', 6: '*fp32', 7: '*fp32', 8: 'i32'}, 'device': 0, 'device_type': 'cuda', 'constants': {}, 'configs': [instance_descriptor(divisible_by_16=(0, 1, 2, 3, 4, 5, 6, 7, 8), equal_to_1=(), ids_of_folded_args=(), divisible_by_8=(8,))]},
    inductor_meta={'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_253', 'mutated_arg_names': ['in_out_ptr0']},
    min_elem_per_thread=0
)
@triton.jit
def triton_(in_out_ptr0, in_ptr0, in_ptr1, in_ptr2, in_ptr3, in_ptr4, in_ptr5, in_ptr6, xnumel, XBLOCK : tl.constexpr):
    xnumel = 6422528
    xoffset = tl.program_id(0) * XBLOCK
    xindex = xoffset + tl.arange(0, XBLOCK)[:]
    xmask = xindex < xnumel
    x2 = xindex
    x0 = xindex % 64
    tmp0 = tl.load(in_ptr0 + (x2), None)
    tmp3 = tl.load(in_out_ptr0 + (x2), None)
    tmp5 = tl.load(in_ptr1 + (x2), None)
    tmp6 = tl.load(in_ptr2 + (x0), None, eviction_policy='evict_last')
    tmp8 = tl.load(in_ptr3 + (x0), None, eviction_policy='evict_last')
    tmp11 = tl.load(in_ptr4 + (x0), None, eviction_policy='evict_last')
    tmp16 = tl.load(in_ptr5 + (x0), None, eviction_policy='evict_last')
    tmp19 = tl.load(in_ptr6 + (x0), None, eviction_policy='evict_last')
    tmp1 = 0.0
    tmp2 = tmp0 <= tmp1
    tmp4 = tl.where(tmp2, tmp1, tmp3)
    tmp7 = tmp5 - tmp6
    tmp9 = 9.964923469387754e-06
    tmp10 = tmp8 * tmp9
    tmp12 = tmp11 * tmp11
    tmp13 = tmp10 * tmp12
    tmp14 = tmp7 * tmp13
    tmp15 = tmp4 - tmp14
    tmp17 = tmp16 * tmp9
    tmp18 = tmp15 - tmp17
    tmp20 = tmp11 * tmp19
    tmp21 = tmp18 * tmp20
    tl.store(in_out_ptr0 + (x2), tmp21, None)
''')


async_compile.wait(globals())
del async_compile

def call(args):
    primals_1, primals_2, primals_4, primals_5, primals_7, primals_8, primals_10, primals_11, primals_13, primals_14, primals_16, primals_17, primals_19, primals_20, primals_22, primals_23, primals_25, primals_26, primals_28, primals_29, primals_31, primals_32, primals_34, primals_35, primals_37, primals_38, primals_40, primals_41, primals_43, primals_44, primals_46, primals_47, primals_49, primals_50, primals_52, primals_53, primals_55, primals_56, primals_58, primals_59, primals_61, primals_62, primals_64, primals_65, primals_67, primals_68, primals_70, primals_71, primals_73, primals_74, primals_76, primals_77, primals_79, primals_80, primals_82, primals_83, primals_85, primals_86, primals_88, primals_89, primals_91, primals_92, primals_94, primals_95, primals_97, primals_98, primals_100, primals_101, primals_103, primals_104, primals_106, primals_107, primals_109, primals_110, primals_112, primals_113, primals_115, primals_116, primals_118, primals_119, primals_121, primals_122, primals_124, primals_125, primals_127, primals_128, primals_130, primals_131, primals_133, primals_134, primals_136, primals_137, primals_139, primals_140, primals_142, primals_143, primals_145, primals_146, primals_148, primals_149, primals_151, primals_152, primals_154, primals_155, primals_157, primals_158, primals_160, primals_161, primals_163, primals_164, primals_166, primals_167, primals_169, primals_170, primals_172, primals_173, primals_175, primals_176, primals_178, primals_179, primals_181, primals_182, primals_184, primals_185, primals_187, primals_188, primals_190, primals_191, primals_193, primals_194, primals_196, primals_197, primals_199, primals_200, primals_202, primals_203, primals_205, primals_206, primals_208, primals_209, primals_211, primals_212, primals_214, primals_215, primals_217, primals_218, primals_220, primals_221, primals_223, primals_224, primals_226, primals_227, primals_229, primals_230, primals_232, primals_233, primals_235, primals_236, primals_238, primals_239, primals_241, primals_242, primals_244, primals_245, primals_247, primals_248, primals_250, primals_251, primals_253, primals_254, primals_256, primals_257, primals_259, primals_260, primals_262, primals_263, primals_265, primals_266, primals_268, primals_269, primals_271, primals_272, primals_274, primals_275, primals_277, primals_278, primals_280, primals_281, primals_283, primals_284, primals_286, primals_287, primals_289, primals_290, primals_292, primals_293, primals_295, primals_296, primals_298, primals_299, primals_301, primals_302, primals_304, primals_305, primals_307, primals_308, primals_310, primals_311, primals_313, primals_314, primals_316, primals_317, primals_319, primals_320, primals_322, primals_323, primals_325, primals_326, primals_328, primals_329, primals_331, primals_332, primals_334, primals_335, primals_337, primals_338, primals_340, primals_341, primals_343, primals_344, primals_346, primals_347, primals_349, primals_350, primals_352, primals_353, primals_355, primals_356, primals_358, primals_359, primals_361, primals_362, primals_364, primals_365, primals_367, primals_368, primals_370, primals_371, primals_373, primals_374, primals_376, primals_377, primals_379, primals_380, primals_382, primals_383, primals_385, primals_386, primals_388, primals_389, primals_391, primals_392, primals_394, primals_395, primals_397, primals_398, primals_400, primals_401, primals_403, primals_404, primals_406, primals_407, primals_409, primals_410, primals_412, primals_413, primals_415, primals_416, primals_418, primals_419, primals_421, primals_422, primals_424, primals_425, primals_427, primals_428, primals_430, primals_431, primals_433, primals_434, primals_436, primals_437, primals_439, primals_440, primals_442, primals_443, primals_445, primals_446, primals_897, convolution, squeeze_1, relu, getitem_2, getitem_3, convolution_1, squeeze_4, getitem_14, convolution_2, squeeze_7, getitem_25, convolution_3, squeeze_10, getitem_36, convolution_4, squeeze_13, getitem_47, convolution_5, squeeze_16, getitem_58, convolution_6, squeeze_19, getitem_69, convolution_7, squeeze_22, getitem_80, convolution_8, squeeze_25, getitem_91, cat, convolution_9, squeeze_28, convolution_10, squeeze_31, relu_9, convolution_11, squeeze_34, getitem_106, convolution_12, squeeze_37, add_66, convolution_13, squeeze_40, add_72, convolution_14, squeeze_43, add_78, convolution_15, squeeze_46, add_84, convolution_16, squeeze_49, add_90, convolution_17, squeeze_52, add_96, convolution_18, squeeze_55, cat_1, convolution_19, squeeze_58, relu_18, convolution_20, squeeze_61, getitem_196, convolution_21, squeeze_64, add_118, convolution_22, squeeze_67, add_124, convolution_23, squeeze_70, add_130, convolution_24, squeeze_73, add_136, convolution_25, squeeze_76, add_142, convolution_26, squeeze_79, add_148, convolution_27, squeeze_82, cat_2, convolution_28, squeeze_85, relu_27, convolution_29, squeeze_88, getitem_286, convolution_30, squeeze_91, getitem_297, convolution_31, squeeze_94, getitem_308, convolution_32, squeeze_97, getitem_319, convolution_33, squeeze_100, getitem_330, convolution_34, squeeze_103, getitem_341, convolution_35, squeeze_106, getitem_352, convolution_36, squeeze_109, getitem_363, cat_3, convolution_37, squeeze_112, convolution_38, squeeze_115, relu_36, convolution_39, squeeze_118, getitem_378, convolution_40, squeeze_121, add_221, convolution_41, squeeze_124, add_227, convolution_42, squeeze_127, add_233, convolution_43, squeeze_130, add_239, convolution_44, squeeze_133, add_245, convolution_45, squeeze_136, add_251, convolution_46, squeeze_139, cat_4, convolution_47, squeeze_142, relu_45, convolution_48, squeeze_145, getitem_468, convolution_49, squeeze_148, add_273, convolution_50, squeeze_151, add_279, convolution_51, squeeze_154, add_285, convolution_52, squeeze_157, add_291, convolution_53, squeeze_160, add_297, convolution_54, squeeze_163, add_303, convolution_55, squeeze_166, cat_5, convolution_56, squeeze_169, relu_54, convolution_57, squeeze_172, getitem_558, convolution_58, squeeze_175, add_325, convolution_59, squeeze_178, add_331, convolution_60, squeeze_181, add_337, convolution_61, squeeze_184, add_343, convolution_62, squeeze_187, add_349, convolution_63, squeeze_190, add_355, convolution_64, squeeze_193, cat_6, convolution_65, squeeze_196, relu_63, convolution_66, squeeze_199, getitem_648, convolution_67, squeeze_202, getitem_659, convolution_68, squeeze_205, getitem_670, convolution_69, squeeze_208, getitem_681, convolution_70, squeeze_211, getitem_692, convolution_71, squeeze_214, getitem_703, convolution_72, squeeze_217, getitem_714, convolution_73, squeeze_220, getitem_725, cat_7, convolution_74, squeeze_223, convolution_75, squeeze_226, relu_72, convolution_76, squeeze_229, getitem_740, convolution_77, squeeze_232, add_428, convolution_78, squeeze_235, add_434, convolution_79, squeeze_238, add_440, convolution_80, squeeze_241, add_446, convolution_81, squeeze_244, add_452, convolution_82, squeeze_247, add_458, convolution_83, squeeze_250, cat_8, convolution_84, squeeze_253, relu_81, convolution_85, squeeze_256, getitem_830, convolution_86, squeeze_259, add_480, convolution_87, squeeze_262, add_486, convolution_88, squeeze_265, add_492, convolution_89, squeeze_268, add_498, convolution_90, squeeze_271, add_504, convolution_91, squeeze_274, add_510, convolution_92, squeeze_277, cat_9, convolution_93, squeeze_280, relu_90, convolution_94, squeeze_283, getitem_920, convolution_95, squeeze_286, add_532, convolution_96, squeeze_289, add_538, convolution_97, squeeze_292, add_544, convolution_98, squeeze_295, add_550, convolution_99, squeeze_298, add_556, convolution_100, squeeze_301, add_562, convolution_101, squeeze_304, cat_10, convolution_102, squeeze_307, relu_99, convolution_103, squeeze_310, getitem_1010, convolution_104, squeeze_313, add_584, convolution_105, squeeze_316, add_590, convolution_106, squeeze_319, add_596, convolution_107, squeeze_322, add_602, convolution_108, squeeze_325, add_608, convolution_109, squeeze_328, add_614, convolution_110, squeeze_331, cat_11, convolution_111, squeeze_334, relu_108, convolution_112, squeeze_337, getitem_1100, convolution_113, squeeze_340, add_636, convolution_114, squeeze_343, add_642, convolution_115, squeeze_346, add_648, convolution_116, squeeze_349, add_654, convolution_117, squeeze_352, add_660, convolution_118, squeeze_355, add_666, convolution_119, squeeze_358, cat_12, convolution_120, squeeze_361, relu_117, convolution_121, squeeze_364, getitem_1190, convolution_122, squeeze_367, getitem_1201, convolution_123, squeeze_370, getitem_1212, convolution_124, squeeze_373, getitem_1223, convolution_125, squeeze_376, getitem_1234, convolution_126, squeeze_379, getitem_1245, convolution_127, squeeze_382, getitem_1256, convolution_128, squeeze_385, getitem_1267, cat_13, convolution_129, squeeze_388, convolution_130, squeeze_391, relu_126, convolution_131, squeeze_394, getitem_1282, convolution_132, squeeze_397, add_739, convolution_133, squeeze_400, add_745, convolution_134, squeeze_403, add_751, convolution_135, squeeze_406, add_757, convolution_136, squeeze_409, add_763, convolution_137, squeeze_412, add_769, convolution_138, squeeze_415, cat_14, convolution_139, squeeze_418, relu_135, convolution_140, squeeze_421, getitem_1372, convolution_141, squeeze_424, add_791, convolution_142, squeeze_427, add_797, convolution_143, squeeze_430, add_803, convolution_144, squeeze_433, add_809, convolution_145, squeeze_436, add_815, convolution_146, squeeze_439, add_821, convolution_147, squeeze_442, cat_15, convolution_148, squeeze_445, view, permute_1, le, unsqueeze_598, le_1, unsqueeze_610, le_2, unsqueeze_622, le_3, unsqueeze_634, le_4, unsqueeze_646, le_5, unsqueeze_658, le_6, unsqueeze_670, le_7, unsqueeze_682, le_8, unsqueeze_694, unsqueeze_706, le_10, unsqueeze_718, le_11, unsqueeze_730, le_12, unsqueeze_742, le_13, unsqueeze_754, le_14, unsqueeze_766, le_15, unsqueeze_778, le_16, unsqueeze_790, le_17, unsqueeze_802, unsqueeze_814, unsqueeze_826, le_19, unsqueeze_838, le_20, unsqueeze_850, le_21, unsqueeze_862, le_22, unsqueeze_874, le_23, unsqueeze_886, le_24, unsqueeze_898, le_25, unsqueeze_910, le_26, unsqueeze_922, unsqueeze_934, le_28, unsqueeze_946, le_29, unsqueeze_958, le_30, unsqueeze_970, le_31, unsqueeze_982, le_32, unsqueeze_994, le_33, unsqueeze_1006, le_34, unsqueeze_1018, le_35, unsqueeze_1030, unsqueeze_1042, le_37, unsqueeze_1054, le_38, unsqueeze_1066, le_39, unsqueeze_1078, le_40, unsqueeze_1090, le_41, unsqueeze_1102, le_42, unsqueeze_1114, le_43, unsqueeze_1126, le_44, unsqueeze_1138, unsqueeze_1150, le_46, unsqueeze_1162, le_47, unsqueeze_1174, le_48, unsqueeze_1186, le_49, unsqueeze_1198, le_50, unsqueeze_1210, le_51, unsqueeze_1222, le_52, unsqueeze_1234, le_53, unsqueeze_1246, unsqueeze_1258, le_55, unsqueeze_1270, le_56, unsqueeze_1282, le_57, unsqueeze_1294, le_58, unsqueeze_1306, le_59, unsqueeze_1318, le_60, unsqueeze_1330, le_61, unsqueeze_1342, le_62, unsqueeze_1354, unsqueeze_1366, le_64, unsqueeze_1378, le_65, unsqueeze_1390, le_66, unsqueeze_1402, le_67, unsqueeze_1414, le_68, unsqueeze_1426, le_69, unsqueeze_1438, le_70, unsqueeze_1450, le_71, unsqueeze_1462, unsqueeze_1474, unsqueeze_1486, le_73, unsqueeze_1498, le_74, unsqueeze_1510, le_75, unsqueeze_1522, le_76, unsqueeze_1534, le_77, unsqueeze_1546, le_78, unsqueeze_1558, le_79, unsqueeze_1570, le_80, unsqueeze_1582, unsqueeze_1594, le_82, unsqueeze_1606, le_83, unsqueeze_1618, le_84, unsqueeze_1630, le_85, unsqueeze_1642, le_86, unsqueeze_1654, le_87, unsqueeze_1666, le_88, unsqueeze_1678, le_89, unsqueeze_1690, unsqueeze_1702, le_91, unsqueeze_1714, le_92, unsqueeze_1726, le_93, unsqueeze_1738, le_94, unsqueeze_1750, le_95, unsqueeze_1762, le_96, unsqueeze_1774, le_97, unsqueeze_1786, le_98, unsqueeze_1798, unsqueeze_1810, le_100, unsqueeze_1822, le_101, unsqueeze_1834, le_102, unsqueeze_1846, le_103, unsqueeze_1858, le_104, unsqueeze_1870, le_105, unsqueeze_1882, le_106, unsqueeze_1894, le_107, unsqueeze_1906, unsqueeze_1918, unsqueeze_1930, le_109, unsqueeze_1942, le_110, unsqueeze_1954, le_111, unsqueeze_1966, le_112, unsqueeze_1978, le_113, unsqueeze_1990, le_114, unsqueeze_2002, le_115, unsqueeze_2014, le_116, unsqueeze_2026, unsqueeze_2038, le_118, unsqueeze_2050, le_119, unsqueeze_2062, le_120, unsqueeze_2074, le_121, unsqueeze_2086, le_122, unsqueeze_2098, le_123, unsqueeze_2110, le_124, unsqueeze_2122, le_125, unsqueeze_2134, unsqueeze_2146, le_127, unsqueeze_2158, le_128, unsqueeze_2170, le_129, unsqueeze_2182, le_130, unsqueeze_2194, le_131, unsqueeze_2206, le_132, unsqueeze_2218, le_133, unsqueeze_2230, le_134, unsqueeze_2242, unsqueeze_2254, unsqueeze_2266, le_136, unsqueeze_2278, le_137, unsqueeze_2290, le_138, unsqueeze_2302, le_139, unsqueeze_2314, le_140, unsqueeze_2326, le_141, unsqueeze_2338, le_142, unsqueeze_2350, le_143, unsqueeze_2362, unsqueeze_2374, tangents_1 = args
    args.clear()
    assert_size_stride(primals_1, (64, 3, 7, 7), (147, 1, 21, 3))
    assert_size_stride(primals_2, (64, ), (1, ))
    assert_size_stride(primals_4, (112, 64, 1, 1), (64, 1, 1, 1))
    assert_size_stride(primals_5, (112, ), (1, ))
    assert_size_stride(primals_7, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_8, (14, ), (1, ))
    assert_size_stride(primals_10, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_11, (14, ), (1, ))
    assert_size_stride(primals_13, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_14, (14, ), (1, ))
    assert_size_stride(primals_16, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_17, (14, ), (1, ))
    assert_size_stride(primals_19, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_20, (14, ), (1, ))
    assert_size_stride(primals_22, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_23, (14, ), (1, ))
    assert_size_stride(primals_25, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_26, (14, ), (1, ))
    assert_size_stride(primals_28, (256, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(primals_29, (256, ), (1, ))
    assert_size_stride(primals_31, (256, 64, 1, 1), (64, 1, 1, 1))
    assert_size_stride(primals_32, (256, ), (1, ))
    assert_size_stride(primals_34, (112, 256, 1, 1), (256, 1, 1, 1))
    assert_size_stride(primals_35, (112, ), (1, ))
    assert_size_stride(primals_37, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_38, (14, ), (1, ))
    assert_size_stride(primals_40, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_41, (14, ), (1, ))
    assert_size_stride(primals_43, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_44, (14, ), (1, ))
    assert_size_stride(primals_46, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_47, (14, ), (1, ))
    assert_size_stride(primals_49, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_50, (14, ), (1, ))
    assert_size_stride(primals_52, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_53, (14, ), (1, ))
    assert_size_stride(primals_55, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_56, (14, ), (1, ))
    assert_size_stride(primals_58, (256, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(primals_59, (256, ), (1, ))
    assert_size_stride(primals_61, (112, 256, 1, 1), (256, 1, 1, 1))
    assert_size_stride(primals_62, (112, ), (1, ))
    assert_size_stride(primals_64, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_65, (14, ), (1, ))
    assert_size_stride(primals_67, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_68, (14, ), (1, ))
    assert_size_stride(primals_70, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_71, (14, ), (1, ))
    assert_size_stride(primals_73, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_74, (14, ), (1, ))
    assert_size_stride(primals_76, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_77, (14, ), (1, ))
    assert_size_stride(primals_79, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_80, (14, ), (1, ))
    assert_size_stride(primals_82, (14, 14, 3, 3), (126, 1, 42, 14))
    assert_size_stride(primals_83, (14, ), (1, ))
    assert_size_stride(primals_85, (256, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(primals_86, (256, ), (1, ))
    assert_size_stride(primals_88, (224, 256, 1, 1), (256, 1, 1, 1))
    assert_size_stride(primals_89, (224, ), (1, ))
    assert_size_stride(primals_91, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_92, (28, ), (1, ))
    assert_size_stride(primals_94, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_95, (28, ), (1, ))
    assert_size_stride(primals_97, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_98, (28, ), (1, ))
    assert_size_stride(primals_100, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_101, (28, ), (1, ))
    assert_size_stride(primals_103, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_104, (28, ), (1, ))
    assert_size_stride(primals_106, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_107, (28, ), (1, ))
    assert_size_stride(primals_109, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_110, (28, ), (1, ))
    assert_size_stride(primals_112, (512, 224, 1, 1), (224, 1, 1, 1))
    assert_size_stride(primals_113, (512, ), (1, ))
    assert_size_stride(primals_115, (512, 256, 1, 1), (256, 1, 1, 1))
    assert_size_stride(primals_116, (512, ), (1, ))
    assert_size_stride(primals_118, (224, 512, 1, 1), (512, 1, 1, 1))
    assert_size_stride(primals_119, (224, ), (1, ))
    assert_size_stride(primals_121, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_122, (28, ), (1, ))
    assert_size_stride(primals_124, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_125, (28, ), (1, ))
    assert_size_stride(primals_127, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_128, (28, ), (1, ))
    assert_size_stride(primals_130, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_131, (28, ), (1, ))
    assert_size_stride(primals_133, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_134, (28, ), (1, ))
    assert_size_stride(primals_136, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_137, (28, ), (1, ))
    assert_size_stride(primals_139, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_140, (28, ), (1, ))
    assert_size_stride(primals_142, (512, 224, 1, 1), (224, 1, 1, 1))
    assert_size_stride(primals_143, (512, ), (1, ))
    assert_size_stride(primals_145, (224, 512, 1, 1), (512, 1, 1, 1))
    assert_size_stride(primals_146, (224, ), (1, ))
    assert_size_stride(primals_148, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_149, (28, ), (1, ))
    assert_size_stride(primals_151, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_152, (28, ), (1, ))
    assert_size_stride(primals_154, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_155, (28, ), (1, ))
    assert_size_stride(primals_157, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_158, (28, ), (1, ))
    assert_size_stride(primals_160, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_161, (28, ), (1, ))
    assert_size_stride(primals_163, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_164, (28, ), (1, ))
    assert_size_stride(primals_166, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_167, (28, ), (1, ))
    assert_size_stride(primals_169, (512, 224, 1, 1), (224, 1, 1, 1))
    assert_size_stride(primals_170, (512, ), (1, ))
    assert_size_stride(primals_172, (224, 512, 1, 1), (512, 1, 1, 1))
    assert_size_stride(primals_173, (224, ), (1, ))
    assert_size_stride(primals_175, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_176, (28, ), (1, ))
    assert_size_stride(primals_178, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_179, (28, ), (1, ))
    assert_size_stride(primals_181, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_182, (28, ), (1, ))
    assert_size_stride(primals_184, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_185, (28, ), (1, ))
    assert_size_stride(primals_187, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_188, (28, ), (1, ))
    assert_size_stride(primals_190, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_191, (28, ), (1, ))
    assert_size_stride(primals_193, (28, 28, 3, 3), (252, 1, 84, 28))
    assert_size_stride(primals_194, (28, ), (1, ))
    assert_size_stride(primals_196, (512, 224, 1, 1), (224, 1, 1, 1))
    assert_size_stride(primals_197, (512, ), (1, ))
    assert_size_stride(primals_199, (448, 512, 1, 1), (512, 1, 1, 1))
    assert_size_stride(primals_200, (448, ), (1, ))
    assert_size_stride(primals_202, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_203, (56, ), (1, ))
    assert_size_stride(primals_205, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_206, (56, ), (1, ))
    assert_size_stride(primals_208, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_209, (56, ), (1, ))
    assert_size_stride(primals_211, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_212, (56, ), (1, ))
    assert_size_stride(primals_214, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_215, (56, ), (1, ))
    assert_size_stride(primals_217, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_218, (56, ), (1, ))
    assert_size_stride(primals_220, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_221, (56, ), (1, ))
    assert_size_stride(primals_223, (1024, 448, 1, 1), (448, 1, 1, 1))
    assert_size_stride(primals_224, (1024, ), (1, ))
    assert_size_stride(primals_226, (1024, 512, 1, 1), (512, 1, 1, 1))
    assert_size_stride(primals_227, (1024, ), (1, ))
    assert_size_stride(primals_229, (448, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(primals_230, (448, ), (1, ))
    assert_size_stride(primals_232, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_233, (56, ), (1, ))
    assert_size_stride(primals_235, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_236, (56, ), (1, ))
    assert_size_stride(primals_238, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_239, (56, ), (1, ))
    assert_size_stride(primals_241, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_242, (56, ), (1, ))
    assert_size_stride(primals_244, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_245, (56, ), (1, ))
    assert_size_stride(primals_247, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_248, (56, ), (1, ))
    assert_size_stride(primals_250, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_251, (56, ), (1, ))
    assert_size_stride(primals_253, (1024, 448, 1, 1), (448, 1, 1, 1))
    assert_size_stride(primals_254, (1024, ), (1, ))
    assert_size_stride(primals_256, (448, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(primals_257, (448, ), (1, ))
    assert_size_stride(primals_259, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_260, (56, ), (1, ))
    assert_size_stride(primals_262, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_263, (56, ), (1, ))
    assert_size_stride(primals_265, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_266, (56, ), (1, ))
    assert_size_stride(primals_268, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_269, (56, ), (1, ))
    assert_size_stride(primals_271, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_272, (56, ), (1, ))
    assert_size_stride(primals_274, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_275, (56, ), (1, ))
    assert_size_stride(primals_277, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_278, (56, ), (1, ))
    assert_size_stride(primals_280, (1024, 448, 1, 1), (448, 1, 1, 1))
    assert_size_stride(primals_281, (1024, ), (1, ))
    assert_size_stride(primals_283, (448, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(primals_284, (448, ), (1, ))
    assert_size_stride(primals_286, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_287, (56, ), (1, ))
    assert_size_stride(primals_289, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_290, (56, ), (1, ))
    assert_size_stride(primals_292, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_293, (56, ), (1, ))
    assert_size_stride(primals_295, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_296, (56, ), (1, ))
    assert_size_stride(primals_298, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_299, (56, ), (1, ))
    assert_size_stride(primals_301, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_302, (56, ), (1, ))
    assert_size_stride(primals_304, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_305, (56, ), (1, ))
    assert_size_stride(primals_307, (1024, 448, 1, 1), (448, 1, 1, 1))
    assert_size_stride(primals_308, (1024, ), (1, ))
    assert_size_stride(primals_310, (448, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(primals_311, (448, ), (1, ))
    assert_size_stride(primals_313, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_314, (56, ), (1, ))
    assert_size_stride(primals_316, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_317, (56, ), (1, ))
    assert_size_stride(primals_319, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_320, (56, ), (1, ))
    assert_size_stride(primals_322, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_323, (56, ), (1, ))
    assert_size_stride(primals_325, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_326, (56, ), (1, ))
    assert_size_stride(primals_328, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_329, (56, ), (1, ))
    assert_size_stride(primals_331, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_332, (56, ), (1, ))
    assert_size_stride(primals_334, (1024, 448, 1, 1), (448, 1, 1, 1))
    assert_size_stride(primals_335, (1024, ), (1, ))
    assert_size_stride(primals_337, (448, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(primals_338, (448, ), (1, ))
    assert_size_stride(primals_340, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_341, (56, ), (1, ))
    assert_size_stride(primals_343, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_344, (56, ), (1, ))
    assert_size_stride(primals_346, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_347, (56, ), (1, ))
    assert_size_stride(primals_349, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_350, (56, ), (1, ))
    assert_size_stride(primals_352, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_353, (56, ), (1, ))
    assert_size_stride(primals_355, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_356, (56, ), (1, ))
    assert_size_stride(primals_358, (56, 56, 3, 3), (504, 1, 168, 56))
    assert_size_stride(primals_359, (56, ), (1, ))
    assert_size_stride(primals_361, (1024, 448, 1, 1), (448, 1, 1, 1))
    assert_size_stride(primals_362, (1024, ), (1, ))
    assert_size_stride(primals_364, (896, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(primals_365, (896, ), (1, ))
    assert_size_stride(primals_367, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_368, (112, ), (1, ))
    assert_size_stride(primals_370, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_371, (112, ), (1, ))
    assert_size_stride(primals_373, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_374, (112, ), (1, ))
    assert_size_stride(primals_376, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_377, (112, ), (1, ))
    assert_size_stride(primals_379, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_380, (112, ), (1, ))
    assert_size_stride(primals_382, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_383, (112, ), (1, ))
    assert_size_stride(primals_385, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_386, (112, ), (1, ))
    assert_size_stride(primals_388, (2048, 896, 1, 1), (896, 1, 1, 1))
    assert_size_stride(primals_389, (2048, ), (1, ))
    assert_size_stride(primals_391, (2048, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(primals_392, (2048, ), (1, ))
    assert_size_stride(primals_394, (896, 2048, 1, 1), (2048, 1, 1, 1))
    assert_size_stride(primals_395, (896, ), (1, ))
    assert_size_stride(primals_397, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_398, (112, ), (1, ))
    assert_size_stride(primals_400, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_401, (112, ), (1, ))
    assert_size_stride(primals_403, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_404, (112, ), (1, ))
    assert_size_stride(primals_406, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_407, (112, ), (1, ))
    assert_size_stride(primals_409, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_410, (112, ), (1, ))
    assert_size_stride(primals_412, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_413, (112, ), (1, ))
    assert_size_stride(primals_415, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_416, (112, ), (1, ))
    assert_size_stride(primals_418, (2048, 896, 1, 1), (896, 1, 1, 1))
    assert_size_stride(primals_419, (2048, ), (1, ))
    assert_size_stride(primals_421, (896, 2048, 1, 1), (2048, 1, 1, 1))
    assert_size_stride(primals_422, (896, ), (1, ))
    assert_size_stride(primals_424, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_425, (112, ), (1, ))
    assert_size_stride(primals_427, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_428, (112, ), (1, ))
    assert_size_stride(primals_430, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_431, (112, ), (1, ))
    assert_size_stride(primals_433, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_434, (112, ), (1, ))
    assert_size_stride(primals_436, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_437, (112, ), (1, ))
    assert_size_stride(primals_439, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_440, (112, ), (1, ))
    assert_size_stride(primals_442, (112, 112, 3, 3), (1008, 1, 336, 112))
    assert_size_stride(primals_443, (112, ), (1, ))
    assert_size_stride(primals_445, (2048, 896, 1, 1), (896, 1, 1, 1))
    assert_size_stride(primals_446, (2048, ), (1, ))
    assert_size_stride(primals_897, (8, 3, 224, 224), (150528, 1, 672, 3))
    assert_size_stride(convolution, (8, 64, 112, 112), (802816, 1, 7168, 64))
    assert_size_stride(squeeze_1, (64, ), (1, ))
    assert_size_stride(relu, (8, 64, 112, 112), (802816, 1, 7168, 64))
    assert_size_stride(getitem_2, (8, 64, 56, 56), (200704, 1, 3584, 64))
    assert_size_stride(getitem_3, (8, 64, 56, 56), (200704, 1, 3584, 64))
    assert_size_stride(convolution_1, (8, 112, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(squeeze_4, (112, ), (1, ))
    assert_size_stride(getitem_14, (8, 14, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(convolution_2, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_7, (14, ), (1, ))
    assert_size_stride(getitem_25, (8, 14, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(convolution_3, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_10, (14, ), (1, ))
    assert_size_stride(getitem_36, (8, 14, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(convolution_4, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_13, (14, ), (1, ))
    assert_size_stride(getitem_47, (8, 14, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(convolution_5, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_16, (14, ), (1, ))
    assert_size_stride(getitem_58, (8, 14, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(convolution_6, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_19, (14, ), (1, ))
    assert_size_stride(getitem_69, (8, 14, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(convolution_7, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_22, (14, ), (1, ))
    assert_size_stride(getitem_80, (8, 14, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(convolution_8, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_25, (14, ), (1, ))
    assert_size_stride(getitem_91, (8, 14, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(cat, (8, 112, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(convolution_9, (8, 256, 56, 56), (802816, 1, 14336, 256))
    assert_size_stride(squeeze_28, (256, ), (1, ))
    assert_size_stride(convolution_10, (8, 256, 56, 56), (802816, 1, 14336, 256))
    assert_size_stride(squeeze_31, (256, ), (1, ))
    assert_size_stride(relu_9, (8, 256, 56, 56), (802816, 1, 14336, 256))
    assert_size_stride(convolution_11, (8, 112, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(squeeze_34, (112, ), (1, ))
    assert_size_stride(getitem_106, (8, 14, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(convolution_12, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_37, (14, ), (1, ))
    assert_size_stride(add_66, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(convolution_13, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_40, (14, ), (1, ))
    assert_size_stride(add_72, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(convolution_14, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_43, (14, ), (1, ))
    assert_size_stride(add_78, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(convolution_15, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_46, (14, ), (1, ))
    assert_size_stride(add_84, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(convolution_16, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_49, (14, ), (1, ))
    assert_size_stride(add_90, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(convolution_17, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_52, (14, ), (1, ))
    assert_size_stride(add_96, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(convolution_18, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_55, (14, ), (1, ))
    assert_size_stride(cat_1, (8, 112, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(convolution_19, (8, 256, 56, 56), (802816, 1, 14336, 256))
    assert_size_stride(squeeze_58, (256, ), (1, ))
    assert_size_stride(relu_18, (8, 256, 56, 56), (802816, 1, 14336, 256))
    assert_size_stride(convolution_20, (8, 112, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(squeeze_61, (112, ), (1, ))
    assert_size_stride(getitem_196, (8, 14, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(convolution_21, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_64, (14, ), (1, ))
    assert_size_stride(add_118, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(convolution_22, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_67, (14, ), (1, ))
    assert_size_stride(add_124, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(convolution_23, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_70, (14, ), (1, ))
    assert_size_stride(add_130, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(convolution_24, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_73, (14, ), (1, ))
    assert_size_stride(add_136, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(convolution_25, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_76, (14, ), (1, ))
    assert_size_stride(add_142, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(convolution_26, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_79, (14, ), (1, ))
    assert_size_stride(add_148, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(convolution_27, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(squeeze_82, (14, ), (1, ))
    assert_size_stride(cat_2, (8, 112, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(convolution_28, (8, 256, 56, 56), (802816, 1, 14336, 256))
    assert_size_stride(squeeze_85, (256, ), (1, ))
    assert_size_stride(relu_27, (8, 256, 56, 56), (802816, 1, 14336, 256))
    assert_size_stride(convolution_29, (8, 224, 56, 56), (702464, 1, 12544, 224))
    assert_size_stride(squeeze_88, (224, ), (1, ))
    assert_size_stride(getitem_286, (8, 28, 56, 56), (702464, 1, 12544, 224))
    assert_size_stride(convolution_30, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_91, (28, ), (1, ))
    assert_size_stride(getitem_297, (8, 28, 56, 56), (702464, 1, 12544, 224))
    assert_size_stride(convolution_31, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_94, (28, ), (1, ))
    assert_size_stride(getitem_308, (8, 28, 56, 56), (702464, 1, 12544, 224))
    assert_size_stride(convolution_32, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_97, (28, ), (1, ))
    assert_size_stride(getitem_319, (8, 28, 56, 56), (702464, 1, 12544, 224))
    assert_size_stride(convolution_33, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_100, (28, ), (1, ))
    assert_size_stride(getitem_330, (8, 28, 56, 56), (702464, 1, 12544, 224))
    assert_size_stride(convolution_34, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_103, (28, ), (1, ))
    assert_size_stride(getitem_341, (8, 28, 56, 56), (702464, 1, 12544, 224))
    assert_size_stride(convolution_35, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_106, (28, ), (1, ))
    assert_size_stride(getitem_352, (8, 28, 56, 56), (702464, 1, 12544, 224))
    assert_size_stride(convolution_36, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_109, (28, ), (1, ))
    assert_size_stride(getitem_363, (8, 28, 56, 56), (702464, 1, 12544, 224))
    assert_size_stride(cat_3, (8, 224, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(convolution_37, (8, 512, 28, 28), (401408, 1, 14336, 512))
    assert_size_stride(squeeze_112, (512, ), (1, ))
    assert_size_stride(convolution_38, (8, 512, 28, 28), (401408, 1, 14336, 512))
    assert_size_stride(squeeze_115, (512, ), (1, ))
    assert_size_stride(relu_36, (8, 512, 28, 28), (401408, 1, 14336, 512))
    assert_size_stride(convolution_39, (8, 224, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(squeeze_118, (224, ), (1, ))
    assert_size_stride(getitem_378, (8, 28, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(convolution_40, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_121, (28, ), (1, ))
    assert_size_stride(add_221, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_41, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_124, (28, ), (1, ))
    assert_size_stride(add_227, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_42, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_127, (28, ), (1, ))
    assert_size_stride(add_233, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_43, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_130, (28, ), (1, ))
    assert_size_stride(add_239, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_44, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_133, (28, ), (1, ))
    assert_size_stride(add_245, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_45, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_136, (28, ), (1, ))
    assert_size_stride(add_251, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_46, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_139, (28, ), (1, ))
    assert_size_stride(cat_4, (8, 224, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(convolution_47, (8, 512, 28, 28), (401408, 1, 14336, 512))
    assert_size_stride(squeeze_142, (512, ), (1, ))
    assert_size_stride(relu_45, (8, 512, 28, 28), (401408, 1, 14336, 512))
    assert_size_stride(convolution_48, (8, 224, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(squeeze_145, (224, ), (1, ))
    assert_size_stride(getitem_468, (8, 28, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(convolution_49, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_148, (28, ), (1, ))
    assert_size_stride(add_273, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_50, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_151, (28, ), (1, ))
    assert_size_stride(add_279, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_51, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_154, (28, ), (1, ))
    assert_size_stride(add_285, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_52, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_157, (28, ), (1, ))
    assert_size_stride(add_291, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_53, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_160, (28, ), (1, ))
    assert_size_stride(add_297, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_54, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_163, (28, ), (1, ))
    assert_size_stride(add_303, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_55, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_166, (28, ), (1, ))
    assert_size_stride(cat_5, (8, 224, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(convolution_56, (8, 512, 28, 28), (401408, 1, 14336, 512))
    assert_size_stride(squeeze_169, (512, ), (1, ))
    assert_size_stride(relu_54, (8, 512, 28, 28), (401408, 1, 14336, 512))
    assert_size_stride(convolution_57, (8, 224, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(squeeze_172, (224, ), (1, ))
    assert_size_stride(getitem_558, (8, 28, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(convolution_58, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_175, (28, ), (1, ))
    assert_size_stride(add_325, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_59, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_178, (28, ), (1, ))
    assert_size_stride(add_331, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_60, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_181, (28, ), (1, ))
    assert_size_stride(add_337, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_61, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_184, (28, ), (1, ))
    assert_size_stride(add_343, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_62, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_187, (28, ), (1, ))
    assert_size_stride(add_349, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_63, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_190, (28, ), (1, ))
    assert_size_stride(add_355, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(convolution_64, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(squeeze_193, (28, ), (1, ))
    assert_size_stride(cat_6, (8, 224, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(convolution_65, (8, 512, 28, 28), (401408, 1, 14336, 512))
    assert_size_stride(squeeze_196, (512, ), (1, ))
    assert_size_stride(relu_63, (8, 512, 28, 28), (401408, 1, 14336, 512))
    assert_size_stride(convolution_66, (8, 448, 28, 28), (351232, 1, 12544, 448))
    assert_size_stride(squeeze_199, (448, ), (1, ))
    assert_size_stride(getitem_648, (8, 56, 28, 28), (351232, 1, 12544, 448))
    assert_size_stride(convolution_67, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_202, (56, ), (1, ))
    assert_size_stride(getitem_659, (8, 56, 28, 28), (351232, 1, 12544, 448))
    assert_size_stride(convolution_68, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_205, (56, ), (1, ))
    assert_size_stride(getitem_670, (8, 56, 28, 28), (351232, 1, 12544, 448))
    assert_size_stride(convolution_69, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_208, (56, ), (1, ))
    assert_size_stride(getitem_681, (8, 56, 28, 28), (351232, 1, 12544, 448))
    assert_size_stride(convolution_70, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_211, (56, ), (1, ))
    assert_size_stride(getitem_692, (8, 56, 28, 28), (351232, 1, 12544, 448))
    assert_size_stride(convolution_71, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_214, (56, ), (1, ))
    assert_size_stride(getitem_703, (8, 56, 28, 28), (351232, 1, 12544, 448))
    assert_size_stride(convolution_72, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_217, (56, ), (1, ))
    assert_size_stride(getitem_714, (8, 56, 28, 28), (351232, 1, 12544, 448))
    assert_size_stride(convolution_73, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_220, (56, ), (1, ))
    assert_size_stride(getitem_725, (8, 56, 28, 28), (351232, 1, 12544, 448))
    assert_size_stride(cat_7, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(convolution_74, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(squeeze_223, (1024, ), (1, ))
    assert_size_stride(convolution_75, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(squeeze_226, (1024, ), (1, ))
    assert_size_stride(relu_72, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(convolution_76, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(squeeze_229, (448, ), (1, ))
    assert_size_stride(getitem_740, (8, 56, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(convolution_77, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_232, (56, ), (1, ))
    assert_size_stride(add_428, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_78, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_235, (56, ), (1, ))
    assert_size_stride(add_434, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_79, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_238, (56, ), (1, ))
    assert_size_stride(add_440, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_80, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_241, (56, ), (1, ))
    assert_size_stride(add_446, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_81, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_244, (56, ), (1, ))
    assert_size_stride(add_452, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_82, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_247, (56, ), (1, ))
    assert_size_stride(add_458, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_83, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_250, (56, ), (1, ))
    assert_size_stride(cat_8, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(convolution_84, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(squeeze_253, (1024, ), (1, ))
    assert_size_stride(relu_81, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(convolution_85, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(squeeze_256, (448, ), (1, ))
    assert_size_stride(getitem_830, (8, 56, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(convolution_86, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_259, (56, ), (1, ))
    assert_size_stride(add_480, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_87, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_262, (56, ), (1, ))
    assert_size_stride(add_486, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_88, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_265, (56, ), (1, ))
    assert_size_stride(add_492, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_89, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_268, (56, ), (1, ))
    assert_size_stride(add_498, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_90, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_271, (56, ), (1, ))
    assert_size_stride(add_504, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_91, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_274, (56, ), (1, ))
    assert_size_stride(add_510, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_92, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_277, (56, ), (1, ))
    assert_size_stride(cat_9, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(convolution_93, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(squeeze_280, (1024, ), (1, ))
    assert_size_stride(relu_90, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(convolution_94, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(squeeze_283, (448, ), (1, ))
    assert_size_stride(getitem_920, (8, 56, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(convolution_95, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_286, (56, ), (1, ))
    assert_size_stride(add_532, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_96, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_289, (56, ), (1, ))
    assert_size_stride(add_538, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_97, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_292, (56, ), (1, ))
    assert_size_stride(add_544, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_98, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_295, (56, ), (1, ))
    assert_size_stride(add_550, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_99, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_298, (56, ), (1, ))
    assert_size_stride(add_556, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_100, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_301, (56, ), (1, ))
    assert_size_stride(add_562, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_101, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_304, (56, ), (1, ))
    assert_size_stride(cat_10, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(convolution_102, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(squeeze_307, (1024, ), (1, ))
    assert_size_stride(relu_99, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(convolution_103, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(squeeze_310, (448, ), (1, ))
    assert_size_stride(getitem_1010, (8, 56, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(convolution_104, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_313, (56, ), (1, ))
    assert_size_stride(add_584, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_105, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_316, (56, ), (1, ))
    assert_size_stride(add_590, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_106, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_319, (56, ), (1, ))
    assert_size_stride(add_596, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_107, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_322, (56, ), (1, ))
    assert_size_stride(add_602, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_108, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_325, (56, ), (1, ))
    assert_size_stride(add_608, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_109, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_328, (56, ), (1, ))
    assert_size_stride(add_614, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_110, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_331, (56, ), (1, ))
    assert_size_stride(cat_11, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(convolution_111, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(squeeze_334, (1024, ), (1, ))
    assert_size_stride(relu_108, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(convolution_112, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(squeeze_337, (448, ), (1, ))
    assert_size_stride(getitem_1100, (8, 56, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(convolution_113, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_340, (56, ), (1, ))
    assert_size_stride(add_636, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_114, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_343, (56, ), (1, ))
    assert_size_stride(add_642, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_115, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_346, (56, ), (1, ))
    assert_size_stride(add_648, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_116, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_349, (56, ), (1, ))
    assert_size_stride(add_654, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_117, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_352, (56, ), (1, ))
    assert_size_stride(add_660, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_118, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_355, (56, ), (1, ))
    assert_size_stride(add_666, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(convolution_119, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(squeeze_358, (56, ), (1, ))
    assert_size_stride(cat_12, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(convolution_120, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(squeeze_361, (1024, ), (1, ))
    assert_size_stride(relu_117, (8, 1024, 14, 14), (200704, 1, 14336, 1024))
    assert_size_stride(convolution_121, (8, 896, 14, 14), (175616, 1, 12544, 896))
    assert_size_stride(squeeze_364, (896, ), (1, ))
    assert_size_stride(getitem_1190, (8, 112, 14, 14), (175616, 1, 12544, 896))
    assert_size_stride(convolution_122, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_367, (112, ), (1, ))
    assert_size_stride(getitem_1201, (8, 112, 14, 14), (175616, 1, 12544, 896))
    assert_size_stride(convolution_123, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_370, (112, ), (1, ))
    assert_size_stride(getitem_1212, (8, 112, 14, 14), (175616, 1, 12544, 896))
    assert_size_stride(convolution_124, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_373, (112, ), (1, ))
    assert_size_stride(getitem_1223, (8, 112, 14, 14), (175616, 1, 12544, 896))
    assert_size_stride(convolution_125, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_376, (112, ), (1, ))
    assert_size_stride(getitem_1234, (8, 112, 14, 14), (175616, 1, 12544, 896))
    assert_size_stride(convolution_126, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_379, (112, ), (1, ))
    assert_size_stride(getitem_1245, (8, 112, 14, 14), (175616, 1, 12544, 896))
    assert_size_stride(convolution_127, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_382, (112, ), (1, ))
    assert_size_stride(getitem_1256, (8, 112, 14, 14), (175616, 1, 12544, 896))
    assert_size_stride(convolution_128, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_385, (112, ), (1, ))
    assert_size_stride(getitem_1267, (8, 112, 14, 14), (175616, 1, 12544, 896))
    assert_size_stride(cat_13, (8, 896, 7, 7), (43904, 1, 6272, 896))
    assert_size_stride(convolution_129, (8, 2048, 7, 7), (100352, 1, 14336, 2048))
    assert_size_stride(squeeze_388, (2048, ), (1, ))
    assert_size_stride(convolution_130, (8, 2048, 7, 7), (100352, 1, 14336, 2048))
    assert_size_stride(squeeze_391, (2048, ), (1, ))
    assert_size_stride(relu_126, (8, 2048, 7, 7), (100352, 1, 14336, 2048))
    assert_size_stride(convolution_131, (8, 896, 7, 7), (43904, 1, 6272, 896))
    assert_size_stride(squeeze_394, (896, ), (1, ))
    assert_size_stride(getitem_1282, (8, 112, 7, 7), (43904, 1, 6272, 896))
    assert_size_stride(convolution_132, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_397, (112, ), (1, ))
    assert_size_stride(add_739, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(convolution_133, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_400, (112, ), (1, ))
    assert_size_stride(add_745, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(convolution_134, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_403, (112, ), (1, ))
    assert_size_stride(add_751, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(convolution_135, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_406, (112, ), (1, ))
    assert_size_stride(add_757, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(convolution_136, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_409, (112, ), (1, ))
    assert_size_stride(add_763, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(convolution_137, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_412, (112, ), (1, ))
    assert_size_stride(add_769, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(convolution_138, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_415, (112, ), (1, ))
    assert_size_stride(cat_14, (8, 896, 7, 7), (43904, 1, 6272, 896))
    assert_size_stride(convolution_139, (8, 2048, 7, 7), (100352, 1, 14336, 2048))
    assert_size_stride(squeeze_418, (2048, ), (1, ))
    assert_size_stride(relu_135, (8, 2048, 7, 7), (100352, 1, 14336, 2048))
    assert_size_stride(convolution_140, (8, 896, 7, 7), (43904, 1, 6272, 896))
    assert_size_stride(squeeze_421, (896, ), (1, ))
    assert_size_stride(getitem_1372, (8, 112, 7, 7), (43904, 1, 6272, 896))
    assert_size_stride(convolution_141, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_424, (112, ), (1, ))
    assert_size_stride(add_791, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(convolution_142, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_427, (112, ), (1, ))
    assert_size_stride(add_797, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(convolution_143, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_430, (112, ), (1, ))
    assert_size_stride(add_803, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(convolution_144, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_433, (112, ), (1, ))
    assert_size_stride(add_809, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(convolution_145, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_436, (112, ), (1, ))
    assert_size_stride(add_815, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(convolution_146, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_439, (112, ), (1, ))
    assert_size_stride(add_821, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(convolution_147, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(squeeze_442, (112, ), (1, ))
    assert_size_stride(cat_15, (8, 896, 7, 7), (43904, 1, 6272, 896))
    assert_size_stride(convolution_148, (8, 2048, 7, 7), (100352, 1, 14336, 2048))
    assert_size_stride(squeeze_445, (2048, ), (1, ))
    assert_size_stride(view, (8, 2048), (2048, 1))
    assert_size_stride(permute_1, (1000, 2048), (2048, 1))
    assert_size_stride(le, (8, 2048, 7, 7), (100352, 1, 14336, 2048))
    assert_size_stride(unsqueeze_598, (1, 2048, 1, 1), (2048, 1, 1, 1))
    assert_size_stride(le_1, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_610, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_2, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_622, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_3, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_634, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_4, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_646, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_5, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_658, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_6, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_670, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_7, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_682, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_8, (8, 896, 7, 7), (43904, 1, 6272, 896))
    assert_size_stride(unsqueeze_694, (1, 896, 1, 1), (896, 1, 1, 1))
    assert_size_stride(unsqueeze_706, (1, 2048, 1, 1), (2048, 1, 1, 1))
    assert_size_stride(le_10, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_718, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_11, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_730, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_12, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_742, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_13, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_754, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_14, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_766, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_15, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_778, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_16, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_790, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_17, (8, 896, 7, 7), (43904, 1, 6272, 896))
    assert_size_stride(unsqueeze_802, (1, 896, 1, 1), (896, 1, 1, 1))
    assert_size_stride(unsqueeze_814, (1, 2048, 1, 1), (2048, 1, 1, 1))
    assert_size_stride(unsqueeze_826, (1, 2048, 1, 1), (2048, 1, 1, 1))
    assert_size_stride(le_19, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_838, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_20, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_850, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_21, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_862, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_22, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_874, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_23, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_886, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_24, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_898, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_25, (8, 112, 7, 7), (5488, 1, 784, 112))
    assert_size_stride(unsqueeze_910, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(le_26, (8, 896, 14, 14), (175616, 1, 12544, 896))
    assert_size_stride(unsqueeze_922, (1, 896, 1, 1), (896, 1, 1, 1))
    assert_size_stride(unsqueeze_934, (1, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(le_28, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_946, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_29, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_958, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_30, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_970, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_31, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_982, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_32, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_994, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_33, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1006, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_34, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1018, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_35, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(unsqueeze_1030, (1, 448, 1, 1), (448, 1, 1, 1))
    assert_size_stride(unsqueeze_1042, (1, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(le_37, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1054, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_38, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1066, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_39, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1078, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_40, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1090, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_41, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1102, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_42, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1114, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_43, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1126, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_44, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(unsqueeze_1138, (1, 448, 1, 1), (448, 1, 1, 1))
    assert_size_stride(unsqueeze_1150, (1, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(le_46, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1162, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_47, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1174, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_48, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1186, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_49, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1198, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_50, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1210, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_51, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1222, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_52, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1234, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_53, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(unsqueeze_1246, (1, 448, 1, 1), (448, 1, 1, 1))
    assert_size_stride(unsqueeze_1258, (1, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(le_55, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1270, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_56, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1282, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_57, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1294, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_58, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1306, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_59, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1318, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_60, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1330, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_61, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1342, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_62, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(unsqueeze_1354, (1, 448, 1, 1), (448, 1, 1, 1))
    assert_size_stride(unsqueeze_1366, (1, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(le_64, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1378, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_65, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1390, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_66, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1402, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_67, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1414, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_68, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1426, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_69, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1438, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_70, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1450, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_71, (8, 448, 14, 14), (87808, 1, 6272, 448))
    assert_size_stride(unsqueeze_1462, (1, 448, 1, 1), (448, 1, 1, 1))
    assert_size_stride(unsqueeze_1474, (1, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(unsqueeze_1486, (1, 1024, 1, 1), (1024, 1, 1, 1))
    assert_size_stride(le_73, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1498, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_74, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1510, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_75, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1522, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_76, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1534, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_77, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1546, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_78, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1558, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_79, (8, 56, 14, 14), (10976, 1, 784, 56))
    assert_size_stride(unsqueeze_1570, (1, 56, 1, 1), (56, 1, 1, 1))
    assert_size_stride(le_80, (8, 448, 28, 28), (351232, 1, 12544, 448))
    assert_size_stride(unsqueeze_1582, (1, 448, 1, 1), (448, 1, 1, 1))
    assert_size_stride(unsqueeze_1594, (1, 512, 1, 1), (512, 1, 1, 1))
    assert_size_stride(le_82, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1606, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_83, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1618, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_84, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1630, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_85, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1642, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_86, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1654, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_87, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1666, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_88, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1678, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_89, (8, 224, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(unsqueeze_1690, (1, 224, 1, 1), (224, 1, 1, 1))
    assert_size_stride(unsqueeze_1702, (1, 512, 1, 1), (512, 1, 1, 1))
    assert_size_stride(le_91, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1714, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_92, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1726, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_93, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1738, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_94, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1750, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_95, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1762, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_96, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1774, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_97, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1786, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_98, (8, 224, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(unsqueeze_1798, (1, 224, 1, 1), (224, 1, 1, 1))
    assert_size_stride(unsqueeze_1810, (1, 512, 1, 1), (512, 1, 1, 1))
    assert_size_stride(le_100, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1822, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_101, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1834, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_102, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1846, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_103, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1858, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_104, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1870, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_105, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1882, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_106, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1894, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_107, (8, 224, 28, 28), (175616, 1, 6272, 224))
    assert_size_stride(unsqueeze_1906, (1, 224, 1, 1), (224, 1, 1, 1))
    assert_size_stride(unsqueeze_1918, (1, 512, 1, 1), (512, 1, 1, 1))
    assert_size_stride(unsqueeze_1930, (1, 512, 1, 1), (512, 1, 1, 1))
    assert_size_stride(le_109, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1942, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_110, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1954, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_111, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1966, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_112, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1978, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_113, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_1990, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_114, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_2002, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_115, (8, 28, 28, 28), (21952, 1, 784, 28))
    assert_size_stride(unsqueeze_2014, (1, 28, 1, 1), (28, 1, 1, 1))
    assert_size_stride(le_116, (8, 224, 56, 56), (702464, 1, 12544, 224))
    assert_size_stride(unsqueeze_2026, (1, 224, 1, 1), (224, 1, 1, 1))
    assert_size_stride(unsqueeze_2038, (1, 256, 1, 1), (256, 1, 1, 1))
    assert_size_stride(le_118, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2050, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_119, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2062, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_120, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2074, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_121, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2086, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_122, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2098, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_123, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2110, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_124, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2122, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_125, (8, 112, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(unsqueeze_2134, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(unsqueeze_2146, (1, 256, 1, 1), (256, 1, 1, 1))
    assert_size_stride(le_127, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2158, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_128, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2170, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_129, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2182, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_130, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2194, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_131, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2206, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_132, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2218, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_133, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2230, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_134, (8, 112, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(unsqueeze_2242, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(unsqueeze_2254, (1, 256, 1, 1), (256, 1, 1, 1))
    assert_size_stride(unsqueeze_2266, (1, 256, 1, 1), (256, 1, 1, 1))
    assert_size_stride(le_136, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2278, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_137, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2290, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_138, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2302, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_139, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2314, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_140, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2326, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_141, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2338, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_142, (8, 14, 56, 56), (43904, 1, 784, 14))
    assert_size_stride(unsqueeze_2350, (1, 14, 1, 1), (14, 1, 1, 1))
    assert_size_stride(le_143, (8, 112, 56, 56), (351232, 1, 6272, 112))
    assert_size_stride(unsqueeze_2362, (1, 112, 1, 1), (112, 1, 1, 1))
    assert_size_stride(unsqueeze_2374, (1, 64, 1, 1), (64, 1, 1, 1))
    assert_size_stride(tangents_1, (8, 1000), (1000, 1))
    with torch.cuda._DeviceGuard(0):
        torch.cuda.set_device(0) # no-op to ensure context
        buf0 = empty((8, 2048), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.mm]
        extern_kernels.mm(tangents_1, permute_1, out=buf0)
        del permute_1
        buf1 = empty((1000, 2048), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.mm]
        extern_kernels.mm(reinterpret_tensor(tangents_1, (1000, 8), (1, 1000), 0), view, out=buf1)
        del view
        buf2 = empty((1, 1000), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.sum]
        stream0 = get_cuda_stream(0)
        triton_per_fused_sum_0.run(tangents_1, buf2, 1000, 8, grid=grid(1000), stream=stream0)
        del tangents_1
        buf3 = empty_strided((2048, 4), (1, 2048), device='cuda', dtype=torch.float32)
        buf5 = empty_strided((2048, 4), (1, 2048), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.div, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_div_native_batch_norm_backward_threshold_backward_1.run(le, buf0, convolution_148, unsqueeze_598, buf3, buf5, 8192, 98, grid=grid(8192), stream=stream0)
        buf4 = empty((2048, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.div, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_div_native_batch_norm_backward_threshold_backward_2.run(buf3, buf4, 2048, 4, grid=grid(2048), stream=stream0)
        buf6 = empty((2048, ), device='cuda', dtype=torch.float32)
        buf7 = empty((2048, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.div, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_div_native_batch_norm_backward_threshold_backward_3.run(buf5, squeeze_445, buf6, buf7, 2048, 4, grid=grid(2048), stream=stream0)
        buf8 = empty_strided((8, 2048, 7, 7), (100352, 1, 14336, 2048), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.div, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_div_native_batch_norm_backward_threshold_backward_4.run(le, buf0, convolution_148, unsqueeze_598, buf6, squeeze_445, buf4, primals_446, buf8, 802816, grid=grid(802816), stream=stream0)
        del convolution_148
        del primals_446
        del squeeze_445
        del unsqueeze_598
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.div, aten.native_batch_norm_backward, aten.threshold_backward]
        buf9 = aten.convolution_backward(buf8, cat_15, primals_445, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del cat_15
        del primals_445
        buf10 = buf9[0]
        buf11 = buf9[1]
        del buf9
        buf12 = empty_strided((112, 4), (1, 112), device='cuda', dtype=torch.float32)
        buf14 = empty_strided((112, 4), (1, 112), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_5.run(le_1, buf10, convolution_147, unsqueeze_610, buf12, buf14, 448, 98, grid=grid(448), stream=stream0)
        buf13 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_6.run(buf12, buf13, 112, 4, grid=grid(112), stream=stream0)
        buf15 = empty((112, ), device='cuda', dtype=torch.float32)
        buf16 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf14, squeeze_442, buf15, buf16, 112, 4, grid=grid(112), stream=stream0)
        buf17 = empty_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_8.run(le_1, buf10, convolution_147, unsqueeze_610, buf15, squeeze_442, buf13, primals_443, buf17, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_147
        del le_1
        del primals_443
        del squeeze_442
        del unsqueeze_610
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf18 = aten.convolution_backward(buf17, add_821, primals_442, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_821
        del primals_442
        buf19 = buf18[0]
        buf20 = buf18[1]
        del buf18
        buf21 = buf15; del buf15  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_9.run(le_2, buf10, buf19, buf21, 112, 392, grid=grid(112), stream=stream0)
        buf22 = buf14; del buf14  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_10.run(le_2, buf10, buf19, convolution_146, unsqueeze_622, buf22, 448, 98, grid=grid(448), stream=stream0)
        buf23 = empty((112, ), device='cuda', dtype=torch.float32)
        buf25 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf22, squeeze_439, buf23, buf25, 112, 4, grid=grid(112), stream=stream0)
        buf24 = buf17; del buf17  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_11.run(le_2, buf10, buf19, convolution_146, unsqueeze_622, buf23, squeeze_439, buf21, primals_440, buf24, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_146
        del le_2
        del primals_440
        del squeeze_439
        del unsqueeze_622
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf26 = aten.convolution_backward(buf24, add_815, primals_439, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_815
        del primals_439
        buf27 = buf26[0]
        buf28 = buf26[1]
        del buf26
        buf29 = buf23; del buf23  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_12.run(le_3, buf10, buf27, buf29, 112, 392, grid=grid(112), stream=stream0)
        buf30 = buf22; del buf22  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_13.run(le_3, buf10, buf27, convolution_145, unsqueeze_634, buf30, 448, 98, grid=grid(448), stream=stream0)
        buf31 = empty((112, ), device='cuda', dtype=torch.float32)
        buf33 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf30, squeeze_436, buf31, buf33, 112, 4, grid=grid(112), stream=stream0)
        buf32 = buf24; del buf24  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_14.run(le_3, buf10, buf27, convolution_145, unsqueeze_634, buf31, squeeze_436, buf29, primals_437, buf32, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_145
        del le_3
        del primals_437
        del squeeze_436
        del unsqueeze_634
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf34 = aten.convolution_backward(buf32, add_809, primals_436, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_809
        del primals_436
        buf35 = buf34[0]
        buf36 = buf34[1]
        del buf34
        buf37 = buf31; del buf31  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_15.run(le_4, buf10, buf35, buf37, 112, 392, grid=grid(112), stream=stream0)
        buf38 = buf30; del buf30  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_16.run(le_4, buf10, buf35, convolution_144, unsqueeze_646, buf38, 448, 98, grid=grid(448), stream=stream0)
        buf39 = empty((112, ), device='cuda', dtype=torch.float32)
        buf41 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf38, squeeze_433, buf39, buf41, 112, 4, grid=grid(112), stream=stream0)
        buf40 = buf32; del buf32  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_17.run(le_4, buf10, buf35, convolution_144, unsqueeze_646, buf39, squeeze_433, buf37, primals_434, buf40, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_144
        del le_4
        del primals_434
        del squeeze_433
        del unsqueeze_646
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf42 = aten.convolution_backward(buf40, add_803, primals_433, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_803
        del primals_433
        buf43 = buf42[0]
        buf44 = buf42[1]
        del buf42
        buf45 = buf39; del buf39  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_18.run(le_5, buf10, buf43, buf45, 112, 392, grid=grid(112), stream=stream0)
        buf46 = buf38; del buf38  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_19.run(le_5, buf10, buf43, convolution_143, unsqueeze_658, buf46, 448, 98, grid=grid(448), stream=stream0)
        buf47 = empty((112, ), device='cuda', dtype=torch.float32)
        buf49 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf46, squeeze_430, buf47, buf49, 112, 4, grid=grid(112), stream=stream0)
        buf48 = buf40; del buf40  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_20.run(le_5, buf10, buf43, convolution_143, unsqueeze_658, buf47, squeeze_430, buf45, primals_431, buf48, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_143
        del le_5
        del primals_431
        del squeeze_430
        del unsqueeze_658
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf50 = aten.convolution_backward(buf48, add_797, primals_430, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_797
        del primals_430
        buf51 = buf50[0]
        buf52 = buf50[1]
        del buf50
        buf53 = buf47; del buf47  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_21.run(le_6, buf10, buf51, buf53, 112, 392, grid=grid(112), stream=stream0)
        buf54 = buf46; del buf46  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_22.run(le_6, buf10, buf51, convolution_142, unsqueeze_670, buf54, 448, 98, grid=grid(448), stream=stream0)
        buf55 = empty((112, ), device='cuda', dtype=torch.float32)
        buf57 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf54, squeeze_427, buf55, buf57, 112, 4, grid=grid(112), stream=stream0)
        buf56 = buf48; del buf48  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_23.run(le_6, buf10, buf51, convolution_142, unsqueeze_670, buf55, squeeze_427, buf53, primals_428, buf56, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_142
        del le_6
        del primals_428
        del squeeze_427
        del unsqueeze_670
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf58 = aten.convolution_backward(buf56, add_791, primals_427, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_791
        del primals_427
        buf59 = buf58[0]
        buf60 = buf58[1]
        del buf58
        buf61 = buf55; del buf55  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_24.run(le_7, buf10, buf59, buf61, 112, 392, grid=grid(112), stream=stream0)
        buf62 = buf54; del buf54  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_25.run(le_7, buf10, buf59, convolution_141, unsqueeze_682, buf62, 448, 98, grid=grid(448), stream=stream0)
        buf63 = empty((112, ), device='cuda', dtype=torch.float32)
        buf65 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf62, squeeze_424, buf63, buf65, 112, 4, grid=grid(112), stream=stream0)
        buf64 = buf56; del buf56  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_26.run(le_7, buf10, buf59, convolution_141, unsqueeze_682, buf63, squeeze_424, buf61, primals_425, buf64, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_141
        del le_7
        del primals_425
        del squeeze_424
        del unsqueeze_682
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf66 = aten.convolution_backward(buf64, getitem_1372, primals_424, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf64
        del getitem_1372
        del primals_424
        buf67 = buf66[0]
        buf68 = buf66[1]
        del buf66
        buf77 = empty((8, 896, 7, 7), device='cuda', dtype=torch.float32)
        buf69 = reinterpret_tensor(buf77, (8, 112, 7, 7), (43904, 49, 7, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf67, buf69, 43904, grid=grid(43904), stream=stream0)
        del buf67
        buf70 = reinterpret_tensor(buf77, (8, 112, 7, 7), (43904, 49, 7, 1), 5488)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf59, buf70, 43904, grid=grid(43904), stream=stream0)
        del buf59
        buf71 = reinterpret_tensor(buf77, (8, 112, 7, 7), (43904, 49, 7, 1), 10976)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf51, buf71, 43904, grid=grid(43904), stream=stream0)
        del buf51
        buf72 = reinterpret_tensor(buf77, (8, 112, 7, 7), (43904, 49, 7, 1), 16464)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf43, buf72, 43904, grid=grid(43904), stream=stream0)
        del buf43
        buf73 = reinterpret_tensor(buf77, (8, 112, 7, 7), (43904, 49, 7, 1), 21952)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf35, buf73, 43904, grid=grid(43904), stream=stream0)
        del buf35
        buf74 = reinterpret_tensor(buf77, (8, 112, 7, 7), (43904, 49, 7, 1), 27440)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf27, buf74, 43904, grid=grid(43904), stream=stream0)
        del buf27
        buf75 = reinterpret_tensor(buf77, (8, 112, 7, 7), (43904, 49, 7, 1), 32928)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf19, buf75, 43904, grid=grid(43904), stream=stream0)
        buf76 = reinterpret_tensor(buf77, (8, 112, 7, 7), (43904, 49, 7, 1), 38416)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_28.run(buf10, buf76, 43904, grid=grid(43904), stream=stream0)
        buf78 = empty_strided((896, 4), (1, 896), device='cuda', dtype=torch.float32)
        buf80 = empty_strided((896, 4), (1, 896), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_29.run(le_8, buf77, convolution_140, unsqueeze_694, buf78, buf80, 3584, 98, grid=grid(3584), stream=stream0)
        del buf69
        del buf70
        del buf71
        del buf72
        del buf73
        del buf74
        del buf75
        del buf76
        buf79 = empty((896, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_30.run(buf78, buf79, 896, 4, grid=grid(896), stream=stream0)
        buf81 = empty((896, ), device='cuda', dtype=torch.float32)
        buf82 = empty((896, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_31.run(buf80, squeeze_421, buf81, buf82, 896, 4, grid=grid(896), stream=stream0)
        buf83 = reinterpret_tensor(buf10, (8, 896, 7, 7), (43904, 1, 6272, 896), 0); del buf10  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_32.run(le_8, buf77, convolution_140, unsqueeze_694, buf81, squeeze_421, buf79, primals_422, buf83, 392, 896, grid=grid(392, 896), stream=stream0)
        del buf77
        del convolution_140
        del le_8
        del primals_422
        del squeeze_421
        del unsqueeze_694
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf84 = aten.convolution_backward(buf83, relu_135, primals_421, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_421
        buf85 = buf84[0]
        buf86 = buf84[1]
        del buf84
        buf87 = buf5; del buf5  # reuse
        buf89 = buf3; del buf3  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.div, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_div_native_batch_norm_backward_threshold_backward_33.run(relu_135, le, buf0, buf85, convolution_139, unsqueeze_706, buf87, buf89, 8192, 98, grid=grid(8192), stream=stream0)
        buf88 = buf6; del buf6  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.div, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_div_native_batch_norm_backward_threshold_backward_2.run(buf87, buf88, 2048, 4, grid=grid(2048), stream=stream0)
        buf90 = empty((2048, ), device='cuda', dtype=torch.float32)
        buf92 = empty((2048, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.div, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_div_native_batch_norm_backward_threshold_backward_3.run(buf89, squeeze_418, buf90, buf92, 2048, 4, grid=grid(2048), stream=stream0)
        buf91 = buf8; del buf8  # reuse
        buf93 = buf91; del buf91  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.convolution_backward, aten.div, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_convolution_backward_div_native_batch_norm_backward_threshold_backward_34.run(buf93, relu_135, le, buf0, buf85, convolution_139, unsqueeze_706, buf90, squeeze_418, buf88, primals_419, 392, 2048, grid=grid(392, 2048), stream=stream0)
        del convolution_139
        del primals_419
        del squeeze_418
        del unsqueeze_706
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf94 = aten.convolution_backward(buf93, cat_14, primals_418, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del cat_14
        del primals_418
        buf95 = buf94[0]
        buf96 = buf94[1]
        del buf94
        buf97 = buf62; del buf62  # reuse
        buf99 = buf12; del buf12  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_5.run(le_10, buf95, convolution_138, unsqueeze_718, buf97, buf99, 448, 98, grid=grid(448), stream=stream0)
        buf98 = buf63; del buf63  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_6.run(buf97, buf98, 112, 4, grid=grid(112), stream=stream0)
        buf100 = empty((112, ), device='cuda', dtype=torch.float32)
        buf101 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf99, squeeze_415, buf100, buf101, 112, 4, grid=grid(112), stream=stream0)
        buf102 = reinterpret_tensor(buf19, (8, 112, 7, 7), (5488, 1, 784, 112), 0); del buf19  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_8.run(le_10, buf95, convolution_138, unsqueeze_718, buf100, squeeze_415, buf98, primals_416, buf102, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_138
        del le_10
        del primals_416
        del squeeze_415
        del unsqueeze_718
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf103 = aten.convolution_backward(buf102, add_769, primals_415, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_769
        del primals_415
        buf104 = buf103[0]
        buf105 = buf103[1]
        del buf103
        buf106 = buf100; del buf100  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_9.run(le_11, buf95, buf104, buf106, 112, 392, grid=grid(112), stream=stream0)
        buf107 = buf99; del buf99  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_10.run(le_11, buf95, buf104, convolution_137, unsqueeze_730, buf107, 448, 98, grid=grid(448), stream=stream0)
        buf108 = empty((112, ), device='cuda', dtype=torch.float32)
        buf110 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf107, squeeze_412, buf108, buf110, 112, 4, grid=grid(112), stream=stream0)
        buf109 = buf102; del buf102  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_11.run(le_11, buf95, buf104, convolution_137, unsqueeze_730, buf108, squeeze_412, buf106, primals_413, buf109, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_137
        del le_11
        del primals_413
        del squeeze_412
        del unsqueeze_730
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf111 = aten.convolution_backward(buf109, add_763, primals_412, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_763
        del primals_412
        buf112 = buf111[0]
        buf113 = buf111[1]
        del buf111
        buf114 = buf108; del buf108  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_12.run(le_12, buf95, buf112, buf114, 112, 392, grid=grid(112), stream=stream0)
        buf115 = buf107; del buf107  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_13.run(le_12, buf95, buf112, convolution_136, unsqueeze_742, buf115, 448, 98, grid=grid(448), stream=stream0)
        buf116 = empty((112, ), device='cuda', dtype=torch.float32)
        buf118 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf115, squeeze_409, buf116, buf118, 112, 4, grid=grid(112), stream=stream0)
        buf117 = buf109; del buf109  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_14.run(le_12, buf95, buf112, convolution_136, unsqueeze_742, buf116, squeeze_409, buf114, primals_410, buf117, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_136
        del le_12
        del primals_410
        del squeeze_409
        del unsqueeze_742
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf119 = aten.convolution_backward(buf117, add_757, primals_409, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_757
        del primals_409
        buf120 = buf119[0]
        buf121 = buf119[1]
        del buf119
        buf122 = buf116; del buf116  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_15.run(le_13, buf95, buf120, buf122, 112, 392, grid=grid(112), stream=stream0)
        buf123 = buf115; del buf115  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_16.run(le_13, buf95, buf120, convolution_135, unsqueeze_754, buf123, 448, 98, grid=grid(448), stream=stream0)
        buf124 = empty((112, ), device='cuda', dtype=torch.float32)
        buf126 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf123, squeeze_406, buf124, buf126, 112, 4, grid=grid(112), stream=stream0)
        buf125 = buf117; del buf117  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_17.run(le_13, buf95, buf120, convolution_135, unsqueeze_754, buf124, squeeze_406, buf122, primals_407, buf125, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_135
        del le_13
        del primals_407
        del squeeze_406
        del unsqueeze_754
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf127 = aten.convolution_backward(buf125, add_751, primals_406, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_751
        del primals_406
        buf128 = buf127[0]
        buf129 = buf127[1]
        del buf127
        buf130 = buf124; del buf124  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_18.run(le_14, buf95, buf128, buf130, 112, 392, grid=grid(112), stream=stream0)
        buf131 = buf123; del buf123  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_19.run(le_14, buf95, buf128, convolution_134, unsqueeze_766, buf131, 448, 98, grid=grid(448), stream=stream0)
        buf132 = empty((112, ), device='cuda', dtype=torch.float32)
        buf134 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf131, squeeze_403, buf132, buf134, 112, 4, grid=grid(112), stream=stream0)
        buf133 = buf125; del buf125  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_20.run(le_14, buf95, buf128, convolution_134, unsqueeze_766, buf132, squeeze_403, buf130, primals_404, buf133, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_134
        del le_14
        del primals_404
        del squeeze_403
        del unsqueeze_766
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf135 = aten.convolution_backward(buf133, add_745, primals_403, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_745
        del primals_403
        buf136 = buf135[0]
        buf137 = buf135[1]
        del buf135
        buf138 = buf132; del buf132  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_21.run(le_15, buf95, buf136, buf138, 112, 392, grid=grid(112), stream=stream0)
        buf139 = buf131; del buf131  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_22.run(le_15, buf95, buf136, convolution_133, unsqueeze_778, buf139, 448, 98, grid=grid(448), stream=stream0)
        buf140 = empty((112, ), device='cuda', dtype=torch.float32)
        buf142 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf139, squeeze_400, buf140, buf142, 112, 4, grid=grid(112), stream=stream0)
        buf141 = buf133; del buf133  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_23.run(le_15, buf95, buf136, convolution_133, unsqueeze_778, buf140, squeeze_400, buf138, primals_401, buf141, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_133
        del le_15
        del primals_401
        del squeeze_400
        del unsqueeze_778
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf143 = aten.convolution_backward(buf141, add_739, primals_400, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_739
        del primals_400
        buf144 = buf143[0]
        buf145 = buf143[1]
        del buf143
        buf146 = buf140; del buf140  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_24.run(le_16, buf95, buf144, buf146, 112, 392, grid=grid(112), stream=stream0)
        buf147 = buf139; del buf139  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_25.run(le_16, buf95, buf144, convolution_132, unsqueeze_790, buf147, 448, 98, grid=grid(448), stream=stream0)
        buf148 = empty((112, ), device='cuda', dtype=torch.float32)
        buf150 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf147, squeeze_397, buf148, buf150, 112, 4, grid=grid(112), stream=stream0)
        buf149 = buf141; del buf141  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_26.run(le_16, buf95, buf144, convolution_132, unsqueeze_790, buf148, squeeze_397, buf146, primals_398, buf149, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_132
        del le_16
        del primals_398
        del squeeze_397
        del unsqueeze_790
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf151 = aten.convolution_backward(buf149, getitem_1282, primals_397, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf149
        del getitem_1282
        del primals_397
        buf152 = buf151[0]
        buf153 = buf151[1]
        del buf151
        buf162 = reinterpret_tensor(buf83, (8, 896, 7, 7), (43904, 49, 7, 1), 0); del buf83  # reuse
        buf154 = reinterpret_tensor(buf162, (8, 112, 7, 7), (43904, 49, 7, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf152, buf154, 43904, grid=grid(43904), stream=stream0)
        del buf152
        buf155 = reinterpret_tensor(buf162, (8, 112, 7, 7), (43904, 49, 7, 1), 5488)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf144, buf155, 43904, grid=grid(43904), stream=stream0)
        del buf144
        buf156 = reinterpret_tensor(buf162, (8, 112, 7, 7), (43904, 49, 7, 1), 10976)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf136, buf156, 43904, grid=grid(43904), stream=stream0)
        del buf136
        buf157 = reinterpret_tensor(buf162, (8, 112, 7, 7), (43904, 49, 7, 1), 16464)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf128, buf157, 43904, grid=grid(43904), stream=stream0)
        del buf128
        buf158 = reinterpret_tensor(buf162, (8, 112, 7, 7), (43904, 49, 7, 1), 21952)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf120, buf158, 43904, grid=grid(43904), stream=stream0)
        del buf120
        buf159 = reinterpret_tensor(buf162, (8, 112, 7, 7), (43904, 49, 7, 1), 27440)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf112, buf159, 43904, grid=grid(43904), stream=stream0)
        del buf112
        buf160 = reinterpret_tensor(buf162, (8, 112, 7, 7), (43904, 49, 7, 1), 32928)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_27.run(buf104, buf160, 43904, grid=grid(43904), stream=stream0)
        buf161 = reinterpret_tensor(buf162, (8, 112, 7, 7), (43904, 49, 7, 1), 38416)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_28.run(buf95, buf161, 43904, grid=grid(43904), stream=stream0)
        buf163 = buf80; del buf80  # reuse
        buf165 = buf78; del buf78  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_29.run(le_17, buf162, convolution_131, unsqueeze_802, buf163, buf165, 3584, 98, grid=grid(3584), stream=stream0)
        del buf154
        del buf155
        del buf156
        del buf157
        del buf158
        del buf159
        del buf160
        del buf161
        buf164 = buf81; del buf81  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_30.run(buf163, buf164, 896, 4, grid=grid(896), stream=stream0)
        del buf163
        buf166 = empty((896, ), device='cuda', dtype=torch.float32)
        buf167 = empty((896, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_31.run(buf165, squeeze_394, buf166, buf167, 896, 4, grid=grid(896), stream=stream0)
        del buf165
        buf168 = reinterpret_tensor(buf95, (8, 896, 7, 7), (43904, 1, 6272, 896), 0); del buf95  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_32.run(le_17, buf162, convolution_131, unsqueeze_802, buf166, squeeze_394, buf164, primals_395, buf168, 392, 896, grid=grid(392, 896), stream=stream0)
        del buf162
        del convolution_131
        del le_17
        del primals_395
        del squeeze_394
        del unsqueeze_802
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf169 = aten.convolution_backward(buf168, relu_126, primals_394, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf168
        del primals_394
        buf170 = buf169[0]
        buf171 = buf169[1]
        del buf169
        buf172 = buf93; del buf93  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.div, aten.threshold_backward]
        triton_poi_fused_add_div_threshold_backward_35.run(relu_126, relu_135, le, buf0, buf85, buf170, buf172, 392, 2048, grid=grid(392, 2048), stream=stream0)
        del buf0
        del le
        del relu_126
        del relu_135
        buf173 = buf89; del buf89  # reuse
        buf175 = buf87; del buf87  # reuse
        buf182 = empty_strided((2048, 4), (1, 2048), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_36.run(buf172, convolution_130, unsqueeze_814, convolution_129, unsqueeze_826, buf173, buf175, buf182, 8192, 98, grid=grid(8192), stream=stream0)
        buf174 = buf90; del buf90  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_div_native_batch_norm_backward_threshold_backward_2.run(buf173, buf174, 2048, 4, grid=grid(2048), stream=stream0)
        del buf173
        buf176 = empty((2048, ), device='cuda', dtype=torch.float32)
        buf177 = empty((2048, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_div_native_batch_norm_backward_threshold_backward_3.run(buf175, squeeze_391, buf176, buf177, 2048, 4, grid=grid(2048), stream=stream0)
        del buf175
        buf183 = empty((2048, ), device='cuda', dtype=torch.float32)
        buf184 = empty((2048, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_div_native_batch_norm_backward_threshold_backward_3.run(buf182, squeeze_388, buf183, buf184, 2048, 4, grid=grid(2048), stream=stream0)
        del buf182
        buf178 = reinterpret_tensor(buf85, (8, 2048, 7, 7), (100352, 1, 14336, 2048), 0); del buf85  # reuse
        buf185 = reinterpret_tensor(buf170, (8, 2048, 7, 7), (100352, 1, 14336, 2048), 0); del buf170  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_37.run(buf172, convolution_130, unsqueeze_814, buf176, squeeze_391, buf174, primals_392, convolution_129, unsqueeze_826, buf183, squeeze_388, primals_389, buf178, buf185, 802816, grid=grid(802816), stream=stream0)
        del buf172
        del buf176
        del buf183
        del convolution_129
        del convolution_130
        del primals_389
        del primals_392
        del squeeze_388
        del squeeze_391
        del unsqueeze_814
        del unsqueeze_826
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf179 = aten.convolution_backward(buf178, relu_117, primals_391, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf178
        del primals_391
        buf180 = buf179[0]
        buf181 = buf179[1]
        del buf179
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf186 = aten.convolution_backward(buf185, cat_13, primals_388, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf185
        del cat_13
        del primals_388
        buf187 = buf186[0]
        buf188 = buf186[1]
        del buf186
        buf260 = empty((8, 896, 14, 14), device='cuda', dtype=torch.float32)
        buf189 = reinterpret_tensor(buf260, (8, 112, 14, 14), (175616, 196, 14, 1), 153664)  # alias
        # Source Nodes: [], Original ATen: [aten.avg_pool2d_backward]
        triton_poi_fused_avg_pool2d_backward_38.run(buf187, buf189, 175616, grid=grid(175616), stream=stream0)
        buf190 = buf147; del buf147  # reuse
        buf192 = buf97; del buf97  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_5.run(le_19, buf187, convolution_128, unsqueeze_838, buf190, buf192, 448, 98, grid=grid(448), stream=stream0)
        buf191 = buf148; del buf148  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_6.run(buf190, buf191, 112, 4, grid=grid(112), stream=stream0)
        buf193 = empty((112, ), device='cuda', dtype=torch.float32)
        buf194 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf192, squeeze_385, buf193, buf194, 112, 4, grid=grid(112), stream=stream0)
        buf195 = reinterpret_tensor(buf104, (8, 112, 7, 7), (5488, 1, 784, 112), 0); del buf104  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_8.run(le_19, buf187, convolution_128, unsqueeze_838, buf193, squeeze_385, buf191, primals_386, buf195, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_128
        del le_19
        del primals_386
        del squeeze_385
        del unsqueeze_838
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf196 = aten.convolution_backward(buf195, getitem_1256, primals_385, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_1256
        del primals_385
        buf197 = buf196[0]
        buf198 = buf196[1]
        del buf196
        buf199 = buf192; del buf192  # reuse
        buf201 = buf190; del buf190  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_39.run(le_20, buf187, convolution_127, unsqueeze_850, buf199, buf201, 448, 98, grid=grid(448), stream=stream0)
        buf200 = buf193; del buf193  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_6.run(buf199, buf200, 112, 4, grid=grid(112), stream=stream0)
        buf202 = empty((112, ), device='cuda', dtype=torch.float32)
        buf203 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf201, squeeze_382, buf202, buf203, 112, 4, grid=grid(112), stream=stream0)
        buf204 = buf195; del buf195  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_40.run(le_20, buf187, convolution_127, unsqueeze_850, buf202, squeeze_382, buf200, primals_383, buf204, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_127
        del le_20
        del primals_383
        del squeeze_382
        del unsqueeze_850
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf205 = aten.convolution_backward(buf204, getitem_1245, primals_382, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_1245
        del primals_382
        buf206 = buf205[0]
        buf207 = buf205[1]
        del buf205
        buf208 = buf201; del buf201  # reuse
        buf210 = buf199; del buf199  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_41.run(le_21, buf187, convolution_126, unsqueeze_862, buf208, buf210, 448, 98, grid=grid(448), stream=stream0)
        buf209 = buf202; del buf202  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_6.run(buf208, buf209, 112, 4, grid=grid(112), stream=stream0)
        buf211 = empty((112, ), device='cuda', dtype=torch.float32)
        buf212 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf210, squeeze_379, buf211, buf212, 112, 4, grid=grid(112), stream=stream0)
        buf213 = buf204; del buf204  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_42.run(le_21, buf187, convolution_126, unsqueeze_862, buf211, squeeze_379, buf209, primals_380, buf213, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_126
        del le_21
        del primals_380
        del squeeze_379
        del unsqueeze_862
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf214 = aten.convolution_backward(buf213, getitem_1234, primals_379, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_1234
        del primals_379
        buf215 = buf214[0]
        buf216 = buf214[1]
        del buf214
        buf217 = buf210; del buf210  # reuse
        buf219 = buf208; del buf208  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_43.run(le_22, buf187, convolution_125, unsqueeze_874, buf217, buf219, 448, 98, grid=grid(448), stream=stream0)
        buf218 = buf211; del buf211  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_6.run(buf217, buf218, 112, 4, grid=grid(112), stream=stream0)
        buf220 = empty((112, ), device='cuda', dtype=torch.float32)
        buf221 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf219, squeeze_376, buf220, buf221, 112, 4, grid=grid(112), stream=stream0)
        buf222 = buf213; del buf213  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_44.run(le_22, buf187, convolution_125, unsqueeze_874, buf220, squeeze_376, buf218, primals_377, buf222, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_125
        del le_22
        del primals_377
        del squeeze_376
        del unsqueeze_874
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf223 = aten.convolution_backward(buf222, getitem_1223, primals_376, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_1223
        del primals_376
        buf224 = buf223[0]
        buf225 = buf223[1]
        del buf223
        buf226 = buf219; del buf219  # reuse
        buf228 = buf217; del buf217  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_45.run(le_23, buf187, convolution_124, unsqueeze_886, buf226, buf228, 448, 98, grid=grid(448), stream=stream0)
        buf227 = buf220; del buf220  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_6.run(buf226, buf227, 112, 4, grid=grid(112), stream=stream0)
        buf229 = empty((112, ), device='cuda', dtype=torch.float32)
        buf230 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf228, squeeze_373, buf229, buf230, 112, 4, grid=grid(112), stream=stream0)
        buf231 = buf222; del buf222  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_46.run(le_23, buf187, convolution_124, unsqueeze_886, buf229, squeeze_373, buf227, primals_374, buf231, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_124
        del le_23
        del primals_374
        del squeeze_373
        del unsqueeze_886
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf232 = aten.convolution_backward(buf231, getitem_1212, primals_373, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_1212
        del primals_373
        buf233 = buf232[0]
        buf234 = buf232[1]
        del buf232
        buf235 = buf228; del buf228  # reuse
        buf237 = buf226; del buf226  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_47.run(le_24, buf187, convolution_123, unsqueeze_898, buf235, buf237, 448, 98, grid=grid(448), stream=stream0)
        buf236 = buf229; del buf229  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_6.run(buf235, buf236, 112, 4, grid=grid(112), stream=stream0)
        buf238 = empty((112, ), device='cuda', dtype=torch.float32)
        buf239 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf237, squeeze_370, buf238, buf239, 112, 4, grid=grid(112), stream=stream0)
        buf240 = buf231; del buf231  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_48.run(le_24, buf187, convolution_123, unsqueeze_898, buf238, squeeze_370, buf236, primals_371, buf240, 392, 112, grid=grid(392, 112), stream=stream0)
        del convolution_123
        del le_24
        del primals_371
        del squeeze_370
        del unsqueeze_898
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf241 = aten.convolution_backward(buf240, getitem_1201, primals_370, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_1201
        del primals_370
        buf242 = buf241[0]
        buf243 = buf241[1]
        del buf241
        buf244 = buf237; del buf237  # reuse
        buf246 = buf235; del buf235  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_49.run(le_25, buf187, convolution_122, unsqueeze_910, buf244, buf246, 448, 98, grid=grid(448), stream=stream0)
        buf245 = buf238; del buf238  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_6.run(buf244, buf245, 112, 4, grid=grid(112), stream=stream0)
        buf247 = empty((112, ), device='cuda', dtype=torch.float32)
        buf248 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_7.run(buf246, squeeze_367, buf247, buf248, 112, 4, grid=grid(112), stream=stream0)
        buf249 = buf240; del buf240  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_50.run(le_25, buf187, convolution_122, unsqueeze_910, buf247, squeeze_367, buf245, primals_368, buf249, 392, 112, grid=grid(392, 112), stream=stream0)
        del buf187
        del convolution_122
        del le_25
        del primals_368
        del squeeze_367
        del unsqueeze_910
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf250 = aten.convolution_backward(buf249, getitem_1190, primals_367, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_1190
        del primals_367
        buf251 = buf250[0]
        buf252 = buf250[1]
        del buf250
        buf253 = reinterpret_tensor(buf260, (8, 112, 14, 14), (175616, 196, 14, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf251, buf253, 175616, grid=grid(175616), stream=stream0)
        del buf251
        buf254 = reinterpret_tensor(buf260, (8, 112, 14, 14), (175616, 196, 14, 1), 21952)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf242, buf254, 175616, grid=grid(175616), stream=stream0)
        del buf242
        buf255 = reinterpret_tensor(buf260, (8, 112, 14, 14), (175616, 196, 14, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf233, buf255, 175616, grid=grid(175616), stream=stream0)
        del buf233
        buf256 = reinterpret_tensor(buf260, (8, 112, 14, 14), (175616, 196, 14, 1), 65856)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf224, buf256, 175616, grid=grid(175616), stream=stream0)
        del buf224
        buf257 = reinterpret_tensor(buf260, (8, 112, 14, 14), (175616, 196, 14, 1), 87808)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf215, buf257, 175616, grid=grid(175616), stream=stream0)
        del buf215
        buf258 = reinterpret_tensor(buf260, (8, 112, 14, 14), (175616, 196, 14, 1), 109760)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf206, buf258, 175616, grid=grid(175616), stream=stream0)
        del buf206
        buf259 = reinterpret_tensor(buf260, (8, 112, 14, 14), (175616, 196, 14, 1), 131712)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf197, buf259, 175616, grid=grid(175616), stream=stream0)
        buf261 = empty((896, 13), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_52.run(le_26, buf260, buf261, 11648, 121, grid=grid(11648), stream=stream0)
        del buf189
        del buf253
        del buf254
        del buf255
        del buf256
        del buf257
        del buf258
        del buf259
        buf262 = buf166; del buf166  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_53.run(buf261, buf262, 896, 13, grid=grid(896), stream=stream0)
        buf263 = reinterpret_tensor(buf261, (896, 13), (1, 896), 0); del buf261  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_54.run(le_26, buf260, convolution_121, unsqueeze_922, buf263, 11648, 121, grid=grid(11648), stream=stream0)
        buf264 = empty((896, ), device='cuda', dtype=torch.float32)
        buf265 = empty((896, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_55.run(buf263, squeeze_364, buf264, buf265, 896, 13, grid=grid(896), stream=stream0)
        del buf263
        buf266 = empty_strided((8, 896, 14, 14), (175616, 1, 12544, 896), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_56.run(le_26, buf260, convolution_121, unsqueeze_922, buf264, squeeze_364, buf262, primals_365, buf266, 1568, 896, grid=grid(1568, 896), stream=stream0)
        del buf260
        del buf264
        del convolution_121
        del le_26
        del primals_365
        del squeeze_364
        del unsqueeze_922
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf267 = aten.convolution_backward(buf266, relu_117, primals_364, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_364
        buf268 = buf267[0]
        buf269 = buf267[1]
        del buf267
        buf270 = empty((1024, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_57.run(relu_117, buf180, buf268, buf270, 1024, 1568, grid=grid(1024), stream=stream0)
        buf271 = empty((1024, 13), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_58.run(relu_117, buf180, buf268, convolution_120, unsqueeze_934, buf271, 13312, 121, grid=grid(13312), stream=stream0)
        buf272 = empty((1024, ), device='cuda', dtype=torch.float32)
        buf274 = empty((1024, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_59.run(buf271, squeeze_361, buf272, buf274, 1024, 13, grid=grid(1024), stream=stream0)
        buf273 = empty_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_60.run(relu_117, buf180, buf268, convolution_120, unsqueeze_934, buf272, squeeze_361, buf270, primals_362, buf273, 1568, 1024, grid=grid(1568, 1024), stream=stream0)
        del convolution_120
        del primals_362
        del squeeze_361
        del unsqueeze_934
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf275 = aten.convolution_backward(buf273, cat_12, primals_361, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf273
        del cat_12
        del primals_361
        buf276 = buf275[0]
        buf277 = buf275[1]
        del buf275
        buf278 = empty((56, 13), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_61.run(le_28, buf276, buf278, 728, 121, grid=grid(728), stream=stream0)
        buf279 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_62.run(buf278, buf279, 56, 13, grid=grid(56), stream=stream0)
        buf280 = reinterpret_tensor(buf278, (56, 13), (1, 56), 0); del buf278  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(le_28, buf276, convolution_119, unsqueeze_946, buf280, 728, 121, grid=grid(728), stream=stream0)
        buf281 = empty((56, ), device='cuda', dtype=torch.float32)
        buf282 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf280, squeeze_358, buf281, buf282, 56, 13, grid=grid(56), stream=stream0)
        buf283 = empty_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_65.run(le_28, buf276, convolution_119, unsqueeze_946, buf281, squeeze_358, buf279, primals_359, buf283, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_119
        del le_28
        del primals_359
        del squeeze_358
        del unsqueeze_946
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf284 = aten.convolution_backward(buf283, add_666, primals_358, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_666
        del primals_358
        buf285 = buf284[0]
        buf286 = buf284[1]
        del buf284
        buf287 = buf281; del buf281  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_66.run(le_29, buf276, buf285, buf287, 56, 1568, grid=grid(56), stream=stream0)
        buf288 = reinterpret_tensor(buf280, (56, 13), (13, 1), 0); del buf280  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_67.run(le_29, buf276, buf285, convolution_118, unsqueeze_958, buf288, 728, 121, grid=grid(728), stream=stream0)
        buf289 = empty((56, ), device='cuda', dtype=torch.float32)
        buf291 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf288, squeeze_355, buf289, buf291, 56, 13, grid=grid(56), stream=stream0)
        buf290 = buf283; del buf283  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_69.run(le_29, buf276, buf285, convolution_118, unsqueeze_958, buf289, squeeze_355, buf287, primals_356, buf290, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_118
        del le_29
        del primals_356
        del squeeze_355
        del unsqueeze_958
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf292 = aten.convolution_backward(buf290, add_660, primals_355, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_660
        del primals_355
        buf293 = buf292[0]
        buf294 = buf292[1]
        del buf292
        buf295 = buf289; del buf289  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_70.run(le_30, buf276, buf293, buf295, 56, 1568, grid=grid(56), stream=stream0)
        buf296 = buf288; del buf288  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_71.run(le_30, buf276, buf293, convolution_117, unsqueeze_970, buf296, 728, 121, grid=grid(728), stream=stream0)
        buf297 = empty((56, ), device='cuda', dtype=torch.float32)
        buf299 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf296, squeeze_352, buf297, buf299, 56, 13, grid=grid(56), stream=stream0)
        buf298 = buf290; del buf290  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_72.run(le_30, buf276, buf293, convolution_117, unsqueeze_970, buf297, squeeze_352, buf295, primals_353, buf298, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_117
        del le_30
        del primals_353
        del squeeze_352
        del unsqueeze_970
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf300 = aten.convolution_backward(buf298, add_654, primals_352, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_654
        del primals_352
        buf301 = buf300[0]
        buf302 = buf300[1]
        del buf300
        buf303 = buf297; del buf297  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_73.run(le_31, buf276, buf301, buf303, 56, 1568, grid=grid(56), stream=stream0)
        buf304 = buf296; del buf296  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_74.run(le_31, buf276, buf301, convolution_116, unsqueeze_982, buf304, 728, 121, grid=grid(728), stream=stream0)
        buf305 = empty((56, ), device='cuda', dtype=torch.float32)
        buf307 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf304, squeeze_349, buf305, buf307, 56, 13, grid=grid(56), stream=stream0)
        buf306 = buf298; del buf298  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_75.run(le_31, buf276, buf301, convolution_116, unsqueeze_982, buf305, squeeze_349, buf303, primals_350, buf306, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_116
        del le_31
        del primals_350
        del squeeze_349
        del unsqueeze_982
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf308 = aten.convolution_backward(buf306, add_648, primals_349, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_648
        del primals_349
        buf309 = buf308[0]
        buf310 = buf308[1]
        del buf308
        buf311 = buf305; del buf305  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_76.run(le_32, buf276, buf309, buf311, 56, 1568, grid=grid(56), stream=stream0)
        buf312 = buf304; del buf304  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_77.run(le_32, buf276, buf309, convolution_115, unsqueeze_994, buf312, 728, 121, grid=grid(728), stream=stream0)
        buf313 = empty((56, ), device='cuda', dtype=torch.float32)
        buf315 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf312, squeeze_346, buf313, buf315, 56, 13, grid=grid(56), stream=stream0)
        buf314 = buf306; del buf306  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_78.run(le_32, buf276, buf309, convolution_115, unsqueeze_994, buf313, squeeze_346, buf311, primals_347, buf314, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_115
        del le_32
        del primals_347
        del squeeze_346
        del unsqueeze_994
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf316 = aten.convolution_backward(buf314, add_642, primals_346, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_642
        del primals_346
        buf317 = buf316[0]
        buf318 = buf316[1]
        del buf316
        buf319 = buf313; del buf313  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_79.run(le_33, buf276, buf317, buf319, 56, 1568, grid=grid(56), stream=stream0)
        buf320 = buf312; del buf312  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_80.run(le_33, buf276, buf317, convolution_114, unsqueeze_1006, buf320, 728, 121, grid=grid(728), stream=stream0)
        buf321 = empty((56, ), device='cuda', dtype=torch.float32)
        buf323 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf320, squeeze_343, buf321, buf323, 56, 13, grid=grid(56), stream=stream0)
        buf322 = buf314; del buf314  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_81.run(le_33, buf276, buf317, convolution_114, unsqueeze_1006, buf321, squeeze_343, buf319, primals_344, buf322, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_114
        del le_33
        del primals_344
        del squeeze_343
        del unsqueeze_1006
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf324 = aten.convolution_backward(buf322, add_636, primals_343, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_636
        del primals_343
        buf325 = buf324[0]
        buf326 = buf324[1]
        del buf324
        buf327 = buf321; del buf321  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_82.run(le_34, buf276, buf325, buf327, 56, 1568, grid=grid(56), stream=stream0)
        buf328 = buf320; del buf320  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_83.run(le_34, buf276, buf325, convolution_113, unsqueeze_1018, buf328, 728, 121, grid=grid(728), stream=stream0)
        buf329 = empty((56, ), device='cuda', dtype=torch.float32)
        buf331 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf328, squeeze_340, buf329, buf331, 56, 13, grid=grid(56), stream=stream0)
        buf330 = buf322; del buf322  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_84.run(le_34, buf276, buf325, convolution_113, unsqueeze_1018, buf329, squeeze_340, buf327, primals_341, buf330, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_113
        del le_34
        del primals_341
        del squeeze_340
        del unsqueeze_1018
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf332 = aten.convolution_backward(buf330, getitem_1100, primals_340, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf330
        del getitem_1100
        del primals_340
        buf333 = buf332[0]
        buf334 = buf332[1]
        del buf332
        buf343 = empty((8, 448, 14, 14), device='cuda', dtype=torch.float32)
        buf335 = reinterpret_tensor(buf343, (8, 56, 14, 14), (87808, 196, 14, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf333, buf335, 87808, grid=grid(87808), stream=stream0)
        del buf333
        buf336 = reinterpret_tensor(buf343, (8, 56, 14, 14), (87808, 196, 14, 1), 10976)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf325, buf336, 87808, grid=grid(87808), stream=stream0)
        del buf325
        buf337 = reinterpret_tensor(buf343, (8, 56, 14, 14), (87808, 196, 14, 1), 21952)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf317, buf337, 87808, grid=grid(87808), stream=stream0)
        del buf317
        buf338 = reinterpret_tensor(buf343, (8, 56, 14, 14), (87808, 196, 14, 1), 32928)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf309, buf338, 87808, grid=grid(87808), stream=stream0)
        del buf309
        buf339 = reinterpret_tensor(buf343, (8, 56, 14, 14), (87808, 196, 14, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf301, buf339, 87808, grid=grid(87808), stream=stream0)
        del buf301
        buf340 = reinterpret_tensor(buf343, (8, 56, 14, 14), (87808, 196, 14, 1), 54880)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf293, buf340, 87808, grid=grid(87808), stream=stream0)
        del buf293
        buf341 = reinterpret_tensor(buf343, (8, 56, 14, 14), (87808, 196, 14, 1), 65856)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf285, buf341, 87808, grid=grid(87808), stream=stream0)
        buf342 = reinterpret_tensor(buf343, (8, 56, 14, 14), (87808, 196, 14, 1), 76832)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_86.run(buf276, buf342, 87808, grid=grid(87808), stream=stream0)
        buf344 = empty((448, 13), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_87.run(le_35, buf343, buf344, 5824, 121, grid=grid(5824), stream=stream0)
        del buf335
        del buf336
        del buf337
        del buf338
        del buf339
        del buf340
        del buf341
        del buf342
        buf345 = reinterpret_tensor(buf246, (448, ), (1, ), 0); del buf246  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_88.run(buf344, buf345, 448, 13, grid=grid(448), stream=stream0)
        buf346 = reinterpret_tensor(buf344, (448, 13), (1, 448), 0); del buf344  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_89.run(le_35, buf343, convolution_112, unsqueeze_1030, buf346, 5824, 121, grid=grid(5824), stream=stream0)
        buf347 = reinterpret_tensor(buf244, (448, ), (1, ), 0); del buf244  # reuse
        buf348 = empty((448, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_90.run(buf346, squeeze_337, buf347, buf348, 448, 13, grid=grid(448), stream=stream0)
        buf349 = reinterpret_tensor(buf276, (8, 448, 14, 14), (87808, 1, 6272, 448), 0); del buf276  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_91.run(le_35, buf343, convolution_112, unsqueeze_1030, buf347, squeeze_337, buf345, primals_338, buf349, 1568, 448, grid=grid(1568, 448), stream=stream0)
        del buf343
        del convolution_112
        del le_35
        del primals_338
        del squeeze_337
        del unsqueeze_1030
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf350 = aten.convolution_backward(buf349, relu_108, primals_337, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_337
        buf351 = buf350[0]
        buf352 = buf350[1]
        del buf350
        buf353 = buf180; del buf180  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_92.run(buf353, relu_108, relu_117, buf268, buf351, 8192, 196, grid=grid(8192, 196), stream=stream0)
        del buf268
        del relu_108
        del relu_117
        buf354 = buf272; del buf272  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_93.run(buf353, buf354, 1024, 1568, grid=grid(1024), stream=stream0)
        buf355 = buf271; del buf271  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_94.run(buf353, convolution_111, unsqueeze_1042, buf355, 13312, 121, grid=grid(13312), stream=stream0)
        buf356 = empty((1024, ), device='cuda', dtype=torch.float32)
        buf357 = empty((1024, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_59.run(buf355, squeeze_334, buf356, buf357, 1024, 13, grid=grid(1024), stream=stream0)
        buf358 = reinterpret_tensor(buf351, (8, 1024, 14, 14), (200704, 1, 14336, 1024), 0); del buf351  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_95.run(buf353, convolution_111, unsqueeze_1042, buf356, squeeze_334, buf354, primals_335, buf358, 1568, 1024, grid=grid(1568, 1024), stream=stream0)
        del convolution_111
        del primals_335
        del squeeze_334
        del unsqueeze_1042
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf359 = aten.convolution_backward(buf358, cat_11, primals_334, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del cat_11
        del primals_334
        buf360 = buf359[0]
        buf361 = buf359[1]
        del buf359
        buf362 = buf328; del buf328  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_61.run(le_37, buf360, buf362, 728, 121, grid=grid(728), stream=stream0)
        buf363 = buf329; del buf329  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_62.run(buf362, buf363, 56, 13, grid=grid(56), stream=stream0)
        buf364 = reinterpret_tensor(buf362, (56, 13), (1, 56), 0); del buf362  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(le_37, buf360, convolution_110, unsqueeze_1054, buf364, 728, 121, grid=grid(728), stream=stream0)
        buf365 = empty((56, ), device='cuda', dtype=torch.float32)
        buf366 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf364, squeeze_331, buf365, buf366, 56, 13, grid=grid(56), stream=stream0)
        buf367 = reinterpret_tensor(buf285, (8, 56, 14, 14), (10976, 1, 784, 56), 0); del buf285  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_65.run(le_37, buf360, convolution_110, unsqueeze_1054, buf365, squeeze_331, buf363, primals_332, buf367, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_110
        del le_37
        del primals_332
        del squeeze_331
        del unsqueeze_1054
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf368 = aten.convolution_backward(buf367, add_614, primals_331, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_614
        del primals_331
        buf369 = buf368[0]
        buf370 = buf368[1]
        del buf368
        buf371 = buf365; del buf365  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_66.run(le_38, buf360, buf369, buf371, 56, 1568, grid=grid(56), stream=stream0)
        buf372 = reinterpret_tensor(buf364, (56, 13), (13, 1), 0); del buf364  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_67.run(le_38, buf360, buf369, convolution_109, unsqueeze_1066, buf372, 728, 121, grid=grid(728), stream=stream0)
        buf373 = empty((56, ), device='cuda', dtype=torch.float32)
        buf375 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf372, squeeze_328, buf373, buf375, 56, 13, grid=grid(56), stream=stream0)
        buf374 = buf367; del buf367  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_69.run(le_38, buf360, buf369, convolution_109, unsqueeze_1066, buf373, squeeze_328, buf371, primals_329, buf374, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_109
        del le_38
        del primals_329
        del squeeze_328
        del unsqueeze_1066
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf376 = aten.convolution_backward(buf374, add_608, primals_328, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_608
        del primals_328
        buf377 = buf376[0]
        buf378 = buf376[1]
        del buf376
        buf379 = buf373; del buf373  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_70.run(le_39, buf360, buf377, buf379, 56, 1568, grid=grid(56), stream=stream0)
        buf380 = buf372; del buf372  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_71.run(le_39, buf360, buf377, convolution_108, unsqueeze_1078, buf380, 728, 121, grid=grid(728), stream=stream0)
        buf381 = empty((56, ), device='cuda', dtype=torch.float32)
        buf383 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf380, squeeze_325, buf381, buf383, 56, 13, grid=grid(56), stream=stream0)
        buf382 = buf374; del buf374  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_72.run(le_39, buf360, buf377, convolution_108, unsqueeze_1078, buf381, squeeze_325, buf379, primals_326, buf382, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_108
        del le_39
        del primals_326
        del squeeze_325
        del unsqueeze_1078
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf384 = aten.convolution_backward(buf382, add_602, primals_325, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_602
        del primals_325
        buf385 = buf384[0]
        buf386 = buf384[1]
        del buf384
        buf387 = buf381; del buf381  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_73.run(le_40, buf360, buf385, buf387, 56, 1568, grid=grid(56), stream=stream0)
        buf388 = buf380; del buf380  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_74.run(le_40, buf360, buf385, convolution_107, unsqueeze_1090, buf388, 728, 121, grid=grid(728), stream=stream0)
        buf389 = empty((56, ), device='cuda', dtype=torch.float32)
        buf391 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf388, squeeze_322, buf389, buf391, 56, 13, grid=grid(56), stream=stream0)
        buf390 = buf382; del buf382  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_75.run(le_40, buf360, buf385, convolution_107, unsqueeze_1090, buf389, squeeze_322, buf387, primals_323, buf390, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_107
        del le_40
        del primals_323
        del squeeze_322
        del unsqueeze_1090
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf392 = aten.convolution_backward(buf390, add_596, primals_322, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_596
        del primals_322
        buf393 = buf392[0]
        buf394 = buf392[1]
        del buf392
        buf395 = buf389; del buf389  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_76.run(le_41, buf360, buf393, buf395, 56, 1568, grid=grid(56), stream=stream0)
        buf396 = buf388; del buf388  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_77.run(le_41, buf360, buf393, convolution_106, unsqueeze_1102, buf396, 728, 121, grid=grid(728), stream=stream0)
        buf397 = empty((56, ), device='cuda', dtype=torch.float32)
        buf399 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf396, squeeze_319, buf397, buf399, 56, 13, grid=grid(56), stream=stream0)
        buf398 = buf390; del buf390  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_78.run(le_41, buf360, buf393, convolution_106, unsqueeze_1102, buf397, squeeze_319, buf395, primals_320, buf398, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_106
        del le_41
        del primals_320
        del squeeze_319
        del unsqueeze_1102
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf400 = aten.convolution_backward(buf398, add_590, primals_319, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_590
        del primals_319
        buf401 = buf400[0]
        buf402 = buf400[1]
        del buf400
        buf403 = buf397; del buf397  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_79.run(le_42, buf360, buf401, buf403, 56, 1568, grid=grid(56), stream=stream0)
        buf404 = buf396; del buf396  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_80.run(le_42, buf360, buf401, convolution_105, unsqueeze_1114, buf404, 728, 121, grid=grid(728), stream=stream0)
        buf405 = empty((56, ), device='cuda', dtype=torch.float32)
        buf407 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf404, squeeze_316, buf405, buf407, 56, 13, grid=grid(56), stream=stream0)
        buf406 = buf398; del buf398  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_81.run(le_42, buf360, buf401, convolution_105, unsqueeze_1114, buf405, squeeze_316, buf403, primals_317, buf406, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_105
        del le_42
        del primals_317
        del squeeze_316
        del unsqueeze_1114
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf408 = aten.convolution_backward(buf406, add_584, primals_316, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_584
        del primals_316
        buf409 = buf408[0]
        buf410 = buf408[1]
        del buf408
        buf411 = buf405; del buf405  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_82.run(le_43, buf360, buf409, buf411, 56, 1568, grid=grid(56), stream=stream0)
        buf412 = buf404; del buf404  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_83.run(le_43, buf360, buf409, convolution_104, unsqueeze_1126, buf412, 728, 121, grid=grid(728), stream=stream0)
        buf413 = empty((56, ), device='cuda', dtype=torch.float32)
        buf415 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf412, squeeze_313, buf413, buf415, 56, 13, grid=grid(56), stream=stream0)
        buf414 = buf406; del buf406  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_84.run(le_43, buf360, buf409, convolution_104, unsqueeze_1126, buf413, squeeze_313, buf411, primals_314, buf414, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_104
        del le_43
        del primals_314
        del squeeze_313
        del unsqueeze_1126
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf416 = aten.convolution_backward(buf414, getitem_1010, primals_313, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf414
        del getitem_1010
        del primals_313
        buf417 = buf416[0]
        buf418 = buf416[1]
        del buf416
        buf427 = reinterpret_tensor(buf349, (8, 448, 14, 14), (87808, 196, 14, 1), 0); del buf349  # reuse
        buf419 = reinterpret_tensor(buf427, (8, 56, 14, 14), (87808, 196, 14, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf417, buf419, 87808, grid=grid(87808), stream=stream0)
        del buf417
        buf420 = reinterpret_tensor(buf427, (8, 56, 14, 14), (87808, 196, 14, 1), 10976)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf409, buf420, 87808, grid=grid(87808), stream=stream0)
        del buf409
        buf421 = reinterpret_tensor(buf427, (8, 56, 14, 14), (87808, 196, 14, 1), 21952)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf401, buf421, 87808, grid=grid(87808), stream=stream0)
        del buf401
        buf422 = reinterpret_tensor(buf427, (8, 56, 14, 14), (87808, 196, 14, 1), 32928)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf393, buf422, 87808, grid=grid(87808), stream=stream0)
        del buf393
        buf423 = reinterpret_tensor(buf427, (8, 56, 14, 14), (87808, 196, 14, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf385, buf423, 87808, grid=grid(87808), stream=stream0)
        del buf385
        buf424 = reinterpret_tensor(buf427, (8, 56, 14, 14), (87808, 196, 14, 1), 54880)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf377, buf424, 87808, grid=grid(87808), stream=stream0)
        del buf377
        buf425 = reinterpret_tensor(buf427, (8, 56, 14, 14), (87808, 196, 14, 1), 65856)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf369, buf425, 87808, grid=grid(87808), stream=stream0)
        buf426 = reinterpret_tensor(buf427, (8, 56, 14, 14), (87808, 196, 14, 1), 76832)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_86.run(buf360, buf426, 87808, grid=grid(87808), stream=stream0)
        buf428 = reinterpret_tensor(buf346, (448, 13), (13, 1), 0); del buf346  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_87.run(le_44, buf427, buf428, 5824, 121, grid=grid(5824), stream=stream0)
        del buf419
        del buf420
        del buf421
        del buf422
        del buf423
        del buf424
        del buf425
        del buf426
        buf429 = buf347; del buf347  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_88.run(buf428, buf429, 448, 13, grid=grid(448), stream=stream0)
        buf430 = reinterpret_tensor(buf428, (448, 13), (1, 448), 0); del buf428  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_89.run(le_44, buf427, convolution_103, unsqueeze_1138, buf430, 5824, 121, grid=grid(5824), stream=stream0)
        buf431 = empty((448, ), device='cuda', dtype=torch.float32)
        buf432 = empty((448, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_90.run(buf430, squeeze_310, buf431, buf432, 448, 13, grid=grid(448), stream=stream0)
        buf433 = reinterpret_tensor(buf360, (8, 448, 14, 14), (87808, 1, 6272, 448), 0); del buf360  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_91.run(le_44, buf427, convolution_103, unsqueeze_1138, buf431, squeeze_310, buf429, primals_311, buf433, 1568, 448, grid=grid(1568, 448), stream=stream0)
        del buf427
        del convolution_103
        del le_44
        del primals_311
        del squeeze_310
        del unsqueeze_1138
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf434 = aten.convolution_backward(buf433, relu_99, primals_310, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_310
        buf435 = buf434[0]
        buf436 = buf434[1]
        del buf434
        buf437 = buf356; del buf356  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_57.run(relu_99, buf353, buf435, buf437, 1024, 1568, grid=grid(1024), stream=stream0)
        buf438 = buf355; del buf355  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_58.run(relu_99, buf353, buf435, convolution_102, unsqueeze_1150, buf438, 13312, 121, grid=grid(13312), stream=stream0)
        buf439 = empty((1024, ), device='cuda', dtype=torch.float32)
        buf441 = empty((1024, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_59.run(buf438, squeeze_307, buf439, buf441, 1024, 13, grid=grid(1024), stream=stream0)
        buf440 = buf358; del buf358  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_60.run(relu_99, buf353, buf435, convolution_102, unsqueeze_1150, buf439, squeeze_307, buf437, primals_308, buf440, 1568, 1024, grid=grid(1568, 1024), stream=stream0)
        del convolution_102
        del primals_308
        del squeeze_307
        del unsqueeze_1150
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf442 = aten.convolution_backward(buf440, cat_10, primals_307, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf440
        del cat_10
        del primals_307
        buf443 = buf442[0]
        buf444 = buf442[1]
        del buf442
        buf445 = buf412; del buf412  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_61.run(le_46, buf443, buf445, 728, 121, grid=grid(728), stream=stream0)
        buf446 = buf413; del buf413  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_62.run(buf445, buf446, 56, 13, grid=grid(56), stream=stream0)
        buf447 = reinterpret_tensor(buf445, (56, 13), (1, 56), 0); del buf445  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(le_46, buf443, convolution_101, unsqueeze_1162, buf447, 728, 121, grid=grid(728), stream=stream0)
        buf448 = empty((56, ), device='cuda', dtype=torch.float32)
        buf449 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf447, squeeze_304, buf448, buf449, 56, 13, grid=grid(56), stream=stream0)
        buf450 = reinterpret_tensor(buf369, (8, 56, 14, 14), (10976, 1, 784, 56), 0); del buf369  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_65.run(le_46, buf443, convolution_101, unsqueeze_1162, buf448, squeeze_304, buf446, primals_305, buf450, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_101
        del le_46
        del primals_305
        del squeeze_304
        del unsqueeze_1162
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf451 = aten.convolution_backward(buf450, add_562, primals_304, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_562
        del primals_304
        buf452 = buf451[0]
        buf453 = buf451[1]
        del buf451
        buf454 = buf448; del buf448  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_66.run(le_47, buf443, buf452, buf454, 56, 1568, grid=grid(56), stream=stream0)
        buf455 = reinterpret_tensor(buf447, (56, 13), (13, 1), 0); del buf447  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_67.run(le_47, buf443, buf452, convolution_100, unsqueeze_1174, buf455, 728, 121, grid=grid(728), stream=stream0)
        buf456 = empty((56, ), device='cuda', dtype=torch.float32)
        buf458 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf455, squeeze_301, buf456, buf458, 56, 13, grid=grid(56), stream=stream0)
        buf457 = buf450; del buf450  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_69.run(le_47, buf443, buf452, convolution_100, unsqueeze_1174, buf456, squeeze_301, buf454, primals_302, buf457, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_100
        del le_47
        del primals_302
        del squeeze_301
        del unsqueeze_1174
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf459 = aten.convolution_backward(buf457, add_556, primals_301, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_556
        del primals_301
        buf460 = buf459[0]
        buf461 = buf459[1]
        del buf459
        buf462 = buf456; del buf456  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_70.run(le_48, buf443, buf460, buf462, 56, 1568, grid=grid(56), stream=stream0)
        buf463 = buf455; del buf455  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_71.run(le_48, buf443, buf460, convolution_99, unsqueeze_1186, buf463, 728, 121, grid=grid(728), stream=stream0)
        buf464 = empty((56, ), device='cuda', dtype=torch.float32)
        buf466 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf463, squeeze_298, buf464, buf466, 56, 13, grid=grid(56), stream=stream0)
        buf465 = buf457; del buf457  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_72.run(le_48, buf443, buf460, convolution_99, unsqueeze_1186, buf464, squeeze_298, buf462, primals_299, buf465, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_99
        del le_48
        del primals_299
        del squeeze_298
        del unsqueeze_1186
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf467 = aten.convolution_backward(buf465, add_550, primals_298, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_550
        del primals_298
        buf468 = buf467[0]
        buf469 = buf467[1]
        del buf467
        buf470 = buf464; del buf464  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_73.run(le_49, buf443, buf468, buf470, 56, 1568, grid=grid(56), stream=stream0)
        buf471 = buf463; del buf463  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_74.run(le_49, buf443, buf468, convolution_98, unsqueeze_1198, buf471, 728, 121, grid=grid(728), stream=stream0)
        buf472 = empty((56, ), device='cuda', dtype=torch.float32)
        buf474 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf471, squeeze_295, buf472, buf474, 56, 13, grid=grid(56), stream=stream0)
        buf473 = buf465; del buf465  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_75.run(le_49, buf443, buf468, convolution_98, unsqueeze_1198, buf472, squeeze_295, buf470, primals_296, buf473, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_98
        del le_49
        del primals_296
        del squeeze_295
        del unsqueeze_1198
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf475 = aten.convolution_backward(buf473, add_544, primals_295, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_544
        del primals_295
        buf476 = buf475[0]
        buf477 = buf475[1]
        del buf475
        buf478 = buf472; del buf472  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_76.run(le_50, buf443, buf476, buf478, 56, 1568, grid=grid(56), stream=stream0)
        buf479 = buf471; del buf471  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_77.run(le_50, buf443, buf476, convolution_97, unsqueeze_1210, buf479, 728, 121, grid=grid(728), stream=stream0)
        buf480 = empty((56, ), device='cuda', dtype=torch.float32)
        buf482 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf479, squeeze_292, buf480, buf482, 56, 13, grid=grid(56), stream=stream0)
        buf481 = buf473; del buf473  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_78.run(le_50, buf443, buf476, convolution_97, unsqueeze_1210, buf480, squeeze_292, buf478, primals_293, buf481, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_97
        del le_50
        del primals_293
        del squeeze_292
        del unsqueeze_1210
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf483 = aten.convolution_backward(buf481, add_538, primals_292, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_538
        del primals_292
        buf484 = buf483[0]
        buf485 = buf483[1]
        del buf483
        buf486 = buf480; del buf480  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_79.run(le_51, buf443, buf484, buf486, 56, 1568, grid=grid(56), stream=stream0)
        buf487 = buf479; del buf479  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_80.run(le_51, buf443, buf484, convolution_96, unsqueeze_1222, buf487, 728, 121, grid=grid(728), stream=stream0)
        buf488 = empty((56, ), device='cuda', dtype=torch.float32)
        buf490 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf487, squeeze_289, buf488, buf490, 56, 13, grid=grid(56), stream=stream0)
        buf489 = buf481; del buf481  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_81.run(le_51, buf443, buf484, convolution_96, unsqueeze_1222, buf488, squeeze_289, buf486, primals_290, buf489, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_96
        del le_51
        del primals_290
        del squeeze_289
        del unsqueeze_1222
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf491 = aten.convolution_backward(buf489, add_532, primals_289, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_532
        del primals_289
        buf492 = buf491[0]
        buf493 = buf491[1]
        del buf491
        buf494 = buf488; del buf488  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_82.run(le_52, buf443, buf492, buf494, 56, 1568, grid=grid(56), stream=stream0)
        buf495 = buf487; del buf487  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_83.run(le_52, buf443, buf492, convolution_95, unsqueeze_1234, buf495, 728, 121, grid=grid(728), stream=stream0)
        buf496 = empty((56, ), device='cuda', dtype=torch.float32)
        buf498 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf495, squeeze_286, buf496, buf498, 56, 13, grid=grid(56), stream=stream0)
        buf497 = buf489; del buf489  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_84.run(le_52, buf443, buf492, convolution_95, unsqueeze_1234, buf496, squeeze_286, buf494, primals_287, buf497, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_95
        del le_52
        del primals_287
        del squeeze_286
        del unsqueeze_1234
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf499 = aten.convolution_backward(buf497, getitem_920, primals_286, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf497
        del getitem_920
        del primals_286
        buf500 = buf499[0]
        buf501 = buf499[1]
        del buf499
        buf510 = reinterpret_tensor(buf433, (8, 448, 14, 14), (87808, 196, 14, 1), 0); del buf433  # reuse
        buf502 = reinterpret_tensor(buf510, (8, 56, 14, 14), (87808, 196, 14, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf500, buf502, 87808, grid=grid(87808), stream=stream0)
        del buf500
        buf503 = reinterpret_tensor(buf510, (8, 56, 14, 14), (87808, 196, 14, 1), 10976)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf492, buf503, 87808, grid=grid(87808), stream=stream0)
        del buf492
        buf504 = reinterpret_tensor(buf510, (8, 56, 14, 14), (87808, 196, 14, 1), 21952)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf484, buf504, 87808, grid=grid(87808), stream=stream0)
        del buf484
        buf505 = reinterpret_tensor(buf510, (8, 56, 14, 14), (87808, 196, 14, 1), 32928)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf476, buf505, 87808, grid=grid(87808), stream=stream0)
        del buf476
        buf506 = reinterpret_tensor(buf510, (8, 56, 14, 14), (87808, 196, 14, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf468, buf506, 87808, grid=grid(87808), stream=stream0)
        del buf468
        buf507 = reinterpret_tensor(buf510, (8, 56, 14, 14), (87808, 196, 14, 1), 54880)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf460, buf507, 87808, grid=grid(87808), stream=stream0)
        del buf460
        buf508 = reinterpret_tensor(buf510, (8, 56, 14, 14), (87808, 196, 14, 1), 65856)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf452, buf508, 87808, grid=grid(87808), stream=stream0)
        buf509 = reinterpret_tensor(buf510, (8, 56, 14, 14), (87808, 196, 14, 1), 76832)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_86.run(buf443, buf509, 87808, grid=grid(87808), stream=stream0)
        buf511 = reinterpret_tensor(buf430, (448, 13), (13, 1), 0); del buf430  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_87.run(le_53, buf510, buf511, 5824, 121, grid=grid(5824), stream=stream0)
        del buf502
        del buf503
        del buf504
        del buf505
        del buf506
        del buf507
        del buf508
        del buf509
        buf512 = buf431; del buf431  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_88.run(buf511, buf512, 448, 13, grid=grid(448), stream=stream0)
        buf513 = reinterpret_tensor(buf511, (448, 13), (1, 448), 0); del buf511  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_89.run(le_53, buf510, convolution_94, unsqueeze_1246, buf513, 5824, 121, grid=grid(5824), stream=stream0)
        buf514 = empty((448, ), device='cuda', dtype=torch.float32)
        buf515 = empty((448, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_90.run(buf513, squeeze_283, buf514, buf515, 448, 13, grid=grid(448), stream=stream0)
        buf516 = reinterpret_tensor(buf443, (8, 448, 14, 14), (87808, 1, 6272, 448), 0); del buf443  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_91.run(le_53, buf510, convolution_94, unsqueeze_1246, buf514, squeeze_283, buf512, primals_284, buf516, 1568, 448, grid=grid(1568, 448), stream=stream0)
        del buf510
        del convolution_94
        del le_53
        del primals_284
        del squeeze_283
        del unsqueeze_1246
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf517 = aten.convolution_backward(buf516, relu_90, primals_283, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_283
        buf518 = buf517[0]
        buf519 = buf517[1]
        del buf517
        buf520 = buf353; del buf353  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_92.run(buf520, relu_90, relu_99, buf435, buf518, 8192, 196, grid=grid(8192, 196), stream=stream0)
        del buf435
        del relu_90
        del relu_99
        buf521 = buf439; del buf439  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_93.run(buf520, buf521, 1024, 1568, grid=grid(1024), stream=stream0)
        buf522 = buf438; del buf438  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_94.run(buf520, convolution_93, unsqueeze_1258, buf522, 13312, 121, grid=grid(13312), stream=stream0)
        buf523 = empty((1024, ), device='cuda', dtype=torch.float32)
        buf524 = empty((1024, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_59.run(buf522, squeeze_280, buf523, buf524, 1024, 13, grid=grid(1024), stream=stream0)
        buf525 = reinterpret_tensor(buf518, (8, 1024, 14, 14), (200704, 1, 14336, 1024), 0); del buf518  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_95.run(buf520, convolution_93, unsqueeze_1258, buf523, squeeze_280, buf521, primals_281, buf525, 1568, 1024, grid=grid(1568, 1024), stream=stream0)
        del convolution_93
        del primals_281
        del squeeze_280
        del unsqueeze_1258
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf526 = aten.convolution_backward(buf525, cat_9, primals_280, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del cat_9
        del primals_280
        buf527 = buf526[0]
        buf528 = buf526[1]
        del buf526
        buf529 = buf495; del buf495  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_61.run(le_55, buf527, buf529, 728, 121, grid=grid(728), stream=stream0)
        buf530 = buf496; del buf496  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_62.run(buf529, buf530, 56, 13, grid=grid(56), stream=stream0)
        buf531 = reinterpret_tensor(buf529, (56, 13), (1, 56), 0); del buf529  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(le_55, buf527, convolution_92, unsqueeze_1270, buf531, 728, 121, grid=grid(728), stream=stream0)
        buf532 = empty((56, ), device='cuda', dtype=torch.float32)
        buf533 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf531, squeeze_277, buf532, buf533, 56, 13, grid=grid(56), stream=stream0)
        buf534 = reinterpret_tensor(buf452, (8, 56, 14, 14), (10976, 1, 784, 56), 0); del buf452  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_65.run(le_55, buf527, convolution_92, unsqueeze_1270, buf532, squeeze_277, buf530, primals_278, buf534, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_92
        del le_55
        del primals_278
        del squeeze_277
        del unsqueeze_1270
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf535 = aten.convolution_backward(buf534, add_510, primals_277, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_510
        del primals_277
        buf536 = buf535[0]
        buf537 = buf535[1]
        del buf535
        buf538 = buf532; del buf532  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_66.run(le_56, buf527, buf536, buf538, 56, 1568, grid=grid(56), stream=stream0)
        buf539 = reinterpret_tensor(buf531, (56, 13), (13, 1), 0); del buf531  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_67.run(le_56, buf527, buf536, convolution_91, unsqueeze_1282, buf539, 728, 121, grid=grid(728), stream=stream0)
        buf540 = empty((56, ), device='cuda', dtype=torch.float32)
        buf542 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf539, squeeze_274, buf540, buf542, 56, 13, grid=grid(56), stream=stream0)
        buf541 = buf534; del buf534  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_69.run(le_56, buf527, buf536, convolution_91, unsqueeze_1282, buf540, squeeze_274, buf538, primals_275, buf541, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_91
        del le_56
        del primals_275
        del squeeze_274
        del unsqueeze_1282
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf543 = aten.convolution_backward(buf541, add_504, primals_274, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_504
        del primals_274
        buf544 = buf543[0]
        buf545 = buf543[1]
        del buf543
        buf546 = buf540; del buf540  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_70.run(le_57, buf527, buf544, buf546, 56, 1568, grid=grid(56), stream=stream0)
        buf547 = buf539; del buf539  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_71.run(le_57, buf527, buf544, convolution_90, unsqueeze_1294, buf547, 728, 121, grid=grid(728), stream=stream0)
        buf548 = empty((56, ), device='cuda', dtype=torch.float32)
        buf550 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf547, squeeze_271, buf548, buf550, 56, 13, grid=grid(56), stream=stream0)
        buf549 = buf541; del buf541  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_72.run(le_57, buf527, buf544, convolution_90, unsqueeze_1294, buf548, squeeze_271, buf546, primals_272, buf549, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_90
        del le_57
        del primals_272
        del squeeze_271
        del unsqueeze_1294
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf551 = aten.convolution_backward(buf549, add_498, primals_271, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_498
        del primals_271
        buf552 = buf551[0]
        buf553 = buf551[1]
        del buf551
        buf554 = buf548; del buf548  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_73.run(le_58, buf527, buf552, buf554, 56, 1568, grid=grid(56), stream=stream0)
        buf555 = buf547; del buf547  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_74.run(le_58, buf527, buf552, convolution_89, unsqueeze_1306, buf555, 728, 121, grid=grid(728), stream=stream0)
        buf556 = empty((56, ), device='cuda', dtype=torch.float32)
        buf558 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf555, squeeze_268, buf556, buf558, 56, 13, grid=grid(56), stream=stream0)
        buf557 = buf549; del buf549  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_75.run(le_58, buf527, buf552, convolution_89, unsqueeze_1306, buf556, squeeze_268, buf554, primals_269, buf557, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_89
        del le_58
        del primals_269
        del squeeze_268
        del unsqueeze_1306
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf559 = aten.convolution_backward(buf557, add_492, primals_268, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_492
        del primals_268
        buf560 = buf559[0]
        buf561 = buf559[1]
        del buf559
        buf562 = buf556; del buf556  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_76.run(le_59, buf527, buf560, buf562, 56, 1568, grid=grid(56), stream=stream0)
        buf563 = buf555; del buf555  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_77.run(le_59, buf527, buf560, convolution_88, unsqueeze_1318, buf563, 728, 121, grid=grid(728), stream=stream0)
        buf564 = empty((56, ), device='cuda', dtype=torch.float32)
        buf566 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf563, squeeze_265, buf564, buf566, 56, 13, grid=grid(56), stream=stream0)
        buf565 = buf557; del buf557  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_78.run(le_59, buf527, buf560, convolution_88, unsqueeze_1318, buf564, squeeze_265, buf562, primals_266, buf565, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_88
        del le_59
        del primals_266
        del squeeze_265
        del unsqueeze_1318
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf567 = aten.convolution_backward(buf565, add_486, primals_265, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_486
        del primals_265
        buf568 = buf567[0]
        buf569 = buf567[1]
        del buf567
        buf570 = buf564; del buf564  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_79.run(le_60, buf527, buf568, buf570, 56, 1568, grid=grid(56), stream=stream0)
        buf571 = buf563; del buf563  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_80.run(le_60, buf527, buf568, convolution_87, unsqueeze_1330, buf571, 728, 121, grid=grid(728), stream=stream0)
        buf572 = empty((56, ), device='cuda', dtype=torch.float32)
        buf574 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf571, squeeze_262, buf572, buf574, 56, 13, grid=grid(56), stream=stream0)
        buf573 = buf565; del buf565  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_81.run(le_60, buf527, buf568, convolution_87, unsqueeze_1330, buf572, squeeze_262, buf570, primals_263, buf573, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_87
        del le_60
        del primals_263
        del squeeze_262
        del unsqueeze_1330
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf575 = aten.convolution_backward(buf573, add_480, primals_262, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_480
        del primals_262
        buf576 = buf575[0]
        buf577 = buf575[1]
        del buf575
        buf578 = buf572; del buf572  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_82.run(le_61, buf527, buf576, buf578, 56, 1568, grid=grid(56), stream=stream0)
        buf579 = buf571; del buf571  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_83.run(le_61, buf527, buf576, convolution_86, unsqueeze_1342, buf579, 728, 121, grid=grid(728), stream=stream0)
        buf580 = empty((56, ), device='cuda', dtype=torch.float32)
        buf582 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf579, squeeze_259, buf580, buf582, 56, 13, grid=grid(56), stream=stream0)
        buf581 = buf573; del buf573  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_84.run(le_61, buf527, buf576, convolution_86, unsqueeze_1342, buf580, squeeze_259, buf578, primals_260, buf581, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_86
        del le_61
        del primals_260
        del squeeze_259
        del unsqueeze_1342
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf583 = aten.convolution_backward(buf581, getitem_830, primals_259, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf581
        del getitem_830
        del primals_259
        buf584 = buf583[0]
        buf585 = buf583[1]
        del buf583
        buf594 = reinterpret_tensor(buf516, (8, 448, 14, 14), (87808, 196, 14, 1), 0); del buf516  # reuse
        buf586 = reinterpret_tensor(buf594, (8, 56, 14, 14), (87808, 196, 14, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf584, buf586, 87808, grid=grid(87808), stream=stream0)
        del buf584
        buf587 = reinterpret_tensor(buf594, (8, 56, 14, 14), (87808, 196, 14, 1), 10976)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf576, buf587, 87808, grid=grid(87808), stream=stream0)
        del buf576
        buf588 = reinterpret_tensor(buf594, (8, 56, 14, 14), (87808, 196, 14, 1), 21952)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf568, buf588, 87808, grid=grid(87808), stream=stream0)
        del buf568
        buf589 = reinterpret_tensor(buf594, (8, 56, 14, 14), (87808, 196, 14, 1), 32928)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf560, buf589, 87808, grid=grid(87808), stream=stream0)
        del buf560
        buf590 = reinterpret_tensor(buf594, (8, 56, 14, 14), (87808, 196, 14, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf552, buf590, 87808, grid=grid(87808), stream=stream0)
        del buf552
        buf591 = reinterpret_tensor(buf594, (8, 56, 14, 14), (87808, 196, 14, 1), 54880)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf544, buf591, 87808, grid=grid(87808), stream=stream0)
        del buf544
        buf592 = reinterpret_tensor(buf594, (8, 56, 14, 14), (87808, 196, 14, 1), 65856)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf536, buf592, 87808, grid=grid(87808), stream=stream0)
        buf593 = reinterpret_tensor(buf594, (8, 56, 14, 14), (87808, 196, 14, 1), 76832)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_86.run(buf527, buf593, 87808, grid=grid(87808), stream=stream0)
        buf595 = reinterpret_tensor(buf513, (448, 13), (13, 1), 0); del buf513  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_87.run(le_62, buf594, buf595, 5824, 121, grid=grid(5824), stream=stream0)
        del buf586
        del buf587
        del buf588
        del buf589
        del buf590
        del buf591
        del buf592
        del buf593
        buf596 = buf514; del buf514  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_88.run(buf595, buf596, 448, 13, grid=grid(448), stream=stream0)
        buf597 = reinterpret_tensor(buf595, (448, 13), (1, 448), 0); del buf595  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_89.run(le_62, buf594, convolution_85, unsqueeze_1354, buf597, 5824, 121, grid=grid(5824), stream=stream0)
        buf598 = empty((448, ), device='cuda', dtype=torch.float32)
        buf599 = empty((448, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_90.run(buf597, squeeze_256, buf598, buf599, 448, 13, grid=grid(448), stream=stream0)
        buf600 = reinterpret_tensor(buf527, (8, 448, 14, 14), (87808, 1, 6272, 448), 0); del buf527  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_91.run(le_62, buf594, convolution_85, unsqueeze_1354, buf598, squeeze_256, buf596, primals_257, buf600, 1568, 448, grid=grid(1568, 448), stream=stream0)
        del buf594
        del convolution_85
        del le_62
        del primals_257
        del squeeze_256
        del unsqueeze_1354
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf601 = aten.convolution_backward(buf600, relu_81, primals_256, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_256
        buf602 = buf601[0]
        buf603 = buf601[1]
        del buf601
        buf604 = buf523; del buf523  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_57.run(relu_81, buf520, buf602, buf604, 1024, 1568, grid=grid(1024), stream=stream0)
        buf605 = buf522; del buf522  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_58.run(relu_81, buf520, buf602, convolution_84, unsqueeze_1366, buf605, 13312, 121, grid=grid(13312), stream=stream0)
        buf606 = empty((1024, ), device='cuda', dtype=torch.float32)
        buf608 = empty((1024, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_59.run(buf605, squeeze_253, buf606, buf608, 1024, 13, grid=grid(1024), stream=stream0)
        buf607 = buf525; del buf525  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_60.run(relu_81, buf520, buf602, convolution_84, unsqueeze_1366, buf606, squeeze_253, buf604, primals_254, buf607, 1568, 1024, grid=grid(1568, 1024), stream=stream0)
        del convolution_84
        del primals_254
        del squeeze_253
        del unsqueeze_1366
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf609 = aten.convolution_backward(buf607, cat_8, primals_253, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf607
        del cat_8
        del primals_253
        buf610 = buf609[0]
        buf611 = buf609[1]
        del buf609
        buf612 = buf579; del buf579  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_61.run(le_64, buf610, buf612, 728, 121, grid=grid(728), stream=stream0)
        buf613 = buf580; del buf580  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_62.run(buf612, buf613, 56, 13, grid=grid(56), stream=stream0)
        buf614 = reinterpret_tensor(buf612, (56, 13), (1, 56), 0); del buf612  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(le_64, buf610, convolution_83, unsqueeze_1378, buf614, 728, 121, grid=grid(728), stream=stream0)
        buf615 = empty((56, ), device='cuda', dtype=torch.float32)
        buf616 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf614, squeeze_250, buf615, buf616, 56, 13, grid=grid(56), stream=stream0)
        buf617 = reinterpret_tensor(buf536, (8, 56, 14, 14), (10976, 1, 784, 56), 0); del buf536  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_65.run(le_64, buf610, convolution_83, unsqueeze_1378, buf615, squeeze_250, buf613, primals_251, buf617, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_83
        del le_64
        del primals_251
        del squeeze_250
        del unsqueeze_1378
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf618 = aten.convolution_backward(buf617, add_458, primals_250, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_458
        del primals_250
        buf619 = buf618[0]
        buf620 = buf618[1]
        del buf618
        buf621 = buf615; del buf615  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_66.run(le_65, buf610, buf619, buf621, 56, 1568, grid=grid(56), stream=stream0)
        buf622 = reinterpret_tensor(buf614, (56, 13), (13, 1), 0); del buf614  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_67.run(le_65, buf610, buf619, convolution_82, unsqueeze_1390, buf622, 728, 121, grid=grid(728), stream=stream0)
        buf623 = empty((56, ), device='cuda', dtype=torch.float32)
        buf625 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf622, squeeze_247, buf623, buf625, 56, 13, grid=grid(56), stream=stream0)
        buf624 = buf617; del buf617  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_69.run(le_65, buf610, buf619, convolution_82, unsqueeze_1390, buf623, squeeze_247, buf621, primals_248, buf624, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_82
        del le_65
        del primals_248
        del squeeze_247
        del unsqueeze_1390
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf626 = aten.convolution_backward(buf624, add_452, primals_247, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_452
        del primals_247
        buf627 = buf626[0]
        buf628 = buf626[1]
        del buf626
        buf629 = buf623; del buf623  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_70.run(le_66, buf610, buf627, buf629, 56, 1568, grid=grid(56), stream=stream0)
        buf630 = buf622; del buf622  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_71.run(le_66, buf610, buf627, convolution_81, unsqueeze_1402, buf630, 728, 121, grid=grid(728), stream=stream0)
        buf631 = empty((56, ), device='cuda', dtype=torch.float32)
        buf633 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf630, squeeze_244, buf631, buf633, 56, 13, grid=grid(56), stream=stream0)
        buf632 = buf624; del buf624  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_72.run(le_66, buf610, buf627, convolution_81, unsqueeze_1402, buf631, squeeze_244, buf629, primals_245, buf632, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_81
        del le_66
        del primals_245
        del squeeze_244
        del unsqueeze_1402
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf634 = aten.convolution_backward(buf632, add_446, primals_244, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_446
        del primals_244
        buf635 = buf634[0]
        buf636 = buf634[1]
        del buf634
        buf637 = buf631; del buf631  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_73.run(le_67, buf610, buf635, buf637, 56, 1568, grid=grid(56), stream=stream0)
        buf638 = buf630; del buf630  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_74.run(le_67, buf610, buf635, convolution_80, unsqueeze_1414, buf638, 728, 121, grid=grid(728), stream=stream0)
        buf639 = empty((56, ), device='cuda', dtype=torch.float32)
        buf641 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf638, squeeze_241, buf639, buf641, 56, 13, grid=grid(56), stream=stream0)
        buf640 = buf632; del buf632  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_75.run(le_67, buf610, buf635, convolution_80, unsqueeze_1414, buf639, squeeze_241, buf637, primals_242, buf640, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_80
        del le_67
        del primals_242
        del squeeze_241
        del unsqueeze_1414
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf642 = aten.convolution_backward(buf640, add_440, primals_241, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_440
        del primals_241
        buf643 = buf642[0]
        buf644 = buf642[1]
        del buf642
        buf645 = buf639; del buf639  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_76.run(le_68, buf610, buf643, buf645, 56, 1568, grid=grid(56), stream=stream0)
        buf646 = buf638; del buf638  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_77.run(le_68, buf610, buf643, convolution_79, unsqueeze_1426, buf646, 728, 121, grid=grid(728), stream=stream0)
        buf647 = empty((56, ), device='cuda', dtype=torch.float32)
        buf649 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf646, squeeze_238, buf647, buf649, 56, 13, grid=grid(56), stream=stream0)
        buf648 = buf640; del buf640  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_78.run(le_68, buf610, buf643, convolution_79, unsqueeze_1426, buf647, squeeze_238, buf645, primals_239, buf648, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_79
        del le_68
        del primals_239
        del squeeze_238
        del unsqueeze_1426
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf650 = aten.convolution_backward(buf648, add_434, primals_238, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_434
        del primals_238
        buf651 = buf650[0]
        buf652 = buf650[1]
        del buf650
        buf653 = buf647; del buf647  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_79.run(le_69, buf610, buf651, buf653, 56, 1568, grid=grid(56), stream=stream0)
        buf654 = buf646; del buf646  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_80.run(le_69, buf610, buf651, convolution_78, unsqueeze_1438, buf654, 728, 121, grid=grid(728), stream=stream0)
        buf655 = empty((56, ), device='cuda', dtype=torch.float32)
        buf657 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf654, squeeze_235, buf655, buf657, 56, 13, grid=grid(56), stream=stream0)
        buf656 = buf648; del buf648  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_81.run(le_69, buf610, buf651, convolution_78, unsqueeze_1438, buf655, squeeze_235, buf653, primals_236, buf656, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_78
        del le_69
        del primals_236
        del squeeze_235
        del unsqueeze_1438
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf658 = aten.convolution_backward(buf656, add_428, primals_235, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_428
        del primals_235
        buf659 = buf658[0]
        buf660 = buf658[1]
        del buf658
        buf661 = buf655; del buf655  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_82.run(le_70, buf610, buf659, buf661, 56, 1568, grid=grid(56), stream=stream0)
        buf662 = buf654; del buf654  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_83.run(le_70, buf610, buf659, convolution_77, unsqueeze_1450, buf662, 728, 121, grid=grid(728), stream=stream0)
        buf663 = empty((56, ), device='cuda', dtype=torch.float32)
        buf665 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_68.run(buf662, squeeze_232, buf663, buf665, 56, 13, grid=grid(56), stream=stream0)
        buf664 = buf656; del buf656  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_84.run(le_70, buf610, buf659, convolution_77, unsqueeze_1450, buf663, squeeze_232, buf661, primals_233, buf664, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_77
        del le_70
        del primals_233
        del squeeze_232
        del unsqueeze_1450
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf666 = aten.convolution_backward(buf664, getitem_740, primals_232, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf664
        del getitem_740
        del primals_232
        buf667 = buf666[0]
        buf668 = buf666[1]
        del buf666
        buf677 = reinterpret_tensor(buf600, (8, 448, 14, 14), (87808, 196, 14, 1), 0); del buf600  # reuse
        buf669 = reinterpret_tensor(buf677, (8, 56, 14, 14), (87808, 196, 14, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf667, buf669, 87808, grid=grid(87808), stream=stream0)
        del buf667
        buf670 = reinterpret_tensor(buf677, (8, 56, 14, 14), (87808, 196, 14, 1), 10976)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf659, buf670, 87808, grid=grid(87808), stream=stream0)
        del buf659
        buf671 = reinterpret_tensor(buf677, (8, 56, 14, 14), (87808, 196, 14, 1), 21952)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf651, buf671, 87808, grid=grid(87808), stream=stream0)
        del buf651
        buf672 = reinterpret_tensor(buf677, (8, 56, 14, 14), (87808, 196, 14, 1), 32928)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf643, buf672, 87808, grid=grid(87808), stream=stream0)
        del buf643
        buf673 = reinterpret_tensor(buf677, (8, 56, 14, 14), (87808, 196, 14, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf635, buf673, 87808, grid=grid(87808), stream=stream0)
        del buf635
        buf674 = reinterpret_tensor(buf677, (8, 56, 14, 14), (87808, 196, 14, 1), 54880)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf627, buf674, 87808, grid=grid(87808), stream=stream0)
        del buf627
        buf675 = reinterpret_tensor(buf677, (8, 56, 14, 14), (87808, 196, 14, 1), 65856)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_85.run(buf619, buf675, 87808, grid=grid(87808), stream=stream0)
        buf676 = reinterpret_tensor(buf677, (8, 56, 14, 14), (87808, 196, 14, 1), 76832)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_86.run(buf610, buf676, 87808, grid=grid(87808), stream=stream0)
        buf678 = reinterpret_tensor(buf597, (448, 13), (13, 1), 0); del buf597  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_87.run(le_71, buf677, buf678, 5824, 121, grid=grid(5824), stream=stream0)
        del buf669
        del buf670
        del buf671
        del buf672
        del buf673
        del buf674
        del buf675
        del buf676
        buf679 = buf598; del buf598  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_88.run(buf678, buf679, 448, 13, grid=grid(448), stream=stream0)
        buf680 = reinterpret_tensor(buf678, (448, 13), (1, 448), 0); del buf678  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_89.run(le_71, buf677, convolution_76, unsqueeze_1462, buf680, 5824, 121, grid=grid(5824), stream=stream0)
        buf681 = empty((448, ), device='cuda', dtype=torch.float32)
        buf682 = empty((448, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_90.run(buf680, squeeze_229, buf681, buf682, 448, 13, grid=grid(448), stream=stream0)
        del buf680
        buf683 = reinterpret_tensor(buf610, (8, 448, 14, 14), (87808, 1, 6272, 448), 0); del buf610  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_91.run(le_71, buf677, convolution_76, unsqueeze_1462, buf681, squeeze_229, buf679, primals_230, buf683, 1568, 448, grid=grid(1568, 448), stream=stream0)
        del buf677
        del convolution_76
        del le_71
        del primals_230
        del squeeze_229
        del unsqueeze_1462
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf684 = aten.convolution_backward(buf683, relu_72, primals_229, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf683
        del primals_229
        buf685 = buf684[0]
        buf686 = buf684[1]
        del buf684
        buf687 = buf520; del buf520  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_92.run(buf687, relu_72, relu_81, buf602, buf685, 8192, 196, grid=grid(8192, 196), stream=stream0)
        del relu_72
        del relu_81
        buf688 = buf606; del buf606  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_93.run(buf687, buf688, 1024, 1568, grid=grid(1024), stream=stream0)
        buf689 = buf605; del buf605  # reuse
        buf696 = empty((1024, 13), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_96.run(buf687, convolution_75, unsqueeze_1474, convolution_74, unsqueeze_1486, buf689, buf696, 13312, 121, grid=grid(13312), stream=stream0)
        buf690 = empty((1024, ), device='cuda', dtype=torch.float32)
        buf691 = empty((1024, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_59.run(buf689, squeeze_226, buf690, buf691, 1024, 13, grid=grid(1024), stream=stream0)
        del buf689
        buf697 = empty((1024, ), device='cuda', dtype=torch.float32)
        buf698 = empty((1024, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_59.run(buf696, squeeze_223, buf697, buf698, 1024, 13, grid=grid(1024), stream=stream0)
        del buf696
        buf692 = reinterpret_tensor(buf685, (8, 1024, 14, 14), (200704, 1, 14336, 1024), 0); del buf685  # reuse
        buf699 = reinterpret_tensor(buf602, (8, 1024, 14, 14), (200704, 1, 14336, 1024), 0); del buf602  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_97.run(buf687, convolution_75, unsqueeze_1474, buf690, squeeze_226, buf688, primals_227, convolution_74, unsqueeze_1486, buf697, squeeze_223, primals_224, buf692, buf699, 1568, 1024, grid=grid(1568, 1024), stream=stream0)
        del buf687
        del buf690
        del buf697
        del convolution_74
        del convolution_75
        del primals_224
        del primals_227
        del squeeze_223
        del squeeze_226
        del unsqueeze_1474
        del unsqueeze_1486
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf693 = aten.convolution_backward(buf692, relu_63, primals_226, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf692
        del primals_226
        buf694 = buf693[0]
        buf695 = buf693[1]
        del buf693
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf700 = aten.convolution_backward(buf699, cat_7, primals_223, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf699
        del cat_7
        del primals_223
        buf701 = buf700[0]
        buf702 = buf700[1]
        del buf700
        buf774 = empty((8, 448, 28, 28), device='cuda', dtype=torch.float32)
        buf703 = reinterpret_tensor(buf774, (8, 56, 28, 28), (351232, 784, 28, 1), 307328)  # alias
        # Source Nodes: [], Original ATen: [aten.avg_pool2d_backward]
        triton_poi_fused_avg_pool2d_backward_98.run(buf701, buf703, 351232, grid=grid(351232), stream=stream0)
        buf704 = buf662; del buf662  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_61.run(le_73, buf701, buf704, 728, 121, grid=grid(728), stream=stream0)
        buf705 = buf663; del buf663  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_62.run(buf704, buf705, 56, 13, grid=grid(56), stream=stream0)
        buf706 = reinterpret_tensor(buf704, (56, 13), (1, 56), 0); del buf704  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_63.run(le_73, buf701, convolution_73, unsqueeze_1498, buf706, 728, 121, grid=grid(728), stream=stream0)
        buf707 = empty((56, ), device='cuda', dtype=torch.float32)
        buf708 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf706, squeeze_220, buf707, buf708, 56, 13, grid=grid(56), stream=stream0)
        buf709 = reinterpret_tensor(buf619, (8, 56, 14, 14), (10976, 1, 784, 56), 0); del buf619  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_65.run(le_73, buf701, convolution_73, unsqueeze_1498, buf707, squeeze_220, buf705, primals_221, buf709, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_73
        del le_73
        del primals_221
        del squeeze_220
        del unsqueeze_1498
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf710 = aten.convolution_backward(buf709, getitem_714, primals_220, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_714
        del primals_220
        buf711 = buf710[0]
        buf712 = buf710[1]
        del buf710
        buf713 = reinterpret_tensor(buf706, (56, 13), (13, 1), 0); del buf706  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_99.run(le_74, buf701, buf713, 728, 121, grid=grid(728), stream=stream0)
        buf714 = buf707; del buf707  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_62.run(buf713, buf714, 56, 13, grid=grid(56), stream=stream0)
        buf715 = reinterpret_tensor(buf713, (56, 13), (1, 56), 0); del buf713  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_100.run(le_74, buf701, convolution_72, unsqueeze_1510, buf715, 728, 121, grid=grid(728), stream=stream0)
        buf716 = empty((56, ), device='cuda', dtype=torch.float32)
        buf717 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf715, squeeze_217, buf716, buf717, 56, 13, grid=grid(56), stream=stream0)
        buf718 = buf709; del buf709  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_101.run(le_74, buf701, convolution_72, unsqueeze_1510, buf716, squeeze_217, buf714, primals_218, buf718, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_72
        del le_74
        del primals_218
        del squeeze_217
        del unsqueeze_1510
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf719 = aten.convolution_backward(buf718, getitem_703, primals_217, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_703
        del primals_217
        buf720 = buf719[0]
        buf721 = buf719[1]
        del buf719
        buf722 = reinterpret_tensor(buf715, (56, 13), (13, 1), 0); del buf715  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_102.run(le_75, buf701, buf722, 728, 121, grid=grid(728), stream=stream0)
        buf723 = buf716; del buf716  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_62.run(buf722, buf723, 56, 13, grid=grid(56), stream=stream0)
        buf724 = reinterpret_tensor(buf722, (56, 13), (1, 56), 0); del buf722  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_103.run(le_75, buf701, convolution_71, unsqueeze_1522, buf724, 728, 121, grid=grid(728), stream=stream0)
        buf725 = empty((56, ), device='cuda', dtype=torch.float32)
        buf726 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf724, squeeze_214, buf725, buf726, 56, 13, grid=grid(56), stream=stream0)
        buf727 = buf718; del buf718  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_104.run(le_75, buf701, convolution_71, unsqueeze_1522, buf725, squeeze_214, buf723, primals_215, buf727, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_71
        del le_75
        del primals_215
        del squeeze_214
        del unsqueeze_1522
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf728 = aten.convolution_backward(buf727, getitem_692, primals_214, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_692
        del primals_214
        buf729 = buf728[0]
        buf730 = buf728[1]
        del buf728
        buf731 = reinterpret_tensor(buf724, (56, 13), (13, 1), 0); del buf724  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_105.run(le_76, buf701, buf731, 728, 121, grid=grid(728), stream=stream0)
        buf732 = buf725; del buf725  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_62.run(buf731, buf732, 56, 13, grid=grid(56), stream=stream0)
        buf733 = reinterpret_tensor(buf731, (56, 13), (1, 56), 0); del buf731  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_106.run(le_76, buf701, convolution_70, unsqueeze_1534, buf733, 728, 121, grid=grid(728), stream=stream0)
        buf734 = empty((56, ), device='cuda', dtype=torch.float32)
        buf735 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf733, squeeze_211, buf734, buf735, 56, 13, grid=grid(56), stream=stream0)
        buf736 = buf727; del buf727  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_107.run(le_76, buf701, convolution_70, unsqueeze_1534, buf734, squeeze_211, buf732, primals_212, buf736, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_70
        del le_76
        del primals_212
        del squeeze_211
        del unsqueeze_1534
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf737 = aten.convolution_backward(buf736, getitem_681, primals_211, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_681
        del primals_211
        buf738 = buf737[0]
        buf739 = buf737[1]
        del buf737
        buf740 = reinterpret_tensor(buf733, (56, 13), (13, 1), 0); del buf733  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_108.run(le_77, buf701, buf740, 728, 121, grid=grid(728), stream=stream0)
        buf741 = buf734; del buf734  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_62.run(buf740, buf741, 56, 13, grid=grid(56), stream=stream0)
        buf742 = reinterpret_tensor(buf740, (56, 13), (1, 56), 0); del buf740  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_109.run(le_77, buf701, convolution_69, unsqueeze_1546, buf742, 728, 121, grid=grid(728), stream=stream0)
        buf743 = empty((56, ), device='cuda', dtype=torch.float32)
        buf744 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf742, squeeze_208, buf743, buf744, 56, 13, grid=grid(56), stream=stream0)
        buf745 = buf736; del buf736  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_110.run(le_77, buf701, convolution_69, unsqueeze_1546, buf743, squeeze_208, buf741, primals_209, buf745, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_69
        del le_77
        del primals_209
        del squeeze_208
        del unsqueeze_1546
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf746 = aten.convolution_backward(buf745, getitem_670, primals_208, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_670
        del primals_208
        buf747 = buf746[0]
        buf748 = buf746[1]
        del buf746
        buf749 = reinterpret_tensor(buf742, (56, 13), (13, 1), 0); del buf742  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_111.run(le_78, buf701, buf749, 728, 121, grid=grid(728), stream=stream0)
        buf750 = buf743; del buf743  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_62.run(buf749, buf750, 56, 13, grid=grid(56), stream=stream0)
        buf751 = reinterpret_tensor(buf749, (56, 13), (1, 56), 0); del buf749  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_112.run(le_78, buf701, convolution_68, unsqueeze_1558, buf751, 728, 121, grid=grid(728), stream=stream0)
        buf752 = empty((56, ), device='cuda', dtype=torch.float32)
        buf753 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf751, squeeze_205, buf752, buf753, 56, 13, grid=grid(56), stream=stream0)
        buf754 = buf745; del buf745  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_113.run(le_78, buf701, convolution_68, unsqueeze_1558, buf752, squeeze_205, buf750, primals_206, buf754, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del convolution_68
        del le_78
        del primals_206
        del squeeze_205
        del unsqueeze_1558
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf755 = aten.convolution_backward(buf754, getitem_659, primals_205, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_659
        del primals_205
        buf756 = buf755[0]
        buf757 = buf755[1]
        del buf755
        buf758 = reinterpret_tensor(buf751, (56, 13), (13, 1), 0); del buf751  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_114.run(le_79, buf701, buf758, 728, 121, grid=grid(728), stream=stream0)
        buf759 = buf752; del buf752  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_62.run(buf758, buf759, 56, 13, grid=grid(56), stream=stream0)
        buf760 = reinterpret_tensor(buf758, (56, 13), (1, 56), 0); del buf758  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_115.run(le_79, buf701, convolution_67, unsqueeze_1570, buf760, 728, 121, grid=grid(728), stream=stream0)
        buf761 = empty((56, ), device='cuda', dtype=torch.float32)
        buf762 = empty((56, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_64.run(buf760, squeeze_202, buf761, buf762, 56, 13, grid=grid(56), stream=stream0)
        del buf760
        buf763 = buf754; del buf754  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_116.run(le_79, buf701, convolution_67, unsqueeze_1570, buf761, squeeze_202, buf759, primals_203, buf763, 1568, 56, grid=grid(1568, 56), stream=stream0)
        del buf701
        del convolution_67
        del le_79
        del primals_203
        del squeeze_202
        del unsqueeze_1570
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf764 = aten.convolution_backward(buf763, getitem_648, primals_202, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf763
        del getitem_648
        del primals_202
        buf765 = buf764[0]
        buf766 = buf764[1]
        del buf764
        buf767 = reinterpret_tensor(buf774, (8, 56, 28, 28), (351232, 784, 28, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf765, buf767, 351232, grid=grid(351232), stream=stream0)
        del buf765
        buf768 = reinterpret_tensor(buf774, (8, 56, 28, 28), (351232, 784, 28, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf756, buf768, 351232, grid=grid(351232), stream=stream0)
        del buf756
        buf769 = reinterpret_tensor(buf774, (8, 56, 28, 28), (351232, 784, 28, 1), 87808)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf747, buf769, 351232, grid=grid(351232), stream=stream0)
        del buf747
        buf770 = reinterpret_tensor(buf774, (8, 56, 28, 28), (351232, 784, 28, 1), 131712)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf738, buf770, 351232, grid=grid(351232), stream=stream0)
        del buf738
        buf771 = reinterpret_tensor(buf774, (8, 56, 28, 28), (351232, 784, 28, 1), 175616)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf729, buf771, 351232, grid=grid(351232), stream=stream0)
        del buf729
        buf772 = reinterpret_tensor(buf774, (8, 56, 28, 28), (351232, 784, 28, 1), 219520)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf720, buf772, 351232, grid=grid(351232), stream=stream0)
        del buf720
        buf773 = reinterpret_tensor(buf774, (8, 56, 28, 28), (351232, 784, 28, 1), 263424)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf711, buf773, 351232, grid=grid(351232), stream=stream0)
        buf775 = empty((448, 49), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_118.run(le_80, buf774, buf775, 21952, 128, grid=grid(21952), stream=stream0)
        del buf703
        del buf767
        del buf768
        del buf769
        del buf770
        del buf771
        del buf772
        del buf773
        buf776 = buf681; del buf681  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_119.run(buf775, buf776, 448, 49, grid=grid(448), stream=stream0)
        buf777 = reinterpret_tensor(buf775, (448, 49), (1, 448), 0); del buf775  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_120.run(le_80, buf774, convolution_66, unsqueeze_1582, buf777, 21952, 128, grid=grid(21952), stream=stream0)
        buf778 = empty((448, ), device='cuda', dtype=torch.float32)
        buf779 = empty((448, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_121.run(buf777, squeeze_199, buf778, buf779, 448, 49, grid=grid(448), stream=stream0)
        buf780 = empty_strided((8, 448, 28, 28), (351232, 1, 12544, 448), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_122.run(le_80, buf774, convolution_66, unsqueeze_1582, buf778, squeeze_199, buf776, primals_200, buf780, 6272, 448, grid=grid(6272, 448), stream=stream0)
        del buf774
        del buf778
        del convolution_66
        del le_80
        del primals_200
        del squeeze_199
        del unsqueeze_1582
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf781 = aten.convolution_backward(buf780, relu_63, primals_199, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_199
        buf782 = buf781[0]
        buf783 = buf781[1]
        del buf781
        buf784 = empty((512, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_123.run(relu_63, buf694, buf782, buf784, 512, 6272, grid=grid(512), stream=stream0)
        buf785 = empty((512, 49), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_124.run(relu_63, buf694, buf782, convolution_65, unsqueeze_1594, buf785, 25088, 128, grid=grid(25088), stream=stream0)
        buf786 = empty((512, ), device='cuda', dtype=torch.float32)
        buf788 = empty((512, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_125.run(buf785, squeeze_196, buf786, buf788, 512, 49, grid=grid(512), stream=stream0)
        buf787 = empty_strided((8, 512, 28, 28), (401408, 1, 14336, 512), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_126.run(relu_63, buf694, buf782, convolution_65, unsqueeze_1594, buf786, squeeze_196, buf784, primals_197, buf787, 6272, 512, grid=grid(6272, 512), stream=stream0)
        del convolution_65
        del primals_197
        del squeeze_196
        del unsqueeze_1594
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf789 = aten.convolution_backward(buf787, cat_6, primals_196, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf787
        del cat_6
        del primals_196
        buf790 = buf789[0]
        buf791 = buf789[1]
        del buf789
        buf792 = empty((28, 49), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_127.run(le_82, buf790, buf792, 1372, 128, grid=grid(1372), stream=stream0)
        buf793 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_128.run(buf792, buf793, 28, 49, grid=grid(28), stream=stream0)
        buf794 = reinterpret_tensor(buf792, (28, 49), (1, 28), 0); del buf792  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_129.run(le_82, buf790, convolution_64, unsqueeze_1606, buf794, 1372, 128, grid=grid(1372), stream=stream0)
        buf795 = empty((28, ), device='cuda', dtype=torch.float32)
        buf796 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_130.run(buf794, squeeze_193, buf795, buf796, 28, 49, grid=grid(28), stream=stream0)
        buf797 = reinterpret_tensor(buf197, (8, 28, 28, 28), (21952, 1, 784, 28), 0); del buf197  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_131.run(le_82, buf790, convolution_64, unsqueeze_1606, buf795, squeeze_193, buf793, primals_194, buf797, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_64
        del le_82
        del primals_194
        del squeeze_193
        del unsqueeze_1606
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf798 = aten.convolution_backward(buf797, add_355, primals_193, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_355
        del primals_193
        buf799 = buf798[0]
        buf800 = buf798[1]
        del buf798
        buf801 = buf795; del buf795  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_132.run(le_83, buf790, buf799, buf801, 28, 6272, grid=grid(28), stream=stream0)
        buf802 = reinterpret_tensor(buf794, (28, 49), (49, 1), 0); del buf794  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_133.run(le_83, buf790, buf799, convolution_63, unsqueeze_1618, buf802, 1372, 128, grid=grid(1372), stream=stream0)
        buf803 = empty((28, ), device='cuda', dtype=torch.float32)
        buf805 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf802, squeeze_190, buf803, buf805, 28, 49, grid=grid(28), stream=stream0)
        buf804 = buf797; del buf797  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_135.run(le_83, buf790, buf799, convolution_63, unsqueeze_1618, buf803, squeeze_190, buf801, primals_191, buf804, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_63
        del le_83
        del primals_191
        del squeeze_190
        del unsqueeze_1618
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf806 = aten.convolution_backward(buf804, add_349, primals_190, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_349
        del primals_190
        buf807 = buf806[0]
        buf808 = buf806[1]
        del buf806
        buf809 = buf803; del buf803  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_136.run(le_84, buf790, buf807, buf809, 28, 6272, grid=grid(28), stream=stream0)
        buf810 = buf802; del buf802  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_137.run(le_84, buf790, buf807, convolution_62, unsqueeze_1630, buf810, 1372, 128, grid=grid(1372), stream=stream0)
        buf811 = empty((28, ), device='cuda', dtype=torch.float32)
        buf813 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf810, squeeze_187, buf811, buf813, 28, 49, grid=grid(28), stream=stream0)
        buf812 = buf804; del buf804  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_138.run(le_84, buf790, buf807, convolution_62, unsqueeze_1630, buf811, squeeze_187, buf809, primals_188, buf812, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_62
        del le_84
        del primals_188
        del squeeze_187
        del unsqueeze_1630
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf814 = aten.convolution_backward(buf812, add_343, primals_187, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_343
        del primals_187
        buf815 = buf814[0]
        buf816 = buf814[1]
        del buf814
        buf817 = buf811; del buf811  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_139.run(le_85, buf790, buf815, buf817, 28, 6272, grid=grid(28), stream=stream0)
        buf818 = buf810; del buf810  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_140.run(le_85, buf790, buf815, convolution_61, unsqueeze_1642, buf818, 1372, 128, grid=grid(1372), stream=stream0)
        buf819 = empty((28, ), device='cuda', dtype=torch.float32)
        buf821 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf818, squeeze_184, buf819, buf821, 28, 49, grid=grid(28), stream=stream0)
        buf820 = buf812; del buf812  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_141.run(le_85, buf790, buf815, convolution_61, unsqueeze_1642, buf819, squeeze_184, buf817, primals_185, buf820, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_61
        del le_85
        del primals_185
        del squeeze_184
        del unsqueeze_1642
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf822 = aten.convolution_backward(buf820, add_337, primals_184, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_337
        del primals_184
        buf823 = buf822[0]
        buf824 = buf822[1]
        del buf822
        buf825 = buf819; del buf819  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_142.run(le_86, buf790, buf823, buf825, 28, 6272, grid=grid(28), stream=stream0)
        buf826 = buf818; del buf818  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_143.run(le_86, buf790, buf823, convolution_60, unsqueeze_1654, buf826, 1372, 128, grid=grid(1372), stream=stream0)
        buf827 = empty((28, ), device='cuda', dtype=torch.float32)
        buf829 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf826, squeeze_181, buf827, buf829, 28, 49, grid=grid(28), stream=stream0)
        buf828 = buf820; del buf820  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_144.run(le_86, buf790, buf823, convolution_60, unsqueeze_1654, buf827, squeeze_181, buf825, primals_182, buf828, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_60
        del le_86
        del primals_182
        del squeeze_181
        del unsqueeze_1654
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf830 = aten.convolution_backward(buf828, add_331, primals_181, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_331
        del primals_181
        buf831 = buf830[0]
        buf832 = buf830[1]
        del buf830
        buf833 = buf827; del buf827  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_145.run(le_87, buf790, buf831, buf833, 28, 6272, grid=grid(28), stream=stream0)
        buf834 = buf826; del buf826  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_146.run(le_87, buf790, buf831, convolution_59, unsqueeze_1666, buf834, 1372, 128, grid=grid(1372), stream=stream0)
        buf835 = empty((28, ), device='cuda', dtype=torch.float32)
        buf837 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf834, squeeze_178, buf835, buf837, 28, 49, grid=grid(28), stream=stream0)
        buf836 = buf828; del buf828  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_147.run(le_87, buf790, buf831, convolution_59, unsqueeze_1666, buf835, squeeze_178, buf833, primals_179, buf836, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_59
        del le_87
        del primals_179
        del squeeze_178
        del unsqueeze_1666
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf838 = aten.convolution_backward(buf836, add_325, primals_178, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_325
        del primals_178
        buf839 = buf838[0]
        buf840 = buf838[1]
        del buf838
        buf841 = buf835; del buf835  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_148.run(le_88, buf790, buf839, buf841, 28, 6272, grid=grid(28), stream=stream0)
        buf842 = buf834; del buf834  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_149.run(le_88, buf790, buf839, convolution_58, unsqueeze_1678, buf842, 1372, 128, grid=grid(1372), stream=stream0)
        buf843 = empty((28, ), device='cuda', dtype=torch.float32)
        buf845 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf842, squeeze_175, buf843, buf845, 28, 49, grid=grid(28), stream=stream0)
        buf844 = buf836; del buf836  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_150.run(le_88, buf790, buf839, convolution_58, unsqueeze_1678, buf843, squeeze_175, buf841, primals_176, buf844, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_58
        del le_88
        del primals_176
        del squeeze_175
        del unsqueeze_1678
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf846 = aten.convolution_backward(buf844, getitem_558, primals_175, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf844
        del getitem_558
        del primals_175
        buf847 = buf846[0]
        buf848 = buf846[1]
        del buf846
        buf857 = reinterpret_tensor(buf266, (8, 224, 28, 28), (175616, 784, 28, 1), 0); del buf266  # reuse
        buf849 = reinterpret_tensor(buf857, (8, 28, 28, 28), (175616, 784, 28, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf847, buf849, 175616, grid=grid(175616), stream=stream0)
        del buf847
        buf850 = reinterpret_tensor(buf857, (8, 28, 28, 28), (175616, 784, 28, 1), 21952)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf839, buf850, 175616, grid=grid(175616), stream=stream0)
        del buf839
        buf851 = reinterpret_tensor(buf857, (8, 28, 28, 28), (175616, 784, 28, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf831, buf851, 175616, grid=grid(175616), stream=stream0)
        del buf831
        buf852 = reinterpret_tensor(buf857, (8, 28, 28, 28), (175616, 784, 28, 1), 65856)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf823, buf852, 175616, grid=grid(175616), stream=stream0)
        del buf823
        buf853 = reinterpret_tensor(buf857, (8, 28, 28, 28), (175616, 784, 28, 1), 87808)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf815, buf853, 175616, grid=grid(175616), stream=stream0)
        del buf815
        buf854 = reinterpret_tensor(buf857, (8, 28, 28, 28), (175616, 784, 28, 1), 109760)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf807, buf854, 175616, grid=grid(175616), stream=stream0)
        del buf807
        buf855 = reinterpret_tensor(buf857, (8, 28, 28, 28), (175616, 784, 28, 1), 131712)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf799, buf855, 175616, grid=grid(175616), stream=stream0)
        buf856 = reinterpret_tensor(buf857, (8, 28, 28, 28), (175616, 784, 28, 1), 153664)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_151.run(buf790, buf856, 175616, grid=grid(175616), stream=stream0)
        buf858 = empty((224, 49), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_152.run(le_89, buf857, buf858, 10976, 128, grid=grid(10976), stream=stream0)
        del buf849
        del buf850
        del buf851
        del buf852
        del buf853
        del buf854
        del buf855
        del buf856
        buf859 = empty((224, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_153.run(buf858, buf859, 224, 49, grid=grid(224), stream=stream0)
        buf860 = reinterpret_tensor(buf858, (224, 49), (1, 224), 0); del buf858  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_154.run(le_89, buf857, convolution_57, unsqueeze_1690, buf860, 10976, 128, grid=grid(10976), stream=stream0)
        buf861 = empty((224, ), device='cuda', dtype=torch.float32)
        buf862 = empty((224, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_155.run(buf860, squeeze_172, buf861, buf862, 224, 49, grid=grid(224), stream=stream0)
        buf863 = reinterpret_tensor(buf790, (8, 224, 28, 28), (175616, 1, 6272, 224), 0); del buf790  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_156.run(le_89, buf857, convolution_57, unsqueeze_1690, buf861, squeeze_172, buf859, primals_173, buf863, 6272, 224, grid=grid(6272, 224), stream=stream0)
        del buf857
        del convolution_57
        del le_89
        del primals_173
        del squeeze_172
        del unsqueeze_1690
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf864 = aten.convolution_backward(buf863, relu_54, primals_172, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_172
        buf865 = buf864[0]
        buf866 = buf864[1]
        del buf864
        buf867 = buf694; del buf694  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_157.run(buf867, relu_54, relu_63, buf782, buf865, 4096, 784, grid=grid(4096, 784), stream=stream0)
        del buf782
        del relu_54
        del relu_63
        buf868 = buf786; del buf786  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_158.run(buf867, buf868, 512, 6272, grid=grid(512), stream=stream0)
        buf869 = buf785; del buf785  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_159.run(buf867, convolution_56, unsqueeze_1702, buf869, 25088, 128, grid=grid(25088), stream=stream0)
        buf870 = empty((512, ), device='cuda', dtype=torch.float32)
        buf871 = empty((512, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_125.run(buf869, squeeze_169, buf870, buf871, 512, 49, grid=grid(512), stream=stream0)
        buf872 = reinterpret_tensor(buf865, (8, 512, 28, 28), (401408, 1, 14336, 512), 0); del buf865  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_160.run(buf867, convolution_56, unsqueeze_1702, buf870, squeeze_169, buf868, primals_170, buf872, 6272, 512, grid=grid(6272, 512), stream=stream0)
        del convolution_56
        del primals_170
        del squeeze_169
        del unsqueeze_1702
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf873 = aten.convolution_backward(buf872, cat_5, primals_169, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del cat_5
        del primals_169
        buf874 = buf873[0]
        buf875 = buf873[1]
        del buf873
        buf876 = buf842; del buf842  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_127.run(le_91, buf874, buf876, 1372, 128, grid=grid(1372), stream=stream0)
        buf877 = buf843; del buf843  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_128.run(buf876, buf877, 28, 49, grid=grid(28), stream=stream0)
        buf878 = reinterpret_tensor(buf876, (28, 49), (1, 28), 0); del buf876  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_129.run(le_91, buf874, convolution_55, unsqueeze_1714, buf878, 1372, 128, grid=grid(1372), stream=stream0)
        buf879 = empty((28, ), device='cuda', dtype=torch.float32)
        buf880 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_130.run(buf878, squeeze_166, buf879, buf880, 28, 49, grid=grid(28), stream=stream0)
        buf881 = reinterpret_tensor(buf799, (8, 28, 28, 28), (21952, 1, 784, 28), 0); del buf799  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_131.run(le_91, buf874, convolution_55, unsqueeze_1714, buf879, squeeze_166, buf877, primals_167, buf881, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_55
        del le_91
        del primals_167
        del squeeze_166
        del unsqueeze_1714
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf882 = aten.convolution_backward(buf881, add_303, primals_166, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_303
        del primals_166
        buf883 = buf882[0]
        buf884 = buf882[1]
        del buf882
        buf885 = buf879; del buf879  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_132.run(le_92, buf874, buf883, buf885, 28, 6272, grid=grid(28), stream=stream0)
        buf886 = reinterpret_tensor(buf878, (28, 49), (49, 1), 0); del buf878  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_133.run(le_92, buf874, buf883, convolution_54, unsqueeze_1726, buf886, 1372, 128, grid=grid(1372), stream=stream0)
        buf887 = empty((28, ), device='cuda', dtype=torch.float32)
        buf889 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf886, squeeze_163, buf887, buf889, 28, 49, grid=grid(28), stream=stream0)
        buf888 = buf881; del buf881  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_135.run(le_92, buf874, buf883, convolution_54, unsqueeze_1726, buf887, squeeze_163, buf885, primals_164, buf888, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_54
        del le_92
        del primals_164
        del squeeze_163
        del unsqueeze_1726
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf890 = aten.convolution_backward(buf888, add_297, primals_163, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_297
        del primals_163
        buf891 = buf890[0]
        buf892 = buf890[1]
        del buf890
        buf893 = buf887; del buf887  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_136.run(le_93, buf874, buf891, buf893, 28, 6272, grid=grid(28), stream=stream0)
        buf894 = buf886; del buf886  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_137.run(le_93, buf874, buf891, convolution_53, unsqueeze_1738, buf894, 1372, 128, grid=grid(1372), stream=stream0)
        buf895 = empty((28, ), device='cuda', dtype=torch.float32)
        buf897 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf894, squeeze_160, buf895, buf897, 28, 49, grid=grid(28), stream=stream0)
        buf896 = buf888; del buf888  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_138.run(le_93, buf874, buf891, convolution_53, unsqueeze_1738, buf895, squeeze_160, buf893, primals_161, buf896, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_53
        del le_93
        del primals_161
        del squeeze_160
        del unsqueeze_1738
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf898 = aten.convolution_backward(buf896, add_291, primals_160, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_291
        del primals_160
        buf899 = buf898[0]
        buf900 = buf898[1]
        del buf898
        buf901 = buf895; del buf895  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_139.run(le_94, buf874, buf899, buf901, 28, 6272, grid=grid(28), stream=stream0)
        buf902 = buf894; del buf894  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_140.run(le_94, buf874, buf899, convolution_52, unsqueeze_1750, buf902, 1372, 128, grid=grid(1372), stream=stream0)
        buf903 = empty((28, ), device='cuda', dtype=torch.float32)
        buf905 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf902, squeeze_157, buf903, buf905, 28, 49, grid=grid(28), stream=stream0)
        buf904 = buf896; del buf896  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_141.run(le_94, buf874, buf899, convolution_52, unsqueeze_1750, buf903, squeeze_157, buf901, primals_158, buf904, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_52
        del le_94
        del primals_158
        del squeeze_157
        del unsqueeze_1750
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf906 = aten.convolution_backward(buf904, add_285, primals_157, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_285
        del primals_157
        buf907 = buf906[0]
        buf908 = buf906[1]
        del buf906
        buf909 = buf903; del buf903  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_142.run(le_95, buf874, buf907, buf909, 28, 6272, grid=grid(28), stream=stream0)
        buf910 = buf902; del buf902  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_143.run(le_95, buf874, buf907, convolution_51, unsqueeze_1762, buf910, 1372, 128, grid=grid(1372), stream=stream0)
        buf911 = empty((28, ), device='cuda', dtype=torch.float32)
        buf913 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf910, squeeze_154, buf911, buf913, 28, 49, grid=grid(28), stream=stream0)
        buf912 = buf904; del buf904  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_144.run(le_95, buf874, buf907, convolution_51, unsqueeze_1762, buf911, squeeze_154, buf909, primals_155, buf912, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_51
        del le_95
        del primals_155
        del squeeze_154
        del unsqueeze_1762
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf914 = aten.convolution_backward(buf912, add_279, primals_154, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_279
        del primals_154
        buf915 = buf914[0]
        buf916 = buf914[1]
        del buf914
        buf917 = buf911; del buf911  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_145.run(le_96, buf874, buf915, buf917, 28, 6272, grid=grid(28), stream=stream0)
        buf918 = buf910; del buf910  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_146.run(le_96, buf874, buf915, convolution_50, unsqueeze_1774, buf918, 1372, 128, grid=grid(1372), stream=stream0)
        buf919 = empty((28, ), device='cuda', dtype=torch.float32)
        buf921 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf918, squeeze_151, buf919, buf921, 28, 49, grid=grid(28), stream=stream0)
        buf920 = buf912; del buf912  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_147.run(le_96, buf874, buf915, convolution_50, unsqueeze_1774, buf919, squeeze_151, buf917, primals_152, buf920, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_50
        del le_96
        del primals_152
        del squeeze_151
        del unsqueeze_1774
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf922 = aten.convolution_backward(buf920, add_273, primals_151, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_273
        del primals_151
        buf923 = buf922[0]
        buf924 = buf922[1]
        del buf922
        buf925 = buf919; del buf919  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_148.run(le_97, buf874, buf923, buf925, 28, 6272, grid=grid(28), stream=stream0)
        buf926 = buf918; del buf918  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_149.run(le_97, buf874, buf923, convolution_49, unsqueeze_1786, buf926, 1372, 128, grid=grid(1372), stream=stream0)
        buf927 = empty((28, ), device='cuda', dtype=torch.float32)
        buf929 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf926, squeeze_148, buf927, buf929, 28, 49, grid=grid(28), stream=stream0)
        buf928 = buf920; del buf920  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_150.run(le_97, buf874, buf923, convolution_49, unsqueeze_1786, buf927, squeeze_148, buf925, primals_149, buf928, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_49
        del le_97
        del primals_149
        del squeeze_148
        del unsqueeze_1786
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf930 = aten.convolution_backward(buf928, getitem_468, primals_148, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf928
        del getitem_468
        del primals_148
        buf931 = buf930[0]
        buf932 = buf930[1]
        del buf930
        buf941 = reinterpret_tensor(buf863, (8, 224, 28, 28), (175616, 784, 28, 1), 0); del buf863  # reuse
        buf933 = reinterpret_tensor(buf941, (8, 28, 28, 28), (175616, 784, 28, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf931, buf933, 175616, grid=grid(175616), stream=stream0)
        del buf931
        buf934 = reinterpret_tensor(buf941, (8, 28, 28, 28), (175616, 784, 28, 1), 21952)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf923, buf934, 175616, grid=grid(175616), stream=stream0)
        del buf923
        buf935 = reinterpret_tensor(buf941, (8, 28, 28, 28), (175616, 784, 28, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf915, buf935, 175616, grid=grid(175616), stream=stream0)
        del buf915
        buf936 = reinterpret_tensor(buf941, (8, 28, 28, 28), (175616, 784, 28, 1), 65856)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf907, buf936, 175616, grid=grid(175616), stream=stream0)
        del buf907
        buf937 = reinterpret_tensor(buf941, (8, 28, 28, 28), (175616, 784, 28, 1), 87808)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf899, buf937, 175616, grid=grid(175616), stream=stream0)
        del buf899
        buf938 = reinterpret_tensor(buf941, (8, 28, 28, 28), (175616, 784, 28, 1), 109760)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf891, buf938, 175616, grid=grid(175616), stream=stream0)
        del buf891
        buf939 = reinterpret_tensor(buf941, (8, 28, 28, 28), (175616, 784, 28, 1), 131712)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf883, buf939, 175616, grid=grid(175616), stream=stream0)
        buf940 = reinterpret_tensor(buf941, (8, 28, 28, 28), (175616, 784, 28, 1), 153664)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_151.run(buf874, buf940, 175616, grid=grid(175616), stream=stream0)
        buf942 = reinterpret_tensor(buf860, (224, 49), (49, 1), 0); del buf860  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_152.run(le_98, buf941, buf942, 10976, 128, grid=grid(10976), stream=stream0)
        del buf933
        del buf934
        del buf935
        del buf936
        del buf937
        del buf938
        del buf939
        del buf940
        buf943 = buf861; del buf861  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_153.run(buf942, buf943, 224, 49, grid=grid(224), stream=stream0)
        buf944 = reinterpret_tensor(buf942, (224, 49), (1, 224), 0); del buf942  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_154.run(le_98, buf941, convolution_48, unsqueeze_1798, buf944, 10976, 128, grid=grid(10976), stream=stream0)
        buf945 = empty((224, ), device='cuda', dtype=torch.float32)
        buf946 = empty((224, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_155.run(buf944, squeeze_145, buf945, buf946, 224, 49, grid=grid(224), stream=stream0)
        buf947 = reinterpret_tensor(buf874, (8, 224, 28, 28), (175616, 1, 6272, 224), 0); del buf874  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_156.run(le_98, buf941, convolution_48, unsqueeze_1798, buf945, squeeze_145, buf943, primals_146, buf947, 6272, 224, grid=grid(6272, 224), stream=stream0)
        del buf941
        del convolution_48
        del le_98
        del primals_146
        del squeeze_145
        del unsqueeze_1798
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf948 = aten.convolution_backward(buf947, relu_45, primals_145, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_145
        buf949 = buf948[0]
        buf950 = buf948[1]
        del buf948
        buf951 = buf870; del buf870  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_123.run(relu_45, buf867, buf949, buf951, 512, 6272, grid=grid(512), stream=stream0)
        buf952 = buf869; del buf869  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_124.run(relu_45, buf867, buf949, convolution_47, unsqueeze_1810, buf952, 25088, 128, grid=grid(25088), stream=stream0)
        buf953 = empty((512, ), device='cuda', dtype=torch.float32)
        buf955 = empty((512, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_125.run(buf952, squeeze_142, buf953, buf955, 512, 49, grid=grid(512), stream=stream0)
        buf954 = buf872; del buf872  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_126.run(relu_45, buf867, buf949, convolution_47, unsqueeze_1810, buf953, squeeze_142, buf951, primals_143, buf954, 6272, 512, grid=grid(6272, 512), stream=stream0)
        del convolution_47
        del primals_143
        del squeeze_142
        del unsqueeze_1810
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf956 = aten.convolution_backward(buf954, cat_4, primals_142, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf954
        del cat_4
        del primals_142
        buf957 = buf956[0]
        buf958 = buf956[1]
        del buf956
        buf959 = buf926; del buf926  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_127.run(le_100, buf957, buf959, 1372, 128, grid=grid(1372), stream=stream0)
        buf960 = buf927; del buf927  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_128.run(buf959, buf960, 28, 49, grid=grid(28), stream=stream0)
        buf961 = reinterpret_tensor(buf959, (28, 49), (1, 28), 0); del buf959  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_129.run(le_100, buf957, convolution_46, unsqueeze_1822, buf961, 1372, 128, grid=grid(1372), stream=stream0)
        buf962 = empty((28, ), device='cuda', dtype=torch.float32)
        buf963 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_130.run(buf961, squeeze_139, buf962, buf963, 28, 49, grid=grid(28), stream=stream0)
        buf964 = reinterpret_tensor(buf883, (8, 28, 28, 28), (21952, 1, 784, 28), 0); del buf883  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_131.run(le_100, buf957, convolution_46, unsqueeze_1822, buf962, squeeze_139, buf960, primals_140, buf964, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_46
        del le_100
        del primals_140
        del squeeze_139
        del unsqueeze_1822
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf965 = aten.convolution_backward(buf964, add_251, primals_139, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_251
        del primals_139
        buf966 = buf965[0]
        buf967 = buf965[1]
        del buf965
        buf968 = buf962; del buf962  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_132.run(le_101, buf957, buf966, buf968, 28, 6272, grid=grid(28), stream=stream0)
        buf969 = reinterpret_tensor(buf961, (28, 49), (49, 1), 0); del buf961  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_133.run(le_101, buf957, buf966, convolution_45, unsqueeze_1834, buf969, 1372, 128, grid=grid(1372), stream=stream0)
        buf970 = empty((28, ), device='cuda', dtype=torch.float32)
        buf972 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf969, squeeze_136, buf970, buf972, 28, 49, grid=grid(28), stream=stream0)
        buf971 = buf964; del buf964  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_135.run(le_101, buf957, buf966, convolution_45, unsqueeze_1834, buf970, squeeze_136, buf968, primals_137, buf971, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_45
        del le_101
        del primals_137
        del squeeze_136
        del unsqueeze_1834
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf973 = aten.convolution_backward(buf971, add_245, primals_136, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_245
        del primals_136
        buf974 = buf973[0]
        buf975 = buf973[1]
        del buf973
        buf976 = buf970; del buf970  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_136.run(le_102, buf957, buf974, buf976, 28, 6272, grid=grid(28), stream=stream0)
        buf977 = buf969; del buf969  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_137.run(le_102, buf957, buf974, convolution_44, unsqueeze_1846, buf977, 1372, 128, grid=grid(1372), stream=stream0)
        buf978 = empty((28, ), device='cuda', dtype=torch.float32)
        buf980 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf977, squeeze_133, buf978, buf980, 28, 49, grid=grid(28), stream=stream0)
        buf979 = buf971; del buf971  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_138.run(le_102, buf957, buf974, convolution_44, unsqueeze_1846, buf978, squeeze_133, buf976, primals_134, buf979, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_44
        del le_102
        del primals_134
        del squeeze_133
        del unsqueeze_1846
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf981 = aten.convolution_backward(buf979, add_239, primals_133, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_239
        del primals_133
        buf982 = buf981[0]
        buf983 = buf981[1]
        del buf981
        buf984 = buf978; del buf978  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_139.run(le_103, buf957, buf982, buf984, 28, 6272, grid=grid(28), stream=stream0)
        buf985 = buf977; del buf977  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_140.run(le_103, buf957, buf982, convolution_43, unsqueeze_1858, buf985, 1372, 128, grid=grid(1372), stream=stream0)
        buf986 = empty((28, ), device='cuda', dtype=torch.float32)
        buf988 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf985, squeeze_130, buf986, buf988, 28, 49, grid=grid(28), stream=stream0)
        buf987 = buf979; del buf979  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_141.run(le_103, buf957, buf982, convolution_43, unsqueeze_1858, buf986, squeeze_130, buf984, primals_131, buf987, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_43
        del le_103
        del primals_131
        del squeeze_130
        del unsqueeze_1858
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf989 = aten.convolution_backward(buf987, add_233, primals_130, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_233
        del primals_130
        buf990 = buf989[0]
        buf991 = buf989[1]
        del buf989
        buf992 = buf986; del buf986  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_142.run(le_104, buf957, buf990, buf992, 28, 6272, grid=grid(28), stream=stream0)
        buf993 = buf985; del buf985  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_143.run(le_104, buf957, buf990, convolution_42, unsqueeze_1870, buf993, 1372, 128, grid=grid(1372), stream=stream0)
        buf994 = empty((28, ), device='cuda', dtype=torch.float32)
        buf996 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf993, squeeze_127, buf994, buf996, 28, 49, grid=grid(28), stream=stream0)
        buf995 = buf987; del buf987  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_144.run(le_104, buf957, buf990, convolution_42, unsqueeze_1870, buf994, squeeze_127, buf992, primals_128, buf995, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_42
        del le_104
        del primals_128
        del squeeze_127
        del unsqueeze_1870
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf997 = aten.convolution_backward(buf995, add_227, primals_127, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_227
        del primals_127
        buf998 = buf997[0]
        buf999 = buf997[1]
        del buf997
        buf1000 = buf994; del buf994  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_145.run(le_105, buf957, buf998, buf1000, 28, 6272, grid=grid(28), stream=stream0)
        buf1001 = buf993; del buf993  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_146.run(le_105, buf957, buf998, convolution_41, unsqueeze_1882, buf1001, 1372, 128, grid=grid(1372), stream=stream0)
        buf1002 = empty((28, ), device='cuda', dtype=torch.float32)
        buf1004 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf1001, squeeze_124, buf1002, buf1004, 28, 49, grid=grid(28), stream=stream0)
        buf1003 = buf995; del buf995  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_147.run(le_105, buf957, buf998, convolution_41, unsqueeze_1882, buf1002, squeeze_124, buf1000, primals_125, buf1003, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_41
        del le_105
        del primals_125
        del squeeze_124
        del unsqueeze_1882
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1005 = aten.convolution_backward(buf1003, add_221, primals_124, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_221
        del primals_124
        buf1006 = buf1005[0]
        buf1007 = buf1005[1]
        del buf1005
        buf1008 = buf1002; del buf1002  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_148.run(le_106, buf957, buf1006, buf1008, 28, 6272, grid=grid(28), stream=stream0)
        buf1009 = buf1001; del buf1001  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_149.run(le_106, buf957, buf1006, convolution_40, unsqueeze_1894, buf1009, 1372, 128, grid=grid(1372), stream=stream0)
        buf1010 = empty((28, ), device='cuda', dtype=torch.float32)
        buf1012 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_134.run(buf1009, squeeze_121, buf1010, buf1012, 28, 49, grid=grid(28), stream=stream0)
        buf1011 = buf1003; del buf1003  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_150.run(le_106, buf957, buf1006, convolution_40, unsqueeze_1894, buf1010, squeeze_121, buf1008, primals_122, buf1011, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_40
        del le_106
        del primals_122
        del squeeze_121
        del unsqueeze_1894
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1013 = aten.convolution_backward(buf1011, getitem_378, primals_121, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1011
        del getitem_378
        del primals_121
        buf1014 = buf1013[0]
        buf1015 = buf1013[1]
        del buf1013
        buf1024 = reinterpret_tensor(buf947, (8, 224, 28, 28), (175616, 784, 28, 1), 0); del buf947  # reuse
        buf1016 = reinterpret_tensor(buf1024, (8, 28, 28, 28), (175616, 784, 28, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf1014, buf1016, 175616, grid=grid(175616), stream=stream0)
        del buf1014
        buf1017 = reinterpret_tensor(buf1024, (8, 28, 28, 28), (175616, 784, 28, 1), 21952)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf1006, buf1017, 175616, grid=grid(175616), stream=stream0)
        del buf1006
        buf1018 = reinterpret_tensor(buf1024, (8, 28, 28, 28), (175616, 784, 28, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf998, buf1018, 175616, grid=grid(175616), stream=stream0)
        del buf998
        buf1019 = reinterpret_tensor(buf1024, (8, 28, 28, 28), (175616, 784, 28, 1), 65856)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf990, buf1019, 175616, grid=grid(175616), stream=stream0)
        del buf990
        buf1020 = reinterpret_tensor(buf1024, (8, 28, 28, 28), (175616, 784, 28, 1), 87808)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf982, buf1020, 175616, grid=grid(175616), stream=stream0)
        del buf982
        buf1021 = reinterpret_tensor(buf1024, (8, 28, 28, 28), (175616, 784, 28, 1), 109760)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf974, buf1021, 175616, grid=grid(175616), stream=stream0)
        del buf974
        buf1022 = reinterpret_tensor(buf1024, (8, 28, 28, 28), (175616, 784, 28, 1), 131712)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_51.run(buf966, buf1022, 175616, grid=grid(175616), stream=stream0)
        buf1023 = reinterpret_tensor(buf1024, (8, 28, 28, 28), (175616, 784, 28, 1), 153664)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_151.run(buf957, buf1023, 175616, grid=grid(175616), stream=stream0)
        buf1025 = reinterpret_tensor(buf944, (224, 49), (49, 1), 0); del buf944  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_152.run(le_107, buf1024, buf1025, 10976, 128, grid=grid(10976), stream=stream0)
        del buf1016
        del buf1017
        del buf1018
        del buf1019
        del buf1020
        del buf1021
        del buf1022
        del buf1023
        buf1026 = buf945; del buf945  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_153.run(buf1025, buf1026, 224, 49, grid=grid(224), stream=stream0)
        buf1027 = reinterpret_tensor(buf1025, (224, 49), (1, 224), 0); del buf1025  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_154.run(le_107, buf1024, convolution_39, unsqueeze_1906, buf1027, 10976, 128, grid=grid(10976), stream=stream0)
        buf1028 = empty((224, ), device='cuda', dtype=torch.float32)
        buf1029 = empty((224, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_155.run(buf1027, squeeze_118, buf1028, buf1029, 224, 49, grid=grid(224), stream=stream0)
        del buf1027
        buf1030 = reinterpret_tensor(buf957, (8, 224, 28, 28), (175616, 1, 6272, 224), 0); del buf957  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_156.run(le_107, buf1024, convolution_39, unsqueeze_1906, buf1028, squeeze_118, buf1026, primals_119, buf1030, 6272, 224, grid=grid(6272, 224), stream=stream0)
        del buf1024
        del convolution_39
        del le_107
        del primals_119
        del squeeze_118
        del unsqueeze_1906
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1031 = aten.convolution_backward(buf1030, relu_36, primals_118, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1030
        del primals_118
        buf1032 = buf1031[0]
        buf1033 = buf1031[1]
        del buf1031
        buf1034 = buf1032; del buf1032  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_161.run(buf1034, relu_36, relu_45, buf867, buf949, 4096, 784, grid=grid(4096, 784), stream=stream0)
        del relu_36
        del relu_45
        buf1035 = buf953; del buf953  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_158.run(buf1034, buf1035, 512, 6272, grid=grid(512), stream=stream0)
        buf1036 = buf952; del buf952  # reuse
        buf1043 = empty((512, 49), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_162.run(buf1034, convolution_38, unsqueeze_1918, convolution_37, unsqueeze_1930, buf1036, buf1043, 25088, 128, grid=grid(25088), stream=stream0)
        buf1037 = empty((512, ), device='cuda', dtype=torch.float32)
        buf1038 = empty((512, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_125.run(buf1036, squeeze_115, buf1037, buf1038, 512, 49, grid=grid(512), stream=stream0)
        del buf1036
        buf1044 = empty((512, ), device='cuda', dtype=torch.float32)
        buf1045 = empty((512, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_125.run(buf1043, squeeze_112, buf1044, buf1045, 512, 49, grid=grid(512), stream=stream0)
        del buf1043
        buf1039 = reinterpret_tensor(buf949, (8, 512, 28, 28), (401408, 1, 14336, 512), 0); del buf949  # reuse
        buf1046 = reinterpret_tensor(buf867, (8, 512, 28, 28), (401408, 1, 14336, 512), 0); del buf867  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_163.run(buf1034, convolution_38, unsqueeze_1918, buf1037, squeeze_115, buf1035, primals_116, convolution_37, unsqueeze_1930, buf1044, squeeze_112, primals_113, buf1039, buf1046, 6272, 512, grid=grid(6272, 512), stream=stream0)
        del buf1034
        del buf1037
        del buf1044
        del convolution_37
        del convolution_38
        del primals_113
        del primals_116
        del squeeze_112
        del squeeze_115
        del unsqueeze_1918
        del unsqueeze_1930
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1040 = aten.convolution_backward(buf1039, relu_27, primals_115, [0], [2, 2], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1039
        del primals_115
        buf1041 = buf1040[0]
        buf1042 = buf1040[1]
        del buf1040
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1047 = aten.convolution_backward(buf1046, cat_3, primals_112, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1046
        del cat_3
        del primals_112
        buf1048 = buf1047[0]
        buf1049 = buf1047[1]
        del buf1047
        buf1121 = empty((8, 224, 56, 56), device='cuda', dtype=torch.float32)
        buf1050 = reinterpret_tensor(buf1121, (8, 28, 56, 56), (702464, 3136, 56, 1), 614656)  # alias
        # Source Nodes: [], Original ATen: [aten.avg_pool2d_backward]
        triton_poi_fused_avg_pool2d_backward_164.run(buf1048, buf1050, 702464, grid=grid(702464), stream=stream0)
        buf1051 = buf1009; del buf1009  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_127.run(le_109, buf1048, buf1051, 1372, 128, grid=grid(1372), stream=stream0)
        buf1052 = buf1010; del buf1010  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_128.run(buf1051, buf1052, 28, 49, grid=grid(28), stream=stream0)
        buf1053 = reinterpret_tensor(buf1051, (28, 49), (1, 28), 0); del buf1051  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_129.run(le_109, buf1048, convolution_36, unsqueeze_1942, buf1053, 1372, 128, grid=grid(1372), stream=stream0)
        buf1054 = empty((28, ), device='cuda', dtype=torch.float32)
        buf1055 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_130.run(buf1053, squeeze_109, buf1054, buf1055, 28, 49, grid=grid(28), stream=stream0)
        buf1056 = reinterpret_tensor(buf966, (8, 28, 28, 28), (21952, 1, 784, 28), 0); del buf966  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_131.run(le_109, buf1048, convolution_36, unsqueeze_1942, buf1054, squeeze_109, buf1052, primals_110, buf1056, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_36
        del le_109
        del primals_110
        del squeeze_109
        del unsqueeze_1942
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1057 = aten.convolution_backward(buf1056, getitem_352, primals_109, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_352
        del primals_109
        buf1058 = buf1057[0]
        buf1059 = buf1057[1]
        del buf1057
        buf1060 = reinterpret_tensor(buf1053, (28, 49), (49, 1), 0); del buf1053  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_165.run(le_110, buf1048, buf1060, 1372, 128, grid=grid(1372), stream=stream0)
        buf1061 = buf1054; del buf1054  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_128.run(buf1060, buf1061, 28, 49, grid=grid(28), stream=stream0)
        buf1062 = reinterpret_tensor(buf1060, (28, 49), (1, 28), 0); del buf1060  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_166.run(le_110, buf1048, convolution_35, unsqueeze_1954, buf1062, 1372, 128, grid=grid(1372), stream=stream0)
        buf1063 = empty((28, ), device='cuda', dtype=torch.float32)
        buf1064 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_130.run(buf1062, squeeze_106, buf1063, buf1064, 28, 49, grid=grid(28), stream=stream0)
        buf1065 = buf1056; del buf1056  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_167.run(le_110, buf1048, convolution_35, unsqueeze_1954, buf1063, squeeze_106, buf1061, primals_107, buf1065, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_35
        del le_110
        del primals_107
        del squeeze_106
        del unsqueeze_1954
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1066 = aten.convolution_backward(buf1065, getitem_341, primals_106, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_341
        del primals_106
        buf1067 = buf1066[0]
        buf1068 = buf1066[1]
        del buf1066
        buf1069 = reinterpret_tensor(buf1062, (28, 49), (49, 1), 0); del buf1062  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_168.run(le_111, buf1048, buf1069, 1372, 128, grid=grid(1372), stream=stream0)
        buf1070 = buf1063; del buf1063  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_128.run(buf1069, buf1070, 28, 49, grid=grid(28), stream=stream0)
        buf1071 = reinterpret_tensor(buf1069, (28, 49), (1, 28), 0); del buf1069  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_169.run(le_111, buf1048, convolution_34, unsqueeze_1966, buf1071, 1372, 128, grid=grid(1372), stream=stream0)
        buf1072 = empty((28, ), device='cuda', dtype=torch.float32)
        buf1073 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_130.run(buf1071, squeeze_103, buf1072, buf1073, 28, 49, grid=grid(28), stream=stream0)
        buf1074 = buf1065; del buf1065  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_170.run(le_111, buf1048, convolution_34, unsqueeze_1966, buf1072, squeeze_103, buf1070, primals_104, buf1074, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_34
        del le_111
        del primals_104
        del squeeze_103
        del unsqueeze_1966
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1075 = aten.convolution_backward(buf1074, getitem_330, primals_103, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_330
        del primals_103
        buf1076 = buf1075[0]
        buf1077 = buf1075[1]
        del buf1075
        buf1078 = reinterpret_tensor(buf1071, (28, 49), (49, 1), 0); del buf1071  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_171.run(le_112, buf1048, buf1078, 1372, 128, grid=grid(1372), stream=stream0)
        buf1079 = buf1072; del buf1072  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_128.run(buf1078, buf1079, 28, 49, grid=grid(28), stream=stream0)
        buf1080 = reinterpret_tensor(buf1078, (28, 49), (1, 28), 0); del buf1078  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_172.run(le_112, buf1048, convolution_33, unsqueeze_1978, buf1080, 1372, 128, grid=grid(1372), stream=stream0)
        buf1081 = empty((28, ), device='cuda', dtype=torch.float32)
        buf1082 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_130.run(buf1080, squeeze_100, buf1081, buf1082, 28, 49, grid=grid(28), stream=stream0)
        buf1083 = buf1074; del buf1074  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_173.run(le_112, buf1048, convolution_33, unsqueeze_1978, buf1081, squeeze_100, buf1079, primals_101, buf1083, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_33
        del le_112
        del primals_101
        del squeeze_100
        del unsqueeze_1978
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1084 = aten.convolution_backward(buf1083, getitem_319, primals_100, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_319
        del primals_100
        buf1085 = buf1084[0]
        buf1086 = buf1084[1]
        del buf1084
        buf1087 = reinterpret_tensor(buf1080, (28, 49), (49, 1), 0); del buf1080  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_174.run(le_113, buf1048, buf1087, 1372, 128, grid=grid(1372), stream=stream0)
        buf1088 = buf1081; del buf1081  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_128.run(buf1087, buf1088, 28, 49, grid=grid(28), stream=stream0)
        buf1089 = reinterpret_tensor(buf1087, (28, 49), (1, 28), 0); del buf1087  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_175.run(le_113, buf1048, convolution_32, unsqueeze_1990, buf1089, 1372, 128, grid=grid(1372), stream=stream0)
        buf1090 = empty((28, ), device='cuda', dtype=torch.float32)
        buf1091 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_130.run(buf1089, squeeze_97, buf1090, buf1091, 28, 49, grid=grid(28), stream=stream0)
        buf1092 = buf1083; del buf1083  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_176.run(le_113, buf1048, convolution_32, unsqueeze_1990, buf1090, squeeze_97, buf1088, primals_98, buf1092, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_32
        del le_113
        del primals_98
        del squeeze_97
        del unsqueeze_1990
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1093 = aten.convolution_backward(buf1092, getitem_308, primals_97, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_308
        del primals_97
        buf1094 = buf1093[0]
        buf1095 = buf1093[1]
        del buf1093
        buf1096 = reinterpret_tensor(buf1089, (28, 49), (49, 1), 0); del buf1089  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_177.run(le_114, buf1048, buf1096, 1372, 128, grid=grid(1372), stream=stream0)
        buf1097 = buf1090; del buf1090  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_128.run(buf1096, buf1097, 28, 49, grid=grid(28), stream=stream0)
        buf1098 = reinterpret_tensor(buf1096, (28, 49), (1, 28), 0); del buf1096  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_178.run(le_114, buf1048, convolution_31, unsqueeze_2002, buf1098, 1372, 128, grid=grid(1372), stream=stream0)
        buf1099 = empty((28, ), device='cuda', dtype=torch.float32)
        buf1100 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_130.run(buf1098, squeeze_94, buf1099, buf1100, 28, 49, grid=grid(28), stream=stream0)
        buf1101 = buf1092; del buf1092  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_179.run(le_114, buf1048, convolution_31, unsqueeze_2002, buf1099, squeeze_94, buf1097, primals_95, buf1101, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del convolution_31
        del le_114
        del primals_95
        del squeeze_94
        del unsqueeze_2002
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1102 = aten.convolution_backward(buf1101, getitem_297, primals_94, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_297
        del primals_94
        buf1103 = buf1102[0]
        buf1104 = buf1102[1]
        del buf1102
        buf1105 = reinterpret_tensor(buf1098, (28, 49), (49, 1), 0); del buf1098  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_180.run(le_115, buf1048, buf1105, 1372, 128, grid=grid(1372), stream=stream0)
        buf1106 = buf1099; del buf1099  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_128.run(buf1105, buf1106, 28, 49, grid=grid(28), stream=stream0)
        buf1107 = reinterpret_tensor(buf1105, (28, 49), (1, 28), 0); del buf1105  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_181.run(le_115, buf1048, convolution_30, unsqueeze_2014, buf1107, 1372, 128, grid=grid(1372), stream=stream0)
        buf1108 = empty((28, ), device='cuda', dtype=torch.float32)
        buf1109 = empty((28, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_130.run(buf1107, squeeze_91, buf1108, buf1109, 28, 49, grid=grid(28), stream=stream0)
        del buf1107
        buf1110 = buf1101; del buf1101  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_182.run(le_115, buf1048, convolution_30, unsqueeze_2014, buf1108, squeeze_91, buf1106, primals_92, buf1110, 6272, 28, grid=grid(6272, 28), stream=stream0)
        del buf1048
        del buf1108
        del convolution_30
        del le_115
        del primals_92
        del squeeze_91
        del unsqueeze_2014
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1111 = aten.convolution_backward(buf1110, getitem_286, primals_91, [0], [2, 2], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1110
        del getitem_286
        del primals_91
        buf1112 = buf1111[0]
        buf1113 = buf1111[1]
        del buf1111
        buf1114 = reinterpret_tensor(buf1121, (8, 28, 56, 56), (702464, 3136, 56, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_183.run(buf1112, buf1114, 702464, grid=grid(702464), stream=stream0)
        del buf1112
        buf1115 = reinterpret_tensor(buf1121, (8, 28, 56, 56), (702464, 3136, 56, 1), 87808)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_183.run(buf1103, buf1115, 702464, grid=grid(702464), stream=stream0)
        del buf1103
        buf1116 = reinterpret_tensor(buf1121, (8, 28, 56, 56), (702464, 3136, 56, 1), 175616)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_183.run(buf1094, buf1116, 702464, grid=grid(702464), stream=stream0)
        del buf1094
        buf1117 = reinterpret_tensor(buf1121, (8, 28, 56, 56), (702464, 3136, 56, 1), 263424)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_183.run(buf1085, buf1117, 702464, grid=grid(702464), stream=stream0)
        del buf1085
        buf1118 = reinterpret_tensor(buf1121, (8, 28, 56, 56), (702464, 3136, 56, 1), 351232)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_183.run(buf1076, buf1118, 702464, grid=grid(702464), stream=stream0)
        del buf1076
        buf1119 = reinterpret_tensor(buf1121, (8, 28, 56, 56), (702464, 3136, 56, 1), 439040)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_183.run(buf1067, buf1119, 702464, grid=grid(702464), stream=stream0)
        del buf1067
        buf1120 = reinterpret_tensor(buf1121, (8, 28, 56, 56), (702464, 3136, 56, 1), 526848)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_183.run(buf1058, buf1120, 702464, grid=grid(702464), stream=stream0)
        del buf1058
        buf1122 = reinterpret_tensor(buf249, (224, 196), (196, 1), 0); del buf249  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_184.run(le_116, buf1121, buf1122, 43904, 128, grid=grid(43904), stream=stream0)
        del buf1050
        del buf1114
        del buf1115
        del buf1116
        del buf1117
        del buf1118
        del buf1119
        del buf1120
        buf1123 = buf1028; del buf1028  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_185.run(buf1122, buf1123, 224, 196, grid=grid(224), stream=stream0)
        buf1124 = reinterpret_tensor(buf1122, (224, 196), (1, 224), 0); del buf1122  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_186.run(le_116, buf1121, convolution_29, unsqueeze_2026, buf1124, 43904, 128, grid=grid(43904), stream=stream0)
        buf1125 = empty((224, ), device='cuda', dtype=torch.float32)
        buf1126 = empty((224, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_187.run(buf1124, squeeze_88, buf1125, buf1126, 224, 196, grid=grid(224), stream=stream0)
        del buf1124
        buf1127 = empty_strided((8, 224, 56, 56), (702464, 1, 12544, 224), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_188.run(le_116, buf1121, convolution_29, unsqueeze_2026, buf1125, squeeze_88, buf1123, primals_89, buf1127, 25088, 224, grid=grid(25088, 224), stream=stream0)
        del buf1121
        del buf1125
        del convolution_29
        del le_116
        del primals_89
        del squeeze_88
        del unsqueeze_2026
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1128 = aten.convolution_backward(buf1127, relu_27, primals_88, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1127
        del primals_88
        buf1129 = buf1128[0]
        buf1130 = buf1128[1]
        del buf1128
        buf1131 = empty((256, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_189.run(relu_27, buf1041, buf1129, buf1131, 256, 25088, grid=grid(256), stream=stream0)
        buf1132 = empty((256, 196), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_190.run(relu_27, buf1041, buf1129, convolution_28, unsqueeze_2038, buf1132, 50176, 128, grid=grid(50176), stream=stream0)
        buf1133 = empty((256, ), device='cuda', dtype=torch.float32)
        buf1135 = empty((256, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_191.run(buf1132, squeeze_85, buf1133, buf1135, 256, 196, grid=grid(256), stream=stream0)
        buf1134 = empty_strided((8, 256, 56, 56), (802816, 1, 14336, 256), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_192.run(relu_27, buf1041, buf1129, convolution_28, unsqueeze_2038, buf1133, squeeze_85, buf1131, primals_86, buf1134, 25088, 256, grid=grid(25088, 256), stream=stream0)
        del convolution_28
        del primals_86
        del squeeze_85
        del unsqueeze_2038
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1136 = aten.convolution_backward(buf1134, cat_2, primals_85, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1134
        del cat_2
        del primals_85
        buf1137 = buf1136[0]
        buf1138 = buf1136[1]
        del buf1136
        buf1139 = empty((14, 196), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_193.run(le_118, buf1137, buf1139, 2744, 128, grid=grid(2744), stream=stream0)
        buf1140 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_194.run(buf1139, buf1140, 14, 196, grid=grid(14), stream=stream0)
        buf1141 = reinterpret_tensor(buf1139, (14, 196), (1, 14), 0); del buf1139  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_195.run(le_118, buf1137, convolution_27, unsqueeze_2050, buf1141, 2744, 128, grid=grid(2744), stream=stream0)
        buf1142 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1143 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_196.run(buf1141, squeeze_82, buf1142, buf1143, 14, 196, grid=grid(14), stream=stream0)
        buf1144 = reinterpret_tensor(buf711, (8, 14, 56, 56), (43904, 1, 784, 14), 0); del buf711  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_197.run(le_118, buf1137, convolution_27, unsqueeze_2050, buf1142, squeeze_82, buf1140, primals_83, buf1144, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_27
        del le_118
        del primals_83
        del squeeze_82
        del unsqueeze_2050
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1145 = aten.convolution_backward(buf1144, add_148, primals_82, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_148
        del primals_82
        buf1146 = buf1145[0]
        buf1147 = buf1145[1]
        del buf1145
        buf1148 = reinterpret_tensor(buf761, (14, 4), (1, 14), 0); del buf761  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_198.run(le_119, buf1137, buf1146, buf1148, 56, 6272, grid=grid(56), stream=stream0)
        buf1149 = buf1142; del buf1142  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_199.run(buf1148, buf1149, 14, 4, grid=grid(14), stream=stream0)
        buf1150 = reinterpret_tensor(buf1141, (14, 196), (196, 1), 0); del buf1141  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_200.run(le_119, buf1137, buf1146, convolution_26, unsqueeze_2062, buf1150, 2744, 128, grid=grid(2744), stream=stream0)
        buf1151 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1153 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_201.run(buf1150, squeeze_79, buf1151, buf1153, 14, 196, grid=grid(14), stream=stream0)
        buf1152 = buf1144; del buf1144  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_202.run(le_119, buf1137, buf1146, convolution_26, unsqueeze_2062, buf1151, squeeze_79, buf1149, primals_80, buf1152, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_26
        del le_119
        del primals_80
        del squeeze_79
        del unsqueeze_2062
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1154 = aten.convolution_backward(buf1152, add_142, primals_79, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_142
        del primals_79
        buf1155 = buf1154[0]
        buf1156 = buf1154[1]
        del buf1154
        buf1157 = buf1148; del buf1148  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_203.run(le_120, buf1137, buf1155, buf1157, 56, 6272, grid=grid(56), stream=stream0)
        buf1158 = buf1151; del buf1151  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_199.run(buf1157, buf1158, 14, 4, grid=grid(14), stream=stream0)
        buf1159 = buf1150; del buf1150  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_204.run(le_120, buf1137, buf1155, convolution_25, unsqueeze_2074, buf1159, 2744, 128, grid=grid(2744), stream=stream0)
        buf1160 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1162 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_201.run(buf1159, squeeze_76, buf1160, buf1162, 14, 196, grid=grid(14), stream=stream0)
        buf1161 = buf1152; del buf1152  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_205.run(le_120, buf1137, buf1155, convolution_25, unsqueeze_2074, buf1160, squeeze_76, buf1158, primals_77, buf1161, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_25
        del le_120
        del primals_77
        del squeeze_76
        del unsqueeze_2074
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1163 = aten.convolution_backward(buf1161, add_136, primals_76, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_136
        del primals_76
        buf1164 = buf1163[0]
        buf1165 = buf1163[1]
        del buf1163
        buf1166 = buf1157; del buf1157  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_206.run(le_121, buf1137, buf1164, buf1166, 56, 6272, grid=grid(56), stream=stream0)
        buf1167 = buf1160; del buf1160  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_199.run(buf1166, buf1167, 14, 4, grid=grid(14), stream=stream0)
        buf1168 = buf1159; del buf1159  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_207.run(le_121, buf1137, buf1164, convolution_24, unsqueeze_2086, buf1168, 2744, 128, grid=grid(2744), stream=stream0)
        buf1169 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1171 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_201.run(buf1168, squeeze_73, buf1169, buf1171, 14, 196, grid=grid(14), stream=stream0)
        buf1170 = buf1161; del buf1161  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_208.run(le_121, buf1137, buf1164, convolution_24, unsqueeze_2086, buf1169, squeeze_73, buf1167, primals_74, buf1170, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_24
        del le_121
        del primals_74
        del squeeze_73
        del unsqueeze_2086
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1172 = aten.convolution_backward(buf1170, add_130, primals_73, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_130
        del primals_73
        buf1173 = buf1172[0]
        buf1174 = buf1172[1]
        del buf1172
        buf1175 = buf1166; del buf1166  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_209.run(le_122, buf1137, buf1173, buf1175, 56, 6272, grid=grid(56), stream=stream0)
        buf1176 = buf1169; del buf1169  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_199.run(buf1175, buf1176, 14, 4, grid=grid(14), stream=stream0)
        buf1177 = buf1168; del buf1168  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_210.run(le_122, buf1137, buf1173, convolution_23, unsqueeze_2098, buf1177, 2744, 128, grid=grid(2744), stream=stream0)
        buf1178 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1180 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_201.run(buf1177, squeeze_70, buf1178, buf1180, 14, 196, grid=grid(14), stream=stream0)
        buf1179 = buf1170; del buf1170  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_211.run(le_122, buf1137, buf1173, convolution_23, unsqueeze_2098, buf1178, squeeze_70, buf1176, primals_71, buf1179, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_23
        del le_122
        del primals_71
        del squeeze_70
        del unsqueeze_2098
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1181 = aten.convolution_backward(buf1179, add_124, primals_70, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_124
        del primals_70
        buf1182 = buf1181[0]
        buf1183 = buf1181[1]
        del buf1181
        buf1184 = buf1175; del buf1175  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_212.run(le_123, buf1137, buf1182, buf1184, 56, 6272, grid=grid(56), stream=stream0)
        buf1185 = buf1178; del buf1178  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_199.run(buf1184, buf1185, 14, 4, grid=grid(14), stream=stream0)
        buf1186 = buf1177; del buf1177  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_213.run(le_123, buf1137, buf1182, convolution_22, unsqueeze_2110, buf1186, 2744, 128, grid=grid(2744), stream=stream0)
        buf1187 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1189 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_201.run(buf1186, squeeze_67, buf1187, buf1189, 14, 196, grid=grid(14), stream=stream0)
        buf1188 = buf1179; del buf1179  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_214.run(le_123, buf1137, buf1182, convolution_22, unsqueeze_2110, buf1187, squeeze_67, buf1185, primals_68, buf1188, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_22
        del le_123
        del primals_68
        del squeeze_67
        del unsqueeze_2110
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1190 = aten.convolution_backward(buf1188, add_118, primals_67, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_118
        del primals_67
        buf1191 = buf1190[0]
        buf1192 = buf1190[1]
        del buf1190
        buf1193 = buf1184; del buf1184  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_215.run(le_124, buf1137, buf1191, buf1193, 56, 6272, grid=grid(56), stream=stream0)
        buf1194 = buf1187; del buf1187  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_199.run(buf1193, buf1194, 14, 4, grid=grid(14), stream=stream0)
        buf1195 = buf1186; del buf1186  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_216.run(le_124, buf1137, buf1191, convolution_21, unsqueeze_2122, buf1195, 2744, 128, grid=grid(2744), stream=stream0)
        buf1196 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1198 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_201.run(buf1195, squeeze_64, buf1196, buf1198, 14, 196, grid=grid(14), stream=stream0)
        buf1197 = buf1188; del buf1188  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_217.run(le_124, buf1137, buf1191, convolution_21, unsqueeze_2122, buf1196, squeeze_64, buf1194, primals_65, buf1197, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_21
        del le_124
        del primals_65
        del squeeze_64
        del unsqueeze_2122
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1199 = aten.convolution_backward(buf1197, getitem_196, primals_64, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1197
        del getitem_196
        del primals_64
        buf1200 = buf1199[0]
        buf1201 = buf1199[1]
        del buf1199
        buf1210 = reinterpret_tensor(buf780, (8, 112, 56, 56), (351232, 3136, 56, 1), 0); del buf780  # reuse
        buf1202 = reinterpret_tensor(buf1210, (8, 14, 56, 56), (351232, 3136, 56, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1200, buf1202, 351232, grid=grid(351232), stream=stream0)
        del buf1200
        buf1203 = reinterpret_tensor(buf1210, (8, 14, 56, 56), (351232, 3136, 56, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1191, buf1203, 351232, grid=grid(351232), stream=stream0)
        del buf1191
        buf1204 = reinterpret_tensor(buf1210, (8, 14, 56, 56), (351232, 3136, 56, 1), 87808)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1182, buf1204, 351232, grid=grid(351232), stream=stream0)
        del buf1182
        buf1205 = reinterpret_tensor(buf1210, (8, 14, 56, 56), (351232, 3136, 56, 1), 131712)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1173, buf1205, 351232, grid=grid(351232), stream=stream0)
        del buf1173
        buf1206 = reinterpret_tensor(buf1210, (8, 14, 56, 56), (351232, 3136, 56, 1), 175616)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1164, buf1206, 351232, grid=grid(351232), stream=stream0)
        del buf1164
        buf1207 = reinterpret_tensor(buf1210, (8, 14, 56, 56), (351232, 3136, 56, 1), 219520)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1155, buf1207, 351232, grid=grid(351232), stream=stream0)
        del buf1155
        buf1208 = reinterpret_tensor(buf1210, (8, 14, 56, 56), (351232, 3136, 56, 1), 263424)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1146, buf1208, 351232, grid=grid(351232), stream=stream0)
        buf1209 = reinterpret_tensor(buf1210, (8, 14, 56, 56), (351232, 3136, 56, 1), 307328)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_218.run(buf1137, buf1209, 351232, grid=grid(351232), stream=stream0)
        buf1211 = reinterpret_tensor(buf777, (112, 196), (196, 1), 0); del buf777  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_219.run(le_125, buf1210, buf1211, 21952, 128, grid=grid(21952), stream=stream0)
        del buf1202
        del buf1203
        del buf1204
        del buf1205
        del buf1206
        del buf1207
        del buf1208
        del buf1209
        buf1212 = buf247; del buf247  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_220.run(buf1211, buf1212, 112, 196, grid=grid(112), stream=stream0)
        buf1213 = reinterpret_tensor(buf1211, (112, 196), (1, 112), 0); del buf1211  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_221.run(le_125, buf1210, convolution_20, unsqueeze_2134, buf1213, 21952, 128, grid=grid(21952), stream=stream0)
        buf1214 = empty((112, ), device='cuda', dtype=torch.float32)
        buf1215 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_222.run(buf1213, squeeze_61, buf1214, buf1215, 112, 196, grid=grid(112), stream=stream0)
        buf1216 = reinterpret_tensor(buf1137, (8, 112, 56, 56), (351232, 1, 6272, 112), 0); del buf1137  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_223.run(le_125, buf1210, convolution_20, unsqueeze_2134, buf1214, squeeze_61, buf1212, primals_62, buf1216, 25088, 112, grid=grid(25088, 112), stream=stream0)
        del buf1210
        del convolution_20
        del le_125
        del primals_62
        del squeeze_61
        del unsqueeze_2134
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1217 = aten.convolution_backward(buf1216, relu_18, primals_61, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_61
        buf1218 = buf1217[0]
        buf1219 = buf1217[1]
        del buf1217
        buf1220 = buf1041; del buf1041  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.threshold_backward]
        triton_poi_fused_add_threshold_backward_224.run(buf1220, relu_18, relu_27, buf1129, buf1218, 2048, 3136, grid=grid(2048, 3136), stream=stream0)
        del relu_18
        del relu_27
        buf1221 = buf1133; del buf1133  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_225.run(buf1220, buf1221, 256, 25088, grid=grid(256), stream=stream0)
        buf1222 = buf1132; del buf1132  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_red_fused_native_batch_norm_backward_226.run(buf1220, convolution_19, unsqueeze_2146, buf1222, 50176, 128, grid=grid(50176), stream=stream0)
        buf1223 = empty((256, ), device='cuda', dtype=torch.float32)
        buf1224 = empty((256, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_191.run(buf1222, squeeze_58, buf1223, buf1224, 256, 196, grid=grid(256), stream=stream0)
        buf1225 = reinterpret_tensor(buf1218, (8, 256, 56, 56), (802816, 1, 14336, 256), 0); del buf1218  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_227.run(buf1220, convolution_19, unsqueeze_2146, buf1223, squeeze_58, buf1221, primals_59, buf1225, 25088, 256, grid=grid(25088, 256), stream=stream0)
        del convolution_19
        del primals_59
        del squeeze_58
        del unsqueeze_2146
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward]
        buf1226 = aten.convolution_backward(buf1225, cat_1, primals_58, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del cat_1
        del primals_58
        buf1227 = buf1226[0]
        buf1228 = buf1226[1]
        del buf1226
        buf1229 = buf1195; del buf1195  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_193.run(le_127, buf1227, buf1229, 2744, 128, grid=grid(2744), stream=stream0)
        buf1230 = buf1196; del buf1196  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_194.run(buf1229, buf1230, 14, 196, grid=grid(14), stream=stream0)
        buf1231 = reinterpret_tensor(buf1229, (14, 196), (1, 14), 0); del buf1229  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_195.run(le_127, buf1227, convolution_18, unsqueeze_2158, buf1231, 2744, 128, grid=grid(2744), stream=stream0)
        buf1232 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1233 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_196.run(buf1231, squeeze_55, buf1232, buf1233, 14, 196, grid=grid(14), stream=stream0)
        buf1234 = reinterpret_tensor(buf1146, (8, 14, 56, 56), (43904, 1, 784, 14), 0); del buf1146  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_197.run(le_127, buf1227, convolution_18, unsqueeze_2158, buf1232, squeeze_55, buf1230, primals_56, buf1234, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_18
        del le_127
        del primals_56
        del squeeze_55
        del unsqueeze_2158
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1235 = aten.convolution_backward(buf1234, add_96, primals_55, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_96
        del primals_55
        buf1236 = buf1235[0]
        buf1237 = buf1235[1]
        del buf1235
        buf1238 = buf1193; del buf1193  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_198.run(le_128, buf1227, buf1236, buf1238, 56, 6272, grid=grid(56), stream=stream0)
        buf1239 = buf1232; del buf1232  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_199.run(buf1238, buf1239, 14, 4, grid=grid(14), stream=stream0)
        buf1240 = reinterpret_tensor(buf1231, (14, 196), (196, 1), 0); del buf1231  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_200.run(le_128, buf1227, buf1236, convolution_17, unsqueeze_2170, buf1240, 2744, 128, grid=grid(2744), stream=stream0)
        buf1241 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1243 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_201.run(buf1240, squeeze_52, buf1241, buf1243, 14, 196, grid=grid(14), stream=stream0)
        buf1242 = buf1234; del buf1234  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_202.run(le_128, buf1227, buf1236, convolution_17, unsqueeze_2170, buf1241, squeeze_52, buf1239, primals_53, buf1242, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_17
        del le_128
        del primals_53
        del squeeze_52
        del unsqueeze_2170
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1244 = aten.convolution_backward(buf1242, add_90, primals_52, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_90
        del primals_52
        buf1245 = buf1244[0]
        buf1246 = buf1244[1]
        del buf1244
        buf1247 = buf1238; del buf1238  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_203.run(le_129, buf1227, buf1245, buf1247, 56, 6272, grid=grid(56), stream=stream0)
        buf1248 = buf1241; del buf1241  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_199.run(buf1247, buf1248, 14, 4, grid=grid(14), stream=stream0)
        buf1249 = buf1240; del buf1240  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_204.run(le_129, buf1227, buf1245, convolution_16, unsqueeze_2182, buf1249, 2744, 128, grid=grid(2744), stream=stream0)
        buf1250 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1252 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_201.run(buf1249, squeeze_49, buf1250, buf1252, 14, 196, grid=grid(14), stream=stream0)
        buf1251 = buf1242; del buf1242  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_205.run(le_129, buf1227, buf1245, convolution_16, unsqueeze_2182, buf1250, squeeze_49, buf1248, primals_50, buf1251, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_16
        del le_129
        del primals_50
        del squeeze_49
        del unsqueeze_2182
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1253 = aten.convolution_backward(buf1251, add_84, primals_49, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_84
        del primals_49
        buf1254 = buf1253[0]
        buf1255 = buf1253[1]
        del buf1253
        buf1256 = buf1247; del buf1247  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_206.run(le_130, buf1227, buf1254, buf1256, 56, 6272, grid=grid(56), stream=stream0)
        buf1257 = buf1250; del buf1250  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_199.run(buf1256, buf1257, 14, 4, grid=grid(14), stream=stream0)
        buf1258 = buf1249; del buf1249  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_207.run(le_130, buf1227, buf1254, convolution_15, unsqueeze_2194, buf1258, 2744, 128, grid=grid(2744), stream=stream0)
        buf1259 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1261 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_201.run(buf1258, squeeze_46, buf1259, buf1261, 14, 196, grid=grid(14), stream=stream0)
        buf1260 = buf1251; del buf1251  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_208.run(le_130, buf1227, buf1254, convolution_15, unsqueeze_2194, buf1259, squeeze_46, buf1257, primals_47, buf1260, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_15
        del le_130
        del primals_47
        del squeeze_46
        del unsqueeze_2194
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1262 = aten.convolution_backward(buf1260, add_78, primals_46, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_78
        del primals_46
        buf1263 = buf1262[0]
        buf1264 = buf1262[1]
        del buf1262
        buf1265 = buf1256; del buf1256  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_209.run(le_131, buf1227, buf1263, buf1265, 56, 6272, grid=grid(56), stream=stream0)
        buf1266 = buf1259; del buf1259  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_199.run(buf1265, buf1266, 14, 4, grid=grid(14), stream=stream0)
        buf1267 = buf1258; del buf1258  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_210.run(le_131, buf1227, buf1263, convolution_14, unsqueeze_2206, buf1267, 2744, 128, grid=grid(2744), stream=stream0)
        buf1268 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1270 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_201.run(buf1267, squeeze_43, buf1268, buf1270, 14, 196, grid=grid(14), stream=stream0)
        buf1269 = buf1260; del buf1260  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_211.run(le_131, buf1227, buf1263, convolution_14, unsqueeze_2206, buf1268, squeeze_43, buf1266, primals_44, buf1269, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_14
        del le_131
        del primals_44
        del squeeze_43
        del unsqueeze_2206
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1271 = aten.convolution_backward(buf1269, add_72, primals_43, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_72
        del primals_43
        buf1272 = buf1271[0]
        buf1273 = buf1271[1]
        del buf1271
        buf1274 = buf1265; del buf1265  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_212.run(le_132, buf1227, buf1272, buf1274, 56, 6272, grid=grid(56), stream=stream0)
        buf1275 = buf1268; del buf1268  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_199.run(buf1274, buf1275, 14, 4, grid=grid(14), stream=stream0)
        buf1276 = buf1267; del buf1267  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_213.run(le_132, buf1227, buf1272, convolution_13, unsqueeze_2218, buf1276, 2744, 128, grid=grid(2744), stream=stream0)
        buf1277 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1279 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_201.run(buf1276, squeeze_40, buf1277, buf1279, 14, 196, grid=grid(14), stream=stream0)
        buf1278 = buf1269; del buf1269  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_214.run(le_132, buf1227, buf1272, convolution_13, unsqueeze_2218, buf1277, squeeze_40, buf1275, primals_41, buf1278, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_13
        del le_132
        del primals_41
        del squeeze_40
        del unsqueeze_2218
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1280 = aten.convolution_backward(buf1278, add_66, primals_40, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del add_66
        del primals_40
        buf1281 = buf1280[0]
        buf1282 = buf1280[1]
        del buf1280
        buf1283 = buf1274; del buf1274  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_215.run(le_133, buf1227, buf1281, buf1283, 56, 6272, grid=grid(56), stream=stream0)
        buf1284 = buf1277; del buf1277  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_199.run(buf1283, buf1284, 14, 4, grid=grid(14), stream=stream0)
        del buf1283
        buf1285 = buf1276; del buf1276  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_216.run(le_133, buf1227, buf1281, convolution_12, unsqueeze_2230, buf1285, 2744, 128, grid=grid(2744), stream=stream0)
        buf1286 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1288 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_201.run(buf1285, squeeze_37, buf1286, buf1288, 14, 196, grid=grid(14), stream=stream0)
        buf1287 = buf1278; del buf1278  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_217.run(le_133, buf1227, buf1281, convolution_12, unsqueeze_2230, buf1286, squeeze_37, buf1284, primals_38, buf1287, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_12
        del le_133
        del primals_38
        del squeeze_37
        del unsqueeze_2230
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1289 = aten.convolution_backward(buf1287, getitem_106, primals_37, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1287
        del getitem_106
        del primals_37
        buf1290 = buf1289[0]
        buf1291 = buf1289[1]
        del buf1289
        buf1300 = reinterpret_tensor(buf1216, (8, 112, 56, 56), (351232, 3136, 56, 1), 0); del buf1216  # reuse
        buf1292 = reinterpret_tensor(buf1300, (8, 14, 56, 56), (351232, 3136, 56, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1290, buf1292, 351232, grid=grid(351232), stream=stream0)
        del buf1290
        buf1293 = reinterpret_tensor(buf1300, (8, 14, 56, 56), (351232, 3136, 56, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1281, buf1293, 351232, grid=grid(351232), stream=stream0)
        del buf1281
        buf1294 = reinterpret_tensor(buf1300, (8, 14, 56, 56), (351232, 3136, 56, 1), 87808)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1272, buf1294, 351232, grid=grid(351232), stream=stream0)
        del buf1272
        buf1295 = reinterpret_tensor(buf1300, (8, 14, 56, 56), (351232, 3136, 56, 1), 131712)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1263, buf1295, 351232, grid=grid(351232), stream=stream0)
        del buf1263
        buf1296 = reinterpret_tensor(buf1300, (8, 14, 56, 56), (351232, 3136, 56, 1), 175616)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1254, buf1296, 351232, grid=grid(351232), stream=stream0)
        del buf1254
        buf1297 = reinterpret_tensor(buf1300, (8, 14, 56, 56), (351232, 3136, 56, 1), 219520)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1245, buf1297, 351232, grid=grid(351232), stream=stream0)
        del buf1245
        buf1298 = reinterpret_tensor(buf1300, (8, 14, 56, 56), (351232, 3136, 56, 1), 263424)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1236, buf1298, 351232, grid=grid(351232), stream=stream0)
        buf1299 = reinterpret_tensor(buf1300, (8, 14, 56, 56), (351232, 3136, 56, 1), 307328)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_218.run(buf1227, buf1299, 351232, grid=grid(351232), stream=stream0)
        buf1301 = reinterpret_tensor(buf1213, (112, 196), (196, 1), 0); del buf1213  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_219.run(le_134, buf1300, buf1301, 21952, 128, grid=grid(21952), stream=stream0)
        del buf1292
        del buf1293
        del buf1294
        del buf1295
        del buf1296
        del buf1297
        del buf1298
        del buf1299
        buf1302 = buf1214; del buf1214  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_220.run(buf1301, buf1302, 112, 196, grid=grid(112), stream=stream0)
        buf1303 = reinterpret_tensor(buf1301, (112, 196), (1, 112), 0); del buf1301  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_221.run(le_134, buf1300, convolution_11, unsqueeze_2242, buf1303, 21952, 128, grid=grid(21952), stream=stream0)
        buf1304 = empty((112, ), device='cuda', dtype=torch.float32)
        buf1305 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_222.run(buf1303, squeeze_34, buf1304, buf1305, 112, 196, grid=grid(112), stream=stream0)
        buf1306 = reinterpret_tensor(buf1227, (8, 112, 56, 56), (351232, 1, 6272, 112), 0); del buf1227  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_223.run(le_134, buf1300, convolution_11, unsqueeze_2242, buf1304, squeeze_34, buf1302, primals_35, buf1306, 25088, 112, grid=grid(25088, 112), stream=stream0)
        del buf1300
        del convolution_11
        del le_134
        del primals_35
        del squeeze_34
        del unsqueeze_2242
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1307 = aten.convolution_backward(buf1306, relu_9, primals_34, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del primals_34
        buf1308 = buf1307[0]
        buf1309 = buf1307[1]
        del buf1307
        buf1310 = buf1223; del buf1223  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_189.run(relu_9, buf1220, buf1308, buf1310, 256, 25088, grid=grid(256), stream=stream0)
        buf1311 = buf1222; del buf1222  # reuse
        buf1318 = empty((256, 196), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_add_native_batch_norm_backward_threshold_backward_228.run(relu_9, buf1220, buf1308, convolution_10, unsqueeze_2254, convolution_9, unsqueeze_2266, buf1311, buf1318, 50176, 128, grid=grid(50176), stream=stream0)
        buf1312 = empty((256, ), device='cuda', dtype=torch.float32)
        buf1314 = empty((256, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_191.run(buf1311, squeeze_31, buf1312, buf1314, 256, 196, grid=grid(256), stream=stream0)
        buf1319 = empty((256, ), device='cuda', dtype=torch.float32)
        buf1321 = empty((256, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_add_native_batch_norm_backward_threshold_backward_191.run(buf1318, squeeze_28, buf1319, buf1321, 256, 196, grid=grid(256), stream=stream0)
        buf1313 = buf1225; del buf1225  # reuse
        buf1320 = reinterpret_tensor(buf1129, (8, 256, 56, 56), (802816, 1, 14336, 256), 0); del buf1129  # reuse
        # Source Nodes: [], Original ATen: [aten.add, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_add_native_batch_norm_backward_threshold_backward_229.run(relu_9, buf1220, buf1308, convolution_10, unsqueeze_2254, buf1312, squeeze_31, buf1310, primals_32, convolution_9, unsqueeze_2266, buf1319, squeeze_28, primals_29, buf1313, buf1320, 25088, 256, grid=grid(25088, 256), stream=stream0)
        del buf1220
        del buf1308
        del buf1312
        del buf1319
        del convolution_10
        del convolution_9
        del primals_29
        del primals_32
        del relu_9
        del squeeze_28
        del squeeze_31
        del unsqueeze_2254
        del unsqueeze_2266
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1315 = aten.convolution_backward(buf1313, getitem_2, primals_31, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1313
        del primals_31
        buf1316 = buf1315[0]
        buf1317 = buf1315[1]
        del buf1315
        # Source Nodes: [], Original ATen: [aten.convolution_backward]
        buf1322 = aten.convolution_backward(buf1320, cat, primals_28, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1320
        del cat
        del primals_28
        buf1323 = buf1322[0]
        buf1324 = buf1322[1]
        del buf1322
        buf1396 = reinterpret_tensor(buf1306, (8, 112, 56, 56), (351232, 3136, 56, 1), 0); del buf1306  # reuse
        buf1325 = reinterpret_tensor(buf1396, (8, 14, 56, 56), (351232, 3136, 56, 1), 307328)  # alias
        # Source Nodes: [], Original ATen: [aten.avg_pool2d_backward]
        triton_poi_fused_avg_pool2d_backward_230.run(buf1323, buf1325, 351232, grid=grid(351232), stream=stream0)
        buf1326 = buf1285; del buf1285  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_193.run(le_136, buf1323, buf1326, 2744, 128, grid=grid(2744), stream=stream0)
        buf1327 = buf1286; del buf1286  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_194.run(buf1326, buf1327, 14, 196, grid=grid(14), stream=stream0)
        buf1328 = reinterpret_tensor(buf1326, (14, 196), (1, 14), 0); del buf1326  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_195.run(le_136, buf1323, convolution_8, unsqueeze_2278, buf1328, 2744, 128, grid=grid(2744), stream=stream0)
        buf1329 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1330 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_196.run(buf1328, squeeze_25, buf1329, buf1330, 14, 196, grid=grid(14), stream=stream0)
        buf1331 = reinterpret_tensor(buf1236, (8, 14, 56, 56), (43904, 1, 784, 14), 0); del buf1236  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_197.run(le_136, buf1323, convolution_8, unsqueeze_2278, buf1329, squeeze_25, buf1327, primals_26, buf1331, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_8
        del le_136
        del primals_26
        del squeeze_25
        del unsqueeze_2278
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1332 = aten.convolution_backward(buf1331, getitem_80, primals_25, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_80
        del primals_25
        buf1333 = buf1332[0]
        buf1334 = buf1332[1]
        del buf1332
        buf1335 = reinterpret_tensor(buf1328, (14, 196), (196, 1), 0); del buf1328  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_231.run(le_137, buf1323, buf1335, 2744, 128, grid=grid(2744), stream=stream0)
        buf1336 = buf1329; del buf1329  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_194.run(buf1335, buf1336, 14, 196, grid=grid(14), stream=stream0)
        buf1337 = reinterpret_tensor(buf1335, (14, 196), (1, 14), 0); del buf1335  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_232.run(le_137, buf1323, convolution_7, unsqueeze_2290, buf1337, 2744, 128, grid=grid(2744), stream=stream0)
        buf1338 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1339 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_196.run(buf1337, squeeze_22, buf1338, buf1339, 14, 196, grid=grid(14), stream=stream0)
        buf1340 = buf1331; del buf1331  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_233.run(le_137, buf1323, convolution_7, unsqueeze_2290, buf1338, squeeze_22, buf1336, primals_23, buf1340, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_7
        del le_137
        del primals_23
        del squeeze_22
        del unsqueeze_2290
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1341 = aten.convolution_backward(buf1340, getitem_69, primals_22, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_69
        del primals_22
        buf1342 = buf1341[0]
        buf1343 = buf1341[1]
        del buf1341
        buf1344 = reinterpret_tensor(buf1337, (14, 196), (196, 1), 0); del buf1337  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_234.run(le_138, buf1323, buf1344, 2744, 128, grid=grid(2744), stream=stream0)
        buf1345 = buf1338; del buf1338  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_194.run(buf1344, buf1345, 14, 196, grid=grid(14), stream=stream0)
        buf1346 = reinterpret_tensor(buf1344, (14, 196), (1, 14), 0); del buf1344  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_235.run(le_138, buf1323, convolution_6, unsqueeze_2302, buf1346, 2744, 128, grid=grid(2744), stream=stream0)
        buf1347 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1348 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_196.run(buf1346, squeeze_19, buf1347, buf1348, 14, 196, grid=grid(14), stream=stream0)
        buf1349 = buf1340; del buf1340  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_236.run(le_138, buf1323, convolution_6, unsqueeze_2302, buf1347, squeeze_19, buf1345, primals_20, buf1349, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_6
        del le_138
        del primals_20
        del squeeze_19
        del unsqueeze_2302
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1350 = aten.convolution_backward(buf1349, getitem_58, primals_19, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_58
        del primals_19
        buf1351 = buf1350[0]
        buf1352 = buf1350[1]
        del buf1350
        buf1353 = reinterpret_tensor(buf1346, (14, 196), (196, 1), 0); del buf1346  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_237.run(le_139, buf1323, buf1353, 2744, 128, grid=grid(2744), stream=stream0)
        buf1354 = buf1347; del buf1347  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_194.run(buf1353, buf1354, 14, 196, grid=grid(14), stream=stream0)
        buf1355 = reinterpret_tensor(buf1353, (14, 196), (1, 14), 0); del buf1353  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_238.run(le_139, buf1323, convolution_5, unsqueeze_2314, buf1355, 2744, 128, grid=grid(2744), stream=stream0)
        buf1356 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1357 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_196.run(buf1355, squeeze_16, buf1356, buf1357, 14, 196, grid=grid(14), stream=stream0)
        buf1358 = buf1349; del buf1349  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_239.run(le_139, buf1323, convolution_5, unsqueeze_2314, buf1356, squeeze_16, buf1354, primals_17, buf1358, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_5
        del le_139
        del primals_17
        del squeeze_16
        del unsqueeze_2314
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1359 = aten.convolution_backward(buf1358, getitem_47, primals_16, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_47
        del primals_16
        buf1360 = buf1359[0]
        buf1361 = buf1359[1]
        del buf1359
        buf1362 = reinterpret_tensor(buf1355, (14, 196), (196, 1), 0); del buf1355  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_240.run(le_140, buf1323, buf1362, 2744, 128, grid=grid(2744), stream=stream0)
        buf1363 = buf1356; del buf1356  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_194.run(buf1362, buf1363, 14, 196, grid=grid(14), stream=stream0)
        buf1364 = reinterpret_tensor(buf1362, (14, 196), (1, 14), 0); del buf1362  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_241.run(le_140, buf1323, convolution_4, unsqueeze_2326, buf1364, 2744, 128, grid=grid(2744), stream=stream0)
        buf1365 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1366 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_196.run(buf1364, squeeze_13, buf1365, buf1366, 14, 196, grid=grid(14), stream=stream0)
        buf1367 = buf1358; del buf1358  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_242.run(le_140, buf1323, convolution_4, unsqueeze_2326, buf1365, squeeze_13, buf1363, primals_14, buf1367, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_4
        del le_140
        del primals_14
        del squeeze_13
        del unsqueeze_2326
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1368 = aten.convolution_backward(buf1367, getitem_36, primals_13, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_36
        del primals_13
        buf1369 = buf1368[0]
        buf1370 = buf1368[1]
        del buf1368
        buf1371 = reinterpret_tensor(buf1364, (14, 196), (196, 1), 0); del buf1364  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_243.run(le_141, buf1323, buf1371, 2744, 128, grid=grid(2744), stream=stream0)
        buf1372 = buf1365; del buf1365  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_194.run(buf1371, buf1372, 14, 196, grid=grid(14), stream=stream0)
        buf1373 = reinterpret_tensor(buf1371, (14, 196), (1, 14), 0); del buf1371  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_244.run(le_141, buf1323, convolution_3, unsqueeze_2338, buf1373, 2744, 128, grid=grid(2744), stream=stream0)
        buf1374 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1375 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_196.run(buf1373, squeeze_10, buf1374, buf1375, 14, 196, grid=grid(14), stream=stream0)
        buf1376 = buf1367; del buf1367  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_245.run(le_141, buf1323, convolution_3, unsqueeze_2338, buf1374, squeeze_10, buf1372, primals_11, buf1376, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del convolution_3
        del le_141
        del primals_11
        del squeeze_10
        del unsqueeze_2338
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1377 = aten.convolution_backward(buf1376, getitem_25, primals_10, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del getitem_25
        del primals_10
        buf1378 = buf1377[0]
        buf1379 = buf1377[1]
        del buf1377
        buf1380 = reinterpret_tensor(buf1373, (14, 196), (196, 1), 0); del buf1373  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_246.run(le_142, buf1323, buf1380, 2744, 128, grid=grid(2744), stream=stream0)
        buf1381 = buf1374; del buf1374  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_194.run(buf1380, buf1381, 14, 196, grid=grid(14), stream=stream0)
        buf1382 = reinterpret_tensor(buf1380, (14, 196), (1, 14), 0); del buf1380  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_247.run(le_142, buf1323, convolution_2, unsqueeze_2350, buf1382, 2744, 128, grid=grid(2744), stream=stream0)
        buf1383 = empty((14, ), device='cuda', dtype=torch.float32)
        buf1384 = empty((14, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_196.run(buf1382, squeeze_7, buf1383, buf1384, 14, 196, grid=grid(14), stream=stream0)
        del buf1382
        buf1385 = buf1376; del buf1376  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_248.run(le_142, buf1323, convolution_2, unsqueeze_2350, buf1383, squeeze_7, buf1381, primals_8, buf1385, 25088, 14, grid=grid(25088, 14), stream=stream0)
        del buf1383
        del convolution_2
        del le_142
        del primals_8
        del squeeze_7
        del unsqueeze_2350
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1386 = aten.convolution_backward(buf1385, getitem_14, primals_7, [0], [1, 1], [1, 1], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1385
        del getitem_14
        del primals_7
        buf1387 = buf1386[0]
        buf1388 = buf1386[1]
        del buf1386
        buf1389 = reinterpret_tensor(buf1396, (8, 14, 56, 56), (351232, 3136, 56, 1), 0)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1387, buf1389, 351232, grid=grid(351232), stream=stream0)
        del buf1387
        buf1390 = reinterpret_tensor(buf1396, (8, 14, 56, 56), (351232, 3136, 56, 1), 43904)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1378, buf1390, 351232, grid=grid(351232), stream=stream0)
        del buf1378
        buf1391 = reinterpret_tensor(buf1396, (8, 14, 56, 56), (351232, 3136, 56, 1), 87808)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1369, buf1391, 351232, grid=grid(351232), stream=stream0)
        del buf1369
        buf1392 = reinterpret_tensor(buf1396, (8, 14, 56, 56), (351232, 3136, 56, 1), 131712)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1360, buf1392, 351232, grid=grid(351232), stream=stream0)
        del buf1360
        buf1393 = reinterpret_tensor(buf1396, (8, 14, 56, 56), (351232, 3136, 56, 1), 175616)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1351, buf1393, 351232, grid=grid(351232), stream=stream0)
        del buf1351
        buf1394 = reinterpret_tensor(buf1396, (8, 14, 56, 56), (351232, 3136, 56, 1), 219520)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1342, buf1394, 351232, grid=grid(351232), stream=stream0)
        del buf1342
        buf1395 = reinterpret_tensor(buf1396, (8, 14, 56, 56), (351232, 3136, 56, 1), 263424)  # alias
        # Source Nodes: [], Original ATen: [aten.cat]
        triton_poi_fused_cat_117.run(buf1333, buf1395, 351232, grid=grid(351232), stream=stream0)
        del buf1333
        buf1397 = reinterpret_tensor(buf1303, (112, 196), (196, 1), 0); del buf1303  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_219.run(le_143, buf1396, buf1397, 21952, 128, grid=grid(21952), stream=stream0)
        del buf1325
        del buf1389
        del buf1390
        del buf1391
        del buf1392
        del buf1393
        del buf1394
        del buf1395
        buf1398 = buf1304; del buf1304  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_per_fused_native_batch_norm_backward_threshold_backward_220.run(buf1397, buf1398, 112, 196, grid=grid(112), stream=stream0)
        buf1399 = reinterpret_tensor(buf1397, (112, 196), (1, 112), 0); del buf1397  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_221.run(le_143, buf1396, convolution_1, unsqueeze_2362, buf1399, 21952, 128, grid=grid(21952), stream=stream0)
        buf1400 = empty((112, ), device='cuda', dtype=torch.float32)
        buf1401 = empty((112, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_222.run(buf1399, squeeze_4, buf1400, buf1401, 112, 196, grid=grid(112), stream=stream0)
        del buf1399
        buf1402 = reinterpret_tensor(buf1323, (8, 112, 56, 56), (351232, 1, 6272, 112), 0); del buf1323  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_223.run(le_143, buf1396, convolution_1, unsqueeze_2362, buf1400, squeeze_4, buf1398, primals_5, buf1402, 25088, 112, grid=grid(25088, 112), stream=stream0)
        del buf1396
        del buf1400
        del convolution_1
        del le_143
        del primals_5
        del squeeze_4
        del unsqueeze_2362
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1403 = aten.convolution_backward(buf1402, getitem_2, primals_4, [0], [1, 1], [0, 0], [1, 1], False, [0, 0], 1, [True, True, False])
        del buf1402
        del getitem_2
        del primals_4
        buf1404 = buf1403[0]
        buf1405 = buf1403[1]
        del buf1403
        buf1406 = buf1316; del buf1316  # reuse
        # Source Nodes: [], Original ATen: [aten.add]
        triton_poi_fused_add_249.run(buf1406, buf1404, 1605632, grid=grid(1605632), stream=stream0)
        del buf1404
        # Source Nodes: [], Original ATen: [aten.add, aten.max_pool2d_with_indices_backward]
        buf1407 = aten.max_pool2d_with_indices_backward(buf1406, relu, [3, 3], [2, 2], [1, 1], [1, 1], False, getitem_3)
        del buf1406
        del getitem_3
        buf1408 = buf1407
        del buf1407
        buf1409 = reinterpret_tensor(buf1318, (64, 784), (1, 64), 0); del buf1318  # reuse
        buf1411 = reinterpret_tensor(buf1311, (64, 784), (1, 64), 0); del buf1311  # reuse
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_250.run(relu, buf1408, convolution, unsqueeze_2374, buf1409, buf1411, 50176, 128, grid=grid(50176), stream=stream0)
        buf1410 = empty((64, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_251.run(buf1409, buf1410, 64, 784, grid=grid(64), stream=stream0)
        del buf1409
        buf1412 = empty((64, ), device='cuda', dtype=torch.float32)
        buf1413 = empty((64, ), device='cuda', dtype=torch.float32)
        # Source Nodes: [], Original ATen: [aten.native_batch_norm_backward, aten.threshold_backward]
        triton_red_fused_native_batch_norm_backward_threshold_backward_252.run(buf1411, squeeze_1, buf1412, buf1413, 64, 784, grid=grid(64), stream=stream0)
        del buf1411
        buf1414 = buf1408; del buf1408  # reuse
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        triton_poi_fused_convolution_backward_native_batch_norm_backward_threshold_backward_253.run(buf1414, relu, convolution, unsqueeze_2374, buf1412, squeeze_1, buf1410, primals_2, 6422528, grid=grid(6422528), stream=stream0)
        del buf1412
        del convolution
        del primals_2
        del relu
        del squeeze_1
        del unsqueeze_2374
        # Source Nodes: [], Original ATen: [aten.convolution_backward, aten.native_batch_norm_backward, aten.threshold_backward]
        buf1415 = aten.convolution_backward(buf1414, primals_897, primals_1, [0], [2, 2], [3, 3], [1, 1], False, [0, 0], 1, [False, True, False])
        del buf1414
        del primals_1
        del primals_897
        buf1416 = buf1415[1]
        return (buf1416, buf1413, buf1410, buf1405, buf1401, buf1398, buf1388, buf1384, buf1381, buf1379, buf1375, buf1372, buf1370, buf1366, buf1363, buf1361, buf1357, buf1354, buf1352, buf1348, buf1345, buf1343, buf1339, buf1336, buf1334, buf1330, buf1327, buf1324, buf1321, buf1310, buf1317, buf1314, buf1310, buf1309, buf1305, buf1302, buf1291, buf1288, buf1284, buf1282, buf1279, buf1275, buf1273, buf1270, buf1266, buf1264, buf1261, buf1257, buf1255, buf1252, buf1248, buf1246, buf1243, buf1239, buf1237, buf1233, buf1230, buf1228, buf1224, buf1221, buf1219, buf1215, buf1212, buf1201, buf1198, buf1194, buf1192, buf1189, buf1185, buf1183, buf1180, buf1176, buf1174, buf1171, buf1167, buf1165, buf1162, buf1158, buf1156, buf1153, buf1149, buf1147, buf1143, buf1140, buf1138, buf1135, buf1131, buf1130, buf1126, buf1123, buf1113, buf1109, buf1106, buf1104, buf1100, buf1097, buf1095, buf1091, buf1088, buf1086, buf1082, buf1079, buf1077, buf1073, buf1070, buf1068, buf1064, buf1061, buf1059, buf1055, buf1052, buf1049, buf1045, buf1035, buf1042, buf1038, buf1035, buf1033, buf1029, buf1026, buf1015, buf1012, buf1008, buf1007, buf1004, buf1000, buf999, buf996, buf992, buf991, buf988, buf984, buf983, buf980, buf976, buf975, buf972, buf968, buf967, buf963, buf960, buf958, buf955, buf951, buf950, buf946, buf943, buf932, buf929, buf925, buf924, buf921, buf917, buf916, buf913, buf909, buf908, buf905, buf901, buf900, buf897, buf893, buf892, buf889, buf885, buf884, buf880, buf877, buf875, buf871, buf868, buf866, buf862, buf859, buf848, buf845, buf841, buf840, buf837, buf833, buf832, buf829, buf825, buf824, buf821, buf817, buf816, buf813, buf809, buf808, buf805, buf801, buf800, buf796, buf793, buf791, buf788, buf784, buf783, buf779, buf776, buf766, buf762, buf759, buf757, buf753, buf750, buf748, buf744, buf741, buf739, buf735, buf732, buf730, buf726, buf723, buf721, buf717, buf714, buf712, buf708, buf705, buf702, buf698, buf688, buf695, buf691, buf688, buf686, buf682, buf679, buf668, buf665, buf661, buf660, buf657, buf653, buf652, buf649, buf645, buf644, buf641, buf637, buf636, buf633, buf629, buf628, buf625, buf621, buf620, buf616, buf613, buf611, buf608, buf604, buf603, buf599, buf596, buf585, buf582, buf578, buf577, buf574, buf570, buf569, buf566, buf562, buf561, buf558, buf554, buf553, buf550, buf546, buf545, buf542, buf538, buf537, buf533, buf530, buf528, buf524, buf521, buf519, buf515, buf512, buf501, buf498, buf494, buf493, buf490, buf486, buf485, buf482, buf478, buf477, buf474, buf470, buf469, buf466, buf462, buf461, buf458, buf454, buf453, buf449, buf446, buf444, buf441, buf437, buf436, buf432, buf429, buf418, buf415, buf411, buf410, buf407, buf403, buf402, buf399, buf395, buf394, buf391, buf387, buf386, buf383, buf379, buf378, buf375, buf371, buf370, buf366, buf363, buf361, buf357, buf354, buf352, buf348, buf345, buf334, buf331, buf327, buf326, buf323, buf319, buf318, buf315, buf311, buf310, buf307, buf303, buf302, buf299, buf295, buf294, buf291, buf287, buf286, buf282, buf279, buf277, buf274, buf270, buf269, buf265, buf262, buf252, buf248, buf245, buf243, buf239, buf236, buf234, buf230, buf227, buf225, buf221, buf218, buf216, buf212, buf209, buf207, buf203, buf200, buf198, buf194, buf191, buf188, buf184, buf174, buf181, buf177, buf174, buf171, buf167, buf164, buf153, buf150, buf146, buf145, buf142, buf138, buf137, buf134, buf130, buf129, buf126, buf122, buf121, buf118, buf114, buf113, buf110, buf106, buf105, buf101, buf98, buf96, buf92, buf88, buf86, buf82, buf79, buf68, buf65, buf61, buf60, buf57, buf53, buf52, buf49, buf45, buf44, buf41, buf37, buf36, buf33, buf29, buf28, buf25, buf21, buf20, buf16, buf13, buf11, buf7, buf4, reinterpret_tensor(buf1, (1000, 2048), (2048, 1), 0), reinterpret_tensor(buf2, (1000, ), (1, ), 0), None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, )


def benchmark_compiled_module(times=10, repeat=10):
    from torch._dynamo.testing import rand_strided
    from torch._inductor.utils import print_performance
    primals_1 = rand_strided((64, 3, 7, 7), (147, 1, 21, 3), device='cuda:0', dtype=torch.float32)
    primals_2 = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_4 = rand_strided((112, 64, 1, 1), (64, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_5 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_7 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_8 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_10 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_11 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_13 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_14 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_16 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_17 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_19 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_20 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_22 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_23 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_25 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_26 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_28 = rand_strided((256, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_29 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_31 = rand_strided((256, 64, 1, 1), (64, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_32 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_34 = rand_strided((112, 256, 1, 1), (256, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_35 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_37 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_38 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_40 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_41 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_43 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_44 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_46 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_47 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_49 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_50 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_52 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_53 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_55 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_56 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_58 = rand_strided((256, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_59 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_61 = rand_strided((112, 256, 1, 1), (256, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_62 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_64 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_65 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_67 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_68 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_70 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_71 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_73 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_74 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_76 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_77 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_79 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_80 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_82 = rand_strided((14, 14, 3, 3), (126, 1, 42, 14), device='cuda:0', dtype=torch.float32)
    primals_83 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_85 = rand_strided((256, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_86 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_88 = rand_strided((224, 256, 1, 1), (256, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_89 = rand_strided((224, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_91 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_92 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_94 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_95 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_97 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_98 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_100 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_101 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_103 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_104 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_106 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_107 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_109 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_110 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_112 = rand_strided((512, 224, 1, 1), (224, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_113 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_115 = rand_strided((512, 256, 1, 1), (256, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_116 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_118 = rand_strided((224, 512, 1, 1), (512, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_119 = rand_strided((224, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_121 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_122 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_124 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_125 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_127 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_128 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_130 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_131 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_133 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_134 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_136 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_137 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_139 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_140 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_142 = rand_strided((512, 224, 1, 1), (224, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_143 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_145 = rand_strided((224, 512, 1, 1), (512, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_146 = rand_strided((224, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_148 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_149 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_151 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_152 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_154 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_155 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_157 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_158 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_160 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_161 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_163 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_164 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_166 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_167 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_169 = rand_strided((512, 224, 1, 1), (224, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_170 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_172 = rand_strided((224, 512, 1, 1), (512, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_173 = rand_strided((224, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_175 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_176 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_178 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_179 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_181 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_182 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_184 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_185 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_187 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_188 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_190 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_191 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_193 = rand_strided((28, 28, 3, 3), (252, 1, 84, 28), device='cuda:0', dtype=torch.float32)
    primals_194 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_196 = rand_strided((512, 224, 1, 1), (224, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_197 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_199 = rand_strided((448, 512, 1, 1), (512, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_200 = rand_strided((448, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_202 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_203 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_205 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_206 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_208 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_209 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_211 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_212 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_214 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_215 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_217 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_218 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_220 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_221 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_223 = rand_strided((1024, 448, 1, 1), (448, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_224 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_226 = rand_strided((1024, 512, 1, 1), (512, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_227 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_229 = rand_strided((448, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_230 = rand_strided((448, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_232 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_233 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_235 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_236 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_238 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_239 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_241 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_242 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_244 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_245 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_247 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_248 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_250 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_251 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_253 = rand_strided((1024, 448, 1, 1), (448, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_254 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_256 = rand_strided((448, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_257 = rand_strided((448, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_259 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_260 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_262 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_263 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_265 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_266 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_268 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_269 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_271 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_272 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_274 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_275 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_277 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_278 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_280 = rand_strided((1024, 448, 1, 1), (448, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_281 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_283 = rand_strided((448, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_284 = rand_strided((448, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_286 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_287 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_289 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_290 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_292 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_293 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_295 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_296 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_298 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_299 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_301 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_302 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_304 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_305 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_307 = rand_strided((1024, 448, 1, 1), (448, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_308 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_310 = rand_strided((448, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_311 = rand_strided((448, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_313 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_314 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_316 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_317 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_319 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_320 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_322 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_323 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_325 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_326 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_328 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_329 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_331 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_332 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_334 = rand_strided((1024, 448, 1, 1), (448, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_335 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_337 = rand_strided((448, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_338 = rand_strided((448, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_340 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_341 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_343 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_344 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_346 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_347 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_349 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_350 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_352 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_353 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_355 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_356 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_358 = rand_strided((56, 56, 3, 3), (504, 1, 168, 56), device='cuda:0', dtype=torch.float32)
    primals_359 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_361 = rand_strided((1024, 448, 1, 1), (448, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_362 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_364 = rand_strided((896, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_365 = rand_strided((896, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_367 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_368 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_370 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_371 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_373 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_374 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_376 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_377 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_379 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_380 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_382 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_383 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_385 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_386 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_388 = rand_strided((2048, 896, 1, 1), (896, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_389 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_391 = rand_strided((2048, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_392 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_394 = rand_strided((896, 2048, 1, 1), (2048, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_395 = rand_strided((896, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_397 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_398 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_400 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_401 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_403 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_404 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_406 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_407 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_409 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_410 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_412 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_413 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_415 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_416 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_418 = rand_strided((2048, 896, 1, 1), (896, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_419 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_421 = rand_strided((896, 2048, 1, 1), (2048, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_422 = rand_strided((896, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_424 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_425 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_427 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_428 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_430 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_431 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_433 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_434 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_436 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_437 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_439 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_440 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_442 = rand_strided((112, 112, 3, 3), (1008, 1, 336, 112), device='cuda:0', dtype=torch.float32)
    primals_443 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_445 = rand_strided((2048, 896, 1, 1), (896, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    primals_446 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float32)
    primals_897 = rand_strided((8, 3, 224, 224), (150528, 1, 672, 3), device='cuda:0', dtype=torch.float32)
    convolution = rand_strided((8, 64, 112, 112), (802816, 1, 7168, 64), device='cuda:0', dtype=torch.float32)
    squeeze_1 = rand_strided((64, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu = rand_strided((8, 64, 112, 112), (802816, 1, 7168, 64), device='cuda:0', dtype=torch.float32)
    getitem_2 = rand_strided((8, 64, 56, 56), (200704, 1, 3584, 64), device='cuda:0', dtype=torch.float32)
    getitem_3 = rand_strided((8, 64, 56, 56), (200704, 1, 3584, 64), device='cuda:0', dtype=torch.int64)
    convolution_1 = rand_strided((8, 112, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    squeeze_4 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_14 = rand_strided((8, 14, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    convolution_2 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_7 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_25 = rand_strided((8, 14, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    convolution_3 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_10 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_36 = rand_strided((8, 14, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    convolution_4 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_13 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_47 = rand_strided((8, 14, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    convolution_5 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_16 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_58 = rand_strided((8, 14, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    convolution_6 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_19 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_69 = rand_strided((8, 14, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    convolution_7 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_22 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_80 = rand_strided((8, 14, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    convolution_8 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_25 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_91 = rand_strided((8, 14, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    cat = rand_strided((8, 112, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    convolution_9 = rand_strided((8, 256, 56, 56), (802816, 1, 14336, 256), device='cuda:0', dtype=torch.float32)
    squeeze_28 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_10 = rand_strided((8, 256, 56, 56), (802816, 1, 14336, 256), device='cuda:0', dtype=torch.float32)
    squeeze_31 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_9 = rand_strided((8, 256, 56, 56), (802816, 1, 14336, 256), device='cuda:0', dtype=torch.float32)
    convolution_11 = rand_strided((8, 112, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    squeeze_34 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_106 = rand_strided((8, 14, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    convolution_12 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_37 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_66 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    convolution_13 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_40 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_72 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    convolution_14 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_43 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_78 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    convolution_15 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_46 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_84 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    convolution_16 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_49 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_90 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    convolution_17 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_52 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_96 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    convolution_18 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_55 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    cat_1 = rand_strided((8, 112, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    convolution_19 = rand_strided((8, 256, 56, 56), (802816, 1, 14336, 256), device='cuda:0', dtype=torch.float32)
    squeeze_58 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_18 = rand_strided((8, 256, 56, 56), (802816, 1, 14336, 256), device='cuda:0', dtype=torch.float32)
    convolution_20 = rand_strided((8, 112, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    squeeze_61 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_196 = rand_strided((8, 14, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    convolution_21 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_64 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_118 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    convolution_22 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_67 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_124 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    convolution_23 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_70 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_130 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    convolution_24 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_73 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_136 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    convolution_25 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_76 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_142 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    convolution_26 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_79 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_148 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    convolution_27 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.float32)
    squeeze_82 = rand_strided((14, ), (1, ), device='cuda:0', dtype=torch.float32)
    cat_2 = rand_strided((8, 112, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.float32)
    convolution_28 = rand_strided((8, 256, 56, 56), (802816, 1, 14336, 256), device='cuda:0', dtype=torch.float32)
    squeeze_85 = rand_strided((256, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_27 = rand_strided((8, 256, 56, 56), (802816, 1, 14336, 256), device='cuda:0', dtype=torch.float32)
    convolution_29 = rand_strided((8, 224, 56, 56), (702464, 1, 12544, 224), device='cuda:0', dtype=torch.float32)
    squeeze_88 = rand_strided((224, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_286 = rand_strided((8, 28, 56, 56), (702464, 1, 12544, 224), device='cuda:0', dtype=torch.float32)
    convolution_30 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_91 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_297 = rand_strided((8, 28, 56, 56), (702464, 1, 12544, 224), device='cuda:0', dtype=torch.float32)
    convolution_31 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_94 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_308 = rand_strided((8, 28, 56, 56), (702464, 1, 12544, 224), device='cuda:0', dtype=torch.float32)
    convolution_32 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_97 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_319 = rand_strided((8, 28, 56, 56), (702464, 1, 12544, 224), device='cuda:0', dtype=torch.float32)
    convolution_33 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_100 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_330 = rand_strided((8, 28, 56, 56), (702464, 1, 12544, 224), device='cuda:0', dtype=torch.float32)
    convolution_34 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_103 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_341 = rand_strided((8, 28, 56, 56), (702464, 1, 12544, 224), device='cuda:0', dtype=torch.float32)
    convolution_35 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_106 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_352 = rand_strided((8, 28, 56, 56), (702464, 1, 12544, 224), device='cuda:0', dtype=torch.float32)
    convolution_36 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_109 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_363 = rand_strided((8, 28, 56, 56), (702464, 1, 12544, 224), device='cuda:0', dtype=torch.float32)
    cat_3 = rand_strided((8, 224, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.float32)
    convolution_37 = rand_strided((8, 512, 28, 28), (401408, 1, 14336, 512), device='cuda:0', dtype=torch.float32)
    squeeze_112 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_38 = rand_strided((8, 512, 28, 28), (401408, 1, 14336, 512), device='cuda:0', dtype=torch.float32)
    squeeze_115 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_36 = rand_strided((8, 512, 28, 28), (401408, 1, 14336, 512), device='cuda:0', dtype=torch.float32)
    convolution_39 = rand_strided((8, 224, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.float32)
    squeeze_118 = rand_strided((224, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_378 = rand_strided((8, 28, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.float32)
    convolution_40 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_121 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_221 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_41 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_124 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_227 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_42 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_127 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_233 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_43 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_130 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_239 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_44 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_133 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_245 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_45 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_136 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_251 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_46 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_139 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    cat_4 = rand_strided((8, 224, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.float32)
    convolution_47 = rand_strided((8, 512, 28, 28), (401408, 1, 14336, 512), device='cuda:0', dtype=torch.float32)
    squeeze_142 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_45 = rand_strided((8, 512, 28, 28), (401408, 1, 14336, 512), device='cuda:0', dtype=torch.float32)
    convolution_48 = rand_strided((8, 224, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.float32)
    squeeze_145 = rand_strided((224, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_468 = rand_strided((8, 28, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.float32)
    convolution_49 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_148 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_273 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_50 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_151 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_279 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_51 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_154 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_285 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_52 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_157 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_291 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_53 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_160 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_297 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_54 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_163 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_303 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_55 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_166 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    cat_5 = rand_strided((8, 224, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.float32)
    convolution_56 = rand_strided((8, 512, 28, 28), (401408, 1, 14336, 512), device='cuda:0', dtype=torch.float32)
    squeeze_169 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_54 = rand_strided((8, 512, 28, 28), (401408, 1, 14336, 512), device='cuda:0', dtype=torch.float32)
    convolution_57 = rand_strided((8, 224, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.float32)
    squeeze_172 = rand_strided((224, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_558 = rand_strided((8, 28, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.float32)
    convolution_58 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_175 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_325 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_59 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_178 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_331 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_60 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_181 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_337 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_61 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_184 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_343 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_62 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_187 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_349 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_63 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_190 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_355 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    convolution_64 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.float32)
    squeeze_193 = rand_strided((28, ), (1, ), device='cuda:0', dtype=torch.float32)
    cat_6 = rand_strided((8, 224, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.float32)
    convolution_65 = rand_strided((8, 512, 28, 28), (401408, 1, 14336, 512), device='cuda:0', dtype=torch.float32)
    squeeze_196 = rand_strided((512, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_63 = rand_strided((8, 512, 28, 28), (401408, 1, 14336, 512), device='cuda:0', dtype=torch.float32)
    convolution_66 = rand_strided((8, 448, 28, 28), (351232, 1, 12544, 448), device='cuda:0', dtype=torch.float32)
    squeeze_199 = rand_strided((448, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_648 = rand_strided((8, 56, 28, 28), (351232, 1, 12544, 448), device='cuda:0', dtype=torch.float32)
    convolution_67 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_202 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_659 = rand_strided((8, 56, 28, 28), (351232, 1, 12544, 448), device='cuda:0', dtype=torch.float32)
    convolution_68 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_205 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_670 = rand_strided((8, 56, 28, 28), (351232, 1, 12544, 448), device='cuda:0', dtype=torch.float32)
    convolution_69 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_208 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_681 = rand_strided((8, 56, 28, 28), (351232, 1, 12544, 448), device='cuda:0', dtype=torch.float32)
    convolution_70 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_211 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_692 = rand_strided((8, 56, 28, 28), (351232, 1, 12544, 448), device='cuda:0', dtype=torch.float32)
    convolution_71 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_214 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_703 = rand_strided((8, 56, 28, 28), (351232, 1, 12544, 448), device='cuda:0', dtype=torch.float32)
    convolution_72 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_217 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_714 = rand_strided((8, 56, 28, 28), (351232, 1, 12544, 448), device='cuda:0', dtype=torch.float32)
    convolution_73 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_220 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_725 = rand_strided((8, 56, 28, 28), (351232, 1, 12544, 448), device='cuda:0', dtype=torch.float32)
    cat_7 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    convolution_74 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    squeeze_223 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_75 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    squeeze_226 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_72 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    convolution_76 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    squeeze_229 = rand_strided((448, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_740 = rand_strided((8, 56, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    convolution_77 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_232 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_428 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_78 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_235 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_434 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_79 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_238 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_440 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_80 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_241 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_446 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_81 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_244 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_452 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_82 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_247 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_458 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_83 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_250 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    cat_8 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    convolution_84 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    squeeze_253 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_81 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    convolution_85 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    squeeze_256 = rand_strided((448, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_830 = rand_strided((8, 56, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    convolution_86 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_259 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_480 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_87 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_262 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_486 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_88 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_265 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_492 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_89 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_268 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_498 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_90 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_271 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_504 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_91 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_274 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_510 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_92 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_277 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    cat_9 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    convolution_93 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    squeeze_280 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_90 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    convolution_94 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    squeeze_283 = rand_strided((448, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_920 = rand_strided((8, 56, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    convolution_95 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_286 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_532 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_96 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_289 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_538 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_97 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_292 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_544 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_98 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_295 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_550 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_99 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_298 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_556 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_100 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_301 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_562 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_101 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_304 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    cat_10 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    convolution_102 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    squeeze_307 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_99 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    convolution_103 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    squeeze_310 = rand_strided((448, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_1010 = rand_strided((8, 56, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    convolution_104 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_313 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_584 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_105 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_316 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_590 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_106 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_319 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_596 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_107 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_322 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_602 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_108 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_325 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_608 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_109 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_328 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_614 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_110 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_331 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    cat_11 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    convolution_111 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    squeeze_334 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_108 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    convolution_112 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    squeeze_337 = rand_strided((448, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_1100 = rand_strided((8, 56, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    convolution_113 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_340 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_636 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_114 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_343 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_642 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_115 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_346 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_648 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_116 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_349 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_654 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_117 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_352 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_660 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_118 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_355 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_666 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    convolution_119 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.float32)
    squeeze_358 = rand_strided((56, ), (1, ), device='cuda:0', dtype=torch.float32)
    cat_12 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.float32)
    convolution_120 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    squeeze_361 = rand_strided((1024, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_117 = rand_strided((8, 1024, 14, 14), (200704, 1, 14336, 1024), device='cuda:0', dtype=torch.float32)
    convolution_121 = rand_strided((8, 896, 14, 14), (175616, 1, 12544, 896), device='cuda:0', dtype=torch.float32)
    squeeze_364 = rand_strided((896, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_1190 = rand_strided((8, 112, 14, 14), (175616, 1, 12544, 896), device='cuda:0', dtype=torch.float32)
    convolution_122 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_367 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_1201 = rand_strided((8, 112, 14, 14), (175616, 1, 12544, 896), device='cuda:0', dtype=torch.float32)
    convolution_123 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_370 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_1212 = rand_strided((8, 112, 14, 14), (175616, 1, 12544, 896), device='cuda:0', dtype=torch.float32)
    convolution_124 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_373 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_1223 = rand_strided((8, 112, 14, 14), (175616, 1, 12544, 896), device='cuda:0', dtype=torch.float32)
    convolution_125 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_376 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_1234 = rand_strided((8, 112, 14, 14), (175616, 1, 12544, 896), device='cuda:0', dtype=torch.float32)
    convolution_126 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_379 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_1245 = rand_strided((8, 112, 14, 14), (175616, 1, 12544, 896), device='cuda:0', dtype=torch.float32)
    convolution_127 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_382 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_1256 = rand_strided((8, 112, 14, 14), (175616, 1, 12544, 896), device='cuda:0', dtype=torch.float32)
    convolution_128 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_385 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_1267 = rand_strided((8, 112, 14, 14), (175616, 1, 12544, 896), device='cuda:0', dtype=torch.float32)
    cat_13 = rand_strided((8, 896, 7, 7), (43904, 1, 6272, 896), device='cuda:0', dtype=torch.float32)
    convolution_129 = rand_strided((8, 2048, 7, 7), (100352, 1, 14336, 2048), device='cuda:0', dtype=torch.float32)
    squeeze_388 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float32)
    convolution_130 = rand_strided((8, 2048, 7, 7), (100352, 1, 14336, 2048), device='cuda:0', dtype=torch.float32)
    squeeze_391 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_126 = rand_strided((8, 2048, 7, 7), (100352, 1, 14336, 2048), device='cuda:0', dtype=torch.float32)
    convolution_131 = rand_strided((8, 896, 7, 7), (43904, 1, 6272, 896), device='cuda:0', dtype=torch.float32)
    squeeze_394 = rand_strided((896, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_1282 = rand_strided((8, 112, 7, 7), (43904, 1, 6272, 896), device='cuda:0', dtype=torch.float32)
    convolution_132 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_397 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_739 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    convolution_133 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_400 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_745 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    convolution_134 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_403 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_751 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    convolution_135 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_406 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_757 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    convolution_136 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_409 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_763 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    convolution_137 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_412 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_769 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    convolution_138 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_415 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    cat_14 = rand_strided((8, 896, 7, 7), (43904, 1, 6272, 896), device='cuda:0', dtype=torch.float32)
    convolution_139 = rand_strided((8, 2048, 7, 7), (100352, 1, 14336, 2048), device='cuda:0', dtype=torch.float32)
    squeeze_418 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float32)
    relu_135 = rand_strided((8, 2048, 7, 7), (100352, 1, 14336, 2048), device='cuda:0', dtype=torch.float32)
    convolution_140 = rand_strided((8, 896, 7, 7), (43904, 1, 6272, 896), device='cuda:0', dtype=torch.float32)
    squeeze_421 = rand_strided((896, ), (1, ), device='cuda:0', dtype=torch.float32)
    getitem_1372 = rand_strided((8, 112, 7, 7), (43904, 1, 6272, 896), device='cuda:0', dtype=torch.float32)
    convolution_141 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_424 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_791 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    convolution_142 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_427 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_797 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    convolution_143 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_430 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_803 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    convolution_144 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_433 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_809 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    convolution_145 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_436 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_815 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    convolution_146 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_439 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    add_821 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    convolution_147 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.float32)
    squeeze_442 = rand_strided((112, ), (1, ), device='cuda:0', dtype=torch.float32)
    cat_15 = rand_strided((8, 896, 7, 7), (43904, 1, 6272, 896), device='cuda:0', dtype=torch.float32)
    convolution_148 = rand_strided((8, 2048, 7, 7), (100352, 1, 14336, 2048), device='cuda:0', dtype=torch.float32)
    squeeze_445 = rand_strided((2048, ), (1, ), device='cuda:0', dtype=torch.float32)
    view = rand_strided((8, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)
    permute_1 = rand_strided((1000, 2048), (2048, 1), device='cuda:0', dtype=torch.float32)
    le = rand_strided((8, 2048, 7, 7), (100352, 1, 14336, 2048), device='cuda:0', dtype=torch.bool)
    unsqueeze_598 = rand_strided((1, 2048, 1, 1), (2048, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_1 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_610 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_2 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_622 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_3 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_634 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_4 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_646 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_5 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_658 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_6 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_670 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_7 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_682 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_8 = rand_strided((8, 896, 7, 7), (43904, 1, 6272, 896), device='cuda:0', dtype=torch.bool)
    unsqueeze_694 = rand_strided((1, 896, 1, 1), (896, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_706 = rand_strided((1, 2048, 1, 1), (2048, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_10 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_718 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_11 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_730 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_12 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_742 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_13 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_754 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_14 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_766 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_15 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_778 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_16 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_790 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_17 = rand_strided((8, 896, 7, 7), (43904, 1, 6272, 896), device='cuda:0', dtype=torch.bool)
    unsqueeze_802 = rand_strided((1, 896, 1, 1), (896, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_814 = rand_strided((1, 2048, 1, 1), (2048, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_826 = rand_strided((1, 2048, 1, 1), (2048, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_19 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_838 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_20 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_850 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_21 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_862 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_22 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_874 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_23 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_886 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_24 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_898 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_25 = rand_strided((8, 112, 7, 7), (5488, 1, 784, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_910 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_26 = rand_strided((8, 896, 14, 14), (175616, 1, 12544, 896), device='cuda:0', dtype=torch.bool)
    unsqueeze_922 = rand_strided((1, 896, 1, 1), (896, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_934 = rand_strided((1, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_28 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_946 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_29 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_958 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_30 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_970 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_31 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_982 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_32 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_994 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_33 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1006 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_34 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1018 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_35 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.bool)
    unsqueeze_1030 = rand_strided((1, 448, 1, 1), (448, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1042 = rand_strided((1, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_37 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1054 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_38 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1066 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_39 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1078 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_40 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1090 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_41 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1102 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_42 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1114 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_43 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1126 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_44 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.bool)
    unsqueeze_1138 = rand_strided((1, 448, 1, 1), (448, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1150 = rand_strided((1, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_46 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1162 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_47 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1174 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_48 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1186 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_49 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1198 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_50 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1210 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_51 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1222 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_52 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1234 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_53 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.bool)
    unsqueeze_1246 = rand_strided((1, 448, 1, 1), (448, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1258 = rand_strided((1, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_55 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1270 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_56 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1282 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_57 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1294 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_58 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1306 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_59 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1318 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_60 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1330 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_61 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1342 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_62 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.bool)
    unsqueeze_1354 = rand_strided((1, 448, 1, 1), (448, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1366 = rand_strided((1, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_64 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1378 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_65 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1390 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_66 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1402 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_67 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1414 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_68 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1426 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_69 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1438 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_70 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1450 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_71 = rand_strided((8, 448, 14, 14), (87808, 1, 6272, 448), device='cuda:0', dtype=torch.bool)
    unsqueeze_1462 = rand_strided((1, 448, 1, 1), (448, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1474 = rand_strided((1, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1486 = rand_strided((1, 1024, 1, 1), (1024, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_73 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1498 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_74 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1510 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_75 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1522 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_76 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1534 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_77 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1546 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_78 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1558 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_79 = rand_strided((8, 56, 14, 14), (10976, 1, 784, 56), device='cuda:0', dtype=torch.bool)
    unsqueeze_1570 = rand_strided((1, 56, 1, 1), (56, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_80 = rand_strided((8, 448, 28, 28), (351232, 1, 12544, 448), device='cuda:0', dtype=torch.bool)
    unsqueeze_1582 = rand_strided((1, 448, 1, 1), (448, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1594 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_82 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1606 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_83 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1618 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_84 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1630 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_85 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1642 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_86 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1654 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_87 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1666 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_88 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1678 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_89 = rand_strided((8, 224, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.bool)
    unsqueeze_1690 = rand_strided((1, 224, 1, 1), (224, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1702 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_91 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1714 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_92 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1726 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_93 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1738 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_94 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1750 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_95 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1762 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_96 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1774 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_97 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1786 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_98 = rand_strided((8, 224, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.bool)
    unsqueeze_1798 = rand_strided((1, 224, 1, 1), (224, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1810 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_100 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1822 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_101 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1834 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_102 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1846 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_103 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1858 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_104 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1870 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_105 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1882 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_106 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1894 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_107 = rand_strided((8, 224, 28, 28), (175616, 1, 6272, 224), device='cuda:0', dtype=torch.bool)
    unsqueeze_1906 = rand_strided((1, 224, 1, 1), (224, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1918 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_1930 = rand_strided((1, 512, 1, 1), (512, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_109 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1942 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_110 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1954 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_111 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1966 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_112 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1978 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_113 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_1990 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_114 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_2002 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_115 = rand_strided((8, 28, 28, 28), (21952, 1, 784, 28), device='cuda:0', dtype=torch.bool)
    unsqueeze_2014 = rand_strided((1, 28, 1, 1), (28, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_116 = rand_strided((8, 224, 56, 56), (702464, 1, 12544, 224), device='cuda:0', dtype=torch.bool)
    unsqueeze_2026 = rand_strided((1, 224, 1, 1), (224, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2038 = rand_strided((1, 256, 1, 1), (256, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_118 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2050 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_119 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2062 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_120 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2074 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_121 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2086 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_122 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2098 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_123 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2110 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_124 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2122 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_125 = rand_strided((8, 112, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_2134 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2146 = rand_strided((1, 256, 1, 1), (256, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_127 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2158 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_128 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2170 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_129 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2182 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_130 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2194 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_131 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2206 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_132 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2218 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_133 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2230 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_134 = rand_strided((8, 112, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_2242 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2254 = rand_strided((1, 256, 1, 1), (256, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2266 = rand_strided((1, 256, 1, 1), (256, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_136 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2278 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_137 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2290 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_138 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2302 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_139 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2314 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_140 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2326 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_141 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2338 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_142 = rand_strided((8, 14, 56, 56), (43904, 1, 784, 14), device='cuda:0', dtype=torch.bool)
    unsqueeze_2350 = rand_strided((1, 14, 1, 1), (14, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    le_143 = rand_strided((8, 112, 56, 56), (351232, 1, 6272, 112), device='cuda:0', dtype=torch.bool)
    unsqueeze_2362 = rand_strided((1, 112, 1, 1), (112, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    unsqueeze_2374 = rand_strided((1, 64, 1, 1), (64, 1, 1, 1), device='cuda:0', dtype=torch.float32)
    tangents_1 = rand_strided((8, 1000), (1000, 1), device='cuda:0', dtype=torch.float32)
    return print_performance(lambda: call([primals_1, primals_2, primals_4, primals_5, primals_7, primals_8, primals_10, primals_11, primals_13, primals_14, primals_16, primals_17, primals_19, primals_20, primals_22, primals_23, primals_25, primals_26, primals_28, primals_29, primals_31, primals_32, primals_34, primals_35, primals_37, primals_38, primals_40, primals_41, primals_43, primals_44, primals_46, primals_47, primals_49, primals_50, primals_52, primals_53, primals_55, primals_56, primals_58, primals_59, primals_61, primals_62, primals_64, primals_65, primals_67, primals_68, primals_70, primals_71, primals_73, primals_74, primals_76, primals_77, primals_79, primals_80, primals_82, primals_83, primals_85, primals_86, primals_88, primals_89, primals_91, primals_92, primals_94, primals_95, primals_97, primals_98, primals_100, primals_101, primals_103, primals_104, primals_106, primals_107, primals_109, primals_110, primals_112, primals_113, primals_115, primals_116, primals_118, primals_119, primals_121, primals_122, primals_124, primals_125, primals_127, primals_128, primals_130, primals_131, primals_133, primals_134, primals_136, primals_137, primals_139, primals_140, primals_142, primals_143, primals_145, primals_146, primals_148, primals_149, primals_151, primals_152, primals_154, primals_155, primals_157, primals_158, primals_160, primals_161, primals_163, primals_164, primals_166, primals_167, primals_169, primals_170, primals_172, primals_173, primals_175, primals_176, primals_178, primals_179, primals_181, primals_182, primals_184, primals_185, primals_187, primals_188, primals_190, primals_191, primals_193, primals_194, primals_196, primals_197, primals_199, primals_200, primals_202, primals_203, primals_205, primals_206, primals_208, primals_209, primals_211, primals_212, primals_214, primals_215, primals_217, primals_218, primals_220, primals_221, primals_223, primals_224, primals_226, primals_227, primals_229, primals_230, primals_232, primals_233, primals_235, primals_236, primals_238, primals_239, primals_241, primals_242, primals_244, primals_245, primals_247, primals_248, primals_250, primals_251, primals_253, primals_254, primals_256, primals_257, primals_259, primals_260, primals_262, primals_263, primals_265, primals_266, primals_268, primals_269, primals_271, primals_272, primals_274, primals_275, primals_277, primals_278, primals_280, primals_281, primals_283, primals_284, primals_286, primals_287, primals_289, primals_290, primals_292, primals_293, primals_295, primals_296, primals_298, primals_299, primals_301, primals_302, primals_304, primals_305, primals_307, primals_308, primals_310, primals_311, primals_313, primals_314, primals_316, primals_317, primals_319, primals_320, primals_322, primals_323, primals_325, primals_326, primals_328, primals_329, primals_331, primals_332, primals_334, primals_335, primals_337, primals_338, primals_340, primals_341, primals_343, primals_344, primals_346, primals_347, primals_349, primals_350, primals_352, primals_353, primals_355, primals_356, primals_358, primals_359, primals_361, primals_362, primals_364, primals_365, primals_367, primals_368, primals_370, primals_371, primals_373, primals_374, primals_376, primals_377, primals_379, primals_380, primals_382, primals_383, primals_385, primals_386, primals_388, primals_389, primals_391, primals_392, primals_394, primals_395, primals_397, primals_398, primals_400, primals_401, primals_403, primals_404, primals_406, primals_407, primals_409, primals_410, primals_412, primals_413, primals_415, primals_416, primals_418, primals_419, primals_421, primals_422, primals_424, primals_425, primals_427, primals_428, primals_430, primals_431, primals_433, primals_434, primals_436, primals_437, primals_439, primals_440, primals_442, primals_443, primals_445, primals_446, primals_897, convolution, squeeze_1, relu, getitem_2, getitem_3, convolution_1, squeeze_4, getitem_14, convolution_2, squeeze_7, getitem_25, convolution_3, squeeze_10, getitem_36, convolution_4, squeeze_13, getitem_47, convolution_5, squeeze_16, getitem_58, convolution_6, squeeze_19, getitem_69, convolution_7, squeeze_22, getitem_80, convolution_8, squeeze_25, getitem_91, cat, convolution_9, squeeze_28, convolution_10, squeeze_31, relu_9, convolution_11, squeeze_34, getitem_106, convolution_12, squeeze_37, add_66, convolution_13, squeeze_40, add_72, convolution_14, squeeze_43, add_78, convolution_15, squeeze_46, add_84, convolution_16, squeeze_49, add_90, convolution_17, squeeze_52, add_96, convolution_18, squeeze_55, cat_1, convolution_19, squeeze_58, relu_18, convolution_20, squeeze_61, getitem_196, convolution_21, squeeze_64, add_118, convolution_22, squeeze_67, add_124, convolution_23, squeeze_70, add_130, convolution_24, squeeze_73, add_136, convolution_25, squeeze_76, add_142, convolution_26, squeeze_79, add_148, convolution_27, squeeze_82, cat_2, convolution_28, squeeze_85, relu_27, convolution_29, squeeze_88, getitem_286, convolution_30, squeeze_91, getitem_297, convolution_31, squeeze_94, getitem_308, convolution_32, squeeze_97, getitem_319, convolution_33, squeeze_100, getitem_330, convolution_34, squeeze_103, getitem_341, convolution_35, squeeze_106, getitem_352, convolution_36, squeeze_109, getitem_363, cat_3, convolution_37, squeeze_112, convolution_38, squeeze_115, relu_36, convolution_39, squeeze_118, getitem_378, convolution_40, squeeze_121, add_221, convolution_41, squeeze_124, add_227, convolution_42, squeeze_127, add_233, convolution_43, squeeze_130, add_239, convolution_44, squeeze_133, add_245, convolution_45, squeeze_136, add_251, convolution_46, squeeze_139, cat_4, convolution_47, squeeze_142, relu_45, convolution_48, squeeze_145, getitem_468, convolution_49, squeeze_148, add_273, convolution_50, squeeze_151, add_279, convolution_51, squeeze_154, add_285, convolution_52, squeeze_157, add_291, convolution_53, squeeze_160, add_297, convolution_54, squeeze_163, add_303, convolution_55, squeeze_166, cat_5, convolution_56, squeeze_169, relu_54, convolution_57, squeeze_172, getitem_558, convolution_58, squeeze_175, add_325, convolution_59, squeeze_178, add_331, convolution_60, squeeze_181, add_337, convolution_61, squeeze_184, add_343, convolution_62, squeeze_187, add_349, convolution_63, squeeze_190, add_355, convolution_64, squeeze_193, cat_6, convolution_65, squeeze_196, relu_63, convolution_66, squeeze_199, getitem_648, convolution_67, squeeze_202, getitem_659, convolution_68, squeeze_205, getitem_670, convolution_69, squeeze_208, getitem_681, convolution_70, squeeze_211, getitem_692, convolution_71, squeeze_214, getitem_703, convolution_72, squeeze_217, getitem_714, convolution_73, squeeze_220, getitem_725, cat_7, convolution_74, squeeze_223, convolution_75, squeeze_226, relu_72, convolution_76, squeeze_229, getitem_740, convolution_77, squeeze_232, add_428, convolution_78, squeeze_235, add_434, convolution_79, squeeze_238, add_440, convolution_80, squeeze_241, add_446, convolution_81, squeeze_244, add_452, convolution_82, squeeze_247, add_458, convolution_83, squeeze_250, cat_8, convolution_84, squeeze_253, relu_81, convolution_85, squeeze_256, getitem_830, convolution_86, squeeze_259, add_480, convolution_87, squeeze_262, add_486, convolution_88, squeeze_265, add_492, convolution_89, squeeze_268, add_498, convolution_90, squeeze_271, add_504, convolution_91, squeeze_274, add_510, convolution_92, squeeze_277, cat_9, convolution_93, squeeze_280, relu_90, convolution_94, squeeze_283, getitem_920, convolution_95, squeeze_286, add_532, convolution_96, squeeze_289, add_538, convolution_97, squeeze_292, add_544, convolution_98, squeeze_295, add_550, convolution_99, squeeze_298, add_556, convolution_100, squeeze_301, add_562, convolution_101, squeeze_304, cat_10, convolution_102, squeeze_307, relu_99, convolution_103, squeeze_310, getitem_1010, convolution_104, squeeze_313, add_584, convolution_105, squeeze_316, add_590, convolution_106, squeeze_319, add_596, convolution_107, squeeze_322, add_602, convolution_108, squeeze_325, add_608, convolution_109, squeeze_328, add_614, convolution_110, squeeze_331, cat_11, convolution_111, squeeze_334, relu_108, convolution_112, squeeze_337, getitem_1100, convolution_113, squeeze_340, add_636, convolution_114, squeeze_343, add_642, convolution_115, squeeze_346, add_648, convolution_116, squeeze_349, add_654, convolution_117, squeeze_352, add_660, convolution_118, squeeze_355, add_666, convolution_119, squeeze_358, cat_12, convolution_120, squeeze_361, relu_117, convolution_121, squeeze_364, getitem_1190, convolution_122, squeeze_367, getitem_1201, convolution_123, squeeze_370, getitem_1212, convolution_124, squeeze_373, getitem_1223, convolution_125, squeeze_376, getitem_1234, convolution_126, squeeze_379, getitem_1245, convolution_127, squeeze_382, getitem_1256, convolution_128, squeeze_385, getitem_1267, cat_13, convolution_129, squeeze_388, convolution_130, squeeze_391, relu_126, convolution_131, squeeze_394, getitem_1282, convolution_132, squeeze_397, add_739, convolution_133, squeeze_400, add_745, convolution_134, squeeze_403, add_751, convolution_135, squeeze_406, add_757, convolution_136, squeeze_409, add_763, convolution_137, squeeze_412, add_769, convolution_138, squeeze_415, cat_14, convolution_139, squeeze_418, relu_135, convolution_140, squeeze_421, getitem_1372, convolution_141, squeeze_424, add_791, convolution_142, squeeze_427, add_797, convolution_143, squeeze_430, add_803, convolution_144, squeeze_433, add_809, convolution_145, squeeze_436, add_815, convolution_146, squeeze_439, add_821, convolution_147, squeeze_442, cat_15, convolution_148, squeeze_445, view, permute_1, le, unsqueeze_598, le_1, unsqueeze_610, le_2, unsqueeze_622, le_3, unsqueeze_634, le_4, unsqueeze_646, le_5, unsqueeze_658, le_6, unsqueeze_670, le_7, unsqueeze_682, le_8, unsqueeze_694, unsqueeze_706, le_10, unsqueeze_718, le_11, unsqueeze_730, le_12, unsqueeze_742, le_13, unsqueeze_754, le_14, unsqueeze_766, le_15, unsqueeze_778, le_16, unsqueeze_790, le_17, unsqueeze_802, unsqueeze_814, unsqueeze_826, le_19, unsqueeze_838, le_20, unsqueeze_850, le_21, unsqueeze_862, le_22, unsqueeze_874, le_23, unsqueeze_886, le_24, unsqueeze_898, le_25, unsqueeze_910, le_26, unsqueeze_922, unsqueeze_934, le_28, unsqueeze_946, le_29, unsqueeze_958, le_30, unsqueeze_970, le_31, unsqueeze_982, le_32, unsqueeze_994, le_33, unsqueeze_1006, le_34, unsqueeze_1018, le_35, unsqueeze_1030, unsqueeze_1042, le_37, unsqueeze_1054, le_38, unsqueeze_1066, le_39, unsqueeze_1078, le_40, unsqueeze_1090, le_41, unsqueeze_1102, le_42, unsqueeze_1114, le_43, unsqueeze_1126, le_44, unsqueeze_1138, unsqueeze_1150, le_46, unsqueeze_1162, le_47, unsqueeze_1174, le_48, unsqueeze_1186, le_49, unsqueeze_1198, le_50, unsqueeze_1210, le_51, unsqueeze_1222, le_52, unsqueeze_1234, le_53, unsqueeze_1246, unsqueeze_1258, le_55, unsqueeze_1270, le_56, unsqueeze_1282, le_57, unsqueeze_1294, le_58, unsqueeze_1306, le_59, unsqueeze_1318, le_60, unsqueeze_1330, le_61, unsqueeze_1342, le_62, unsqueeze_1354, unsqueeze_1366, le_64, unsqueeze_1378, le_65, unsqueeze_1390, le_66, unsqueeze_1402, le_67, unsqueeze_1414, le_68, unsqueeze_1426, le_69, unsqueeze_1438, le_70, unsqueeze_1450, le_71, unsqueeze_1462, unsqueeze_1474, unsqueeze_1486, le_73, unsqueeze_1498, le_74, unsqueeze_1510, le_75, unsqueeze_1522, le_76, unsqueeze_1534, le_77, unsqueeze_1546, le_78, unsqueeze_1558, le_79, unsqueeze_1570, le_80, unsqueeze_1582, unsqueeze_1594, le_82, unsqueeze_1606, le_83, unsqueeze_1618, le_84, unsqueeze_1630, le_85, unsqueeze_1642, le_86, unsqueeze_1654, le_87, unsqueeze_1666, le_88, unsqueeze_1678, le_89, unsqueeze_1690, unsqueeze_1702, le_91, unsqueeze_1714, le_92, unsqueeze_1726, le_93, unsqueeze_1738, le_94, unsqueeze_1750, le_95, unsqueeze_1762, le_96, unsqueeze_1774, le_97, unsqueeze_1786, le_98, unsqueeze_1798, unsqueeze_1810, le_100, unsqueeze_1822, le_101, unsqueeze_1834, le_102, unsqueeze_1846, le_103, unsqueeze_1858, le_104, unsqueeze_1870, le_105, unsqueeze_1882, le_106, unsqueeze_1894, le_107, unsqueeze_1906, unsqueeze_1918, unsqueeze_1930, le_109, unsqueeze_1942, le_110, unsqueeze_1954, le_111, unsqueeze_1966, le_112, unsqueeze_1978, le_113, unsqueeze_1990, le_114, unsqueeze_2002, le_115, unsqueeze_2014, le_116, unsqueeze_2026, unsqueeze_2038, le_118, unsqueeze_2050, le_119, unsqueeze_2062, le_120, unsqueeze_2074, le_121, unsqueeze_2086, le_122, unsqueeze_2098, le_123, unsqueeze_2110, le_124, unsqueeze_2122, le_125, unsqueeze_2134, unsqueeze_2146, le_127, unsqueeze_2158, le_128, unsqueeze_2170, le_129, unsqueeze_2182, le_130, unsqueeze_2194, le_131, unsqueeze_2206, le_132, unsqueeze_2218, le_133, unsqueeze_2230, le_134, unsqueeze_2242, unsqueeze_2254, unsqueeze_2266, le_136, unsqueeze_2278, le_137, unsqueeze_2290, le_138, unsqueeze_2302, le_139, unsqueeze_2314, le_140, unsqueeze_2326, le_141, unsqueeze_2338, le_142, unsqueeze_2350, le_143, unsqueeze_2362, unsqueeze_2374, tangents_1]), times=times, repeat=repeat)


if __name__ == "__main__":
    from torch._inductor.wrapper_benchmark import compiled_module_main
    compiled_module_main('res2net50_14w_8s', benchmark_compiled_module)
