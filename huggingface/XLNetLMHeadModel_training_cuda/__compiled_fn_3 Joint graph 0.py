from __future__ import annotations



def forward(self, primals, tangents):
    primals_1: "f32[1024, 16, 64]"; primals_2: "f32[1024, 16, 64]"; primals_3: "f32[1024, 16, 64]"; primals_4: "f32[1024, 16, 64]"; primals_5: "f32[16, 64]"; primals_6: "f32[16, 64]"; primals_7: "f32[1024, 16, 64]"; primals_8: "f32[1024, 16, 64]"; primals_9: "f32[1024, 16, 64]"; primals_10: "f32[1024, 16, 64]"; primals_11: "f32[1024, 16, 64]"; primals_12: "f32[16, 64]"; primals_13: "f32[16, 64]"; primals_14: "f32[1024, 16, 64]"; primals_15: "f32[1024, 16, 64]"; primals_16: "f32[1024, 16, 64]"; primals_17: "f32[1024, 16, 64]"; primals_18: "f32[1024, 16, 64]"; primals_19: "f32[16, 64]"; primals_20: "f32[16, 64]"; primals_21: "f32[1024, 16, 64]"; primals_22: "f32[1024, 16, 64]"; primals_23: "f32[1024, 16, 64]"; primals_24: "f32[1024, 16, 64]"; primals_25: "f32[1024, 16, 64]"; primals_26: "f32[16, 64]"; primals_27: "f32[16, 64]"; primals_28: "f32[1024, 16, 64]"; primals_29: "f32[1024, 16, 64]"; primals_30: "f32[1024, 16, 64]"; primals_31: "f32[1024, 16, 64]"; primals_32: "f32[1024, 16, 64]"; primals_33: "f32[16, 64]"; primals_34: "f32[16, 64]"; primals_35: "f32[1024, 16, 64]"; primals_36: "f32[1024, 16, 64]"; primals_37: "f32[1024, 16, 64]"; primals_38: "f32[1024, 16, 64]"; primals_39: "f32[1024, 16, 64]"; primals_40: "f32[16, 64]"; primals_41: "f32[16, 64]"; primals_42: "f32[1024, 16, 64]"; primals_43: "f32[1024, 16, 64]"; primals_44: "f32[1024, 16, 64]"; primals_45: "f32[1024, 16, 64]"; primals_46: "f32[1024, 16, 64]"; primals_47: "f32[16, 64]"; primals_48: "f32[16, 64]"; primals_49: "f32[1024, 16, 64]"; primals_50: "f32[1024, 16, 64]"; primals_51: "f32[1024, 16, 64]"; primals_52: "f32[1024, 16, 64]"; primals_53: "f32[1024, 16, 64]"; primals_54: "f32[16, 64]"; primals_55: "f32[16, 64]"; primals_56: "f32[1024, 16, 64]"; primals_57: "f32[1024, 16, 64]"; primals_58: "f32[1024, 16, 64]"; primals_59: "f32[1024, 16, 64]"; primals_60: "f32[1024, 16, 64]"; primals_61: "f32[16, 64]"; primals_62: "f32[16, 64]"; primals_63: "f32[1024, 16, 64]"; primals_64: "f32[1024, 16, 64]"; primals_65: "f32[1024, 16, 64]"; primals_66: "f32[1024, 16, 64]"; primals_67: "f32[1024, 16, 64]"; primals_68: "f32[16, 64]"; primals_69: "f32[16, 64]"; primals_70: "f32[1024, 16, 64]"; primals_71: "f32[1024, 16, 64]"; primals_72: "f32[1024, 16, 64]"; primals_73: "f32[1024, 16, 64]"; primals_74: "f32[1024, 16, 64]"; primals_75: "f32[16, 64]"; primals_76: "f32[16, 64]"; primals_77: "f32[1024, 16, 64]"; primals_78: "f32[1024, 16, 64]"; primals_79: "f32[1024, 16, 64]"; primals_80: "f32[1024, 16, 64]"; primals_81: "f32[1024, 16, 64]"; primals_82: "f32[16, 64]"; primals_83: "f32[16, 64]"; primals_84: "f32[1024, 16, 64]"; primals_85: "f32[1024, 16, 64]"; primals_86: "f32[1024, 16, 64]"; primals_87: "f32[1024, 16, 64]"; primals_88: "f32[1024, 16, 64]"; primals_89: "f32[16, 64]"; primals_90: "f32[16, 64]"; primals_91: "f32[1024, 16, 64]"; primals_92: "f32[1024, 16, 64]"; primals_93: "f32[1024, 16, 64]"; primals_94: "f32[1024, 16, 64]"; primals_95: "f32[1024, 16, 64]"; primals_96: "f32[16, 64]"; primals_97: "f32[16, 64]"; primals_98: "f32[1024, 16, 64]"; primals_99: "f32[1024, 16, 64]"; primals_100: "f32[1024, 16, 64]"; primals_101: "f32[1024, 16, 64]"; primals_102: "f32[1024, 16, 64]"; primals_103: "f32[16, 64]"; primals_104: "f32[16, 64]"; primals_105: "f32[1024, 16, 64]"; primals_106: "f32[1024, 16, 64]"; primals_107: "f32[1024, 16, 64]"; primals_108: "f32[1024, 16, 64]"; primals_109: "f32[1024, 16, 64]"; primals_110: "f32[16, 64]"; primals_111: "f32[16, 64]"; primals_112: "f32[1024, 16, 64]"; primals_113: "f32[1024, 16, 64]"; primals_114: "f32[1024, 16, 64]"; primals_115: "f32[1024, 16, 64]"; primals_116: "f32[1024, 16, 64]"; primals_117: "f32[16, 64]"; primals_118: "f32[16, 64]"; primals_119: "f32[1024, 16, 64]"; primals_120: "f32[1024, 16, 64]"; primals_121: "f32[1024, 16, 64]"; primals_122: "f32[1024, 16, 64]"; primals_123: "f32[1024, 16, 64]"; primals_124: "f32[16, 64]"; primals_125: "f32[16, 64]"; primals_126: "f32[1024, 16, 64]"; primals_127: "f32[1024, 16, 64]"; primals_128: "f32[1024, 16, 64]"; primals_129: "f32[1024, 16, 64]"; primals_130: "f32[1024, 16, 64]"; primals_131: "f32[16, 64]"; primals_132: "f32[16, 64]"; primals_133: "f32[1024, 16, 64]"; primals_134: "f32[1024, 16, 64]"; primals_135: "f32[1024, 16, 64]"; primals_136: "f32[1024, 16, 64]"; primals_137: "f32[1024, 16, 64]"; primals_138: "f32[16, 64]"; primals_139: "f32[16, 64]"; primals_140: "f32[1024, 16, 64]"; primals_141: "f32[1024, 16, 64]"; primals_142: "f32[1024, 16, 64]"; primals_143: "f32[1024, 16, 64]"; primals_144: "f32[1024, 16, 64]"; primals_145: "f32[16, 64]"; primals_146: "f32[16, 64]"; primals_147: "f32[1024, 16, 64]"; primals_148: "f32[1024, 16, 64]"; primals_149: "f32[1024, 16, 64]"; primals_150: "f32[1024, 16, 64]"; primals_151: "f32[1024, 16, 64]"; primals_152: "f32[16, 64]"; primals_153: "f32[16, 64]"; primals_154: "f32[1024, 16, 64]"; primals_155: "f32[1024, 16, 64]"; primals_156: "f32[1024, 16, 64]"; primals_157: "f32[1024, 16, 64]"; primals_158: "f32[1024, 16, 64]"; primals_159: "f32[16, 64]"; primals_160: "f32[16, 64]"; primals_161: "f32[1024, 16, 64]"; primals_162: "f32[1024, 16, 64]"; primals_163: "f32[1024, 16, 64]"; primals_164: "f32[1024, 16, 64]"; primals_165: "f32[1024, 16, 64]"; primals_166: "f32[16, 64]"; primals_167: "f32[16, 64]"; primals_168: "f32[1024, 16, 64]"; primals_169: "f32[32000, 1024]"; primals_170: "f32[1024]"; primals_171: "f32[1024]"; primals_172: "f32[4096, 1024]"; primals_173: "f32[4096]"; primals_174: "f32[1024, 4096]"; primals_175: "f32[1024]"; primals_176: "f32[1024]"; primals_177: "f32[1024]"; primals_178: "f32[1024]"; primals_179: "f32[1024]"; primals_180: "f32[4096, 1024]"; primals_181: "f32[4096]"; primals_182: "f32[1024, 4096]"; primals_183: "f32[1024]"; primals_184: "f32[1024]"; primals_185: "f32[1024]"; primals_186: "f32[1024]"; primals_187: "f32[1024]"; primals_188: "f32[4096, 1024]"; primals_189: "f32[4096]"; primals_190: "f32[1024, 4096]"; primals_191: "f32[1024]"; primals_192: "f32[1024]"; primals_193: "f32[1024]"; primals_194: "f32[1024]"; primals_195: "f32[1024]"; primals_196: "f32[4096, 1024]"; primals_197: "f32[4096]"; primals_198: "f32[1024, 4096]"; primals_199: "f32[1024]"; primals_200: "f32[1024]"; primals_201: "f32[1024]"; primals_202: "f32[1024]"; primals_203: "f32[1024]"; primals_204: "f32[4096, 1024]"; primals_205: "f32[4096]"; primals_206: "f32[1024, 4096]"; primals_207: "f32[1024]"; primals_208: "f32[1024]"; primals_209: "f32[1024]"; primals_210: "f32[1024]"; primals_211: "f32[1024]"; primals_212: "f32[4096, 1024]"; primals_213: "f32[4096]"; primals_214: "f32[1024, 4096]"; primals_215: "f32[1024]"; primals_216: "f32[1024]"; primals_217: "f32[1024]"; primals_218: "f32[1024]"; primals_219: "f32[1024]"; primals_220: "f32[4096, 1024]"; primals_221: "f32[4096]"; primals_222: "f32[1024, 4096]"; primals_223: "f32[1024]"; primals_224: "f32[1024]"; primals_225: "f32[1024]"; primals_226: "f32[1024]"; primals_227: "f32[1024]"; primals_228: "f32[4096, 1024]"; primals_229: "f32[4096]"; primals_230: "f32[1024, 4096]"; primals_231: "f32[1024]"; primals_232: "f32[1024]"; primals_233: "f32[1024]"; primals_234: "f32[1024]"; primals_235: "f32[1024]"; primals_236: "f32[4096, 1024]"; primals_237: "f32[4096]"; primals_238: "f32[1024, 4096]"; primals_239: "f32[1024]"; primals_240: "f32[1024]"; primals_241: "f32[1024]"; primals_242: "f32[1024]"; primals_243: "f32[1024]"; primals_244: "f32[4096, 1024]"; primals_245: "f32[4096]"; primals_246: "f32[1024, 4096]"; primals_247: "f32[1024]"; primals_248: "f32[1024]"; primals_249: "f32[1024]"; primals_250: "f32[1024]"; primals_251: "f32[1024]"; primals_252: "f32[4096, 1024]"; primals_253: "f32[4096]"; primals_254: "f32[1024, 4096]"; primals_255: "f32[1024]"; primals_256: "f32[1024]"; primals_257: "f32[1024]"; primals_258: "f32[1024]"; primals_259: "f32[1024]"; primals_260: "f32[4096, 1024]"; primals_261: "f32[4096]"; primals_262: "f32[1024, 4096]"; primals_263: "f32[1024]"; primals_264: "f32[1024]"; primals_265: "f32[1024]"; primals_266: "f32[1024]"; primals_267: "f32[1024]"; primals_268: "f32[4096, 1024]"; primals_269: "f32[4096]"; primals_270: "f32[1024, 4096]"; primals_271: "f32[1024]"; primals_272: "f32[1024]"; primals_273: "f32[1024]"; primals_274: "f32[1024]"; primals_275: "f32[1024]"; primals_276: "f32[4096, 1024]"; primals_277: "f32[4096]"; primals_278: "f32[1024, 4096]"; primals_279: "f32[1024]"; primals_280: "f32[1024]"; primals_281: "f32[1024]"; primals_282: "f32[1024]"; primals_283: "f32[1024]"; primals_284: "f32[4096, 1024]"; primals_285: "f32[4096]"; primals_286: "f32[1024, 4096]"; primals_287: "f32[1024]"; primals_288: "f32[1024]"; primals_289: "f32[1024]"; primals_290: "f32[1024]"; primals_291: "f32[1024]"; primals_292: "f32[4096, 1024]"; primals_293: "f32[4096]"; primals_294: "f32[1024, 4096]"; primals_295: "f32[1024]"; primals_296: "f32[1024]"; primals_297: "f32[1024]"; primals_298: "f32[1024]"; primals_299: "f32[1024]"; primals_300: "f32[4096, 1024]"; primals_301: "f32[4096]"; primals_302: "f32[1024, 4096]"; primals_303: "f32[1024]"; primals_304: "f32[1024]"; primals_305: "f32[1024]"; primals_306: "f32[1024]"; primals_307: "f32[1024]"; primals_308: "f32[4096, 1024]"; primals_309: "f32[4096]"; primals_310: "f32[1024, 4096]"; primals_311: "f32[1024]"; primals_312: "f32[1024]"; primals_313: "f32[1024]"; primals_314: "f32[1024]"; primals_315: "f32[1024]"; primals_316: "f32[4096, 1024]"; primals_317: "f32[4096]"; primals_318: "f32[1024, 4096]"; primals_319: "f32[1024]"; primals_320: "f32[1024]"; primals_321: "f32[1024]"; primals_322: "f32[1024]"; primals_323: "f32[1024]"; primals_324: "f32[4096, 1024]"; primals_325: "f32[4096]"; primals_326: "f32[1024, 4096]"; primals_327: "f32[1024]"; primals_328: "f32[1024]"; primals_329: "f32[1024]"; primals_330: "f32[1024]"; primals_331: "f32[1024]"; primals_332: "f32[4096, 1024]"; primals_333: "f32[4096]"; primals_334: "f32[1024, 4096]"; primals_335: "f32[1024]"; primals_336: "f32[1024]"; primals_337: "f32[1024]"; primals_338: "f32[1024]"; primals_339: "f32[1024]"; primals_340: "f32[4096, 1024]"; primals_341: "f32[4096]"; primals_342: "f32[1024, 4096]"; primals_343: "f32[1024]"; primals_344: "f32[1024]"; primals_345: "f32[1024]"; primals_346: "f32[1024]"; primals_347: "f32[1024]"; primals_348: "f32[4096, 1024]"; primals_349: "f32[4096]"; primals_350: "f32[1024, 4096]"; primals_351: "f32[1024]"; primals_352: "f32[1024]"; primals_353: "f32[1024]"; primals_354: "f32[1024]"; primals_355: "f32[1024]"; primals_356: "f32[4096, 1024]"; primals_357: "f32[4096]"; primals_358: "f32[1024, 4096]"; primals_359: "f32[1024]"; primals_360: "f32[1024]"; primals_361: "f32[1024]"; primals_362: "f32[32000, 1024]"; primals_363: "f32[32000]"; primals_364: "i64[1, 512]"; primals_365: "i64[1, 512]"; tangents_1: "f32[]"; tangents_2: "f32[1, 512, 32000]"; 

    primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38, primals_39, primals_40, primals_41, primals_42, primals_43, primals_44, primals_45, primals_46, primals_47, primals_48, primals_49, primals_50, primals_51, primals_52, primals_53, primals_54, primals_55, primals_56, primals_57, primals_58, primals_59, primals_60, primals_61, primals_62, primals_63, primals_64, primals_65, primals_66, primals_67, primals_68, primals_69, primals_70, primals_71, primals_72, primals_73, primals_74, primals_75, primals_76, primals_77, primals_78, primals_79, primals_80, primals_81, primals_82, primals_83, primals_84, primals_85, primals_86, primals_87, primals_88, primals_89, primals_90, primals_91, primals_92, primals_93, primals_94, primals_95, primals_96, primals_97, primals_98, primals_99, primals_100, primals_101, primals_102, primals_103, primals_104, primals_105, primals_106, primals_107, primals_108, primals_109, primals_110, primals_111, primals_112, primals_113, primals_114, primals_115, primals_116, primals_117, primals_118, primals_119, primals_120, primals_121, primals_122, primals_123, primals_124, primals_125, primals_126, primals_127, primals_128, primals_129, primals_130, primals_131, primals_132, primals_133, primals_134, primals_135, primals_136, primals_137, primals_138, primals_139, primals_140, primals_141, primals_142, primals_143, primals_144, primals_145, primals_146, primals_147, primals_148, primals_149, primals_150, primals_151, primals_152, primals_153, primals_154, primals_155, primals_156, primals_157, primals_158, primals_159, primals_160, primals_161, primals_162, primals_163, primals_164, primals_165, primals_166, primals_167, primals_168, primals_169, primals_170, primals_171, primals_172, primals_173, primals_174, primals_175, primals_176, primals_177, primals_178, primals_179, primals_180, primals_181, primals_182, primals_183, primals_184, primals_185, primals_186, primals_187, primals_188, primals_189, primals_190, primals_191, primals_192, primals_193, primals_194, primals_195, primals_196, primals_197, primals_198, primals_199, primals_200, primals_201, primals_202, primals_203, primals_204, primals_205, primals_206, primals_207, primals_208, primals_209, primals_210, primals_211, primals_212, primals_213, primals_214, primals_215, primals_216, primals_217, primals_218, primals_219, primals_220, primals_221, primals_222, primals_223, primals_224, primals_225, primals_226, primals_227, primals_228, primals_229, primals_230, primals_231, primals_232, primals_233, primals_234, primals_235, primals_236, primals_237, primals_238, primals_239, primals_240, primals_241, primals_242, primals_243, primals_244, primals_245, primals_246, primals_247, primals_248, primals_249, primals_250, primals_251, primals_252, primals_253, primals_254, primals_255, primals_256, primals_257, primals_258, primals_259, primals_260, primals_261, primals_262, primals_263, primals_264, primals_265, primals_266, primals_267, primals_268, primals_269, primals_270, primals_271, primals_272, primals_273, primals_274, primals_275, primals_276, primals_277, primals_278, primals_279, primals_280, primals_281, primals_282, primals_283, primals_284, primals_285, primals_286, primals_287, primals_288, primals_289, primals_290, primals_291, primals_292, primals_293, primals_294, primals_295, primals_296, primals_297, primals_298, primals_299, primals_300, primals_301, primals_302, primals_303, primals_304, primals_305, primals_306, primals_307, primals_308, primals_309, primals_310, primals_311, primals_312, primals_313, primals_314, primals_315, primals_316, primals_317, primals_318, primals_319, primals_320, primals_321, primals_322, primals_323, primals_324, primals_325, primals_326, primals_327, primals_328, primals_329, primals_330, primals_331, primals_332, primals_333, primals_334, primals_335, primals_336, primals_337, primals_338, primals_339, primals_340, primals_341, primals_342, primals_343, primals_344, primals_345, primals_346, primals_347, primals_348, primals_349, primals_350, primals_351, primals_352, primals_353, primals_354, primals_355, primals_356, primals_357, primals_358, primals_359, primals_360, primals_361, primals_362, primals_363, primals_364, primals_365, tangents_1, tangents_2, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1107, code: input_ids = input_ids.transpose(0, 1).contiguous()
    permute: "i64[512, 1]" = torch.ops.aten.permute.default(primals_364, [1, 0]);  primals_364 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1176, code: word_emb_k = self.word_embedding(input_ids)
    embedding: "f32[512, 1, 1024]" = torch.ops.aten.embedding.default(primals_169, permute);  primals_169 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1177, code: output_h = self.dropout(word_emb_k)
    native_dropout = torch.ops.aten.native_dropout.default(embedding, 0.1, True);  embedding = None
    getitem: "f32[512, 1, 1024]" = native_dropout[0]
    getitem_1: "b8[512, 1, 1024]" = native_dropout[1];  native_dropout = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1023, code: freq_seq = torch.arange(0, self.d_model, 2.0, dtype=torch.float)
    iota: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
    convert_element_type: "f64[512]" = torch.ops.prims.convert_element_type.default(iota, torch.float64);  iota = None
    mul: "f64[512]" = torch.ops.aten.mul.Tensor(convert_element_type, 2.0);  convert_element_type = None
    add: "f64[512]" = torch.ops.aten.add.Tensor(mul, 0);  mul = None
    convert_element_type_1: "f32[512]" = torch.ops.prims.convert_element_type.default(add, torch.float32);  add = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1024, code: inv_freq = 1 / torch.pow(10000, (freq_seq / self.d_model))
    div: "f32[512]" = torch.ops.aten.div.Tensor(convert_element_type_1, 1024);  convert_element_type_1 = None
    pow_1: "f32[512]" = torch.ops.aten.pow.Scalar(10000, div);  div = None
    reciprocal: "f32[512]" = torch.ops.aten.reciprocal.default(pow_1);  pow_1 = None
    mul_1: "f32[512]" = torch.ops.aten.mul.Tensor(reciprocal, 1);  reciprocal = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1052, code: fwd_pos_seq = torch.arange(beg, end, -1.0)
    iota_1: "i64[1024]" = torch.ops.prims.iota.default(1024, start = 0, step = 1, dtype = torch.int64, device = device(type='cpu'), requires_grad = False)
    convert_element_type_2: "f64[1024]" = torch.ops.prims.convert_element_type.default(iota_1, torch.float64);  iota_1 = None
    mul_2: "f64[1024]" = torch.ops.aten.mul.Tensor(convert_element_type_2, -1.0);  convert_element_type_2 = None
    add_1: "f64[1024]" = torch.ops.aten.add.Tensor(mul_2, 512);  mul_2 = None
    convert_element_type_3: "f32[1024]" = torch.ops.prims.convert_element_type.default(add_1, torch.float32);  add_1 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1012, code: sinusoid_inp = torch.einsum("i,d->id", pos_seq, inv_freq)
    unsqueeze: "f32[1024, 1]" = torch.ops.aten.unsqueeze.default(convert_element_type_3, 1);  convert_element_type_3 = None
    permute_1: "f32[1024, 1]" = torch.ops.aten.permute.default(unsqueeze, [0, 1]);  unsqueeze = None
    unsqueeze_1: "f32[512, 1]" = torch.ops.aten.unsqueeze.default(mul_1, 1);  mul_1 = None
    permute_2: "f32[1, 512]" = torch.ops.aten.permute.default(unsqueeze_1, [1, 0]);  unsqueeze_1 = None
    mul_3: "f32[1024, 512]" = torch.ops.aten.mul.Tensor(permute_1, permute_2);  permute_1 = permute_2 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1013, code: pos_emb = torch.cat([torch.sin(sinusoid_inp), torch.cos(sinusoid_inp)], dim=-1)
    sin: "f32[1024, 512]" = torch.ops.aten.sin.default(mul_3)
    cos: "f32[1024, 512]" = torch.ops.aten.cos.default(mul_3);  mul_3 = None
    cat: "f32[1024, 1024]" = torch.ops.aten.cat.default([sin, cos], 1);  sin = cos = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1014, code: pos_emb = pos_emb[:, None, :]
    slice_1: "f32[1024, 1024]" = torch.ops.aten.slice.Tensor(cat, 0, 0, 9223372036854775807);  cat = None
    unsqueeze_2: "f32[1024, 1, 1024]" = torch.ops.aten.unsqueeze.default(slice_1, 1);  slice_1 = None
    slice_2: "f32[1024, 1, 1024]" = torch.ops.aten.slice.Tensor(unsqueeze_2, 2, 0, 9223372036854775807);  unsqueeze_2 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1017, code: pos_emb = pos_emb.expand(-1, bsz, -1)
    expand: "f32[1024, 1, 1024]" = torch.ops.aten.expand.default(slice_2, [-1, 1, -1]);  slice_2 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1204, code: pos_emb = pos_emb.to(output_h.device)
    device_put: "f32[1024, 1, 1024]" = torch.ops.prims.device_put.default(expand, device(type='cuda', index=0));  expand = None
    convert_element_type_4: "f32[1024, 1, 1024]" = torch.ops.prims.convert_element_type.default(device_put, torch.float32);  device_put = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1205, code: pos_emb = self.dropout(pos_emb)
    native_dropout_1 = torch.ops.aten.native_dropout.default(convert_element_type_4, 0.1, True);  convert_element_type_4 = None
    getitem_2: "f32[1024, 1, 1024]" = native_dropout_1[0];  native_dropout_1 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_3: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem, 3)
    unsqueeze_4: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_3, 4);  unsqueeze_3 = None
    permute_3: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_4, [0, 1, 3, 4, 2]);  unsqueeze_4 = None
    unsqueeze_5: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_1, 3);  primals_1 = None
    unsqueeze_6: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_5, 4);  unsqueeze_5 = None
    permute_4: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_6, [3, 4, 1, 2, 0]);  unsqueeze_6 = None
    permute_5: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_3, [0, 4, 1, 2, 3]);  permute_3 = None
    view: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_5, [1, 512, 1024]);  permute_5 = None
    permute_6: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_4, [4, 1, 2, 3, 0]);  permute_4 = None
    view_1: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_6, [1, 1024, 1024]);  permute_6 = None
    bmm: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view, view_1)
    view_2: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm, [512, 1, 1, 16, 64]);  bmm = None
    permute_7: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_2, [0, 2, 3, 4, 1]);  view_2 = None
    view_3: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_7, [512, 1, 16, 64]);  permute_7 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_7: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem, 3)
    unsqueeze_8: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_7, 4);  unsqueeze_7 = None
    permute_8: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_8, [0, 1, 3, 4, 2]);  unsqueeze_8 = None
    unsqueeze_9: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_2, 3);  primals_2 = None
    unsqueeze_10: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_9, 4);  unsqueeze_9 = None
    permute_9: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_10, [3, 4, 1, 2, 0]);  unsqueeze_10 = None
    permute_10: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_8, [0, 4, 1, 2, 3]);  permute_8 = None
    view_4: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_10, [1, 512, 1024]);  permute_10 = None
    permute_11: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_9, [4, 1, 2, 3, 0]);  permute_9 = None
    view_5: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_11, [1, 1024, 1024]);  permute_11 = None
    bmm_1: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_4, view_5)
    view_6: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_1, [512, 1, 1, 16, 64]);  bmm_1 = None
    permute_12: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_6, [0, 2, 3, 4, 1]);  view_6 = None
    view_7: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_12, [512, 1, 16, 64]);  permute_12 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_11: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem, 3)
    unsqueeze_12: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_11, 4);  unsqueeze_11 = None
    permute_13: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_12, [0, 1, 3, 4, 2]);  unsqueeze_12 = None
    unsqueeze_13: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_3, 3);  primals_3 = None
    unsqueeze_14: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_13, 4);  unsqueeze_13 = None
    permute_14: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_14, [3, 4, 1, 2, 0]);  unsqueeze_14 = None
    permute_15: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_13, [0, 4, 1, 2, 3]);  permute_13 = None
    view_8: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_15, [1, 512, 1024]);  permute_15 = None
    permute_16: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_14, [4, 1, 2, 3, 0]);  permute_14 = None
    view_9: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_16, [1, 1024, 1024]);  permute_16 = None
    bmm_2: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_8, view_9)
    view_10: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_2, [512, 1, 1, 16, 64]);  bmm_2 = None
    permute_17: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_10, [0, 2, 3, 4, 1]);  view_10 = None
    view_11: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_17, [512, 1, 16, 64]);  permute_17 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_15: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_16: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_15, 4);  unsqueeze_15 = None
    permute_18: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_16, [0, 1, 3, 4, 2]);  unsqueeze_16 = None
    unsqueeze_17: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_4, 3);  primals_4 = None
    unsqueeze_18: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_17, 4);  unsqueeze_17 = None
    permute_19: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_18, [3, 4, 1, 2, 0]);  unsqueeze_18 = None
    permute_20: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_18, [0, 4, 1, 2, 3]);  permute_18 = None
    view_12: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_20, [1, 1024, 1024]);  permute_20 = None
    permute_21: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_19, [4, 1, 2, 3, 0]);  permute_19 = None
    view_13: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_21, [1, 1024, 1024]);  permute_21 = None
    bmm_3: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_12, view_13);  view_13 = None
    view_14: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_3, [1024, 1, 1, 16, 64]);  bmm_3 = None
    permute_22: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_14, [0, 2, 3, 4, 1]);  view_14 = None
    view_15: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_22, [1024, 1, 16, 64]);  permute_22 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_2: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_3, primals_5);  primals_5 = None
    unsqueeze_19: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_2, 4);  add_2 = None
    permute_23: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_19, [1, 2, 0, 4, 3]);  unsqueeze_19 = None
    unsqueeze_20: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_7, 4);  view_7 = None
    permute_24: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_20, [1, 2, 4, 0, 3]);  unsqueeze_20 = None
    permute_25: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_23, [1, 2, 4, 0, 3]);  permute_23 = None
    view_16: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_25, [16, 512, 64]);  permute_25 = None
    permute_26: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_24, [1, 4, 0, 3, 2]);  permute_24 = None
    view_17: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_26, [16, 64, 512]);  permute_26 = None
    bmm_4: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_16, view_17)
    view_18: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_4, [16, 512, 1, 1, 512]);  bmm_4 = None
    permute_27: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_18, [3, 0, 1, 4, 2]);  view_18 = None
    view_19: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_27, [1, 16, 512, 512]);  permute_27 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_3: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_3, primals_6);  view_3 = primals_6 = None
    unsqueeze_21: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_3, 4);  add_3 = None
    permute_28: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_21, [1, 2, 0, 4, 3]);  unsqueeze_21 = None
    unsqueeze_22: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_15, 4);  view_15 = None
    permute_29: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_22, [1, 2, 4, 0, 3]);  unsqueeze_22 = None
    permute_30: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_28, [1, 2, 4, 0, 3]);  permute_28 = None
    view_20: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_30, [16, 512, 64]);  permute_30 = None
    permute_31: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_29, [1, 4, 0, 3, 2]);  permute_29 = None
    view_21: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_31, [16, 64, 1024]);  permute_31 = None
    bmm_5: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_20, view_21)
    view_22: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_5, [16, 512, 1, 1, 1024]);  bmm_5 = None
    permute_32: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_22, [3, 0, 1, 4, 2]);  view_22 = None
    view_23: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_32, [1, 16, 512, 1024]);  permute_32 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_24: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_23, [1, 16, 1024, 512]);  view_23 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_3: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_24, 0, 0, 9223372036854775807);  view_24 = None
    slice_4: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_3, 1, 0, 9223372036854775807);  slice_3 = None
    slice_5: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_4, 2, 1, 9223372036854775807);  slice_4 = None
    slice_6: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_5, 3, 0, 9223372036854775807);  slice_5 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_25: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_6, [1, 16, 512, 1023]);  slice_6 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_2: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_7: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_25, 0, 0, 9223372036854775807);  view_25 = None
    slice_8: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_7, 1, 0, 9223372036854775807);  slice_7 = None
    slice_9: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_8, 2, 0, 9223372036854775807);  slice_8 = None
    index: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_9, [None, None, None, iota_2]);  slice_9 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_4: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_19, index);  view_19 = index = None
    add_5: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_4, 0);  add_4 = None
    mul_4: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_5, 0.125);  add_5 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_4, [3], True)
    sub: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_4, amax);  mul_4 = amax = None
    exp: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub);  sub = None
    sum_1: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp, [3], True)
    div_1: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp, sum_1);  exp = sum_1 = None
    alias: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_1)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_2 = torch.ops.aten.native_dropout.default(div_1, 0.1, True);  div_1 = None
    getitem_4: "f32[1, 16, 512, 512]" = native_dropout_2[0]
    getitem_5: "b8[1, 16, 512, 512]" = native_dropout_2[1];  native_dropout_2 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_23: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_4, 4);  getitem_4 = None
    permute_33: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_23, [2, 0, 1, 4, 3]);  unsqueeze_23 = None
    unsqueeze_24: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_11, 4);  view_11 = None
    permute_34: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_24, [4, 1, 2, 3, 0]);  unsqueeze_24 = None
    permute_35: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_33, [2, 0, 4, 1, 3]);  permute_33 = None
    view_26: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_35, [16, 512, 512]);  permute_35 = None
    permute_36: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_34, [2, 4, 1, 3, 0]);  permute_34 = None
    view_27: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_36, [16, 512, 64]);  permute_36 = None
    bmm_6: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_26, view_27)
    view_28: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_6, [16, 512, 1, 1, 64]);  bmm_6 = None
    permute_37: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_28, [1, 3, 0, 4, 2]);  view_28 = None
    view_29: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_37, [512, 1, 16, 64]);  permute_37 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_25: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_29, 4);  view_29 = None
    permute_38: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_25, [0, 1, 4, 3, 2]);  unsqueeze_25 = None
    unsqueeze_26: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_7, 3);  primals_7 = None
    unsqueeze_27: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_26, 4);  unsqueeze_26 = None
    permute_39: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_27, [3, 4, 0, 2, 1]);  unsqueeze_27 = None
    permute_40: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_38, [0, 3, 4, 1, 2]);  permute_38 = None
    clone: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_40, memory_format = torch.contiguous_format);  permute_40 = None
    view_30: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone, [1, 512, 1024]);  clone = None
    permute_41: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_39, [3, 4, 1, 2, 0]);  permute_39 = None
    clone_1: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_41, memory_format = torch.contiguous_format);  permute_41 = None
    view_31: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_1, [1, 1024, 1024]);  clone_1 = None
    bmm_7: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_30, view_31)
    view_32: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_7, [512, 1, 1, 1, 1024]);  bmm_7 = None
    permute_42: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_32, [0, 3, 4, 1, 2]);  view_32 = None
    view_33: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_42, [512, 1, 1024]);  permute_42 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_3 = torch.ops.aten.native_dropout.default(view_33, 0.1, True);  view_33 = None
    getitem_6: "f32[512, 1, 1024]" = native_dropout_3[0]
    getitem_7: "b8[512, 1, 1024]" = native_dropout_3[1];  native_dropout_3 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_6: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_6, getitem);  getitem_6 = getitem = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean = torch.ops.aten.var_mean.correction(add_6, [2], correction = 0, keepdim = True)
    getitem_8: "f32[512, 1, 1]" = var_mean[0]
    getitem_9: "f32[512, 1, 1]" = var_mean[1];  var_mean = None
    add_7: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_8, 1e-12);  getitem_8 = None
    rsqrt: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_7);  add_7 = None
    sub_1: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_6, getitem_9)
    mul_5: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_1, rsqrt);  sub_1 = None
    mul_6: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_5, primals_170);  mul_5 = None
    add_8: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_6, primals_171);  mul_6 = primals_171 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_34: "f32[512, 1024]" = torch.ops.aten.view.default(add_8, [512, 1024])
    permute_43: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_172, [1, 0]);  primals_172 = None
    addmm: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_173, view_34, permute_43);  primals_173 = None
    view_35: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm, [512, 1, 4096]);  addmm = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_7: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_35, 0.5)
    mul_8: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_35, 0.7071067811865476)
    erf: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_8);  mul_8 = None
    add_9: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf, 1);  erf = None
    mul_9: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_7, add_9);  mul_7 = add_9 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_4 = torch.ops.aten.native_dropout.default(mul_9, 0.1, True);  mul_9 = None
    getitem_10: "f32[512, 1, 4096]" = native_dropout_4[0]
    getitem_11: "b8[512, 1, 4096]" = native_dropout_4[1];  native_dropout_4 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_36: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_10, [512, 4096]);  getitem_10 = None
    permute_44: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_174, [1, 0]);  primals_174 = None
    addmm_1: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_175, view_36, permute_44);  primals_175 = None
    view_37: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_1, [512, 1, 1024]);  addmm_1 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_5 = torch.ops.aten.native_dropout.default(view_37, 0.1, True);  view_37 = None
    getitem_12: "f32[512, 1, 1024]" = native_dropout_5[0]
    getitem_13: "b8[512, 1, 1024]" = native_dropout_5[1];  native_dropout_5 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_10: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_12, add_8);  getitem_12 = add_8 = None
    var_mean_1 = torch.ops.aten.var_mean.correction(add_10, [2], correction = 0, keepdim = True)
    getitem_14: "f32[512, 1, 1]" = var_mean_1[0]
    getitem_15: "f32[512, 1, 1]" = var_mean_1[1];  var_mean_1 = None
    add_11: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_14, 1e-12);  getitem_14 = None
    rsqrt_1: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_11);  add_11 = None
    sub_2: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_10, getitem_15)
    mul_10: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_2, rsqrt_1);  sub_2 = None
    mul_11: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_10, primals_176);  mul_10 = None
    add_12: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_11, primals_177);  mul_11 = primals_177 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_28: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_12, 3)
    unsqueeze_29: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_28, 4);  unsqueeze_28 = None
    permute_45: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_29, [0, 1, 3, 4, 2]);  unsqueeze_29 = None
    unsqueeze_30: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_8, 3);  primals_8 = None
    unsqueeze_31: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_30, 4);  unsqueeze_30 = None
    permute_46: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_31, [3, 4, 1, 2, 0]);  unsqueeze_31 = None
    permute_47: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_45, [0, 4, 1, 2, 3]);  permute_45 = None
    view_38: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_47, [1, 512, 1024]);  permute_47 = None
    permute_48: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_46, [4, 1, 2, 3, 0]);  permute_46 = None
    view_39: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_48, [1, 1024, 1024]);  permute_48 = None
    bmm_8: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_38, view_39)
    view_40: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_8, [512, 1, 1, 16, 64]);  bmm_8 = None
    permute_49: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_40, [0, 2, 3, 4, 1]);  view_40 = None
    view_41: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_49, [512, 1, 16, 64]);  permute_49 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_32: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_12, 3)
    unsqueeze_33: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_32, 4);  unsqueeze_32 = None
    permute_50: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_33, [0, 1, 3, 4, 2]);  unsqueeze_33 = None
    unsqueeze_34: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_9, 3);  primals_9 = None
    unsqueeze_35: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_34, 4);  unsqueeze_34 = None
    permute_51: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_35, [3, 4, 1, 2, 0]);  unsqueeze_35 = None
    permute_52: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_50, [0, 4, 1, 2, 3]);  permute_50 = None
    view_42: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_52, [1, 512, 1024]);  permute_52 = None
    permute_53: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_51, [4, 1, 2, 3, 0]);  permute_51 = None
    view_43: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_53, [1, 1024, 1024]);  permute_53 = None
    bmm_9: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_42, view_43)
    view_44: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_9, [512, 1, 1, 16, 64]);  bmm_9 = None
    permute_54: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_44, [0, 2, 3, 4, 1]);  view_44 = None
    view_45: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_54, [512, 1, 16, 64]);  permute_54 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_36: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_12, 3)
    unsqueeze_37: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_36, 4);  unsqueeze_36 = None
    permute_55: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_37, [0, 1, 3, 4, 2]);  unsqueeze_37 = None
    unsqueeze_38: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_10, 3);  primals_10 = None
    unsqueeze_39: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_38, 4);  unsqueeze_38 = None
    permute_56: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_39, [3, 4, 1, 2, 0]);  unsqueeze_39 = None
    permute_57: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_55, [0, 4, 1, 2, 3]);  permute_55 = None
    view_46: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_57, [1, 512, 1024]);  permute_57 = None
    permute_58: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_56, [4, 1, 2, 3, 0]);  permute_56 = None
    view_47: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_58, [1, 1024, 1024]);  permute_58 = None
    bmm_10: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_46, view_47)
    view_48: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_10, [512, 1, 1, 16, 64]);  bmm_10 = None
    permute_59: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_48, [0, 2, 3, 4, 1]);  view_48 = None
    view_49: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_59, [512, 1, 16, 64]);  permute_59 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_40: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_41: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_40, 4);  unsqueeze_40 = None
    permute_60: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_41, [0, 1, 3, 4, 2]);  unsqueeze_41 = None
    unsqueeze_42: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_11, 3);  primals_11 = None
    unsqueeze_43: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_42, 4);  unsqueeze_42 = None
    permute_61: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_43, [3, 4, 1, 2, 0]);  unsqueeze_43 = None
    permute_62: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_60, [0, 4, 1, 2, 3]);  permute_60 = None
    view_50: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_62, [1, 1024, 1024]);  permute_62 = None
    permute_63: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_61, [4, 1, 2, 3, 0]);  permute_61 = None
    view_51: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_63, [1, 1024, 1024]);  permute_63 = None
    bmm_11: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_50, view_51);  view_51 = None
    view_52: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_11, [1024, 1, 1, 16, 64]);  bmm_11 = None
    permute_64: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_52, [0, 2, 3, 4, 1]);  view_52 = None
    view_53: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_64, [1024, 1, 16, 64]);  permute_64 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_13: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_41, primals_12);  primals_12 = None
    unsqueeze_44: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_13, 4);  add_13 = None
    permute_65: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_44, [1, 2, 0, 4, 3]);  unsqueeze_44 = None
    unsqueeze_45: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_45, 4);  view_45 = None
    permute_66: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_45, [1, 2, 4, 0, 3]);  unsqueeze_45 = None
    permute_67: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_65, [1, 2, 4, 0, 3]);  permute_65 = None
    view_54: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_67, [16, 512, 64]);  permute_67 = None
    permute_68: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_66, [1, 4, 0, 3, 2]);  permute_66 = None
    view_55: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_68, [16, 64, 512]);  permute_68 = None
    bmm_12: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_54, view_55)
    view_56: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_12, [16, 512, 1, 1, 512]);  bmm_12 = None
    permute_69: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_56, [3, 0, 1, 4, 2]);  view_56 = None
    view_57: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_69, [1, 16, 512, 512]);  permute_69 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_14: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_41, primals_13);  view_41 = primals_13 = None
    unsqueeze_46: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_14, 4);  add_14 = None
    permute_70: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_46, [1, 2, 0, 4, 3]);  unsqueeze_46 = None
    unsqueeze_47: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_53, 4);  view_53 = None
    permute_71: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_47, [1, 2, 4, 0, 3]);  unsqueeze_47 = None
    permute_72: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_70, [1, 2, 4, 0, 3]);  permute_70 = None
    view_58: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_72, [16, 512, 64]);  permute_72 = None
    permute_73: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_71, [1, 4, 0, 3, 2]);  permute_71 = None
    view_59: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_73, [16, 64, 1024]);  permute_73 = None
    bmm_13: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_58, view_59)
    view_60: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_13, [16, 512, 1, 1, 1024]);  bmm_13 = None
    permute_74: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_60, [3, 0, 1, 4, 2]);  view_60 = None
    view_61: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_74, [1, 16, 512, 1024]);  permute_74 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_62: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_61, [1, 16, 1024, 512]);  view_61 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_10: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_62, 0, 0, 9223372036854775807);  view_62 = None
    slice_11: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_10, 1, 0, 9223372036854775807);  slice_10 = None
    slice_12: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_11, 2, 1, 9223372036854775807);  slice_11 = None
    slice_13: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_12, 3, 0, 9223372036854775807);  slice_12 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_63: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_13, [1, 16, 512, 1023]);  slice_13 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_3: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_14: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_63, 0, 0, 9223372036854775807);  view_63 = None
    slice_15: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_14, 1, 0, 9223372036854775807);  slice_14 = None
    slice_16: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_15, 2, 0, 9223372036854775807);  slice_15 = None
    index_1: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_16, [None, None, None, iota_3]);  slice_16 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_15: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_57, index_1);  view_57 = index_1 = None
    add_16: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_15, 0);  add_15 = None
    mul_12: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_16, 0.125);  add_16 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_1: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_12, [3], True)
    sub_3: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_12, amax_1);  mul_12 = amax_1 = None
    exp_1: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_3);  sub_3 = None
    sum_2: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_1, [3], True)
    div_2: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_1, sum_2);  exp_1 = sum_2 = None
    alias_1: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_2)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_6 = torch.ops.aten.native_dropout.default(div_2, 0.1, True);  div_2 = None
    getitem_16: "f32[1, 16, 512, 512]" = native_dropout_6[0]
    getitem_17: "b8[1, 16, 512, 512]" = native_dropout_6[1];  native_dropout_6 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_48: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_16, 4);  getitem_16 = None
    permute_75: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_48, [2, 0, 1, 4, 3]);  unsqueeze_48 = None
    unsqueeze_49: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_49, 4);  view_49 = None
    permute_76: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_49, [4, 1, 2, 3, 0]);  unsqueeze_49 = None
    permute_77: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_75, [2, 0, 4, 1, 3]);  permute_75 = None
    view_64: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_77, [16, 512, 512]);  permute_77 = None
    permute_78: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_76, [2, 4, 1, 3, 0]);  permute_76 = None
    view_65: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_78, [16, 512, 64]);  permute_78 = None
    bmm_14: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_64, view_65)
    view_66: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_14, [16, 512, 1, 1, 64]);  bmm_14 = None
    permute_79: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_66, [1, 3, 0, 4, 2]);  view_66 = None
    view_67: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_79, [512, 1, 16, 64]);  permute_79 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_50: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_67, 4);  view_67 = None
    permute_80: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_50, [0, 1, 4, 3, 2]);  unsqueeze_50 = None
    unsqueeze_51: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_14, 3);  primals_14 = None
    unsqueeze_52: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_51, 4);  unsqueeze_51 = None
    permute_81: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_52, [3, 4, 0, 2, 1]);  unsqueeze_52 = None
    permute_82: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_80, [0, 3, 4, 1, 2]);  permute_80 = None
    clone_2: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_82, memory_format = torch.contiguous_format);  permute_82 = None
    view_68: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_2, [1, 512, 1024]);  clone_2 = None
    permute_83: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_81, [3, 4, 1, 2, 0]);  permute_81 = None
    clone_3: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_83, memory_format = torch.contiguous_format);  permute_83 = None
    view_69: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_3, [1, 1024, 1024]);  clone_3 = None
    bmm_15: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_68, view_69)
    view_70: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_15, [512, 1, 1, 1, 1024]);  bmm_15 = None
    permute_84: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_70, [0, 3, 4, 1, 2]);  view_70 = None
    view_71: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_84, [512, 1, 1024]);  permute_84 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_7 = torch.ops.aten.native_dropout.default(view_71, 0.1, True);  view_71 = None
    getitem_18: "f32[512, 1, 1024]" = native_dropout_7[0]
    getitem_19: "b8[512, 1, 1024]" = native_dropout_7[1];  native_dropout_7 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_17: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_18, add_12);  getitem_18 = add_12 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_2 = torch.ops.aten.var_mean.correction(add_17, [2], correction = 0, keepdim = True)
    getitem_20: "f32[512, 1, 1]" = var_mean_2[0]
    getitem_21: "f32[512, 1, 1]" = var_mean_2[1];  var_mean_2 = None
    add_18: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_20, 1e-12);  getitem_20 = None
    rsqrt_2: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_18);  add_18 = None
    sub_4: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_17, getitem_21)
    mul_13: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_4, rsqrt_2);  sub_4 = None
    mul_14: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_13, primals_178);  mul_13 = None
    add_19: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_14, primals_179);  mul_14 = primals_179 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_72: "f32[512, 1024]" = torch.ops.aten.view.default(add_19, [512, 1024])
    permute_85: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_180, [1, 0]);  primals_180 = None
    addmm_2: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_181, view_72, permute_85);  primals_181 = None
    view_73: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_2, [512, 1, 4096]);  addmm_2 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_15: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_73, 0.5)
    mul_16: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_73, 0.7071067811865476)
    erf_1: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_16);  mul_16 = None
    add_20: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_1, 1);  erf_1 = None
    mul_17: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_15, add_20);  mul_15 = add_20 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_8 = torch.ops.aten.native_dropout.default(mul_17, 0.1, True);  mul_17 = None
    getitem_22: "f32[512, 1, 4096]" = native_dropout_8[0]
    getitem_23: "b8[512, 1, 4096]" = native_dropout_8[1];  native_dropout_8 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_74: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_22, [512, 4096]);  getitem_22 = None
    permute_86: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_182, [1, 0]);  primals_182 = None
    addmm_3: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_183, view_74, permute_86);  primals_183 = None
    view_75: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_3, [512, 1, 1024]);  addmm_3 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_9 = torch.ops.aten.native_dropout.default(view_75, 0.1, True);  view_75 = None
    getitem_24: "f32[512, 1, 1024]" = native_dropout_9[0]
    getitem_25: "b8[512, 1, 1024]" = native_dropout_9[1];  native_dropout_9 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_21: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_24, add_19);  getitem_24 = add_19 = None
    var_mean_3 = torch.ops.aten.var_mean.correction(add_21, [2], correction = 0, keepdim = True)
    getitem_26: "f32[512, 1, 1]" = var_mean_3[0]
    getitem_27: "f32[512, 1, 1]" = var_mean_3[1];  var_mean_3 = None
    add_22: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_26, 1e-12);  getitem_26 = None
    rsqrt_3: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_22);  add_22 = None
    sub_5: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_21, getitem_27)
    mul_18: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_5, rsqrt_3);  sub_5 = None
    mul_19: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_18, primals_184);  mul_18 = None
    add_23: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_19, primals_185);  mul_19 = primals_185 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_53: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_23, 3)
    unsqueeze_54: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_53, 4);  unsqueeze_53 = None
    permute_87: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_54, [0, 1, 3, 4, 2]);  unsqueeze_54 = None
    unsqueeze_55: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_15, 3);  primals_15 = None
    unsqueeze_56: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_55, 4);  unsqueeze_55 = None
    permute_88: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_56, [3, 4, 1, 2, 0]);  unsqueeze_56 = None
    permute_89: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_87, [0, 4, 1, 2, 3]);  permute_87 = None
    view_76: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_89, [1, 512, 1024]);  permute_89 = None
    permute_90: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_88, [4, 1, 2, 3, 0]);  permute_88 = None
    view_77: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_90, [1, 1024, 1024]);  permute_90 = None
    bmm_16: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_76, view_77)
    view_78: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_16, [512, 1, 1, 16, 64]);  bmm_16 = None
    permute_91: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_78, [0, 2, 3, 4, 1]);  view_78 = None
    view_79: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_91, [512, 1, 16, 64]);  permute_91 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_57: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_23, 3)
    unsqueeze_58: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_57, 4);  unsqueeze_57 = None
    permute_92: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_58, [0, 1, 3, 4, 2]);  unsqueeze_58 = None
    unsqueeze_59: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_16, 3);  primals_16 = None
    unsqueeze_60: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_59, 4);  unsqueeze_59 = None
    permute_93: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_60, [3, 4, 1, 2, 0]);  unsqueeze_60 = None
    permute_94: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_92, [0, 4, 1, 2, 3]);  permute_92 = None
    view_80: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_94, [1, 512, 1024]);  permute_94 = None
    permute_95: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_93, [4, 1, 2, 3, 0]);  permute_93 = None
    view_81: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_95, [1, 1024, 1024]);  permute_95 = None
    bmm_17: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_80, view_81)
    view_82: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_17, [512, 1, 1, 16, 64]);  bmm_17 = None
    permute_96: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_82, [0, 2, 3, 4, 1]);  view_82 = None
    view_83: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_96, [512, 1, 16, 64]);  permute_96 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_61: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_23, 3)
    unsqueeze_62: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_61, 4);  unsqueeze_61 = None
    permute_97: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_62, [0, 1, 3, 4, 2]);  unsqueeze_62 = None
    unsqueeze_63: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_17, 3);  primals_17 = None
    unsqueeze_64: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_63, 4);  unsqueeze_63 = None
    permute_98: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_64, [3, 4, 1, 2, 0]);  unsqueeze_64 = None
    permute_99: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_97, [0, 4, 1, 2, 3]);  permute_97 = None
    view_84: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_99, [1, 512, 1024]);  permute_99 = None
    permute_100: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_98, [4, 1, 2, 3, 0]);  permute_98 = None
    view_85: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_100, [1, 1024, 1024]);  permute_100 = None
    bmm_18: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_84, view_85)
    view_86: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_18, [512, 1, 1, 16, 64]);  bmm_18 = None
    permute_101: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_86, [0, 2, 3, 4, 1]);  view_86 = None
    view_87: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_101, [512, 1, 16, 64]);  permute_101 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_65: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_66: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_65, 4);  unsqueeze_65 = None
    permute_102: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_66, [0, 1, 3, 4, 2]);  unsqueeze_66 = None
    unsqueeze_67: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_18, 3);  primals_18 = None
    unsqueeze_68: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_67, 4);  unsqueeze_67 = None
    permute_103: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_68, [3, 4, 1, 2, 0]);  unsqueeze_68 = None
    permute_104: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_102, [0, 4, 1, 2, 3]);  permute_102 = None
    view_88: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_104, [1, 1024, 1024]);  permute_104 = None
    permute_105: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_103, [4, 1, 2, 3, 0]);  permute_103 = None
    view_89: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_105, [1, 1024, 1024]);  permute_105 = None
    bmm_19: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_88, view_89);  view_89 = None
    view_90: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_19, [1024, 1, 1, 16, 64]);  bmm_19 = None
    permute_106: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_90, [0, 2, 3, 4, 1]);  view_90 = None
    view_91: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_106, [1024, 1, 16, 64]);  permute_106 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_24: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_79, primals_19);  primals_19 = None
    unsqueeze_69: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_24, 4);  add_24 = None
    permute_107: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_69, [1, 2, 0, 4, 3]);  unsqueeze_69 = None
    unsqueeze_70: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_83, 4);  view_83 = None
    permute_108: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_70, [1, 2, 4, 0, 3]);  unsqueeze_70 = None
    permute_109: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_107, [1, 2, 4, 0, 3]);  permute_107 = None
    view_92: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_109, [16, 512, 64]);  permute_109 = None
    permute_110: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_108, [1, 4, 0, 3, 2]);  permute_108 = None
    view_93: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_110, [16, 64, 512]);  permute_110 = None
    bmm_20: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_92, view_93)
    view_94: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_20, [16, 512, 1, 1, 512]);  bmm_20 = None
    permute_111: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_94, [3, 0, 1, 4, 2]);  view_94 = None
    view_95: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_111, [1, 16, 512, 512]);  permute_111 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_25: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_79, primals_20);  view_79 = primals_20 = None
    unsqueeze_71: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_25, 4);  add_25 = None
    permute_112: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_71, [1, 2, 0, 4, 3]);  unsqueeze_71 = None
    unsqueeze_72: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_91, 4);  view_91 = None
    permute_113: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_72, [1, 2, 4, 0, 3]);  unsqueeze_72 = None
    permute_114: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_112, [1, 2, 4, 0, 3]);  permute_112 = None
    view_96: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_114, [16, 512, 64]);  permute_114 = None
    permute_115: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_113, [1, 4, 0, 3, 2]);  permute_113 = None
    view_97: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_115, [16, 64, 1024]);  permute_115 = None
    bmm_21: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_96, view_97)
    view_98: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_21, [16, 512, 1, 1, 1024]);  bmm_21 = None
    permute_116: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_98, [3, 0, 1, 4, 2]);  view_98 = None
    view_99: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_116, [1, 16, 512, 1024]);  permute_116 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_100: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_99, [1, 16, 1024, 512]);  view_99 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_17: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_100, 0, 0, 9223372036854775807);  view_100 = None
    slice_18: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_17, 1, 0, 9223372036854775807);  slice_17 = None
    slice_19: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_18, 2, 1, 9223372036854775807);  slice_18 = None
    slice_20: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_19, 3, 0, 9223372036854775807);  slice_19 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_101: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_20, [1, 16, 512, 1023]);  slice_20 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_4: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_21: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_101, 0, 0, 9223372036854775807);  view_101 = None
    slice_22: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_21, 1, 0, 9223372036854775807);  slice_21 = None
    slice_23: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_22, 2, 0, 9223372036854775807);  slice_22 = None
    index_2: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_23, [None, None, None, iota_4]);  slice_23 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_26: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_95, index_2);  view_95 = index_2 = None
    add_27: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_26, 0);  add_26 = None
    mul_20: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_27, 0.125);  add_27 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_2: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_20, [3], True)
    sub_6: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_20, amax_2);  mul_20 = amax_2 = None
    exp_2: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_6);  sub_6 = None
    sum_3: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_2, [3], True)
    div_3: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_2, sum_3);  exp_2 = sum_3 = None
    alias_2: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_3)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_10 = torch.ops.aten.native_dropout.default(div_3, 0.1, True);  div_3 = None
    getitem_28: "f32[1, 16, 512, 512]" = native_dropout_10[0]
    getitem_29: "b8[1, 16, 512, 512]" = native_dropout_10[1];  native_dropout_10 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_73: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_28, 4);  getitem_28 = None
    permute_117: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_73, [2, 0, 1, 4, 3]);  unsqueeze_73 = None
    unsqueeze_74: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_87, 4);  view_87 = None
    permute_118: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_74, [4, 1, 2, 3, 0]);  unsqueeze_74 = None
    permute_119: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_117, [2, 0, 4, 1, 3]);  permute_117 = None
    view_102: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_119, [16, 512, 512]);  permute_119 = None
    permute_120: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_118, [2, 4, 1, 3, 0]);  permute_118 = None
    view_103: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_120, [16, 512, 64]);  permute_120 = None
    bmm_22: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_102, view_103)
    view_104: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_22, [16, 512, 1, 1, 64]);  bmm_22 = None
    permute_121: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_104, [1, 3, 0, 4, 2]);  view_104 = None
    view_105: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_121, [512, 1, 16, 64]);  permute_121 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_75: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_105, 4);  view_105 = None
    permute_122: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_75, [0, 1, 4, 3, 2]);  unsqueeze_75 = None
    unsqueeze_76: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_21, 3);  primals_21 = None
    unsqueeze_77: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_76, 4);  unsqueeze_76 = None
    permute_123: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_77, [3, 4, 0, 2, 1]);  unsqueeze_77 = None
    permute_124: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_122, [0, 3, 4, 1, 2]);  permute_122 = None
    clone_4: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_124, memory_format = torch.contiguous_format);  permute_124 = None
    view_106: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_4, [1, 512, 1024]);  clone_4 = None
    permute_125: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_123, [3, 4, 1, 2, 0]);  permute_123 = None
    clone_5: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_125, memory_format = torch.contiguous_format);  permute_125 = None
    view_107: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_5, [1, 1024, 1024]);  clone_5 = None
    bmm_23: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_106, view_107)
    view_108: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_23, [512, 1, 1, 1, 1024]);  bmm_23 = None
    permute_126: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_108, [0, 3, 4, 1, 2]);  view_108 = None
    view_109: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_126, [512, 1, 1024]);  permute_126 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_11 = torch.ops.aten.native_dropout.default(view_109, 0.1, True);  view_109 = None
    getitem_30: "f32[512, 1, 1024]" = native_dropout_11[0]
    getitem_31: "b8[512, 1, 1024]" = native_dropout_11[1];  native_dropout_11 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_28: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_30, add_23);  getitem_30 = add_23 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_4 = torch.ops.aten.var_mean.correction(add_28, [2], correction = 0, keepdim = True)
    getitem_32: "f32[512, 1, 1]" = var_mean_4[0]
    getitem_33: "f32[512, 1, 1]" = var_mean_4[1];  var_mean_4 = None
    add_29: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_32, 1e-12);  getitem_32 = None
    rsqrt_4: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_29);  add_29 = None
    sub_7: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_28, getitem_33)
    mul_21: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_7, rsqrt_4);  sub_7 = None
    mul_22: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_21, primals_186);  mul_21 = None
    add_30: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_22, primals_187);  mul_22 = primals_187 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_110: "f32[512, 1024]" = torch.ops.aten.view.default(add_30, [512, 1024])
    permute_127: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_188, [1, 0]);  primals_188 = None
    addmm_4: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_189, view_110, permute_127);  primals_189 = None
    view_111: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_4, [512, 1, 4096]);  addmm_4 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_23: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_111, 0.5)
    mul_24: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_111, 0.7071067811865476)
    erf_2: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_24);  mul_24 = None
    add_31: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_2, 1);  erf_2 = None
    mul_25: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_23, add_31);  mul_23 = add_31 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_12 = torch.ops.aten.native_dropout.default(mul_25, 0.1, True);  mul_25 = None
    getitem_34: "f32[512, 1, 4096]" = native_dropout_12[0]
    getitem_35: "b8[512, 1, 4096]" = native_dropout_12[1];  native_dropout_12 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_112: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_34, [512, 4096]);  getitem_34 = None
    permute_128: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_190, [1, 0]);  primals_190 = None
    addmm_5: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_191, view_112, permute_128);  primals_191 = None
    view_113: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_5, [512, 1, 1024]);  addmm_5 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_13 = torch.ops.aten.native_dropout.default(view_113, 0.1, True);  view_113 = None
    getitem_36: "f32[512, 1, 1024]" = native_dropout_13[0]
    getitem_37: "b8[512, 1, 1024]" = native_dropout_13[1];  native_dropout_13 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_32: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_36, add_30);  getitem_36 = add_30 = None
    var_mean_5 = torch.ops.aten.var_mean.correction(add_32, [2], correction = 0, keepdim = True)
    getitem_38: "f32[512, 1, 1]" = var_mean_5[0]
    getitem_39: "f32[512, 1, 1]" = var_mean_5[1];  var_mean_5 = None
    add_33: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_38, 1e-12);  getitem_38 = None
    rsqrt_5: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_33);  add_33 = None
    sub_8: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_32, getitem_39)
    mul_26: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_8, rsqrt_5);  sub_8 = None
    mul_27: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_26, primals_192);  mul_26 = None
    add_34: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_27, primals_193);  mul_27 = primals_193 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_78: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_34, 3)
    unsqueeze_79: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_78, 4);  unsqueeze_78 = None
    permute_129: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_79, [0, 1, 3, 4, 2]);  unsqueeze_79 = None
    unsqueeze_80: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_22, 3);  primals_22 = None
    unsqueeze_81: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_80, 4);  unsqueeze_80 = None
    permute_130: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_81, [3, 4, 1, 2, 0]);  unsqueeze_81 = None
    permute_131: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_129, [0, 4, 1, 2, 3]);  permute_129 = None
    view_114: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_131, [1, 512, 1024]);  permute_131 = None
    permute_132: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_130, [4, 1, 2, 3, 0]);  permute_130 = None
    view_115: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_132, [1, 1024, 1024]);  permute_132 = None
    bmm_24: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_114, view_115)
    view_116: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_24, [512, 1, 1, 16, 64]);  bmm_24 = None
    permute_133: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_116, [0, 2, 3, 4, 1]);  view_116 = None
    view_117: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_133, [512, 1, 16, 64]);  permute_133 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_82: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_34, 3)
    unsqueeze_83: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_82, 4);  unsqueeze_82 = None
    permute_134: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_83, [0, 1, 3, 4, 2]);  unsqueeze_83 = None
    unsqueeze_84: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_23, 3);  primals_23 = None
    unsqueeze_85: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_84, 4);  unsqueeze_84 = None
    permute_135: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_85, [3, 4, 1, 2, 0]);  unsqueeze_85 = None
    permute_136: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_134, [0, 4, 1, 2, 3]);  permute_134 = None
    view_118: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_136, [1, 512, 1024]);  permute_136 = None
    permute_137: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_135, [4, 1, 2, 3, 0]);  permute_135 = None
    view_119: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_137, [1, 1024, 1024]);  permute_137 = None
    bmm_25: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_118, view_119)
    view_120: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_25, [512, 1, 1, 16, 64]);  bmm_25 = None
    permute_138: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_120, [0, 2, 3, 4, 1]);  view_120 = None
    view_121: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_138, [512, 1, 16, 64]);  permute_138 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_86: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_34, 3)
    unsqueeze_87: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_86, 4);  unsqueeze_86 = None
    permute_139: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_87, [0, 1, 3, 4, 2]);  unsqueeze_87 = None
    unsqueeze_88: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_24, 3);  primals_24 = None
    unsqueeze_89: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_88, 4);  unsqueeze_88 = None
    permute_140: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_89, [3, 4, 1, 2, 0]);  unsqueeze_89 = None
    permute_141: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_139, [0, 4, 1, 2, 3]);  permute_139 = None
    view_122: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_141, [1, 512, 1024]);  permute_141 = None
    permute_142: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_140, [4, 1, 2, 3, 0]);  permute_140 = None
    view_123: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_142, [1, 1024, 1024]);  permute_142 = None
    bmm_26: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_122, view_123)
    view_124: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_26, [512, 1, 1, 16, 64]);  bmm_26 = None
    permute_143: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_124, [0, 2, 3, 4, 1]);  view_124 = None
    view_125: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_143, [512, 1, 16, 64]);  permute_143 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_90: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_91: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_90, 4);  unsqueeze_90 = None
    permute_144: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_91, [0, 1, 3, 4, 2]);  unsqueeze_91 = None
    unsqueeze_92: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_25, 3);  primals_25 = None
    unsqueeze_93: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_92, 4);  unsqueeze_92 = None
    permute_145: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_93, [3, 4, 1, 2, 0]);  unsqueeze_93 = None
    permute_146: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_144, [0, 4, 1, 2, 3]);  permute_144 = None
    view_126: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_146, [1, 1024, 1024]);  permute_146 = None
    permute_147: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_145, [4, 1, 2, 3, 0]);  permute_145 = None
    view_127: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_147, [1, 1024, 1024]);  permute_147 = None
    bmm_27: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_126, view_127);  view_127 = None
    view_128: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_27, [1024, 1, 1, 16, 64]);  bmm_27 = None
    permute_148: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_128, [0, 2, 3, 4, 1]);  view_128 = None
    view_129: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_148, [1024, 1, 16, 64]);  permute_148 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_35: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_117, primals_26);  primals_26 = None
    unsqueeze_94: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_35, 4);  add_35 = None
    permute_149: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_94, [1, 2, 0, 4, 3]);  unsqueeze_94 = None
    unsqueeze_95: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_121, 4);  view_121 = None
    permute_150: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_95, [1, 2, 4, 0, 3]);  unsqueeze_95 = None
    permute_151: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_149, [1, 2, 4, 0, 3]);  permute_149 = None
    view_130: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_151, [16, 512, 64]);  permute_151 = None
    permute_152: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_150, [1, 4, 0, 3, 2]);  permute_150 = None
    view_131: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_152, [16, 64, 512]);  permute_152 = None
    bmm_28: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_130, view_131)
    view_132: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_28, [16, 512, 1, 1, 512]);  bmm_28 = None
    permute_153: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_132, [3, 0, 1, 4, 2]);  view_132 = None
    view_133: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_153, [1, 16, 512, 512]);  permute_153 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_36: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_117, primals_27);  view_117 = primals_27 = None
    unsqueeze_96: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_36, 4);  add_36 = None
    permute_154: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_96, [1, 2, 0, 4, 3]);  unsqueeze_96 = None
    unsqueeze_97: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_129, 4);  view_129 = None
    permute_155: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_97, [1, 2, 4, 0, 3]);  unsqueeze_97 = None
    permute_156: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_154, [1, 2, 4, 0, 3]);  permute_154 = None
    view_134: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_156, [16, 512, 64]);  permute_156 = None
    permute_157: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_155, [1, 4, 0, 3, 2]);  permute_155 = None
    view_135: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_157, [16, 64, 1024]);  permute_157 = None
    bmm_29: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_134, view_135)
    view_136: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_29, [16, 512, 1, 1, 1024]);  bmm_29 = None
    permute_158: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_136, [3, 0, 1, 4, 2]);  view_136 = None
    view_137: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_158, [1, 16, 512, 1024]);  permute_158 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_138: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_137, [1, 16, 1024, 512]);  view_137 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_24: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_138, 0, 0, 9223372036854775807);  view_138 = None
    slice_25: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_24, 1, 0, 9223372036854775807);  slice_24 = None
    slice_26: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_25, 2, 1, 9223372036854775807);  slice_25 = None
    slice_27: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_26, 3, 0, 9223372036854775807);  slice_26 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_139: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_27, [1, 16, 512, 1023]);  slice_27 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_5: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_28: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_139, 0, 0, 9223372036854775807);  view_139 = None
    slice_29: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_28, 1, 0, 9223372036854775807);  slice_28 = None
    slice_30: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_29, 2, 0, 9223372036854775807);  slice_29 = None
    index_3: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_30, [None, None, None, iota_5]);  slice_30 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_37: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_133, index_3);  view_133 = index_3 = None
    add_38: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_37, 0);  add_37 = None
    mul_28: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_38, 0.125);  add_38 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_3: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_28, [3], True)
    sub_9: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_28, amax_3);  mul_28 = amax_3 = None
    exp_3: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_9);  sub_9 = None
    sum_4: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_3, [3], True)
    div_4: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_3, sum_4);  exp_3 = sum_4 = None
    alias_3: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_4)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_14 = torch.ops.aten.native_dropout.default(div_4, 0.1, True);  div_4 = None
    getitem_40: "f32[1, 16, 512, 512]" = native_dropout_14[0]
    getitem_41: "b8[1, 16, 512, 512]" = native_dropout_14[1];  native_dropout_14 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_98: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_40, 4);  getitem_40 = None
    permute_159: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_98, [2, 0, 1, 4, 3]);  unsqueeze_98 = None
    unsqueeze_99: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_125, 4);  view_125 = None
    permute_160: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_99, [4, 1, 2, 3, 0]);  unsqueeze_99 = None
    permute_161: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_159, [2, 0, 4, 1, 3]);  permute_159 = None
    view_140: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_161, [16, 512, 512]);  permute_161 = None
    permute_162: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_160, [2, 4, 1, 3, 0]);  permute_160 = None
    view_141: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_162, [16, 512, 64]);  permute_162 = None
    bmm_30: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_140, view_141)
    view_142: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_30, [16, 512, 1, 1, 64]);  bmm_30 = None
    permute_163: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_142, [1, 3, 0, 4, 2]);  view_142 = None
    view_143: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_163, [512, 1, 16, 64]);  permute_163 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_100: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_143, 4);  view_143 = None
    permute_164: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_100, [0, 1, 4, 3, 2]);  unsqueeze_100 = None
    unsqueeze_101: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_28, 3);  primals_28 = None
    unsqueeze_102: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_101, 4);  unsqueeze_101 = None
    permute_165: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_102, [3, 4, 0, 2, 1]);  unsqueeze_102 = None
    permute_166: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_164, [0, 3, 4, 1, 2]);  permute_164 = None
    clone_6: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_166, memory_format = torch.contiguous_format);  permute_166 = None
    view_144: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_6, [1, 512, 1024]);  clone_6 = None
    permute_167: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_165, [3, 4, 1, 2, 0]);  permute_165 = None
    clone_7: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_167, memory_format = torch.contiguous_format);  permute_167 = None
    view_145: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_7, [1, 1024, 1024]);  clone_7 = None
    bmm_31: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_144, view_145)
    view_146: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_31, [512, 1, 1, 1, 1024]);  bmm_31 = None
    permute_168: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_146, [0, 3, 4, 1, 2]);  view_146 = None
    view_147: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_168, [512, 1, 1024]);  permute_168 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_15 = torch.ops.aten.native_dropout.default(view_147, 0.1, True);  view_147 = None
    getitem_42: "f32[512, 1, 1024]" = native_dropout_15[0]
    getitem_43: "b8[512, 1, 1024]" = native_dropout_15[1];  native_dropout_15 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_39: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_42, add_34);  getitem_42 = add_34 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_6 = torch.ops.aten.var_mean.correction(add_39, [2], correction = 0, keepdim = True)
    getitem_44: "f32[512, 1, 1]" = var_mean_6[0]
    getitem_45: "f32[512, 1, 1]" = var_mean_6[1];  var_mean_6 = None
    add_40: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_44, 1e-12);  getitem_44 = None
    rsqrt_6: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_40);  add_40 = None
    sub_10: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_39, getitem_45)
    mul_29: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_10, rsqrt_6);  sub_10 = None
    mul_30: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_29, primals_194);  mul_29 = None
    add_41: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_30, primals_195);  mul_30 = primals_195 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_148: "f32[512, 1024]" = torch.ops.aten.view.default(add_41, [512, 1024])
    permute_169: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_196, [1, 0]);  primals_196 = None
    addmm_6: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_197, view_148, permute_169);  primals_197 = None
    view_149: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_6, [512, 1, 4096]);  addmm_6 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_31: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_149, 0.5)
    mul_32: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_149, 0.7071067811865476)
    erf_3: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_32);  mul_32 = None
    add_42: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_3, 1);  erf_3 = None
    mul_33: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_31, add_42);  mul_31 = add_42 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_16 = torch.ops.aten.native_dropout.default(mul_33, 0.1, True);  mul_33 = None
    getitem_46: "f32[512, 1, 4096]" = native_dropout_16[0]
    getitem_47: "b8[512, 1, 4096]" = native_dropout_16[1];  native_dropout_16 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_150: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_46, [512, 4096]);  getitem_46 = None
    permute_170: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_198, [1, 0]);  primals_198 = None
    addmm_7: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_199, view_150, permute_170);  primals_199 = None
    view_151: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_7, [512, 1, 1024]);  addmm_7 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_17 = torch.ops.aten.native_dropout.default(view_151, 0.1, True);  view_151 = None
    getitem_48: "f32[512, 1, 1024]" = native_dropout_17[0]
    getitem_49: "b8[512, 1, 1024]" = native_dropout_17[1];  native_dropout_17 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_43: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_48, add_41);  getitem_48 = add_41 = None
    var_mean_7 = torch.ops.aten.var_mean.correction(add_43, [2], correction = 0, keepdim = True)
    getitem_50: "f32[512, 1, 1]" = var_mean_7[0]
    getitem_51: "f32[512, 1, 1]" = var_mean_7[1];  var_mean_7 = None
    add_44: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_50, 1e-12);  getitem_50 = None
    rsqrt_7: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_44);  add_44 = None
    sub_11: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_43, getitem_51)
    mul_34: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_11, rsqrt_7);  sub_11 = None
    mul_35: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_34, primals_200);  mul_34 = None
    add_45: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_35, primals_201);  mul_35 = primals_201 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_103: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_45, 3)
    unsqueeze_104: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_103, 4);  unsqueeze_103 = None
    permute_171: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_104, [0, 1, 3, 4, 2]);  unsqueeze_104 = None
    unsqueeze_105: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_29, 3);  primals_29 = None
    unsqueeze_106: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_105, 4);  unsqueeze_105 = None
    permute_172: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_106, [3, 4, 1, 2, 0]);  unsqueeze_106 = None
    permute_173: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_171, [0, 4, 1, 2, 3]);  permute_171 = None
    view_152: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_173, [1, 512, 1024]);  permute_173 = None
    permute_174: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_172, [4, 1, 2, 3, 0]);  permute_172 = None
    view_153: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_174, [1, 1024, 1024]);  permute_174 = None
    bmm_32: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_152, view_153)
    view_154: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_32, [512, 1, 1, 16, 64]);  bmm_32 = None
    permute_175: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_154, [0, 2, 3, 4, 1]);  view_154 = None
    view_155: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_175, [512, 1, 16, 64]);  permute_175 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_107: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_45, 3)
    unsqueeze_108: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_107, 4);  unsqueeze_107 = None
    permute_176: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_108, [0, 1, 3, 4, 2]);  unsqueeze_108 = None
    unsqueeze_109: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_30, 3);  primals_30 = None
    unsqueeze_110: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_109, 4);  unsqueeze_109 = None
    permute_177: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_110, [3, 4, 1, 2, 0]);  unsqueeze_110 = None
    permute_178: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_176, [0, 4, 1, 2, 3]);  permute_176 = None
    view_156: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_178, [1, 512, 1024]);  permute_178 = None
    permute_179: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_177, [4, 1, 2, 3, 0]);  permute_177 = None
    view_157: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_179, [1, 1024, 1024]);  permute_179 = None
    bmm_33: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_156, view_157)
    view_158: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_33, [512, 1, 1, 16, 64]);  bmm_33 = None
    permute_180: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_158, [0, 2, 3, 4, 1]);  view_158 = None
    view_159: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_180, [512, 1, 16, 64]);  permute_180 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_111: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_45, 3)
    unsqueeze_112: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_111, 4);  unsqueeze_111 = None
    permute_181: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_112, [0, 1, 3, 4, 2]);  unsqueeze_112 = None
    unsqueeze_113: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_31, 3);  primals_31 = None
    unsqueeze_114: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_113, 4);  unsqueeze_113 = None
    permute_182: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_114, [3, 4, 1, 2, 0]);  unsqueeze_114 = None
    permute_183: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_181, [0, 4, 1, 2, 3]);  permute_181 = None
    view_160: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_183, [1, 512, 1024]);  permute_183 = None
    permute_184: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_182, [4, 1, 2, 3, 0]);  permute_182 = None
    view_161: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_184, [1, 1024, 1024]);  permute_184 = None
    bmm_34: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_160, view_161)
    view_162: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_34, [512, 1, 1, 16, 64]);  bmm_34 = None
    permute_185: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_162, [0, 2, 3, 4, 1]);  view_162 = None
    view_163: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_185, [512, 1, 16, 64]);  permute_185 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_115: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_116: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_115, 4);  unsqueeze_115 = None
    permute_186: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_116, [0, 1, 3, 4, 2]);  unsqueeze_116 = None
    unsqueeze_117: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_32, 3);  primals_32 = None
    unsqueeze_118: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_117, 4);  unsqueeze_117 = None
    permute_187: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_118, [3, 4, 1, 2, 0]);  unsqueeze_118 = None
    permute_188: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_186, [0, 4, 1, 2, 3]);  permute_186 = None
    view_164: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_188, [1, 1024, 1024]);  permute_188 = None
    permute_189: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_187, [4, 1, 2, 3, 0]);  permute_187 = None
    view_165: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_189, [1, 1024, 1024]);  permute_189 = None
    bmm_35: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_164, view_165);  view_165 = None
    view_166: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_35, [1024, 1, 1, 16, 64]);  bmm_35 = None
    permute_190: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_166, [0, 2, 3, 4, 1]);  view_166 = None
    view_167: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_190, [1024, 1, 16, 64]);  permute_190 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_46: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_155, primals_33);  primals_33 = None
    unsqueeze_119: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_46, 4);  add_46 = None
    permute_191: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_119, [1, 2, 0, 4, 3]);  unsqueeze_119 = None
    unsqueeze_120: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_159, 4);  view_159 = None
    permute_192: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_120, [1, 2, 4, 0, 3]);  unsqueeze_120 = None
    permute_193: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_191, [1, 2, 4, 0, 3]);  permute_191 = None
    view_168: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_193, [16, 512, 64]);  permute_193 = None
    permute_194: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_192, [1, 4, 0, 3, 2]);  permute_192 = None
    view_169: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_194, [16, 64, 512]);  permute_194 = None
    bmm_36: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_168, view_169)
    view_170: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_36, [16, 512, 1, 1, 512]);  bmm_36 = None
    permute_195: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_170, [3, 0, 1, 4, 2]);  view_170 = None
    view_171: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_195, [1, 16, 512, 512]);  permute_195 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_47: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_155, primals_34);  view_155 = primals_34 = None
    unsqueeze_121: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_47, 4);  add_47 = None
    permute_196: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_121, [1, 2, 0, 4, 3]);  unsqueeze_121 = None
    unsqueeze_122: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_167, 4);  view_167 = None
    permute_197: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_122, [1, 2, 4, 0, 3]);  unsqueeze_122 = None
    permute_198: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_196, [1, 2, 4, 0, 3]);  permute_196 = None
    view_172: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_198, [16, 512, 64]);  permute_198 = None
    permute_199: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_197, [1, 4, 0, 3, 2]);  permute_197 = None
    view_173: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_199, [16, 64, 1024]);  permute_199 = None
    bmm_37: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_172, view_173)
    view_174: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_37, [16, 512, 1, 1, 1024]);  bmm_37 = None
    permute_200: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_174, [3, 0, 1, 4, 2]);  view_174 = None
    view_175: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_200, [1, 16, 512, 1024]);  permute_200 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_176: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_175, [1, 16, 1024, 512]);  view_175 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_31: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_176, 0, 0, 9223372036854775807);  view_176 = None
    slice_32: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_31, 1, 0, 9223372036854775807);  slice_31 = None
    slice_33: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_32, 2, 1, 9223372036854775807);  slice_32 = None
    slice_34: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_33, 3, 0, 9223372036854775807);  slice_33 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_177: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_34, [1, 16, 512, 1023]);  slice_34 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_6: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_35: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_177, 0, 0, 9223372036854775807);  view_177 = None
    slice_36: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_35, 1, 0, 9223372036854775807);  slice_35 = None
    slice_37: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_36, 2, 0, 9223372036854775807);  slice_36 = None
    index_4: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_37, [None, None, None, iota_6]);  slice_37 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_48: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_171, index_4);  view_171 = index_4 = None
    add_49: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_48, 0);  add_48 = None
    mul_36: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_49, 0.125);  add_49 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_4: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_36, [3], True)
    sub_12: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_36, amax_4);  mul_36 = amax_4 = None
    exp_4: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_12);  sub_12 = None
    sum_5: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_4, [3], True)
    div_5: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_4, sum_5);  exp_4 = sum_5 = None
    alias_4: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_5)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_18 = torch.ops.aten.native_dropout.default(div_5, 0.1, True);  div_5 = None
    getitem_52: "f32[1, 16, 512, 512]" = native_dropout_18[0]
    getitem_53: "b8[1, 16, 512, 512]" = native_dropout_18[1];  native_dropout_18 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_123: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_52, 4);  getitem_52 = None
    permute_201: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_123, [2, 0, 1, 4, 3]);  unsqueeze_123 = None
    unsqueeze_124: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_163, 4);  view_163 = None
    permute_202: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_124, [4, 1, 2, 3, 0]);  unsqueeze_124 = None
    permute_203: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_201, [2, 0, 4, 1, 3]);  permute_201 = None
    view_178: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_203, [16, 512, 512]);  permute_203 = None
    permute_204: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_202, [2, 4, 1, 3, 0]);  permute_202 = None
    view_179: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_204, [16, 512, 64]);  permute_204 = None
    bmm_38: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_178, view_179)
    view_180: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_38, [16, 512, 1, 1, 64]);  bmm_38 = None
    permute_205: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_180, [1, 3, 0, 4, 2]);  view_180 = None
    view_181: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_205, [512, 1, 16, 64]);  permute_205 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_125: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_181, 4);  view_181 = None
    permute_206: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_125, [0, 1, 4, 3, 2]);  unsqueeze_125 = None
    unsqueeze_126: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_35, 3);  primals_35 = None
    unsqueeze_127: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_126, 4);  unsqueeze_126 = None
    permute_207: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_127, [3, 4, 0, 2, 1]);  unsqueeze_127 = None
    permute_208: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_206, [0, 3, 4, 1, 2]);  permute_206 = None
    clone_8: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_208, memory_format = torch.contiguous_format);  permute_208 = None
    view_182: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_8, [1, 512, 1024]);  clone_8 = None
    permute_209: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_207, [3, 4, 1, 2, 0]);  permute_207 = None
    clone_9: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_209, memory_format = torch.contiguous_format);  permute_209 = None
    view_183: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_9, [1, 1024, 1024]);  clone_9 = None
    bmm_39: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_182, view_183)
    view_184: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_39, [512, 1, 1, 1, 1024]);  bmm_39 = None
    permute_210: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_184, [0, 3, 4, 1, 2]);  view_184 = None
    view_185: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_210, [512, 1, 1024]);  permute_210 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_19 = torch.ops.aten.native_dropout.default(view_185, 0.1, True);  view_185 = None
    getitem_54: "f32[512, 1, 1024]" = native_dropout_19[0]
    getitem_55: "b8[512, 1, 1024]" = native_dropout_19[1];  native_dropout_19 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_50: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_54, add_45);  getitem_54 = add_45 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_8 = torch.ops.aten.var_mean.correction(add_50, [2], correction = 0, keepdim = True)
    getitem_56: "f32[512, 1, 1]" = var_mean_8[0]
    getitem_57: "f32[512, 1, 1]" = var_mean_8[1];  var_mean_8 = None
    add_51: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_56, 1e-12);  getitem_56 = None
    rsqrt_8: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_51);  add_51 = None
    sub_13: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_50, getitem_57)
    mul_37: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_13, rsqrt_8);  sub_13 = None
    mul_38: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_37, primals_202);  mul_37 = None
    add_52: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_38, primals_203);  mul_38 = primals_203 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_186: "f32[512, 1024]" = torch.ops.aten.view.default(add_52, [512, 1024])
    permute_211: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_204, [1, 0]);  primals_204 = None
    addmm_8: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_205, view_186, permute_211);  primals_205 = None
    view_187: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_8, [512, 1, 4096]);  addmm_8 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_39: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_187, 0.5)
    mul_40: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_187, 0.7071067811865476)
    erf_4: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_40);  mul_40 = None
    add_53: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_4, 1);  erf_4 = None
    mul_41: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_39, add_53);  mul_39 = add_53 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_20 = torch.ops.aten.native_dropout.default(mul_41, 0.1, True);  mul_41 = None
    getitem_58: "f32[512, 1, 4096]" = native_dropout_20[0]
    getitem_59: "b8[512, 1, 4096]" = native_dropout_20[1];  native_dropout_20 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_188: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_58, [512, 4096]);  getitem_58 = None
    permute_212: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_206, [1, 0]);  primals_206 = None
    addmm_9: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_207, view_188, permute_212);  primals_207 = None
    view_189: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_9, [512, 1, 1024]);  addmm_9 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_21 = torch.ops.aten.native_dropout.default(view_189, 0.1, True);  view_189 = None
    getitem_60: "f32[512, 1, 1024]" = native_dropout_21[0]
    getitem_61: "b8[512, 1, 1024]" = native_dropout_21[1];  native_dropout_21 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_54: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_60, add_52);  getitem_60 = add_52 = None
    var_mean_9 = torch.ops.aten.var_mean.correction(add_54, [2], correction = 0, keepdim = True)
    getitem_62: "f32[512, 1, 1]" = var_mean_9[0]
    getitem_63: "f32[512, 1, 1]" = var_mean_9[1];  var_mean_9 = None
    add_55: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_62, 1e-12);  getitem_62 = None
    rsqrt_9: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_55);  add_55 = None
    sub_14: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_54, getitem_63)
    mul_42: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_14, rsqrt_9);  sub_14 = None
    mul_43: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_42, primals_208);  mul_42 = None
    add_56: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_43, primals_209);  mul_43 = primals_209 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_128: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_56, 3)
    unsqueeze_129: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_128, 4);  unsqueeze_128 = None
    permute_213: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_129, [0, 1, 3, 4, 2]);  unsqueeze_129 = None
    unsqueeze_130: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_36, 3);  primals_36 = None
    unsqueeze_131: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_130, 4);  unsqueeze_130 = None
    permute_214: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_131, [3, 4, 1, 2, 0]);  unsqueeze_131 = None
    permute_215: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_213, [0, 4, 1, 2, 3]);  permute_213 = None
    view_190: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_215, [1, 512, 1024]);  permute_215 = None
    permute_216: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_214, [4, 1, 2, 3, 0]);  permute_214 = None
    view_191: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_216, [1, 1024, 1024]);  permute_216 = None
    bmm_40: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_190, view_191)
    view_192: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_40, [512, 1, 1, 16, 64]);  bmm_40 = None
    permute_217: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_192, [0, 2, 3, 4, 1]);  view_192 = None
    view_193: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_217, [512, 1, 16, 64]);  permute_217 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_132: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_56, 3)
    unsqueeze_133: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_132, 4);  unsqueeze_132 = None
    permute_218: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_133, [0, 1, 3, 4, 2]);  unsqueeze_133 = None
    unsqueeze_134: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_37, 3);  primals_37 = None
    unsqueeze_135: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_134, 4);  unsqueeze_134 = None
    permute_219: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_135, [3, 4, 1, 2, 0]);  unsqueeze_135 = None
    permute_220: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_218, [0, 4, 1, 2, 3]);  permute_218 = None
    view_194: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_220, [1, 512, 1024]);  permute_220 = None
    permute_221: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_219, [4, 1, 2, 3, 0]);  permute_219 = None
    view_195: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_221, [1, 1024, 1024]);  permute_221 = None
    bmm_41: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_194, view_195)
    view_196: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_41, [512, 1, 1, 16, 64]);  bmm_41 = None
    permute_222: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_196, [0, 2, 3, 4, 1]);  view_196 = None
    view_197: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_222, [512, 1, 16, 64]);  permute_222 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_136: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_56, 3)
    unsqueeze_137: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_136, 4);  unsqueeze_136 = None
    permute_223: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_137, [0, 1, 3, 4, 2]);  unsqueeze_137 = None
    unsqueeze_138: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_38, 3);  primals_38 = None
    unsqueeze_139: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_138, 4);  unsqueeze_138 = None
    permute_224: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_139, [3, 4, 1, 2, 0]);  unsqueeze_139 = None
    permute_225: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_223, [0, 4, 1, 2, 3]);  permute_223 = None
    view_198: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_225, [1, 512, 1024]);  permute_225 = None
    permute_226: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_224, [4, 1, 2, 3, 0]);  permute_224 = None
    view_199: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_226, [1, 1024, 1024]);  permute_226 = None
    bmm_42: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_198, view_199)
    view_200: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_42, [512, 1, 1, 16, 64]);  bmm_42 = None
    permute_227: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_200, [0, 2, 3, 4, 1]);  view_200 = None
    view_201: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_227, [512, 1, 16, 64]);  permute_227 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_140: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_141: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_140, 4);  unsqueeze_140 = None
    permute_228: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_141, [0, 1, 3, 4, 2]);  unsqueeze_141 = None
    unsqueeze_142: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_39, 3);  primals_39 = None
    unsqueeze_143: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_142, 4);  unsqueeze_142 = None
    permute_229: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_143, [3, 4, 1, 2, 0]);  unsqueeze_143 = None
    permute_230: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_228, [0, 4, 1, 2, 3]);  permute_228 = None
    view_202: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_230, [1, 1024, 1024]);  permute_230 = None
    permute_231: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_229, [4, 1, 2, 3, 0]);  permute_229 = None
    view_203: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_231, [1, 1024, 1024]);  permute_231 = None
    bmm_43: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_202, view_203);  view_203 = None
    view_204: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_43, [1024, 1, 1, 16, 64]);  bmm_43 = None
    permute_232: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_204, [0, 2, 3, 4, 1]);  view_204 = None
    view_205: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_232, [1024, 1, 16, 64]);  permute_232 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_57: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_193, primals_40);  primals_40 = None
    unsqueeze_144: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_57, 4);  add_57 = None
    permute_233: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_144, [1, 2, 0, 4, 3]);  unsqueeze_144 = None
    unsqueeze_145: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_197, 4);  view_197 = None
    permute_234: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_145, [1, 2, 4, 0, 3]);  unsqueeze_145 = None
    permute_235: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_233, [1, 2, 4, 0, 3]);  permute_233 = None
    view_206: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_235, [16, 512, 64]);  permute_235 = None
    permute_236: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_234, [1, 4, 0, 3, 2]);  permute_234 = None
    view_207: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_236, [16, 64, 512]);  permute_236 = None
    bmm_44: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_206, view_207)
    view_208: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_44, [16, 512, 1, 1, 512]);  bmm_44 = None
    permute_237: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_208, [3, 0, 1, 4, 2]);  view_208 = None
    view_209: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_237, [1, 16, 512, 512]);  permute_237 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_58: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_193, primals_41);  view_193 = primals_41 = None
    unsqueeze_146: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_58, 4);  add_58 = None
    permute_238: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_146, [1, 2, 0, 4, 3]);  unsqueeze_146 = None
    unsqueeze_147: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_205, 4);  view_205 = None
    permute_239: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_147, [1, 2, 4, 0, 3]);  unsqueeze_147 = None
    permute_240: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_238, [1, 2, 4, 0, 3]);  permute_238 = None
    view_210: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_240, [16, 512, 64]);  permute_240 = None
    permute_241: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_239, [1, 4, 0, 3, 2]);  permute_239 = None
    view_211: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_241, [16, 64, 1024]);  permute_241 = None
    bmm_45: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_210, view_211)
    view_212: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_45, [16, 512, 1, 1, 1024]);  bmm_45 = None
    permute_242: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_212, [3, 0, 1, 4, 2]);  view_212 = None
    view_213: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_242, [1, 16, 512, 1024]);  permute_242 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_214: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_213, [1, 16, 1024, 512]);  view_213 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_38: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_214, 0, 0, 9223372036854775807);  view_214 = None
    slice_39: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_38, 1, 0, 9223372036854775807);  slice_38 = None
    slice_40: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_39, 2, 1, 9223372036854775807);  slice_39 = None
    slice_41: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_40, 3, 0, 9223372036854775807);  slice_40 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_215: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_41, [1, 16, 512, 1023]);  slice_41 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_7: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_42: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_215, 0, 0, 9223372036854775807);  view_215 = None
    slice_43: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_42, 1, 0, 9223372036854775807);  slice_42 = None
    slice_44: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_43, 2, 0, 9223372036854775807);  slice_43 = None
    index_5: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_44, [None, None, None, iota_7]);  slice_44 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_59: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_209, index_5);  view_209 = index_5 = None
    add_60: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_59, 0);  add_59 = None
    mul_44: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_60, 0.125);  add_60 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_5: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_44, [3], True)
    sub_15: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_44, amax_5);  mul_44 = amax_5 = None
    exp_5: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_15);  sub_15 = None
    sum_6: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_5, [3], True)
    div_6: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_5, sum_6);  exp_5 = sum_6 = None
    alias_5: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_6)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_22 = torch.ops.aten.native_dropout.default(div_6, 0.1, True);  div_6 = None
    getitem_64: "f32[1, 16, 512, 512]" = native_dropout_22[0]
    getitem_65: "b8[1, 16, 512, 512]" = native_dropout_22[1];  native_dropout_22 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_148: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_64, 4);  getitem_64 = None
    permute_243: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_148, [2, 0, 1, 4, 3]);  unsqueeze_148 = None
    unsqueeze_149: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_201, 4);  view_201 = None
    permute_244: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_149, [4, 1, 2, 3, 0]);  unsqueeze_149 = None
    permute_245: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_243, [2, 0, 4, 1, 3]);  permute_243 = None
    view_216: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_245, [16, 512, 512]);  permute_245 = None
    permute_246: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_244, [2, 4, 1, 3, 0]);  permute_244 = None
    view_217: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_246, [16, 512, 64]);  permute_246 = None
    bmm_46: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_216, view_217)
    view_218: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_46, [16, 512, 1, 1, 64]);  bmm_46 = None
    permute_247: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_218, [1, 3, 0, 4, 2]);  view_218 = None
    view_219: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_247, [512, 1, 16, 64]);  permute_247 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_150: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_219, 4);  view_219 = None
    permute_248: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_150, [0, 1, 4, 3, 2]);  unsqueeze_150 = None
    unsqueeze_151: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_42, 3);  primals_42 = None
    unsqueeze_152: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_151, 4);  unsqueeze_151 = None
    permute_249: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_152, [3, 4, 0, 2, 1]);  unsqueeze_152 = None
    permute_250: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_248, [0, 3, 4, 1, 2]);  permute_248 = None
    clone_10: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_250, memory_format = torch.contiguous_format);  permute_250 = None
    view_220: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_10, [1, 512, 1024]);  clone_10 = None
    permute_251: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_249, [3, 4, 1, 2, 0]);  permute_249 = None
    clone_11: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_251, memory_format = torch.contiguous_format);  permute_251 = None
    view_221: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_11, [1, 1024, 1024]);  clone_11 = None
    bmm_47: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_220, view_221)
    view_222: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_47, [512, 1, 1, 1, 1024]);  bmm_47 = None
    permute_252: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_222, [0, 3, 4, 1, 2]);  view_222 = None
    view_223: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_252, [512, 1, 1024]);  permute_252 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_23 = torch.ops.aten.native_dropout.default(view_223, 0.1, True);  view_223 = None
    getitem_66: "f32[512, 1, 1024]" = native_dropout_23[0]
    getitem_67: "b8[512, 1, 1024]" = native_dropout_23[1];  native_dropout_23 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_61: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_66, add_56);  getitem_66 = add_56 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_10 = torch.ops.aten.var_mean.correction(add_61, [2], correction = 0, keepdim = True)
    getitem_68: "f32[512, 1, 1]" = var_mean_10[0]
    getitem_69: "f32[512, 1, 1]" = var_mean_10[1];  var_mean_10 = None
    add_62: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_68, 1e-12);  getitem_68 = None
    rsqrt_10: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_62);  add_62 = None
    sub_16: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_61, getitem_69)
    mul_45: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_16, rsqrt_10);  sub_16 = None
    mul_46: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_45, primals_210);  mul_45 = None
    add_63: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_46, primals_211);  mul_46 = primals_211 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_224: "f32[512, 1024]" = torch.ops.aten.view.default(add_63, [512, 1024])
    permute_253: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_212, [1, 0]);  primals_212 = None
    addmm_10: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_213, view_224, permute_253);  primals_213 = None
    view_225: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_10, [512, 1, 4096]);  addmm_10 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_47: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_225, 0.5)
    mul_48: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_225, 0.7071067811865476)
    erf_5: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_48);  mul_48 = None
    add_64: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_5, 1);  erf_5 = None
    mul_49: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_47, add_64);  mul_47 = add_64 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_24 = torch.ops.aten.native_dropout.default(mul_49, 0.1, True);  mul_49 = None
    getitem_70: "f32[512, 1, 4096]" = native_dropout_24[0]
    getitem_71: "b8[512, 1, 4096]" = native_dropout_24[1];  native_dropout_24 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_226: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_70, [512, 4096]);  getitem_70 = None
    permute_254: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_214, [1, 0]);  primals_214 = None
    addmm_11: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_215, view_226, permute_254);  primals_215 = None
    view_227: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_11, [512, 1, 1024]);  addmm_11 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_25 = torch.ops.aten.native_dropout.default(view_227, 0.1, True);  view_227 = None
    getitem_72: "f32[512, 1, 1024]" = native_dropout_25[0]
    getitem_73: "b8[512, 1, 1024]" = native_dropout_25[1];  native_dropout_25 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_65: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_72, add_63);  getitem_72 = add_63 = None
    var_mean_11 = torch.ops.aten.var_mean.correction(add_65, [2], correction = 0, keepdim = True)
    getitem_74: "f32[512, 1, 1]" = var_mean_11[0]
    getitem_75: "f32[512, 1, 1]" = var_mean_11[1];  var_mean_11 = None
    add_66: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_74, 1e-12);  getitem_74 = None
    rsqrt_11: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_66);  add_66 = None
    sub_17: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_65, getitem_75)
    mul_50: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_17, rsqrt_11);  sub_17 = None
    mul_51: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_50, primals_216);  mul_50 = None
    add_67: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_51, primals_217);  mul_51 = primals_217 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_153: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_67, 3)
    unsqueeze_154: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_153, 4);  unsqueeze_153 = None
    permute_255: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_154, [0, 1, 3, 4, 2]);  unsqueeze_154 = None
    unsqueeze_155: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_43, 3);  primals_43 = None
    unsqueeze_156: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_155, 4);  unsqueeze_155 = None
    permute_256: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_156, [3, 4, 1, 2, 0]);  unsqueeze_156 = None
    permute_257: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_255, [0, 4, 1, 2, 3]);  permute_255 = None
    view_228: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_257, [1, 512, 1024]);  permute_257 = None
    permute_258: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_256, [4, 1, 2, 3, 0]);  permute_256 = None
    view_229: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_258, [1, 1024, 1024]);  permute_258 = None
    bmm_48: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_228, view_229)
    view_230: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_48, [512, 1, 1, 16, 64]);  bmm_48 = None
    permute_259: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_230, [0, 2, 3, 4, 1]);  view_230 = None
    view_231: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_259, [512, 1, 16, 64]);  permute_259 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_157: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_67, 3)
    unsqueeze_158: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_157, 4);  unsqueeze_157 = None
    permute_260: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_158, [0, 1, 3, 4, 2]);  unsqueeze_158 = None
    unsqueeze_159: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_44, 3);  primals_44 = None
    unsqueeze_160: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_159, 4);  unsqueeze_159 = None
    permute_261: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_160, [3, 4, 1, 2, 0]);  unsqueeze_160 = None
    permute_262: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_260, [0, 4, 1, 2, 3]);  permute_260 = None
    view_232: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_262, [1, 512, 1024]);  permute_262 = None
    permute_263: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_261, [4, 1, 2, 3, 0]);  permute_261 = None
    view_233: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_263, [1, 1024, 1024]);  permute_263 = None
    bmm_49: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_232, view_233)
    view_234: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_49, [512, 1, 1, 16, 64]);  bmm_49 = None
    permute_264: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_234, [0, 2, 3, 4, 1]);  view_234 = None
    view_235: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_264, [512, 1, 16, 64]);  permute_264 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_161: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_67, 3)
    unsqueeze_162: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_161, 4);  unsqueeze_161 = None
    permute_265: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_162, [0, 1, 3, 4, 2]);  unsqueeze_162 = None
    unsqueeze_163: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_45, 3);  primals_45 = None
    unsqueeze_164: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_163, 4);  unsqueeze_163 = None
    permute_266: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_164, [3, 4, 1, 2, 0]);  unsqueeze_164 = None
    permute_267: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_265, [0, 4, 1, 2, 3]);  permute_265 = None
    view_236: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_267, [1, 512, 1024]);  permute_267 = None
    permute_268: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_266, [4, 1, 2, 3, 0]);  permute_266 = None
    view_237: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_268, [1, 1024, 1024]);  permute_268 = None
    bmm_50: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_236, view_237)
    view_238: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_50, [512, 1, 1, 16, 64]);  bmm_50 = None
    permute_269: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_238, [0, 2, 3, 4, 1]);  view_238 = None
    view_239: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_269, [512, 1, 16, 64]);  permute_269 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_165: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_166: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_165, 4);  unsqueeze_165 = None
    permute_270: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_166, [0, 1, 3, 4, 2]);  unsqueeze_166 = None
    unsqueeze_167: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_46, 3);  primals_46 = None
    unsqueeze_168: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_167, 4);  unsqueeze_167 = None
    permute_271: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_168, [3, 4, 1, 2, 0]);  unsqueeze_168 = None
    permute_272: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_270, [0, 4, 1, 2, 3]);  permute_270 = None
    view_240: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_272, [1, 1024, 1024]);  permute_272 = None
    permute_273: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_271, [4, 1, 2, 3, 0]);  permute_271 = None
    view_241: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_273, [1, 1024, 1024]);  permute_273 = None
    bmm_51: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_240, view_241);  view_241 = None
    view_242: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_51, [1024, 1, 1, 16, 64]);  bmm_51 = None
    permute_274: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_242, [0, 2, 3, 4, 1]);  view_242 = None
    view_243: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_274, [1024, 1, 16, 64]);  permute_274 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_68: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_231, primals_47);  primals_47 = None
    unsqueeze_169: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_68, 4);  add_68 = None
    permute_275: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_169, [1, 2, 0, 4, 3]);  unsqueeze_169 = None
    unsqueeze_170: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_235, 4);  view_235 = None
    permute_276: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_170, [1, 2, 4, 0, 3]);  unsqueeze_170 = None
    permute_277: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_275, [1, 2, 4, 0, 3]);  permute_275 = None
    view_244: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_277, [16, 512, 64]);  permute_277 = None
    permute_278: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_276, [1, 4, 0, 3, 2]);  permute_276 = None
    view_245: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_278, [16, 64, 512]);  permute_278 = None
    bmm_52: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_244, view_245)
    view_246: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_52, [16, 512, 1, 1, 512]);  bmm_52 = None
    permute_279: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_246, [3, 0, 1, 4, 2]);  view_246 = None
    view_247: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_279, [1, 16, 512, 512]);  permute_279 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_69: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_231, primals_48);  view_231 = primals_48 = None
    unsqueeze_171: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_69, 4);  add_69 = None
    permute_280: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_171, [1, 2, 0, 4, 3]);  unsqueeze_171 = None
    unsqueeze_172: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_243, 4);  view_243 = None
    permute_281: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_172, [1, 2, 4, 0, 3]);  unsqueeze_172 = None
    permute_282: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_280, [1, 2, 4, 0, 3]);  permute_280 = None
    view_248: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_282, [16, 512, 64]);  permute_282 = None
    permute_283: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_281, [1, 4, 0, 3, 2]);  permute_281 = None
    view_249: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_283, [16, 64, 1024]);  permute_283 = None
    bmm_53: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_248, view_249)
    view_250: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_53, [16, 512, 1, 1, 1024]);  bmm_53 = None
    permute_284: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_250, [3, 0, 1, 4, 2]);  view_250 = None
    view_251: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_284, [1, 16, 512, 1024]);  permute_284 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_252: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_251, [1, 16, 1024, 512]);  view_251 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_45: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_252, 0, 0, 9223372036854775807);  view_252 = None
    slice_46: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_45, 1, 0, 9223372036854775807);  slice_45 = None
    slice_47: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_46, 2, 1, 9223372036854775807);  slice_46 = None
    slice_48: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_47, 3, 0, 9223372036854775807);  slice_47 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_253: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_48, [1, 16, 512, 1023]);  slice_48 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_8: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_49: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_253, 0, 0, 9223372036854775807);  view_253 = None
    slice_50: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_49, 1, 0, 9223372036854775807);  slice_49 = None
    slice_51: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_50, 2, 0, 9223372036854775807);  slice_50 = None
    index_6: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_51, [None, None, None, iota_8]);  slice_51 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_70: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_247, index_6);  view_247 = index_6 = None
    add_71: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_70, 0);  add_70 = None
    mul_52: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_71, 0.125);  add_71 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_6: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_52, [3], True)
    sub_18: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_52, amax_6);  mul_52 = amax_6 = None
    exp_6: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_18);  sub_18 = None
    sum_7: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_6, [3], True)
    div_7: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_6, sum_7);  exp_6 = sum_7 = None
    alias_6: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_7)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_26 = torch.ops.aten.native_dropout.default(div_7, 0.1, True);  div_7 = None
    getitem_76: "f32[1, 16, 512, 512]" = native_dropout_26[0]
    getitem_77: "b8[1, 16, 512, 512]" = native_dropout_26[1];  native_dropout_26 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_173: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_76, 4);  getitem_76 = None
    permute_285: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_173, [2, 0, 1, 4, 3]);  unsqueeze_173 = None
    unsqueeze_174: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_239, 4);  view_239 = None
    permute_286: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_174, [4, 1, 2, 3, 0]);  unsqueeze_174 = None
    permute_287: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_285, [2, 0, 4, 1, 3]);  permute_285 = None
    view_254: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_287, [16, 512, 512]);  permute_287 = None
    permute_288: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_286, [2, 4, 1, 3, 0]);  permute_286 = None
    view_255: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_288, [16, 512, 64]);  permute_288 = None
    bmm_54: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_254, view_255)
    view_256: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_54, [16, 512, 1, 1, 64]);  bmm_54 = None
    permute_289: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_256, [1, 3, 0, 4, 2]);  view_256 = None
    view_257: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_289, [512, 1, 16, 64]);  permute_289 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_175: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_257, 4);  view_257 = None
    permute_290: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_175, [0, 1, 4, 3, 2]);  unsqueeze_175 = None
    unsqueeze_176: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_49, 3);  primals_49 = None
    unsqueeze_177: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_176, 4);  unsqueeze_176 = None
    permute_291: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_177, [3, 4, 0, 2, 1]);  unsqueeze_177 = None
    permute_292: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_290, [0, 3, 4, 1, 2]);  permute_290 = None
    clone_12: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_292, memory_format = torch.contiguous_format);  permute_292 = None
    view_258: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_12, [1, 512, 1024]);  clone_12 = None
    permute_293: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_291, [3, 4, 1, 2, 0]);  permute_291 = None
    clone_13: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_293, memory_format = torch.contiguous_format);  permute_293 = None
    view_259: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_13, [1, 1024, 1024]);  clone_13 = None
    bmm_55: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_258, view_259)
    view_260: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_55, [512, 1, 1, 1, 1024]);  bmm_55 = None
    permute_294: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_260, [0, 3, 4, 1, 2]);  view_260 = None
    view_261: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_294, [512, 1, 1024]);  permute_294 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_27 = torch.ops.aten.native_dropout.default(view_261, 0.1, True);  view_261 = None
    getitem_78: "f32[512, 1, 1024]" = native_dropout_27[0]
    getitem_79: "b8[512, 1, 1024]" = native_dropout_27[1];  native_dropout_27 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_72: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_78, add_67);  getitem_78 = add_67 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_12 = torch.ops.aten.var_mean.correction(add_72, [2], correction = 0, keepdim = True)
    getitem_80: "f32[512, 1, 1]" = var_mean_12[0]
    getitem_81: "f32[512, 1, 1]" = var_mean_12[1];  var_mean_12 = None
    add_73: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_80, 1e-12);  getitem_80 = None
    rsqrt_12: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_73);  add_73 = None
    sub_19: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_72, getitem_81)
    mul_53: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_19, rsqrt_12);  sub_19 = None
    mul_54: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_53, primals_218);  mul_53 = None
    add_74: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_54, primals_219);  mul_54 = primals_219 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_262: "f32[512, 1024]" = torch.ops.aten.view.default(add_74, [512, 1024])
    permute_295: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_220, [1, 0]);  primals_220 = None
    addmm_12: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_221, view_262, permute_295);  primals_221 = None
    view_263: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_12, [512, 1, 4096]);  addmm_12 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_55: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_263, 0.5)
    mul_56: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_263, 0.7071067811865476)
    erf_6: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_56);  mul_56 = None
    add_75: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_6, 1);  erf_6 = None
    mul_57: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_55, add_75);  mul_55 = add_75 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_28 = torch.ops.aten.native_dropout.default(mul_57, 0.1, True);  mul_57 = None
    getitem_82: "f32[512, 1, 4096]" = native_dropout_28[0]
    getitem_83: "b8[512, 1, 4096]" = native_dropout_28[1];  native_dropout_28 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_264: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_82, [512, 4096]);  getitem_82 = None
    permute_296: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_222, [1, 0]);  primals_222 = None
    addmm_13: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_223, view_264, permute_296);  primals_223 = None
    view_265: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_13, [512, 1, 1024]);  addmm_13 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_29 = torch.ops.aten.native_dropout.default(view_265, 0.1, True);  view_265 = None
    getitem_84: "f32[512, 1, 1024]" = native_dropout_29[0]
    getitem_85: "b8[512, 1, 1024]" = native_dropout_29[1];  native_dropout_29 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_76: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_84, add_74);  getitem_84 = add_74 = None
    var_mean_13 = torch.ops.aten.var_mean.correction(add_76, [2], correction = 0, keepdim = True)
    getitem_86: "f32[512, 1, 1]" = var_mean_13[0]
    getitem_87: "f32[512, 1, 1]" = var_mean_13[1];  var_mean_13 = None
    add_77: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_86, 1e-12);  getitem_86 = None
    rsqrt_13: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_77);  add_77 = None
    sub_20: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_76, getitem_87)
    mul_58: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_20, rsqrt_13);  sub_20 = None
    mul_59: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_58, primals_224);  mul_58 = None
    add_78: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_59, primals_225);  mul_59 = primals_225 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_178: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_78, 3)
    unsqueeze_179: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_178, 4);  unsqueeze_178 = None
    permute_297: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_179, [0, 1, 3, 4, 2]);  unsqueeze_179 = None
    unsqueeze_180: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_50, 3);  primals_50 = None
    unsqueeze_181: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_180, 4);  unsqueeze_180 = None
    permute_298: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_181, [3, 4, 1, 2, 0]);  unsqueeze_181 = None
    permute_299: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_297, [0, 4, 1, 2, 3]);  permute_297 = None
    view_266: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_299, [1, 512, 1024]);  permute_299 = None
    permute_300: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_298, [4, 1, 2, 3, 0]);  permute_298 = None
    view_267: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_300, [1, 1024, 1024]);  permute_300 = None
    bmm_56: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_266, view_267)
    view_268: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_56, [512, 1, 1, 16, 64]);  bmm_56 = None
    permute_301: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_268, [0, 2, 3, 4, 1]);  view_268 = None
    view_269: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_301, [512, 1, 16, 64]);  permute_301 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_182: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_78, 3)
    unsqueeze_183: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_182, 4);  unsqueeze_182 = None
    permute_302: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_183, [0, 1, 3, 4, 2]);  unsqueeze_183 = None
    unsqueeze_184: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_51, 3);  primals_51 = None
    unsqueeze_185: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_184, 4);  unsqueeze_184 = None
    permute_303: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_185, [3, 4, 1, 2, 0]);  unsqueeze_185 = None
    permute_304: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_302, [0, 4, 1, 2, 3]);  permute_302 = None
    view_270: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_304, [1, 512, 1024]);  permute_304 = None
    permute_305: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_303, [4, 1, 2, 3, 0]);  permute_303 = None
    view_271: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_305, [1, 1024, 1024]);  permute_305 = None
    bmm_57: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_270, view_271)
    view_272: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_57, [512, 1, 1, 16, 64]);  bmm_57 = None
    permute_306: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_272, [0, 2, 3, 4, 1]);  view_272 = None
    view_273: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_306, [512, 1, 16, 64]);  permute_306 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_186: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_78, 3)
    unsqueeze_187: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_186, 4);  unsqueeze_186 = None
    permute_307: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_187, [0, 1, 3, 4, 2]);  unsqueeze_187 = None
    unsqueeze_188: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_52, 3);  primals_52 = None
    unsqueeze_189: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_188, 4);  unsqueeze_188 = None
    permute_308: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_189, [3, 4, 1, 2, 0]);  unsqueeze_189 = None
    permute_309: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_307, [0, 4, 1, 2, 3]);  permute_307 = None
    view_274: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_309, [1, 512, 1024]);  permute_309 = None
    permute_310: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_308, [4, 1, 2, 3, 0]);  permute_308 = None
    view_275: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_310, [1, 1024, 1024]);  permute_310 = None
    bmm_58: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_274, view_275)
    view_276: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_58, [512, 1, 1, 16, 64]);  bmm_58 = None
    permute_311: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_276, [0, 2, 3, 4, 1]);  view_276 = None
    view_277: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_311, [512, 1, 16, 64]);  permute_311 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_190: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_191: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_190, 4);  unsqueeze_190 = None
    permute_312: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_191, [0, 1, 3, 4, 2]);  unsqueeze_191 = None
    unsqueeze_192: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_53, 3);  primals_53 = None
    unsqueeze_193: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_192, 4);  unsqueeze_192 = None
    permute_313: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_193, [3, 4, 1, 2, 0]);  unsqueeze_193 = None
    permute_314: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_312, [0, 4, 1, 2, 3]);  permute_312 = None
    view_278: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_314, [1, 1024, 1024]);  permute_314 = None
    permute_315: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_313, [4, 1, 2, 3, 0]);  permute_313 = None
    view_279: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_315, [1, 1024, 1024]);  permute_315 = None
    bmm_59: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_278, view_279);  view_279 = None
    view_280: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_59, [1024, 1, 1, 16, 64]);  bmm_59 = None
    permute_316: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_280, [0, 2, 3, 4, 1]);  view_280 = None
    view_281: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_316, [1024, 1, 16, 64]);  permute_316 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_79: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_269, primals_54);  primals_54 = None
    unsqueeze_194: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_79, 4);  add_79 = None
    permute_317: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_194, [1, 2, 0, 4, 3]);  unsqueeze_194 = None
    unsqueeze_195: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_273, 4);  view_273 = None
    permute_318: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_195, [1, 2, 4, 0, 3]);  unsqueeze_195 = None
    permute_319: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_317, [1, 2, 4, 0, 3]);  permute_317 = None
    view_282: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_319, [16, 512, 64]);  permute_319 = None
    permute_320: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_318, [1, 4, 0, 3, 2]);  permute_318 = None
    view_283: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_320, [16, 64, 512]);  permute_320 = None
    bmm_60: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_282, view_283)
    view_284: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_60, [16, 512, 1, 1, 512]);  bmm_60 = None
    permute_321: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_284, [3, 0, 1, 4, 2]);  view_284 = None
    view_285: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_321, [1, 16, 512, 512]);  permute_321 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_80: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_269, primals_55);  view_269 = primals_55 = None
    unsqueeze_196: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_80, 4);  add_80 = None
    permute_322: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_196, [1, 2, 0, 4, 3]);  unsqueeze_196 = None
    unsqueeze_197: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_281, 4);  view_281 = None
    permute_323: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_197, [1, 2, 4, 0, 3]);  unsqueeze_197 = None
    permute_324: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_322, [1, 2, 4, 0, 3]);  permute_322 = None
    view_286: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_324, [16, 512, 64]);  permute_324 = None
    permute_325: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_323, [1, 4, 0, 3, 2]);  permute_323 = None
    view_287: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_325, [16, 64, 1024]);  permute_325 = None
    bmm_61: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_286, view_287)
    view_288: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_61, [16, 512, 1, 1, 1024]);  bmm_61 = None
    permute_326: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_288, [3, 0, 1, 4, 2]);  view_288 = None
    view_289: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_326, [1, 16, 512, 1024]);  permute_326 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_290: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_289, [1, 16, 1024, 512]);  view_289 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_52: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_290, 0, 0, 9223372036854775807);  view_290 = None
    slice_53: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_52, 1, 0, 9223372036854775807);  slice_52 = None
    slice_54: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_53, 2, 1, 9223372036854775807);  slice_53 = None
    slice_55: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_54, 3, 0, 9223372036854775807);  slice_54 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_291: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_55, [1, 16, 512, 1023]);  slice_55 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_9: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_56: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_291, 0, 0, 9223372036854775807);  view_291 = None
    slice_57: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_56, 1, 0, 9223372036854775807);  slice_56 = None
    slice_58: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_57, 2, 0, 9223372036854775807);  slice_57 = None
    index_7: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_58, [None, None, None, iota_9]);  slice_58 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_81: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_285, index_7);  view_285 = index_7 = None
    add_82: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_81, 0);  add_81 = None
    mul_60: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_82, 0.125);  add_82 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_7: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_60, [3], True)
    sub_21: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_60, amax_7);  mul_60 = amax_7 = None
    exp_7: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_21);  sub_21 = None
    sum_8: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_7, [3], True)
    div_8: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_7, sum_8);  exp_7 = sum_8 = None
    alias_7: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_8)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_30 = torch.ops.aten.native_dropout.default(div_8, 0.1, True);  div_8 = None
    getitem_88: "f32[1, 16, 512, 512]" = native_dropout_30[0]
    getitem_89: "b8[1, 16, 512, 512]" = native_dropout_30[1];  native_dropout_30 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_198: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_88, 4);  getitem_88 = None
    permute_327: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_198, [2, 0, 1, 4, 3]);  unsqueeze_198 = None
    unsqueeze_199: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_277, 4);  view_277 = None
    permute_328: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_199, [4, 1, 2, 3, 0]);  unsqueeze_199 = None
    permute_329: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_327, [2, 0, 4, 1, 3]);  permute_327 = None
    view_292: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_329, [16, 512, 512]);  permute_329 = None
    permute_330: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_328, [2, 4, 1, 3, 0]);  permute_328 = None
    view_293: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_330, [16, 512, 64]);  permute_330 = None
    bmm_62: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_292, view_293)
    view_294: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_62, [16, 512, 1, 1, 64]);  bmm_62 = None
    permute_331: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_294, [1, 3, 0, 4, 2]);  view_294 = None
    view_295: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_331, [512, 1, 16, 64]);  permute_331 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_200: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_295, 4);  view_295 = None
    permute_332: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_200, [0, 1, 4, 3, 2]);  unsqueeze_200 = None
    unsqueeze_201: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_56, 3);  primals_56 = None
    unsqueeze_202: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_201, 4);  unsqueeze_201 = None
    permute_333: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_202, [3, 4, 0, 2, 1]);  unsqueeze_202 = None
    permute_334: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_332, [0, 3, 4, 1, 2]);  permute_332 = None
    clone_14: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_334, memory_format = torch.contiguous_format);  permute_334 = None
    view_296: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_14, [1, 512, 1024]);  clone_14 = None
    permute_335: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_333, [3, 4, 1, 2, 0]);  permute_333 = None
    clone_15: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_335, memory_format = torch.contiguous_format);  permute_335 = None
    view_297: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_15, [1, 1024, 1024]);  clone_15 = None
    bmm_63: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_296, view_297)
    view_298: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_63, [512, 1, 1, 1, 1024]);  bmm_63 = None
    permute_336: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_298, [0, 3, 4, 1, 2]);  view_298 = None
    view_299: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_336, [512, 1, 1024]);  permute_336 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_31 = torch.ops.aten.native_dropout.default(view_299, 0.1, True);  view_299 = None
    getitem_90: "f32[512, 1, 1024]" = native_dropout_31[0]
    getitem_91: "b8[512, 1, 1024]" = native_dropout_31[1];  native_dropout_31 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_83: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_90, add_78);  getitem_90 = add_78 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_14 = torch.ops.aten.var_mean.correction(add_83, [2], correction = 0, keepdim = True)
    getitem_92: "f32[512, 1, 1]" = var_mean_14[0]
    getitem_93: "f32[512, 1, 1]" = var_mean_14[1];  var_mean_14 = None
    add_84: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_92, 1e-12);  getitem_92 = None
    rsqrt_14: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_84);  add_84 = None
    sub_22: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_83, getitem_93)
    mul_61: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_22, rsqrt_14);  sub_22 = None
    mul_62: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_61, primals_226);  mul_61 = None
    add_85: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_62, primals_227);  mul_62 = primals_227 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_300: "f32[512, 1024]" = torch.ops.aten.view.default(add_85, [512, 1024])
    permute_337: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_228, [1, 0]);  primals_228 = None
    addmm_14: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_229, view_300, permute_337);  primals_229 = None
    view_301: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_14, [512, 1, 4096]);  addmm_14 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_63: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_301, 0.5)
    mul_64: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_301, 0.7071067811865476)
    erf_7: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_64);  mul_64 = None
    add_86: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_7, 1);  erf_7 = None
    mul_65: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_63, add_86);  mul_63 = add_86 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_32 = torch.ops.aten.native_dropout.default(mul_65, 0.1, True);  mul_65 = None
    getitem_94: "f32[512, 1, 4096]" = native_dropout_32[0]
    getitem_95: "b8[512, 1, 4096]" = native_dropout_32[1];  native_dropout_32 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_302: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_94, [512, 4096]);  getitem_94 = None
    permute_338: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_230, [1, 0]);  primals_230 = None
    addmm_15: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_231, view_302, permute_338);  primals_231 = None
    view_303: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_15, [512, 1, 1024]);  addmm_15 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_33 = torch.ops.aten.native_dropout.default(view_303, 0.1, True);  view_303 = None
    getitem_96: "f32[512, 1, 1024]" = native_dropout_33[0]
    getitem_97: "b8[512, 1, 1024]" = native_dropout_33[1];  native_dropout_33 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_87: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_96, add_85);  getitem_96 = add_85 = None
    var_mean_15 = torch.ops.aten.var_mean.correction(add_87, [2], correction = 0, keepdim = True)
    getitem_98: "f32[512, 1, 1]" = var_mean_15[0]
    getitem_99: "f32[512, 1, 1]" = var_mean_15[1];  var_mean_15 = None
    add_88: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_98, 1e-12);  getitem_98 = None
    rsqrt_15: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_88);  add_88 = None
    sub_23: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_87, getitem_99)
    mul_66: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_23, rsqrt_15);  sub_23 = None
    mul_67: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_66, primals_232);  mul_66 = None
    add_89: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_67, primals_233);  mul_67 = primals_233 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_203: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_89, 3)
    unsqueeze_204: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_203, 4);  unsqueeze_203 = None
    permute_339: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_204, [0, 1, 3, 4, 2]);  unsqueeze_204 = None
    unsqueeze_205: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_57, 3);  primals_57 = None
    unsqueeze_206: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_205, 4);  unsqueeze_205 = None
    permute_340: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_206, [3, 4, 1, 2, 0]);  unsqueeze_206 = None
    permute_341: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_339, [0, 4, 1, 2, 3]);  permute_339 = None
    view_304: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_341, [1, 512, 1024]);  permute_341 = None
    permute_342: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_340, [4, 1, 2, 3, 0]);  permute_340 = None
    view_305: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_342, [1, 1024, 1024]);  permute_342 = None
    bmm_64: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_304, view_305)
    view_306: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_64, [512, 1, 1, 16, 64]);  bmm_64 = None
    permute_343: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_306, [0, 2, 3, 4, 1]);  view_306 = None
    view_307: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_343, [512, 1, 16, 64]);  permute_343 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_207: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_89, 3)
    unsqueeze_208: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_207, 4);  unsqueeze_207 = None
    permute_344: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_208, [0, 1, 3, 4, 2]);  unsqueeze_208 = None
    unsqueeze_209: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_58, 3);  primals_58 = None
    unsqueeze_210: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_209, 4);  unsqueeze_209 = None
    permute_345: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_210, [3, 4, 1, 2, 0]);  unsqueeze_210 = None
    permute_346: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_344, [0, 4, 1, 2, 3]);  permute_344 = None
    view_308: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_346, [1, 512, 1024]);  permute_346 = None
    permute_347: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_345, [4, 1, 2, 3, 0]);  permute_345 = None
    view_309: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_347, [1, 1024, 1024]);  permute_347 = None
    bmm_65: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_308, view_309)
    view_310: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_65, [512, 1, 1, 16, 64]);  bmm_65 = None
    permute_348: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_310, [0, 2, 3, 4, 1]);  view_310 = None
    view_311: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_348, [512, 1, 16, 64]);  permute_348 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_211: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_89, 3)
    unsqueeze_212: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_211, 4);  unsqueeze_211 = None
    permute_349: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_212, [0, 1, 3, 4, 2]);  unsqueeze_212 = None
    unsqueeze_213: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_59, 3);  primals_59 = None
    unsqueeze_214: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_213, 4);  unsqueeze_213 = None
    permute_350: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_214, [3, 4, 1, 2, 0]);  unsqueeze_214 = None
    permute_351: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_349, [0, 4, 1, 2, 3]);  permute_349 = None
    view_312: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_351, [1, 512, 1024]);  permute_351 = None
    permute_352: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_350, [4, 1, 2, 3, 0]);  permute_350 = None
    view_313: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_352, [1, 1024, 1024]);  permute_352 = None
    bmm_66: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_312, view_313)
    view_314: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_66, [512, 1, 1, 16, 64]);  bmm_66 = None
    permute_353: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_314, [0, 2, 3, 4, 1]);  view_314 = None
    view_315: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_353, [512, 1, 16, 64]);  permute_353 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_215: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_216: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_215, 4);  unsqueeze_215 = None
    permute_354: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_216, [0, 1, 3, 4, 2]);  unsqueeze_216 = None
    unsqueeze_217: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_60, 3);  primals_60 = None
    unsqueeze_218: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_217, 4);  unsqueeze_217 = None
    permute_355: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_218, [3, 4, 1, 2, 0]);  unsqueeze_218 = None
    permute_356: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_354, [0, 4, 1, 2, 3]);  permute_354 = None
    view_316: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_356, [1, 1024, 1024]);  permute_356 = None
    permute_357: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_355, [4, 1, 2, 3, 0]);  permute_355 = None
    view_317: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_357, [1, 1024, 1024]);  permute_357 = None
    bmm_67: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_316, view_317);  view_317 = None
    view_318: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_67, [1024, 1, 1, 16, 64]);  bmm_67 = None
    permute_358: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_318, [0, 2, 3, 4, 1]);  view_318 = None
    view_319: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_358, [1024, 1, 16, 64]);  permute_358 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_90: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_307, primals_61);  primals_61 = None
    unsqueeze_219: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_90, 4);  add_90 = None
    permute_359: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_219, [1, 2, 0, 4, 3]);  unsqueeze_219 = None
    unsqueeze_220: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_311, 4);  view_311 = None
    permute_360: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_220, [1, 2, 4, 0, 3]);  unsqueeze_220 = None
    permute_361: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_359, [1, 2, 4, 0, 3]);  permute_359 = None
    view_320: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_361, [16, 512, 64]);  permute_361 = None
    permute_362: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_360, [1, 4, 0, 3, 2]);  permute_360 = None
    view_321: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_362, [16, 64, 512]);  permute_362 = None
    bmm_68: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_320, view_321)
    view_322: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_68, [16, 512, 1, 1, 512]);  bmm_68 = None
    permute_363: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_322, [3, 0, 1, 4, 2]);  view_322 = None
    view_323: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_363, [1, 16, 512, 512]);  permute_363 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_91: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_307, primals_62);  view_307 = primals_62 = None
    unsqueeze_221: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_91, 4);  add_91 = None
    permute_364: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_221, [1, 2, 0, 4, 3]);  unsqueeze_221 = None
    unsqueeze_222: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_319, 4);  view_319 = None
    permute_365: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_222, [1, 2, 4, 0, 3]);  unsqueeze_222 = None
    permute_366: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_364, [1, 2, 4, 0, 3]);  permute_364 = None
    view_324: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_366, [16, 512, 64]);  permute_366 = None
    permute_367: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_365, [1, 4, 0, 3, 2]);  permute_365 = None
    view_325: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_367, [16, 64, 1024]);  permute_367 = None
    bmm_69: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_324, view_325)
    view_326: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_69, [16, 512, 1, 1, 1024]);  bmm_69 = None
    permute_368: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_326, [3, 0, 1, 4, 2]);  view_326 = None
    view_327: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_368, [1, 16, 512, 1024]);  permute_368 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_328: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_327, [1, 16, 1024, 512]);  view_327 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_59: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_328, 0, 0, 9223372036854775807);  view_328 = None
    slice_60: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_59, 1, 0, 9223372036854775807);  slice_59 = None
    slice_61: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_60, 2, 1, 9223372036854775807);  slice_60 = None
    slice_62: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_61, 3, 0, 9223372036854775807);  slice_61 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_329: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_62, [1, 16, 512, 1023]);  slice_62 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_10: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_63: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_329, 0, 0, 9223372036854775807);  view_329 = None
    slice_64: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_63, 1, 0, 9223372036854775807);  slice_63 = None
    slice_65: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_64, 2, 0, 9223372036854775807);  slice_64 = None
    index_8: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_65, [None, None, None, iota_10]);  slice_65 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_92: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_323, index_8);  view_323 = index_8 = None
    add_93: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_92, 0);  add_92 = None
    mul_68: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_93, 0.125);  add_93 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_8: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_68, [3], True)
    sub_24: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_68, amax_8);  mul_68 = amax_8 = None
    exp_8: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_24);  sub_24 = None
    sum_9: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_8, [3], True)
    div_9: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_8, sum_9);  exp_8 = sum_9 = None
    alias_8: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_9)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_34 = torch.ops.aten.native_dropout.default(div_9, 0.1, True);  div_9 = None
    getitem_100: "f32[1, 16, 512, 512]" = native_dropout_34[0]
    getitem_101: "b8[1, 16, 512, 512]" = native_dropout_34[1];  native_dropout_34 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_223: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_100, 4);  getitem_100 = None
    permute_369: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_223, [2, 0, 1, 4, 3]);  unsqueeze_223 = None
    unsqueeze_224: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_315, 4);  view_315 = None
    permute_370: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_224, [4, 1, 2, 3, 0]);  unsqueeze_224 = None
    permute_371: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_369, [2, 0, 4, 1, 3]);  permute_369 = None
    view_330: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_371, [16, 512, 512]);  permute_371 = None
    permute_372: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_370, [2, 4, 1, 3, 0]);  permute_370 = None
    view_331: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_372, [16, 512, 64]);  permute_372 = None
    bmm_70: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_330, view_331)
    view_332: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_70, [16, 512, 1, 1, 64]);  bmm_70 = None
    permute_373: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_332, [1, 3, 0, 4, 2]);  view_332 = None
    view_333: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_373, [512, 1, 16, 64]);  permute_373 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_225: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_333, 4);  view_333 = None
    permute_374: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_225, [0, 1, 4, 3, 2]);  unsqueeze_225 = None
    unsqueeze_226: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_63, 3);  primals_63 = None
    unsqueeze_227: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_226, 4);  unsqueeze_226 = None
    permute_375: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_227, [3, 4, 0, 2, 1]);  unsqueeze_227 = None
    permute_376: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_374, [0, 3, 4, 1, 2]);  permute_374 = None
    clone_16: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_376, memory_format = torch.contiguous_format);  permute_376 = None
    view_334: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_16, [1, 512, 1024]);  clone_16 = None
    permute_377: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_375, [3, 4, 1, 2, 0]);  permute_375 = None
    clone_17: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_377, memory_format = torch.contiguous_format);  permute_377 = None
    view_335: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_17, [1, 1024, 1024]);  clone_17 = None
    bmm_71: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_334, view_335)
    view_336: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_71, [512, 1, 1, 1, 1024]);  bmm_71 = None
    permute_378: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_336, [0, 3, 4, 1, 2]);  view_336 = None
    view_337: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_378, [512, 1, 1024]);  permute_378 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_35 = torch.ops.aten.native_dropout.default(view_337, 0.1, True);  view_337 = None
    getitem_102: "f32[512, 1, 1024]" = native_dropout_35[0]
    getitem_103: "b8[512, 1, 1024]" = native_dropout_35[1];  native_dropout_35 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_94: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_102, add_89);  getitem_102 = add_89 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_16 = torch.ops.aten.var_mean.correction(add_94, [2], correction = 0, keepdim = True)
    getitem_104: "f32[512, 1, 1]" = var_mean_16[0]
    getitem_105: "f32[512, 1, 1]" = var_mean_16[1];  var_mean_16 = None
    add_95: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_104, 1e-12);  getitem_104 = None
    rsqrt_16: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_95);  add_95 = None
    sub_25: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_94, getitem_105)
    mul_69: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_25, rsqrt_16);  sub_25 = None
    mul_70: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_69, primals_234);  mul_69 = None
    add_96: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_70, primals_235);  mul_70 = primals_235 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_338: "f32[512, 1024]" = torch.ops.aten.view.default(add_96, [512, 1024])
    permute_379: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_236, [1, 0]);  primals_236 = None
    addmm_16: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_237, view_338, permute_379);  primals_237 = None
    view_339: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_16, [512, 1, 4096]);  addmm_16 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_71: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_339, 0.5)
    mul_72: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_339, 0.7071067811865476)
    erf_8: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_72);  mul_72 = None
    add_97: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_8, 1);  erf_8 = None
    mul_73: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_71, add_97);  mul_71 = add_97 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_36 = torch.ops.aten.native_dropout.default(mul_73, 0.1, True);  mul_73 = None
    getitem_106: "f32[512, 1, 4096]" = native_dropout_36[0]
    getitem_107: "b8[512, 1, 4096]" = native_dropout_36[1];  native_dropout_36 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_340: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_106, [512, 4096]);  getitem_106 = None
    permute_380: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_238, [1, 0]);  primals_238 = None
    addmm_17: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_239, view_340, permute_380);  primals_239 = None
    view_341: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_17, [512, 1, 1024]);  addmm_17 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_37 = torch.ops.aten.native_dropout.default(view_341, 0.1, True);  view_341 = None
    getitem_108: "f32[512, 1, 1024]" = native_dropout_37[0]
    getitem_109: "b8[512, 1, 1024]" = native_dropout_37[1];  native_dropout_37 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_98: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_108, add_96);  getitem_108 = add_96 = None
    var_mean_17 = torch.ops.aten.var_mean.correction(add_98, [2], correction = 0, keepdim = True)
    getitem_110: "f32[512, 1, 1]" = var_mean_17[0]
    getitem_111: "f32[512, 1, 1]" = var_mean_17[1];  var_mean_17 = None
    add_99: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_110, 1e-12);  getitem_110 = None
    rsqrt_17: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_99);  add_99 = None
    sub_26: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_98, getitem_111)
    mul_74: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_26, rsqrt_17);  sub_26 = None
    mul_75: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_74, primals_240);  mul_74 = None
    add_100: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_75, primals_241);  mul_75 = primals_241 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_228: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_100, 3)
    unsqueeze_229: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_228, 4);  unsqueeze_228 = None
    permute_381: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_229, [0, 1, 3, 4, 2]);  unsqueeze_229 = None
    unsqueeze_230: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_64, 3);  primals_64 = None
    unsqueeze_231: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_230, 4);  unsqueeze_230 = None
    permute_382: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_231, [3, 4, 1, 2, 0]);  unsqueeze_231 = None
    permute_383: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_381, [0, 4, 1, 2, 3]);  permute_381 = None
    view_342: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_383, [1, 512, 1024]);  permute_383 = None
    permute_384: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_382, [4, 1, 2, 3, 0]);  permute_382 = None
    view_343: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_384, [1, 1024, 1024]);  permute_384 = None
    bmm_72: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_342, view_343)
    view_344: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_72, [512, 1, 1, 16, 64]);  bmm_72 = None
    permute_385: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_344, [0, 2, 3, 4, 1]);  view_344 = None
    view_345: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_385, [512, 1, 16, 64]);  permute_385 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_232: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_100, 3)
    unsqueeze_233: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_232, 4);  unsqueeze_232 = None
    permute_386: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_233, [0, 1, 3, 4, 2]);  unsqueeze_233 = None
    unsqueeze_234: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_65, 3);  primals_65 = None
    unsqueeze_235: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_234, 4);  unsqueeze_234 = None
    permute_387: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_235, [3, 4, 1, 2, 0]);  unsqueeze_235 = None
    permute_388: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_386, [0, 4, 1, 2, 3]);  permute_386 = None
    view_346: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_388, [1, 512, 1024]);  permute_388 = None
    permute_389: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_387, [4, 1, 2, 3, 0]);  permute_387 = None
    view_347: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_389, [1, 1024, 1024]);  permute_389 = None
    bmm_73: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_346, view_347)
    view_348: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_73, [512, 1, 1, 16, 64]);  bmm_73 = None
    permute_390: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_348, [0, 2, 3, 4, 1]);  view_348 = None
    view_349: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_390, [512, 1, 16, 64]);  permute_390 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_236: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_100, 3)
    unsqueeze_237: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_236, 4);  unsqueeze_236 = None
    permute_391: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_237, [0, 1, 3, 4, 2]);  unsqueeze_237 = None
    unsqueeze_238: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_66, 3);  primals_66 = None
    unsqueeze_239: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_238, 4);  unsqueeze_238 = None
    permute_392: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_239, [3, 4, 1, 2, 0]);  unsqueeze_239 = None
    permute_393: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_391, [0, 4, 1, 2, 3]);  permute_391 = None
    view_350: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_393, [1, 512, 1024]);  permute_393 = None
    permute_394: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_392, [4, 1, 2, 3, 0]);  permute_392 = None
    view_351: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_394, [1, 1024, 1024]);  permute_394 = None
    bmm_74: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_350, view_351)
    view_352: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_74, [512, 1, 1, 16, 64]);  bmm_74 = None
    permute_395: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_352, [0, 2, 3, 4, 1]);  view_352 = None
    view_353: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_395, [512, 1, 16, 64]);  permute_395 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_240: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_241: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_240, 4);  unsqueeze_240 = None
    permute_396: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_241, [0, 1, 3, 4, 2]);  unsqueeze_241 = None
    unsqueeze_242: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_67, 3);  primals_67 = None
    unsqueeze_243: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_242, 4);  unsqueeze_242 = None
    permute_397: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_243, [3, 4, 1, 2, 0]);  unsqueeze_243 = None
    permute_398: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_396, [0, 4, 1, 2, 3]);  permute_396 = None
    view_354: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_398, [1, 1024, 1024]);  permute_398 = None
    permute_399: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_397, [4, 1, 2, 3, 0]);  permute_397 = None
    view_355: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_399, [1, 1024, 1024]);  permute_399 = None
    bmm_75: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_354, view_355);  view_355 = None
    view_356: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_75, [1024, 1, 1, 16, 64]);  bmm_75 = None
    permute_400: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_356, [0, 2, 3, 4, 1]);  view_356 = None
    view_357: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_400, [1024, 1, 16, 64]);  permute_400 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_101: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_345, primals_68);  primals_68 = None
    unsqueeze_244: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_101, 4);  add_101 = None
    permute_401: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_244, [1, 2, 0, 4, 3]);  unsqueeze_244 = None
    unsqueeze_245: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_349, 4);  view_349 = None
    permute_402: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_245, [1, 2, 4, 0, 3]);  unsqueeze_245 = None
    permute_403: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_401, [1, 2, 4, 0, 3]);  permute_401 = None
    view_358: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_403, [16, 512, 64]);  permute_403 = None
    permute_404: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_402, [1, 4, 0, 3, 2]);  permute_402 = None
    view_359: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_404, [16, 64, 512]);  permute_404 = None
    bmm_76: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_358, view_359)
    view_360: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_76, [16, 512, 1, 1, 512]);  bmm_76 = None
    permute_405: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_360, [3, 0, 1, 4, 2]);  view_360 = None
    view_361: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_405, [1, 16, 512, 512]);  permute_405 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_102: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_345, primals_69);  view_345 = primals_69 = None
    unsqueeze_246: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_102, 4);  add_102 = None
    permute_406: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_246, [1, 2, 0, 4, 3]);  unsqueeze_246 = None
    unsqueeze_247: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_357, 4);  view_357 = None
    permute_407: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_247, [1, 2, 4, 0, 3]);  unsqueeze_247 = None
    permute_408: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_406, [1, 2, 4, 0, 3]);  permute_406 = None
    view_362: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_408, [16, 512, 64]);  permute_408 = None
    permute_409: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_407, [1, 4, 0, 3, 2]);  permute_407 = None
    view_363: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_409, [16, 64, 1024]);  permute_409 = None
    bmm_77: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_362, view_363)
    view_364: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_77, [16, 512, 1, 1, 1024]);  bmm_77 = None
    permute_410: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_364, [3, 0, 1, 4, 2]);  view_364 = None
    view_365: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_410, [1, 16, 512, 1024]);  permute_410 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_366: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_365, [1, 16, 1024, 512]);  view_365 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_66: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_366, 0, 0, 9223372036854775807);  view_366 = None
    slice_67: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_66, 1, 0, 9223372036854775807);  slice_66 = None
    slice_68: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_67, 2, 1, 9223372036854775807);  slice_67 = None
    slice_69: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_68, 3, 0, 9223372036854775807);  slice_68 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_367: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_69, [1, 16, 512, 1023]);  slice_69 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_11: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_70: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_367, 0, 0, 9223372036854775807);  view_367 = None
    slice_71: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_70, 1, 0, 9223372036854775807);  slice_70 = None
    slice_72: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_71, 2, 0, 9223372036854775807);  slice_71 = None
    index_9: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_72, [None, None, None, iota_11]);  slice_72 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_103: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_361, index_9);  view_361 = index_9 = None
    add_104: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_103, 0);  add_103 = None
    mul_76: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_104, 0.125);  add_104 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_9: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_76, [3], True)
    sub_27: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_76, amax_9);  mul_76 = amax_9 = None
    exp_9: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_27);  sub_27 = None
    sum_10: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_9, [3], True)
    div_10: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_9, sum_10);  exp_9 = sum_10 = None
    alias_9: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_10)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_38 = torch.ops.aten.native_dropout.default(div_10, 0.1, True);  div_10 = None
    getitem_112: "f32[1, 16, 512, 512]" = native_dropout_38[0]
    getitem_113: "b8[1, 16, 512, 512]" = native_dropout_38[1];  native_dropout_38 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_248: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_112, 4);  getitem_112 = None
    permute_411: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_248, [2, 0, 1, 4, 3]);  unsqueeze_248 = None
    unsqueeze_249: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_353, 4);  view_353 = None
    permute_412: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_249, [4, 1, 2, 3, 0]);  unsqueeze_249 = None
    permute_413: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_411, [2, 0, 4, 1, 3]);  permute_411 = None
    view_368: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_413, [16, 512, 512]);  permute_413 = None
    permute_414: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_412, [2, 4, 1, 3, 0]);  permute_412 = None
    view_369: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_414, [16, 512, 64]);  permute_414 = None
    bmm_78: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_368, view_369)
    view_370: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_78, [16, 512, 1, 1, 64]);  bmm_78 = None
    permute_415: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_370, [1, 3, 0, 4, 2]);  view_370 = None
    view_371: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_415, [512, 1, 16, 64]);  permute_415 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_250: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_371, 4);  view_371 = None
    permute_416: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_250, [0, 1, 4, 3, 2]);  unsqueeze_250 = None
    unsqueeze_251: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_70, 3);  primals_70 = None
    unsqueeze_252: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_251, 4);  unsqueeze_251 = None
    permute_417: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_252, [3, 4, 0, 2, 1]);  unsqueeze_252 = None
    permute_418: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_416, [0, 3, 4, 1, 2]);  permute_416 = None
    clone_18: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_418, memory_format = torch.contiguous_format);  permute_418 = None
    view_372: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_18, [1, 512, 1024]);  clone_18 = None
    permute_419: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_417, [3, 4, 1, 2, 0]);  permute_417 = None
    clone_19: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_419, memory_format = torch.contiguous_format);  permute_419 = None
    view_373: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_19, [1, 1024, 1024]);  clone_19 = None
    bmm_79: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_372, view_373)
    view_374: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_79, [512, 1, 1, 1, 1024]);  bmm_79 = None
    permute_420: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_374, [0, 3, 4, 1, 2]);  view_374 = None
    view_375: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_420, [512, 1, 1024]);  permute_420 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_39 = torch.ops.aten.native_dropout.default(view_375, 0.1, True);  view_375 = None
    getitem_114: "f32[512, 1, 1024]" = native_dropout_39[0]
    getitem_115: "b8[512, 1, 1024]" = native_dropout_39[1];  native_dropout_39 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_105: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_114, add_100);  getitem_114 = add_100 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_18 = torch.ops.aten.var_mean.correction(add_105, [2], correction = 0, keepdim = True)
    getitem_116: "f32[512, 1, 1]" = var_mean_18[0]
    getitem_117: "f32[512, 1, 1]" = var_mean_18[1];  var_mean_18 = None
    add_106: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_116, 1e-12);  getitem_116 = None
    rsqrt_18: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_106);  add_106 = None
    sub_28: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_105, getitem_117)
    mul_77: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_28, rsqrt_18);  sub_28 = None
    mul_78: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_77, primals_242);  mul_77 = None
    add_107: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_78, primals_243);  mul_78 = primals_243 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_376: "f32[512, 1024]" = torch.ops.aten.view.default(add_107, [512, 1024])
    permute_421: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_244, [1, 0]);  primals_244 = None
    addmm_18: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_245, view_376, permute_421);  primals_245 = None
    view_377: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_18, [512, 1, 4096]);  addmm_18 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_79: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_377, 0.5)
    mul_80: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_377, 0.7071067811865476)
    erf_9: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_80);  mul_80 = None
    add_108: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_9, 1);  erf_9 = None
    mul_81: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_79, add_108);  mul_79 = add_108 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_40 = torch.ops.aten.native_dropout.default(mul_81, 0.1, True);  mul_81 = None
    getitem_118: "f32[512, 1, 4096]" = native_dropout_40[0]
    getitem_119: "b8[512, 1, 4096]" = native_dropout_40[1];  native_dropout_40 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_378: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_118, [512, 4096]);  getitem_118 = None
    permute_422: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_246, [1, 0]);  primals_246 = None
    addmm_19: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_247, view_378, permute_422);  primals_247 = None
    view_379: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_19, [512, 1, 1024]);  addmm_19 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_41 = torch.ops.aten.native_dropout.default(view_379, 0.1, True);  view_379 = None
    getitem_120: "f32[512, 1, 1024]" = native_dropout_41[0]
    getitem_121: "b8[512, 1, 1024]" = native_dropout_41[1];  native_dropout_41 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_109: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_120, add_107);  getitem_120 = add_107 = None
    var_mean_19 = torch.ops.aten.var_mean.correction(add_109, [2], correction = 0, keepdim = True)
    getitem_122: "f32[512, 1, 1]" = var_mean_19[0]
    getitem_123: "f32[512, 1, 1]" = var_mean_19[1];  var_mean_19 = None
    add_110: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_122, 1e-12);  getitem_122 = None
    rsqrt_19: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_110);  add_110 = None
    sub_29: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_109, getitem_123)
    mul_82: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_29, rsqrt_19);  sub_29 = None
    mul_83: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_82, primals_248);  mul_82 = None
    add_111: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_83, primals_249);  mul_83 = primals_249 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_253: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_111, 3)
    unsqueeze_254: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_253, 4);  unsqueeze_253 = None
    permute_423: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_254, [0, 1, 3, 4, 2]);  unsqueeze_254 = None
    unsqueeze_255: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_71, 3);  primals_71 = None
    unsqueeze_256: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_255, 4);  unsqueeze_255 = None
    permute_424: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_256, [3, 4, 1, 2, 0]);  unsqueeze_256 = None
    permute_425: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_423, [0, 4, 1, 2, 3]);  permute_423 = None
    view_380: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_425, [1, 512, 1024]);  permute_425 = None
    permute_426: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_424, [4, 1, 2, 3, 0]);  permute_424 = None
    view_381: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_426, [1, 1024, 1024]);  permute_426 = None
    bmm_80: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_380, view_381)
    view_382: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_80, [512, 1, 1, 16, 64]);  bmm_80 = None
    permute_427: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_382, [0, 2, 3, 4, 1]);  view_382 = None
    view_383: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_427, [512, 1, 16, 64]);  permute_427 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_257: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_111, 3)
    unsqueeze_258: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_257, 4);  unsqueeze_257 = None
    permute_428: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_258, [0, 1, 3, 4, 2]);  unsqueeze_258 = None
    unsqueeze_259: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_72, 3);  primals_72 = None
    unsqueeze_260: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_259, 4);  unsqueeze_259 = None
    permute_429: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_260, [3, 4, 1, 2, 0]);  unsqueeze_260 = None
    permute_430: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_428, [0, 4, 1, 2, 3]);  permute_428 = None
    view_384: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_430, [1, 512, 1024]);  permute_430 = None
    permute_431: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_429, [4, 1, 2, 3, 0]);  permute_429 = None
    view_385: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_431, [1, 1024, 1024]);  permute_431 = None
    bmm_81: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_384, view_385)
    view_386: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_81, [512, 1, 1, 16, 64]);  bmm_81 = None
    permute_432: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_386, [0, 2, 3, 4, 1]);  view_386 = None
    view_387: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_432, [512, 1, 16, 64]);  permute_432 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_261: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_111, 3)
    unsqueeze_262: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_261, 4);  unsqueeze_261 = None
    permute_433: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_262, [0, 1, 3, 4, 2]);  unsqueeze_262 = None
    unsqueeze_263: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_73, 3);  primals_73 = None
    unsqueeze_264: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_263, 4);  unsqueeze_263 = None
    permute_434: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_264, [3, 4, 1, 2, 0]);  unsqueeze_264 = None
    permute_435: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_433, [0, 4, 1, 2, 3]);  permute_433 = None
    view_388: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_435, [1, 512, 1024]);  permute_435 = None
    permute_436: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_434, [4, 1, 2, 3, 0]);  permute_434 = None
    view_389: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_436, [1, 1024, 1024]);  permute_436 = None
    bmm_82: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_388, view_389)
    view_390: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_82, [512, 1, 1, 16, 64]);  bmm_82 = None
    permute_437: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_390, [0, 2, 3, 4, 1]);  view_390 = None
    view_391: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_437, [512, 1, 16, 64]);  permute_437 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_265: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_266: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_265, 4);  unsqueeze_265 = None
    permute_438: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_266, [0, 1, 3, 4, 2]);  unsqueeze_266 = None
    unsqueeze_267: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_74, 3);  primals_74 = None
    unsqueeze_268: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_267, 4);  unsqueeze_267 = None
    permute_439: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_268, [3, 4, 1, 2, 0]);  unsqueeze_268 = None
    permute_440: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_438, [0, 4, 1, 2, 3]);  permute_438 = None
    view_392: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_440, [1, 1024, 1024]);  permute_440 = None
    permute_441: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_439, [4, 1, 2, 3, 0]);  permute_439 = None
    view_393: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_441, [1, 1024, 1024]);  permute_441 = None
    bmm_83: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_392, view_393);  view_393 = None
    view_394: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_83, [1024, 1, 1, 16, 64]);  bmm_83 = None
    permute_442: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_394, [0, 2, 3, 4, 1]);  view_394 = None
    view_395: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_442, [1024, 1, 16, 64]);  permute_442 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_112: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_383, primals_75);  primals_75 = None
    unsqueeze_269: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_112, 4);  add_112 = None
    permute_443: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_269, [1, 2, 0, 4, 3]);  unsqueeze_269 = None
    unsqueeze_270: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_387, 4);  view_387 = None
    permute_444: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_270, [1, 2, 4, 0, 3]);  unsqueeze_270 = None
    permute_445: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_443, [1, 2, 4, 0, 3]);  permute_443 = None
    view_396: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_445, [16, 512, 64]);  permute_445 = None
    permute_446: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_444, [1, 4, 0, 3, 2]);  permute_444 = None
    view_397: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_446, [16, 64, 512]);  permute_446 = None
    bmm_84: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_396, view_397)
    view_398: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_84, [16, 512, 1, 1, 512]);  bmm_84 = None
    permute_447: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_398, [3, 0, 1, 4, 2]);  view_398 = None
    view_399: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_447, [1, 16, 512, 512]);  permute_447 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_113: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_383, primals_76);  view_383 = primals_76 = None
    unsqueeze_271: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_113, 4);  add_113 = None
    permute_448: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_271, [1, 2, 0, 4, 3]);  unsqueeze_271 = None
    unsqueeze_272: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_395, 4);  view_395 = None
    permute_449: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_272, [1, 2, 4, 0, 3]);  unsqueeze_272 = None
    permute_450: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_448, [1, 2, 4, 0, 3]);  permute_448 = None
    view_400: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_450, [16, 512, 64]);  permute_450 = None
    permute_451: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_449, [1, 4, 0, 3, 2]);  permute_449 = None
    view_401: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_451, [16, 64, 1024]);  permute_451 = None
    bmm_85: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_400, view_401)
    view_402: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_85, [16, 512, 1, 1, 1024]);  bmm_85 = None
    permute_452: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_402, [3, 0, 1, 4, 2]);  view_402 = None
    view_403: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_452, [1, 16, 512, 1024]);  permute_452 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_404: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_403, [1, 16, 1024, 512]);  view_403 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_73: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_404, 0, 0, 9223372036854775807);  view_404 = None
    slice_74: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_73, 1, 0, 9223372036854775807);  slice_73 = None
    slice_75: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_74, 2, 1, 9223372036854775807);  slice_74 = None
    slice_76: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_75, 3, 0, 9223372036854775807);  slice_75 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_405: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_76, [1, 16, 512, 1023]);  slice_76 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_12: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_77: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_405, 0, 0, 9223372036854775807);  view_405 = None
    slice_78: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_77, 1, 0, 9223372036854775807);  slice_77 = None
    slice_79: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_78, 2, 0, 9223372036854775807);  slice_78 = None
    index_10: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_79, [None, None, None, iota_12]);  slice_79 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_114: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_399, index_10);  view_399 = index_10 = None
    add_115: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_114, 0);  add_114 = None
    mul_84: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_115, 0.125);  add_115 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_10: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_84, [3], True)
    sub_30: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_84, amax_10);  mul_84 = amax_10 = None
    exp_10: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_30);  sub_30 = None
    sum_11: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_10, [3], True)
    div_11: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_10, sum_11);  exp_10 = sum_11 = None
    alias_10: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_11)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_42 = torch.ops.aten.native_dropout.default(div_11, 0.1, True);  div_11 = None
    getitem_124: "f32[1, 16, 512, 512]" = native_dropout_42[0]
    getitem_125: "b8[1, 16, 512, 512]" = native_dropout_42[1];  native_dropout_42 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_273: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_124, 4);  getitem_124 = None
    permute_453: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_273, [2, 0, 1, 4, 3]);  unsqueeze_273 = None
    unsqueeze_274: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_391, 4);  view_391 = None
    permute_454: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_274, [4, 1, 2, 3, 0]);  unsqueeze_274 = None
    permute_455: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_453, [2, 0, 4, 1, 3]);  permute_453 = None
    view_406: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_455, [16, 512, 512]);  permute_455 = None
    permute_456: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_454, [2, 4, 1, 3, 0]);  permute_454 = None
    view_407: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_456, [16, 512, 64]);  permute_456 = None
    bmm_86: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_406, view_407)
    view_408: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_86, [16, 512, 1, 1, 64]);  bmm_86 = None
    permute_457: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_408, [1, 3, 0, 4, 2]);  view_408 = None
    view_409: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_457, [512, 1, 16, 64]);  permute_457 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_275: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_409, 4);  view_409 = None
    permute_458: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_275, [0, 1, 4, 3, 2]);  unsqueeze_275 = None
    unsqueeze_276: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_77, 3);  primals_77 = None
    unsqueeze_277: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_276, 4);  unsqueeze_276 = None
    permute_459: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_277, [3, 4, 0, 2, 1]);  unsqueeze_277 = None
    permute_460: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_458, [0, 3, 4, 1, 2]);  permute_458 = None
    clone_20: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_460, memory_format = torch.contiguous_format);  permute_460 = None
    view_410: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_20, [1, 512, 1024]);  clone_20 = None
    permute_461: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_459, [3, 4, 1, 2, 0]);  permute_459 = None
    clone_21: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_461, memory_format = torch.contiguous_format);  permute_461 = None
    view_411: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_21, [1, 1024, 1024]);  clone_21 = None
    bmm_87: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_410, view_411)
    view_412: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_87, [512, 1, 1, 1, 1024]);  bmm_87 = None
    permute_462: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_412, [0, 3, 4, 1, 2]);  view_412 = None
    view_413: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_462, [512, 1, 1024]);  permute_462 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_43 = torch.ops.aten.native_dropout.default(view_413, 0.1, True);  view_413 = None
    getitem_126: "f32[512, 1, 1024]" = native_dropout_43[0]
    getitem_127: "b8[512, 1, 1024]" = native_dropout_43[1];  native_dropout_43 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_116: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_126, add_111);  getitem_126 = add_111 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_20 = torch.ops.aten.var_mean.correction(add_116, [2], correction = 0, keepdim = True)
    getitem_128: "f32[512, 1, 1]" = var_mean_20[0]
    getitem_129: "f32[512, 1, 1]" = var_mean_20[1];  var_mean_20 = None
    add_117: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_128, 1e-12);  getitem_128 = None
    rsqrt_20: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_117);  add_117 = None
    sub_31: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_116, getitem_129)
    mul_85: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_31, rsqrt_20);  sub_31 = None
    mul_86: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_85, primals_250);  mul_85 = None
    add_118: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_86, primals_251);  mul_86 = primals_251 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_414: "f32[512, 1024]" = torch.ops.aten.view.default(add_118, [512, 1024])
    permute_463: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_252, [1, 0]);  primals_252 = None
    addmm_20: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_253, view_414, permute_463);  primals_253 = None
    view_415: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_20, [512, 1, 4096]);  addmm_20 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_87: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_415, 0.5)
    mul_88: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_415, 0.7071067811865476)
    erf_10: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_88);  mul_88 = None
    add_119: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_10, 1);  erf_10 = None
    mul_89: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_87, add_119);  mul_87 = add_119 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_44 = torch.ops.aten.native_dropout.default(mul_89, 0.1, True);  mul_89 = None
    getitem_130: "f32[512, 1, 4096]" = native_dropout_44[0]
    getitem_131: "b8[512, 1, 4096]" = native_dropout_44[1];  native_dropout_44 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_416: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_130, [512, 4096]);  getitem_130 = None
    permute_464: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_254, [1, 0]);  primals_254 = None
    addmm_21: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_255, view_416, permute_464);  primals_255 = None
    view_417: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_21, [512, 1, 1024]);  addmm_21 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_45 = torch.ops.aten.native_dropout.default(view_417, 0.1, True);  view_417 = None
    getitem_132: "f32[512, 1, 1024]" = native_dropout_45[0]
    getitem_133: "b8[512, 1, 1024]" = native_dropout_45[1];  native_dropout_45 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_120: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_132, add_118);  getitem_132 = add_118 = None
    var_mean_21 = torch.ops.aten.var_mean.correction(add_120, [2], correction = 0, keepdim = True)
    getitem_134: "f32[512, 1, 1]" = var_mean_21[0]
    getitem_135: "f32[512, 1, 1]" = var_mean_21[1];  var_mean_21 = None
    add_121: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_134, 1e-12);  getitem_134 = None
    rsqrt_21: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_121);  add_121 = None
    sub_32: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_120, getitem_135)
    mul_90: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_32, rsqrt_21);  sub_32 = None
    mul_91: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_90, primals_256);  mul_90 = None
    add_122: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_91, primals_257);  mul_91 = primals_257 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_278: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_122, 3)
    unsqueeze_279: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_278, 4);  unsqueeze_278 = None
    permute_465: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_279, [0, 1, 3, 4, 2]);  unsqueeze_279 = None
    unsqueeze_280: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_78, 3);  primals_78 = None
    unsqueeze_281: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_280, 4);  unsqueeze_280 = None
    permute_466: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_281, [3, 4, 1, 2, 0]);  unsqueeze_281 = None
    permute_467: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_465, [0, 4, 1, 2, 3]);  permute_465 = None
    view_418: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_467, [1, 512, 1024]);  permute_467 = None
    permute_468: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_466, [4, 1, 2, 3, 0]);  permute_466 = None
    view_419: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_468, [1, 1024, 1024]);  permute_468 = None
    bmm_88: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_418, view_419)
    view_420: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_88, [512, 1, 1, 16, 64]);  bmm_88 = None
    permute_469: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_420, [0, 2, 3, 4, 1]);  view_420 = None
    view_421: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_469, [512, 1, 16, 64]);  permute_469 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_282: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_122, 3)
    unsqueeze_283: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_282, 4);  unsqueeze_282 = None
    permute_470: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_283, [0, 1, 3, 4, 2]);  unsqueeze_283 = None
    unsqueeze_284: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_79, 3);  primals_79 = None
    unsqueeze_285: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_284, 4);  unsqueeze_284 = None
    permute_471: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_285, [3, 4, 1, 2, 0]);  unsqueeze_285 = None
    permute_472: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_470, [0, 4, 1, 2, 3]);  permute_470 = None
    view_422: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_472, [1, 512, 1024]);  permute_472 = None
    permute_473: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_471, [4, 1, 2, 3, 0]);  permute_471 = None
    view_423: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_473, [1, 1024, 1024]);  permute_473 = None
    bmm_89: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_422, view_423)
    view_424: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_89, [512, 1, 1, 16, 64]);  bmm_89 = None
    permute_474: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_424, [0, 2, 3, 4, 1]);  view_424 = None
    view_425: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_474, [512, 1, 16, 64]);  permute_474 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_286: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_122, 3)
    unsqueeze_287: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_286, 4);  unsqueeze_286 = None
    permute_475: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_287, [0, 1, 3, 4, 2]);  unsqueeze_287 = None
    unsqueeze_288: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_80, 3);  primals_80 = None
    unsqueeze_289: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_288, 4);  unsqueeze_288 = None
    permute_476: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_289, [3, 4, 1, 2, 0]);  unsqueeze_289 = None
    permute_477: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_475, [0, 4, 1, 2, 3]);  permute_475 = None
    view_426: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_477, [1, 512, 1024]);  permute_477 = None
    permute_478: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_476, [4, 1, 2, 3, 0]);  permute_476 = None
    view_427: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_478, [1, 1024, 1024]);  permute_478 = None
    bmm_90: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_426, view_427)
    view_428: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_90, [512, 1, 1, 16, 64]);  bmm_90 = None
    permute_479: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_428, [0, 2, 3, 4, 1]);  view_428 = None
    view_429: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_479, [512, 1, 16, 64]);  permute_479 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_290: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_291: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_290, 4);  unsqueeze_290 = None
    permute_480: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_291, [0, 1, 3, 4, 2]);  unsqueeze_291 = None
    unsqueeze_292: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_81, 3);  primals_81 = None
    unsqueeze_293: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_292, 4);  unsqueeze_292 = None
    permute_481: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_293, [3, 4, 1, 2, 0]);  unsqueeze_293 = None
    permute_482: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_480, [0, 4, 1, 2, 3]);  permute_480 = None
    view_430: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_482, [1, 1024, 1024]);  permute_482 = None
    permute_483: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_481, [4, 1, 2, 3, 0]);  permute_481 = None
    view_431: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_483, [1, 1024, 1024]);  permute_483 = None
    bmm_91: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_430, view_431);  view_431 = None
    view_432: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_91, [1024, 1, 1, 16, 64]);  bmm_91 = None
    permute_484: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_432, [0, 2, 3, 4, 1]);  view_432 = None
    view_433: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_484, [1024, 1, 16, 64]);  permute_484 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_123: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_421, primals_82);  primals_82 = None
    unsqueeze_294: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_123, 4);  add_123 = None
    permute_485: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_294, [1, 2, 0, 4, 3]);  unsqueeze_294 = None
    unsqueeze_295: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_425, 4);  view_425 = None
    permute_486: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_295, [1, 2, 4, 0, 3]);  unsqueeze_295 = None
    permute_487: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_485, [1, 2, 4, 0, 3]);  permute_485 = None
    view_434: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_487, [16, 512, 64]);  permute_487 = None
    permute_488: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_486, [1, 4, 0, 3, 2]);  permute_486 = None
    view_435: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_488, [16, 64, 512]);  permute_488 = None
    bmm_92: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_434, view_435)
    view_436: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_92, [16, 512, 1, 1, 512]);  bmm_92 = None
    permute_489: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_436, [3, 0, 1, 4, 2]);  view_436 = None
    view_437: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_489, [1, 16, 512, 512]);  permute_489 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_124: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_421, primals_83);  view_421 = primals_83 = None
    unsqueeze_296: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_124, 4);  add_124 = None
    permute_490: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_296, [1, 2, 0, 4, 3]);  unsqueeze_296 = None
    unsqueeze_297: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_433, 4);  view_433 = None
    permute_491: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_297, [1, 2, 4, 0, 3]);  unsqueeze_297 = None
    permute_492: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_490, [1, 2, 4, 0, 3]);  permute_490 = None
    view_438: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_492, [16, 512, 64]);  permute_492 = None
    permute_493: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_491, [1, 4, 0, 3, 2]);  permute_491 = None
    view_439: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_493, [16, 64, 1024]);  permute_493 = None
    bmm_93: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_438, view_439)
    view_440: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_93, [16, 512, 1, 1, 1024]);  bmm_93 = None
    permute_494: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_440, [3, 0, 1, 4, 2]);  view_440 = None
    view_441: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_494, [1, 16, 512, 1024]);  permute_494 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_442: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_441, [1, 16, 1024, 512]);  view_441 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_80: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_442, 0, 0, 9223372036854775807);  view_442 = None
    slice_81: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_80, 1, 0, 9223372036854775807);  slice_80 = None
    slice_82: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_81, 2, 1, 9223372036854775807);  slice_81 = None
    slice_83: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_82, 3, 0, 9223372036854775807);  slice_82 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_443: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_83, [1, 16, 512, 1023]);  slice_83 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_13: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_84: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_443, 0, 0, 9223372036854775807);  view_443 = None
    slice_85: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_84, 1, 0, 9223372036854775807);  slice_84 = None
    slice_86: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_85, 2, 0, 9223372036854775807);  slice_85 = None
    index_11: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_86, [None, None, None, iota_13]);  slice_86 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_125: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_437, index_11);  view_437 = index_11 = None
    add_126: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_125, 0);  add_125 = None
    mul_92: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_126, 0.125);  add_126 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_11: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_92, [3], True)
    sub_33: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_92, amax_11);  mul_92 = amax_11 = None
    exp_11: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_33);  sub_33 = None
    sum_12: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_11, [3], True)
    div_12: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_11, sum_12);  exp_11 = sum_12 = None
    alias_11: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_12)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_46 = torch.ops.aten.native_dropout.default(div_12, 0.1, True);  div_12 = None
    getitem_136: "f32[1, 16, 512, 512]" = native_dropout_46[0]
    getitem_137: "b8[1, 16, 512, 512]" = native_dropout_46[1];  native_dropout_46 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_298: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_136, 4);  getitem_136 = None
    permute_495: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_298, [2, 0, 1, 4, 3]);  unsqueeze_298 = None
    unsqueeze_299: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_429, 4);  view_429 = None
    permute_496: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_299, [4, 1, 2, 3, 0]);  unsqueeze_299 = None
    permute_497: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_495, [2, 0, 4, 1, 3]);  permute_495 = None
    view_444: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_497, [16, 512, 512]);  permute_497 = None
    permute_498: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_496, [2, 4, 1, 3, 0]);  permute_496 = None
    view_445: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_498, [16, 512, 64]);  permute_498 = None
    bmm_94: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_444, view_445)
    view_446: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_94, [16, 512, 1, 1, 64]);  bmm_94 = None
    permute_499: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_446, [1, 3, 0, 4, 2]);  view_446 = None
    view_447: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_499, [512, 1, 16, 64]);  permute_499 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_300: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_447, 4);  view_447 = None
    permute_500: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_300, [0, 1, 4, 3, 2]);  unsqueeze_300 = None
    unsqueeze_301: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_84, 3);  primals_84 = None
    unsqueeze_302: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_301, 4);  unsqueeze_301 = None
    permute_501: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_302, [3, 4, 0, 2, 1]);  unsqueeze_302 = None
    permute_502: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_500, [0, 3, 4, 1, 2]);  permute_500 = None
    clone_22: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_502, memory_format = torch.contiguous_format);  permute_502 = None
    view_448: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_22, [1, 512, 1024]);  clone_22 = None
    permute_503: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_501, [3, 4, 1, 2, 0]);  permute_501 = None
    clone_23: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_503, memory_format = torch.contiguous_format);  permute_503 = None
    view_449: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_23, [1, 1024, 1024]);  clone_23 = None
    bmm_95: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_448, view_449)
    view_450: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_95, [512, 1, 1, 1, 1024]);  bmm_95 = None
    permute_504: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_450, [0, 3, 4, 1, 2]);  view_450 = None
    view_451: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_504, [512, 1, 1024]);  permute_504 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_47 = torch.ops.aten.native_dropout.default(view_451, 0.1, True);  view_451 = None
    getitem_138: "f32[512, 1, 1024]" = native_dropout_47[0]
    getitem_139: "b8[512, 1, 1024]" = native_dropout_47[1];  native_dropout_47 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_127: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_138, add_122);  getitem_138 = add_122 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_22 = torch.ops.aten.var_mean.correction(add_127, [2], correction = 0, keepdim = True)
    getitem_140: "f32[512, 1, 1]" = var_mean_22[0]
    getitem_141: "f32[512, 1, 1]" = var_mean_22[1];  var_mean_22 = None
    add_128: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_140, 1e-12);  getitem_140 = None
    rsqrt_22: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_128);  add_128 = None
    sub_34: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_127, getitem_141)
    mul_93: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_34, rsqrt_22);  sub_34 = None
    mul_94: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_93, primals_258);  mul_93 = None
    add_129: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_94, primals_259);  mul_94 = primals_259 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_452: "f32[512, 1024]" = torch.ops.aten.view.default(add_129, [512, 1024])
    permute_505: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_260, [1, 0]);  primals_260 = None
    addmm_22: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_261, view_452, permute_505);  primals_261 = None
    view_453: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_22, [512, 1, 4096]);  addmm_22 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_95: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_453, 0.5)
    mul_96: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_453, 0.7071067811865476)
    erf_11: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_96);  mul_96 = None
    add_130: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_11, 1);  erf_11 = None
    mul_97: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_95, add_130);  mul_95 = add_130 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_48 = torch.ops.aten.native_dropout.default(mul_97, 0.1, True);  mul_97 = None
    getitem_142: "f32[512, 1, 4096]" = native_dropout_48[0]
    getitem_143: "b8[512, 1, 4096]" = native_dropout_48[1];  native_dropout_48 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_454: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_142, [512, 4096]);  getitem_142 = None
    permute_506: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_262, [1, 0]);  primals_262 = None
    addmm_23: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_263, view_454, permute_506);  primals_263 = None
    view_455: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_23, [512, 1, 1024]);  addmm_23 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_49 = torch.ops.aten.native_dropout.default(view_455, 0.1, True);  view_455 = None
    getitem_144: "f32[512, 1, 1024]" = native_dropout_49[0]
    getitem_145: "b8[512, 1, 1024]" = native_dropout_49[1];  native_dropout_49 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_131: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_144, add_129);  getitem_144 = add_129 = None
    var_mean_23 = torch.ops.aten.var_mean.correction(add_131, [2], correction = 0, keepdim = True)
    getitem_146: "f32[512, 1, 1]" = var_mean_23[0]
    getitem_147: "f32[512, 1, 1]" = var_mean_23[1];  var_mean_23 = None
    add_132: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_146, 1e-12);  getitem_146 = None
    rsqrt_23: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_132);  add_132 = None
    sub_35: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_131, getitem_147)
    mul_98: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_35, rsqrt_23);  sub_35 = None
    mul_99: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_98, primals_264);  mul_98 = None
    add_133: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_99, primals_265);  mul_99 = primals_265 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_303: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_133, 3)
    unsqueeze_304: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_303, 4);  unsqueeze_303 = None
    permute_507: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_304, [0, 1, 3, 4, 2]);  unsqueeze_304 = None
    unsqueeze_305: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_85, 3);  primals_85 = None
    unsqueeze_306: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_305, 4);  unsqueeze_305 = None
    permute_508: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_306, [3, 4, 1, 2, 0]);  unsqueeze_306 = None
    permute_509: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_507, [0, 4, 1, 2, 3]);  permute_507 = None
    view_456: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_509, [1, 512, 1024]);  permute_509 = None
    permute_510: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_508, [4, 1, 2, 3, 0]);  permute_508 = None
    view_457: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_510, [1, 1024, 1024]);  permute_510 = None
    bmm_96: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_456, view_457)
    view_458: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_96, [512, 1, 1, 16, 64]);  bmm_96 = None
    permute_511: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_458, [0, 2, 3, 4, 1]);  view_458 = None
    view_459: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_511, [512, 1, 16, 64]);  permute_511 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_307: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_133, 3)
    unsqueeze_308: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_307, 4);  unsqueeze_307 = None
    permute_512: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_308, [0, 1, 3, 4, 2]);  unsqueeze_308 = None
    unsqueeze_309: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_86, 3);  primals_86 = None
    unsqueeze_310: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_309, 4);  unsqueeze_309 = None
    permute_513: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_310, [3, 4, 1, 2, 0]);  unsqueeze_310 = None
    permute_514: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_512, [0, 4, 1, 2, 3]);  permute_512 = None
    view_460: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_514, [1, 512, 1024]);  permute_514 = None
    permute_515: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_513, [4, 1, 2, 3, 0]);  permute_513 = None
    view_461: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_515, [1, 1024, 1024]);  permute_515 = None
    bmm_97: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_460, view_461)
    view_462: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_97, [512, 1, 1, 16, 64]);  bmm_97 = None
    permute_516: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_462, [0, 2, 3, 4, 1]);  view_462 = None
    view_463: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_516, [512, 1, 16, 64]);  permute_516 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_311: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_133, 3)
    unsqueeze_312: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_311, 4);  unsqueeze_311 = None
    permute_517: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_312, [0, 1, 3, 4, 2]);  unsqueeze_312 = None
    unsqueeze_313: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_87, 3);  primals_87 = None
    unsqueeze_314: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_313, 4);  unsqueeze_313 = None
    permute_518: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_314, [3, 4, 1, 2, 0]);  unsqueeze_314 = None
    permute_519: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_517, [0, 4, 1, 2, 3]);  permute_517 = None
    view_464: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_519, [1, 512, 1024]);  permute_519 = None
    permute_520: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_518, [4, 1, 2, 3, 0]);  permute_518 = None
    view_465: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_520, [1, 1024, 1024]);  permute_520 = None
    bmm_98: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_464, view_465)
    view_466: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_98, [512, 1, 1, 16, 64]);  bmm_98 = None
    permute_521: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_466, [0, 2, 3, 4, 1]);  view_466 = None
    view_467: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_521, [512, 1, 16, 64]);  permute_521 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_315: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_316: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_315, 4);  unsqueeze_315 = None
    permute_522: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_316, [0, 1, 3, 4, 2]);  unsqueeze_316 = None
    unsqueeze_317: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_88, 3);  primals_88 = None
    unsqueeze_318: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_317, 4);  unsqueeze_317 = None
    permute_523: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_318, [3, 4, 1, 2, 0]);  unsqueeze_318 = None
    permute_524: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_522, [0, 4, 1, 2, 3]);  permute_522 = None
    view_468: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_524, [1, 1024, 1024]);  permute_524 = None
    permute_525: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_523, [4, 1, 2, 3, 0]);  permute_523 = None
    view_469: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_525, [1, 1024, 1024]);  permute_525 = None
    bmm_99: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_468, view_469);  view_469 = None
    view_470: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_99, [1024, 1, 1, 16, 64]);  bmm_99 = None
    permute_526: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_470, [0, 2, 3, 4, 1]);  view_470 = None
    view_471: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_526, [1024, 1, 16, 64]);  permute_526 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_134: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_459, primals_89);  primals_89 = None
    unsqueeze_319: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_134, 4);  add_134 = None
    permute_527: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_319, [1, 2, 0, 4, 3]);  unsqueeze_319 = None
    unsqueeze_320: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_463, 4);  view_463 = None
    permute_528: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_320, [1, 2, 4, 0, 3]);  unsqueeze_320 = None
    permute_529: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_527, [1, 2, 4, 0, 3]);  permute_527 = None
    view_472: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_529, [16, 512, 64]);  permute_529 = None
    permute_530: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_528, [1, 4, 0, 3, 2]);  permute_528 = None
    view_473: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_530, [16, 64, 512]);  permute_530 = None
    bmm_100: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_472, view_473)
    view_474: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_100, [16, 512, 1, 1, 512]);  bmm_100 = None
    permute_531: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_474, [3, 0, 1, 4, 2]);  view_474 = None
    view_475: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_531, [1, 16, 512, 512]);  permute_531 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_135: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_459, primals_90);  view_459 = primals_90 = None
    unsqueeze_321: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_135, 4);  add_135 = None
    permute_532: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_321, [1, 2, 0, 4, 3]);  unsqueeze_321 = None
    unsqueeze_322: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_471, 4);  view_471 = None
    permute_533: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_322, [1, 2, 4, 0, 3]);  unsqueeze_322 = None
    permute_534: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_532, [1, 2, 4, 0, 3]);  permute_532 = None
    view_476: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_534, [16, 512, 64]);  permute_534 = None
    permute_535: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_533, [1, 4, 0, 3, 2]);  permute_533 = None
    view_477: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_535, [16, 64, 1024]);  permute_535 = None
    bmm_101: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_476, view_477)
    view_478: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_101, [16, 512, 1, 1, 1024]);  bmm_101 = None
    permute_536: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_478, [3, 0, 1, 4, 2]);  view_478 = None
    view_479: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_536, [1, 16, 512, 1024]);  permute_536 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_480: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_479, [1, 16, 1024, 512]);  view_479 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_87: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_480, 0, 0, 9223372036854775807);  view_480 = None
    slice_88: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_87, 1, 0, 9223372036854775807);  slice_87 = None
    slice_89: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_88, 2, 1, 9223372036854775807);  slice_88 = None
    slice_90: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_89, 3, 0, 9223372036854775807);  slice_89 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_481: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_90, [1, 16, 512, 1023]);  slice_90 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_14: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_91: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_481, 0, 0, 9223372036854775807);  view_481 = None
    slice_92: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_91, 1, 0, 9223372036854775807);  slice_91 = None
    slice_93: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_92, 2, 0, 9223372036854775807);  slice_92 = None
    index_12: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_93, [None, None, None, iota_14]);  slice_93 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_136: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_475, index_12);  view_475 = index_12 = None
    add_137: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_136, 0);  add_136 = None
    mul_100: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_137, 0.125);  add_137 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_12: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_100, [3], True)
    sub_36: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_100, amax_12);  mul_100 = amax_12 = None
    exp_12: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_36);  sub_36 = None
    sum_13: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_12, [3], True)
    div_13: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_12, sum_13);  exp_12 = sum_13 = None
    alias_12: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_13)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_50 = torch.ops.aten.native_dropout.default(div_13, 0.1, True);  div_13 = None
    getitem_148: "f32[1, 16, 512, 512]" = native_dropout_50[0]
    getitem_149: "b8[1, 16, 512, 512]" = native_dropout_50[1];  native_dropout_50 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_323: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_148, 4);  getitem_148 = None
    permute_537: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_323, [2, 0, 1, 4, 3]);  unsqueeze_323 = None
    unsqueeze_324: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_467, 4);  view_467 = None
    permute_538: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_324, [4, 1, 2, 3, 0]);  unsqueeze_324 = None
    permute_539: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_537, [2, 0, 4, 1, 3]);  permute_537 = None
    view_482: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_539, [16, 512, 512]);  permute_539 = None
    permute_540: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_538, [2, 4, 1, 3, 0]);  permute_538 = None
    view_483: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_540, [16, 512, 64]);  permute_540 = None
    bmm_102: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_482, view_483)
    view_484: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_102, [16, 512, 1, 1, 64]);  bmm_102 = None
    permute_541: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_484, [1, 3, 0, 4, 2]);  view_484 = None
    view_485: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_541, [512, 1, 16, 64]);  permute_541 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_325: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_485, 4);  view_485 = None
    permute_542: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_325, [0, 1, 4, 3, 2]);  unsqueeze_325 = None
    unsqueeze_326: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_91, 3);  primals_91 = None
    unsqueeze_327: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_326, 4);  unsqueeze_326 = None
    permute_543: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_327, [3, 4, 0, 2, 1]);  unsqueeze_327 = None
    permute_544: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_542, [0, 3, 4, 1, 2]);  permute_542 = None
    clone_24: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_544, memory_format = torch.contiguous_format);  permute_544 = None
    view_486: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_24, [1, 512, 1024]);  clone_24 = None
    permute_545: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_543, [3, 4, 1, 2, 0]);  permute_543 = None
    clone_25: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_545, memory_format = torch.contiguous_format);  permute_545 = None
    view_487: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_25, [1, 1024, 1024]);  clone_25 = None
    bmm_103: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_486, view_487)
    view_488: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_103, [512, 1, 1, 1, 1024]);  bmm_103 = None
    permute_546: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_488, [0, 3, 4, 1, 2]);  view_488 = None
    view_489: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_546, [512, 1, 1024]);  permute_546 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_51 = torch.ops.aten.native_dropout.default(view_489, 0.1, True);  view_489 = None
    getitem_150: "f32[512, 1, 1024]" = native_dropout_51[0]
    getitem_151: "b8[512, 1, 1024]" = native_dropout_51[1];  native_dropout_51 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_138: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_150, add_133);  getitem_150 = add_133 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_24 = torch.ops.aten.var_mean.correction(add_138, [2], correction = 0, keepdim = True)
    getitem_152: "f32[512, 1, 1]" = var_mean_24[0]
    getitem_153: "f32[512, 1, 1]" = var_mean_24[1];  var_mean_24 = None
    add_139: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_152, 1e-12);  getitem_152 = None
    rsqrt_24: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_139);  add_139 = None
    sub_37: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_138, getitem_153)
    mul_101: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_37, rsqrt_24);  sub_37 = None
    mul_102: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_101, primals_266);  mul_101 = None
    add_140: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_102, primals_267);  mul_102 = primals_267 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_490: "f32[512, 1024]" = torch.ops.aten.view.default(add_140, [512, 1024])
    permute_547: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_268, [1, 0]);  primals_268 = None
    addmm_24: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_269, view_490, permute_547);  primals_269 = None
    view_491: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_24, [512, 1, 4096]);  addmm_24 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_103: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_491, 0.5)
    mul_104: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_491, 0.7071067811865476)
    erf_12: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_104);  mul_104 = None
    add_141: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_12, 1);  erf_12 = None
    mul_105: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_103, add_141);  mul_103 = add_141 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_52 = torch.ops.aten.native_dropout.default(mul_105, 0.1, True);  mul_105 = None
    getitem_154: "f32[512, 1, 4096]" = native_dropout_52[0]
    getitem_155: "b8[512, 1, 4096]" = native_dropout_52[1];  native_dropout_52 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_492: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_154, [512, 4096]);  getitem_154 = None
    permute_548: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_270, [1, 0]);  primals_270 = None
    addmm_25: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_271, view_492, permute_548);  primals_271 = None
    view_493: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_25, [512, 1, 1024]);  addmm_25 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_53 = torch.ops.aten.native_dropout.default(view_493, 0.1, True);  view_493 = None
    getitem_156: "f32[512, 1, 1024]" = native_dropout_53[0]
    getitem_157: "b8[512, 1, 1024]" = native_dropout_53[1];  native_dropout_53 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_142: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_156, add_140);  getitem_156 = add_140 = None
    var_mean_25 = torch.ops.aten.var_mean.correction(add_142, [2], correction = 0, keepdim = True)
    getitem_158: "f32[512, 1, 1]" = var_mean_25[0]
    getitem_159: "f32[512, 1, 1]" = var_mean_25[1];  var_mean_25 = None
    add_143: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_158, 1e-12);  getitem_158 = None
    rsqrt_25: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_143);  add_143 = None
    sub_38: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_142, getitem_159)
    mul_106: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_38, rsqrt_25);  sub_38 = None
    mul_107: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_106, primals_272);  mul_106 = None
    add_144: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_107, primals_273);  mul_107 = primals_273 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_328: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_144, 3)
    unsqueeze_329: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_328, 4);  unsqueeze_328 = None
    permute_549: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_329, [0, 1, 3, 4, 2]);  unsqueeze_329 = None
    unsqueeze_330: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_92, 3);  primals_92 = None
    unsqueeze_331: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_330, 4);  unsqueeze_330 = None
    permute_550: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_331, [3, 4, 1, 2, 0]);  unsqueeze_331 = None
    permute_551: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_549, [0, 4, 1, 2, 3]);  permute_549 = None
    view_494: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_551, [1, 512, 1024]);  permute_551 = None
    permute_552: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_550, [4, 1, 2, 3, 0]);  permute_550 = None
    view_495: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_552, [1, 1024, 1024]);  permute_552 = None
    bmm_104: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_494, view_495)
    view_496: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_104, [512, 1, 1, 16, 64]);  bmm_104 = None
    permute_553: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_496, [0, 2, 3, 4, 1]);  view_496 = None
    view_497: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_553, [512, 1, 16, 64]);  permute_553 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_332: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_144, 3)
    unsqueeze_333: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_332, 4);  unsqueeze_332 = None
    permute_554: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_333, [0, 1, 3, 4, 2]);  unsqueeze_333 = None
    unsqueeze_334: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_93, 3);  primals_93 = None
    unsqueeze_335: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_334, 4);  unsqueeze_334 = None
    permute_555: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_335, [3, 4, 1, 2, 0]);  unsqueeze_335 = None
    permute_556: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_554, [0, 4, 1, 2, 3]);  permute_554 = None
    view_498: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_556, [1, 512, 1024]);  permute_556 = None
    permute_557: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_555, [4, 1, 2, 3, 0]);  permute_555 = None
    view_499: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_557, [1, 1024, 1024]);  permute_557 = None
    bmm_105: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_498, view_499)
    view_500: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_105, [512, 1, 1, 16, 64]);  bmm_105 = None
    permute_558: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_500, [0, 2, 3, 4, 1]);  view_500 = None
    view_501: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_558, [512, 1, 16, 64]);  permute_558 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_336: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_144, 3)
    unsqueeze_337: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_336, 4);  unsqueeze_336 = None
    permute_559: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_337, [0, 1, 3, 4, 2]);  unsqueeze_337 = None
    unsqueeze_338: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_94, 3);  primals_94 = None
    unsqueeze_339: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_338, 4);  unsqueeze_338 = None
    permute_560: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_339, [3, 4, 1, 2, 0]);  unsqueeze_339 = None
    permute_561: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_559, [0, 4, 1, 2, 3]);  permute_559 = None
    view_502: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_561, [1, 512, 1024]);  permute_561 = None
    permute_562: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_560, [4, 1, 2, 3, 0]);  permute_560 = None
    view_503: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_562, [1, 1024, 1024]);  permute_562 = None
    bmm_106: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_502, view_503)
    view_504: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_106, [512, 1, 1, 16, 64]);  bmm_106 = None
    permute_563: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_504, [0, 2, 3, 4, 1]);  view_504 = None
    view_505: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_563, [512, 1, 16, 64]);  permute_563 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_340: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_341: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_340, 4);  unsqueeze_340 = None
    permute_564: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_341, [0, 1, 3, 4, 2]);  unsqueeze_341 = None
    unsqueeze_342: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_95, 3);  primals_95 = None
    unsqueeze_343: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_342, 4);  unsqueeze_342 = None
    permute_565: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_343, [3, 4, 1, 2, 0]);  unsqueeze_343 = None
    permute_566: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_564, [0, 4, 1, 2, 3]);  permute_564 = None
    view_506: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_566, [1, 1024, 1024]);  permute_566 = None
    permute_567: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_565, [4, 1, 2, 3, 0]);  permute_565 = None
    view_507: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_567, [1, 1024, 1024]);  permute_567 = None
    bmm_107: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_506, view_507);  view_507 = None
    view_508: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_107, [1024, 1, 1, 16, 64]);  bmm_107 = None
    permute_568: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_508, [0, 2, 3, 4, 1]);  view_508 = None
    view_509: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_568, [1024, 1, 16, 64]);  permute_568 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_145: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_497, primals_96);  primals_96 = None
    unsqueeze_344: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_145, 4);  add_145 = None
    permute_569: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_344, [1, 2, 0, 4, 3]);  unsqueeze_344 = None
    unsqueeze_345: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_501, 4);  view_501 = None
    permute_570: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_345, [1, 2, 4, 0, 3]);  unsqueeze_345 = None
    permute_571: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_569, [1, 2, 4, 0, 3]);  permute_569 = None
    view_510: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_571, [16, 512, 64]);  permute_571 = None
    permute_572: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_570, [1, 4, 0, 3, 2]);  permute_570 = None
    view_511: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_572, [16, 64, 512]);  permute_572 = None
    bmm_108: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_510, view_511)
    view_512: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_108, [16, 512, 1, 1, 512]);  bmm_108 = None
    permute_573: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_512, [3, 0, 1, 4, 2]);  view_512 = None
    view_513: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_573, [1, 16, 512, 512]);  permute_573 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_146: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_497, primals_97);  view_497 = primals_97 = None
    unsqueeze_346: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_146, 4);  add_146 = None
    permute_574: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_346, [1, 2, 0, 4, 3]);  unsqueeze_346 = None
    unsqueeze_347: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_509, 4);  view_509 = None
    permute_575: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_347, [1, 2, 4, 0, 3]);  unsqueeze_347 = None
    permute_576: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_574, [1, 2, 4, 0, 3]);  permute_574 = None
    view_514: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_576, [16, 512, 64]);  permute_576 = None
    permute_577: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_575, [1, 4, 0, 3, 2]);  permute_575 = None
    view_515: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_577, [16, 64, 1024]);  permute_577 = None
    bmm_109: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_514, view_515)
    view_516: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_109, [16, 512, 1, 1, 1024]);  bmm_109 = None
    permute_578: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_516, [3, 0, 1, 4, 2]);  view_516 = None
    view_517: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_578, [1, 16, 512, 1024]);  permute_578 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_518: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_517, [1, 16, 1024, 512]);  view_517 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_94: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_518, 0, 0, 9223372036854775807);  view_518 = None
    slice_95: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_94, 1, 0, 9223372036854775807);  slice_94 = None
    slice_96: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_95, 2, 1, 9223372036854775807);  slice_95 = None
    slice_97: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_96, 3, 0, 9223372036854775807);  slice_96 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_519: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_97, [1, 16, 512, 1023]);  slice_97 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_15: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_98: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_519, 0, 0, 9223372036854775807);  view_519 = None
    slice_99: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_98, 1, 0, 9223372036854775807);  slice_98 = None
    slice_100: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_99, 2, 0, 9223372036854775807);  slice_99 = None
    index_13: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_100, [None, None, None, iota_15]);  slice_100 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_147: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_513, index_13);  view_513 = index_13 = None
    add_148: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_147, 0);  add_147 = None
    mul_108: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_148, 0.125);  add_148 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_13: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_108, [3], True)
    sub_39: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_108, amax_13);  mul_108 = amax_13 = None
    exp_13: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_39);  sub_39 = None
    sum_14: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_13, [3], True)
    div_14: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_13, sum_14);  exp_13 = sum_14 = None
    alias_13: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_14)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_54 = torch.ops.aten.native_dropout.default(div_14, 0.1, True);  div_14 = None
    getitem_160: "f32[1, 16, 512, 512]" = native_dropout_54[0]
    getitem_161: "b8[1, 16, 512, 512]" = native_dropout_54[1];  native_dropout_54 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_348: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_160, 4);  getitem_160 = None
    permute_579: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_348, [2, 0, 1, 4, 3]);  unsqueeze_348 = None
    unsqueeze_349: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_505, 4);  view_505 = None
    permute_580: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_349, [4, 1, 2, 3, 0]);  unsqueeze_349 = None
    permute_581: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_579, [2, 0, 4, 1, 3]);  permute_579 = None
    view_520: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_581, [16, 512, 512]);  permute_581 = None
    permute_582: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_580, [2, 4, 1, 3, 0]);  permute_580 = None
    view_521: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_582, [16, 512, 64]);  permute_582 = None
    bmm_110: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_520, view_521)
    view_522: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_110, [16, 512, 1, 1, 64]);  bmm_110 = None
    permute_583: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_522, [1, 3, 0, 4, 2]);  view_522 = None
    view_523: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_583, [512, 1, 16, 64]);  permute_583 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_350: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_523, 4);  view_523 = None
    permute_584: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_350, [0, 1, 4, 3, 2]);  unsqueeze_350 = None
    unsqueeze_351: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_98, 3);  primals_98 = None
    unsqueeze_352: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_351, 4);  unsqueeze_351 = None
    permute_585: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_352, [3, 4, 0, 2, 1]);  unsqueeze_352 = None
    permute_586: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_584, [0, 3, 4, 1, 2]);  permute_584 = None
    clone_26: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_586, memory_format = torch.contiguous_format);  permute_586 = None
    view_524: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_26, [1, 512, 1024]);  clone_26 = None
    permute_587: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_585, [3, 4, 1, 2, 0]);  permute_585 = None
    clone_27: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_587, memory_format = torch.contiguous_format);  permute_587 = None
    view_525: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_27, [1, 1024, 1024]);  clone_27 = None
    bmm_111: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_524, view_525)
    view_526: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_111, [512, 1, 1, 1, 1024]);  bmm_111 = None
    permute_588: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_526, [0, 3, 4, 1, 2]);  view_526 = None
    view_527: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_588, [512, 1, 1024]);  permute_588 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_55 = torch.ops.aten.native_dropout.default(view_527, 0.1, True);  view_527 = None
    getitem_162: "f32[512, 1, 1024]" = native_dropout_55[0]
    getitem_163: "b8[512, 1, 1024]" = native_dropout_55[1];  native_dropout_55 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_149: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_162, add_144);  getitem_162 = add_144 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_26 = torch.ops.aten.var_mean.correction(add_149, [2], correction = 0, keepdim = True)
    getitem_164: "f32[512, 1, 1]" = var_mean_26[0]
    getitem_165: "f32[512, 1, 1]" = var_mean_26[1];  var_mean_26 = None
    add_150: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_164, 1e-12);  getitem_164 = None
    rsqrt_26: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_150);  add_150 = None
    sub_40: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_149, getitem_165)
    mul_109: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_40, rsqrt_26);  sub_40 = None
    mul_110: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_109, primals_274);  mul_109 = None
    add_151: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_110, primals_275);  mul_110 = primals_275 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_528: "f32[512, 1024]" = torch.ops.aten.view.default(add_151, [512, 1024])
    permute_589: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_276, [1, 0]);  primals_276 = None
    addmm_26: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_277, view_528, permute_589);  primals_277 = None
    view_529: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_26, [512, 1, 4096]);  addmm_26 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_111: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_529, 0.5)
    mul_112: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_529, 0.7071067811865476)
    erf_13: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_112);  mul_112 = None
    add_152: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_13, 1);  erf_13 = None
    mul_113: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_111, add_152);  mul_111 = add_152 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_56 = torch.ops.aten.native_dropout.default(mul_113, 0.1, True);  mul_113 = None
    getitem_166: "f32[512, 1, 4096]" = native_dropout_56[0]
    getitem_167: "b8[512, 1, 4096]" = native_dropout_56[1];  native_dropout_56 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_530: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_166, [512, 4096]);  getitem_166 = None
    permute_590: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_278, [1, 0]);  primals_278 = None
    addmm_27: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_279, view_530, permute_590);  primals_279 = None
    view_531: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_27, [512, 1, 1024]);  addmm_27 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_57 = torch.ops.aten.native_dropout.default(view_531, 0.1, True);  view_531 = None
    getitem_168: "f32[512, 1, 1024]" = native_dropout_57[0]
    getitem_169: "b8[512, 1, 1024]" = native_dropout_57[1];  native_dropout_57 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_153: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_168, add_151);  getitem_168 = add_151 = None
    var_mean_27 = torch.ops.aten.var_mean.correction(add_153, [2], correction = 0, keepdim = True)
    getitem_170: "f32[512, 1, 1]" = var_mean_27[0]
    getitem_171: "f32[512, 1, 1]" = var_mean_27[1];  var_mean_27 = None
    add_154: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_170, 1e-12);  getitem_170 = None
    rsqrt_27: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_154);  add_154 = None
    sub_41: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_153, getitem_171)
    mul_114: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_41, rsqrt_27);  sub_41 = None
    mul_115: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_114, primals_280);  mul_114 = None
    add_155: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_115, primals_281);  mul_115 = primals_281 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_353: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_155, 3)
    unsqueeze_354: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_353, 4);  unsqueeze_353 = None
    permute_591: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_354, [0, 1, 3, 4, 2]);  unsqueeze_354 = None
    unsqueeze_355: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_99, 3);  primals_99 = None
    unsqueeze_356: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_355, 4);  unsqueeze_355 = None
    permute_592: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_356, [3, 4, 1, 2, 0]);  unsqueeze_356 = None
    permute_593: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_591, [0, 4, 1, 2, 3]);  permute_591 = None
    view_532: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_593, [1, 512, 1024]);  permute_593 = None
    permute_594: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_592, [4, 1, 2, 3, 0]);  permute_592 = None
    view_533: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_594, [1, 1024, 1024]);  permute_594 = None
    bmm_112: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_532, view_533)
    view_534: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_112, [512, 1, 1, 16, 64]);  bmm_112 = None
    permute_595: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_534, [0, 2, 3, 4, 1]);  view_534 = None
    view_535: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_595, [512, 1, 16, 64]);  permute_595 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_357: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_155, 3)
    unsqueeze_358: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_357, 4);  unsqueeze_357 = None
    permute_596: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_358, [0, 1, 3, 4, 2]);  unsqueeze_358 = None
    unsqueeze_359: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_100, 3);  primals_100 = None
    unsqueeze_360: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_359, 4);  unsqueeze_359 = None
    permute_597: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_360, [3, 4, 1, 2, 0]);  unsqueeze_360 = None
    permute_598: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_596, [0, 4, 1, 2, 3]);  permute_596 = None
    view_536: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_598, [1, 512, 1024]);  permute_598 = None
    permute_599: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_597, [4, 1, 2, 3, 0]);  permute_597 = None
    view_537: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_599, [1, 1024, 1024]);  permute_599 = None
    bmm_113: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_536, view_537)
    view_538: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_113, [512, 1, 1, 16, 64]);  bmm_113 = None
    permute_600: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_538, [0, 2, 3, 4, 1]);  view_538 = None
    view_539: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_600, [512, 1, 16, 64]);  permute_600 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_361: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_155, 3)
    unsqueeze_362: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_361, 4);  unsqueeze_361 = None
    permute_601: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_362, [0, 1, 3, 4, 2]);  unsqueeze_362 = None
    unsqueeze_363: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_101, 3);  primals_101 = None
    unsqueeze_364: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_363, 4);  unsqueeze_363 = None
    permute_602: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_364, [3, 4, 1, 2, 0]);  unsqueeze_364 = None
    permute_603: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_601, [0, 4, 1, 2, 3]);  permute_601 = None
    view_540: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_603, [1, 512, 1024]);  permute_603 = None
    permute_604: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_602, [4, 1, 2, 3, 0]);  permute_602 = None
    view_541: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_604, [1, 1024, 1024]);  permute_604 = None
    bmm_114: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_540, view_541)
    view_542: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_114, [512, 1, 1, 16, 64]);  bmm_114 = None
    permute_605: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_542, [0, 2, 3, 4, 1]);  view_542 = None
    view_543: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_605, [512, 1, 16, 64]);  permute_605 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_365: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_366: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_365, 4);  unsqueeze_365 = None
    permute_606: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_366, [0, 1, 3, 4, 2]);  unsqueeze_366 = None
    unsqueeze_367: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_102, 3);  primals_102 = None
    unsqueeze_368: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_367, 4);  unsqueeze_367 = None
    permute_607: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_368, [3, 4, 1, 2, 0]);  unsqueeze_368 = None
    permute_608: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_606, [0, 4, 1, 2, 3]);  permute_606 = None
    view_544: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_608, [1, 1024, 1024]);  permute_608 = None
    permute_609: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_607, [4, 1, 2, 3, 0]);  permute_607 = None
    view_545: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_609, [1, 1024, 1024]);  permute_609 = None
    bmm_115: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_544, view_545);  view_545 = None
    view_546: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_115, [1024, 1, 1, 16, 64]);  bmm_115 = None
    permute_610: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_546, [0, 2, 3, 4, 1]);  view_546 = None
    view_547: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_610, [1024, 1, 16, 64]);  permute_610 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_156: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_535, primals_103);  primals_103 = None
    unsqueeze_369: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_156, 4);  add_156 = None
    permute_611: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_369, [1, 2, 0, 4, 3]);  unsqueeze_369 = None
    unsqueeze_370: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_539, 4);  view_539 = None
    permute_612: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_370, [1, 2, 4, 0, 3]);  unsqueeze_370 = None
    permute_613: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_611, [1, 2, 4, 0, 3]);  permute_611 = None
    view_548: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_613, [16, 512, 64]);  permute_613 = None
    permute_614: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_612, [1, 4, 0, 3, 2]);  permute_612 = None
    view_549: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_614, [16, 64, 512]);  permute_614 = None
    bmm_116: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_548, view_549)
    view_550: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_116, [16, 512, 1, 1, 512]);  bmm_116 = None
    permute_615: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_550, [3, 0, 1, 4, 2]);  view_550 = None
    view_551: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_615, [1, 16, 512, 512]);  permute_615 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_157: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_535, primals_104);  view_535 = primals_104 = None
    unsqueeze_371: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_157, 4);  add_157 = None
    permute_616: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_371, [1, 2, 0, 4, 3]);  unsqueeze_371 = None
    unsqueeze_372: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_547, 4);  view_547 = None
    permute_617: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_372, [1, 2, 4, 0, 3]);  unsqueeze_372 = None
    permute_618: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_616, [1, 2, 4, 0, 3]);  permute_616 = None
    view_552: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_618, [16, 512, 64]);  permute_618 = None
    permute_619: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_617, [1, 4, 0, 3, 2]);  permute_617 = None
    view_553: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_619, [16, 64, 1024]);  permute_619 = None
    bmm_117: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_552, view_553)
    view_554: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_117, [16, 512, 1, 1, 1024]);  bmm_117 = None
    permute_620: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_554, [3, 0, 1, 4, 2]);  view_554 = None
    view_555: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_620, [1, 16, 512, 1024]);  permute_620 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_556: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_555, [1, 16, 1024, 512]);  view_555 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_101: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_556, 0, 0, 9223372036854775807);  view_556 = None
    slice_102: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_101, 1, 0, 9223372036854775807);  slice_101 = None
    slice_103: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_102, 2, 1, 9223372036854775807);  slice_102 = None
    slice_104: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_103, 3, 0, 9223372036854775807);  slice_103 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_557: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_104, [1, 16, 512, 1023]);  slice_104 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_16: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_105: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_557, 0, 0, 9223372036854775807);  view_557 = None
    slice_106: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_105, 1, 0, 9223372036854775807);  slice_105 = None
    slice_107: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_106, 2, 0, 9223372036854775807);  slice_106 = None
    index_14: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_107, [None, None, None, iota_16]);  slice_107 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_158: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_551, index_14);  view_551 = index_14 = None
    add_159: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_158, 0);  add_158 = None
    mul_116: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_159, 0.125);  add_159 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_14: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_116, [3], True)
    sub_42: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_116, amax_14);  mul_116 = amax_14 = None
    exp_14: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_42);  sub_42 = None
    sum_15: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_14, [3], True)
    div_15: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_14, sum_15);  exp_14 = sum_15 = None
    alias_14: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_15)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_58 = torch.ops.aten.native_dropout.default(div_15, 0.1, True);  div_15 = None
    getitem_172: "f32[1, 16, 512, 512]" = native_dropout_58[0]
    getitem_173: "b8[1, 16, 512, 512]" = native_dropout_58[1];  native_dropout_58 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_373: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_172, 4);  getitem_172 = None
    permute_621: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_373, [2, 0, 1, 4, 3]);  unsqueeze_373 = None
    unsqueeze_374: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_543, 4);  view_543 = None
    permute_622: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_374, [4, 1, 2, 3, 0]);  unsqueeze_374 = None
    permute_623: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_621, [2, 0, 4, 1, 3]);  permute_621 = None
    view_558: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_623, [16, 512, 512]);  permute_623 = None
    permute_624: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_622, [2, 4, 1, 3, 0]);  permute_622 = None
    view_559: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_624, [16, 512, 64]);  permute_624 = None
    bmm_118: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_558, view_559)
    view_560: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_118, [16, 512, 1, 1, 64]);  bmm_118 = None
    permute_625: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_560, [1, 3, 0, 4, 2]);  view_560 = None
    view_561: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_625, [512, 1, 16, 64]);  permute_625 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_375: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_561, 4);  view_561 = None
    permute_626: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_375, [0, 1, 4, 3, 2]);  unsqueeze_375 = None
    unsqueeze_376: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_105, 3);  primals_105 = None
    unsqueeze_377: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_376, 4);  unsqueeze_376 = None
    permute_627: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_377, [3, 4, 0, 2, 1]);  unsqueeze_377 = None
    permute_628: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_626, [0, 3, 4, 1, 2]);  permute_626 = None
    clone_28: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_628, memory_format = torch.contiguous_format);  permute_628 = None
    view_562: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_28, [1, 512, 1024]);  clone_28 = None
    permute_629: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_627, [3, 4, 1, 2, 0]);  permute_627 = None
    clone_29: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_629, memory_format = torch.contiguous_format);  permute_629 = None
    view_563: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_29, [1, 1024, 1024]);  clone_29 = None
    bmm_119: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_562, view_563)
    view_564: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_119, [512, 1, 1, 1, 1024]);  bmm_119 = None
    permute_630: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_564, [0, 3, 4, 1, 2]);  view_564 = None
    view_565: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_630, [512, 1, 1024]);  permute_630 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_59 = torch.ops.aten.native_dropout.default(view_565, 0.1, True);  view_565 = None
    getitem_174: "f32[512, 1, 1024]" = native_dropout_59[0]
    getitem_175: "b8[512, 1, 1024]" = native_dropout_59[1];  native_dropout_59 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_160: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_174, add_155);  getitem_174 = add_155 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_28 = torch.ops.aten.var_mean.correction(add_160, [2], correction = 0, keepdim = True)
    getitem_176: "f32[512, 1, 1]" = var_mean_28[0]
    getitem_177: "f32[512, 1, 1]" = var_mean_28[1];  var_mean_28 = None
    add_161: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_176, 1e-12);  getitem_176 = None
    rsqrt_28: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_161);  add_161 = None
    sub_43: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_160, getitem_177)
    mul_117: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_43, rsqrt_28);  sub_43 = None
    mul_118: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_117, primals_282);  mul_117 = None
    add_162: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_118, primals_283);  mul_118 = primals_283 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_566: "f32[512, 1024]" = torch.ops.aten.view.default(add_162, [512, 1024])
    permute_631: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_284, [1, 0]);  primals_284 = None
    addmm_28: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_285, view_566, permute_631);  primals_285 = None
    view_567: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_28, [512, 1, 4096]);  addmm_28 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_119: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_567, 0.5)
    mul_120: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_567, 0.7071067811865476)
    erf_14: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_120);  mul_120 = None
    add_163: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_14, 1);  erf_14 = None
    mul_121: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_119, add_163);  mul_119 = add_163 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_60 = torch.ops.aten.native_dropout.default(mul_121, 0.1, True);  mul_121 = None
    getitem_178: "f32[512, 1, 4096]" = native_dropout_60[0]
    getitem_179: "b8[512, 1, 4096]" = native_dropout_60[1];  native_dropout_60 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_568: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_178, [512, 4096]);  getitem_178 = None
    permute_632: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_286, [1, 0]);  primals_286 = None
    addmm_29: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_287, view_568, permute_632);  primals_287 = None
    view_569: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_29, [512, 1, 1024]);  addmm_29 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_61 = torch.ops.aten.native_dropout.default(view_569, 0.1, True);  view_569 = None
    getitem_180: "f32[512, 1, 1024]" = native_dropout_61[0]
    getitem_181: "b8[512, 1, 1024]" = native_dropout_61[1];  native_dropout_61 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_164: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_180, add_162);  getitem_180 = add_162 = None
    var_mean_29 = torch.ops.aten.var_mean.correction(add_164, [2], correction = 0, keepdim = True)
    getitem_182: "f32[512, 1, 1]" = var_mean_29[0]
    getitem_183: "f32[512, 1, 1]" = var_mean_29[1];  var_mean_29 = None
    add_165: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_182, 1e-12);  getitem_182 = None
    rsqrt_29: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_165);  add_165 = None
    sub_44: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_164, getitem_183)
    mul_122: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_44, rsqrt_29);  sub_44 = None
    mul_123: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_122, primals_288);  mul_122 = None
    add_166: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_123, primals_289);  mul_123 = primals_289 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_378: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_166, 3)
    unsqueeze_379: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_378, 4);  unsqueeze_378 = None
    permute_633: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_379, [0, 1, 3, 4, 2]);  unsqueeze_379 = None
    unsqueeze_380: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_106, 3);  primals_106 = None
    unsqueeze_381: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_380, 4);  unsqueeze_380 = None
    permute_634: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_381, [3, 4, 1, 2, 0]);  unsqueeze_381 = None
    permute_635: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_633, [0, 4, 1, 2, 3]);  permute_633 = None
    view_570: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_635, [1, 512, 1024]);  permute_635 = None
    permute_636: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_634, [4, 1, 2, 3, 0]);  permute_634 = None
    view_571: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_636, [1, 1024, 1024]);  permute_636 = None
    bmm_120: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_570, view_571)
    view_572: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_120, [512, 1, 1, 16, 64]);  bmm_120 = None
    permute_637: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_572, [0, 2, 3, 4, 1]);  view_572 = None
    view_573: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_637, [512, 1, 16, 64]);  permute_637 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_382: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_166, 3)
    unsqueeze_383: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_382, 4);  unsqueeze_382 = None
    permute_638: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_383, [0, 1, 3, 4, 2]);  unsqueeze_383 = None
    unsqueeze_384: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_107, 3);  primals_107 = None
    unsqueeze_385: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_384, 4);  unsqueeze_384 = None
    permute_639: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_385, [3, 4, 1, 2, 0]);  unsqueeze_385 = None
    permute_640: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_638, [0, 4, 1, 2, 3]);  permute_638 = None
    view_574: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_640, [1, 512, 1024]);  permute_640 = None
    permute_641: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_639, [4, 1, 2, 3, 0]);  permute_639 = None
    view_575: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_641, [1, 1024, 1024]);  permute_641 = None
    bmm_121: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_574, view_575)
    view_576: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_121, [512, 1, 1, 16, 64]);  bmm_121 = None
    permute_642: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_576, [0, 2, 3, 4, 1]);  view_576 = None
    view_577: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_642, [512, 1, 16, 64]);  permute_642 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_386: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_166, 3)
    unsqueeze_387: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_386, 4);  unsqueeze_386 = None
    permute_643: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_387, [0, 1, 3, 4, 2]);  unsqueeze_387 = None
    unsqueeze_388: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_108, 3);  primals_108 = None
    unsqueeze_389: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_388, 4);  unsqueeze_388 = None
    permute_644: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_389, [3, 4, 1, 2, 0]);  unsqueeze_389 = None
    permute_645: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_643, [0, 4, 1, 2, 3]);  permute_643 = None
    view_578: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_645, [1, 512, 1024]);  permute_645 = None
    permute_646: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_644, [4, 1, 2, 3, 0]);  permute_644 = None
    view_579: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_646, [1, 1024, 1024]);  permute_646 = None
    bmm_122: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_578, view_579)
    view_580: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_122, [512, 1, 1, 16, 64]);  bmm_122 = None
    permute_647: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_580, [0, 2, 3, 4, 1]);  view_580 = None
    view_581: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_647, [512, 1, 16, 64]);  permute_647 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_390: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_391: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_390, 4);  unsqueeze_390 = None
    permute_648: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_391, [0, 1, 3, 4, 2]);  unsqueeze_391 = None
    unsqueeze_392: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_109, 3);  primals_109 = None
    unsqueeze_393: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_392, 4);  unsqueeze_392 = None
    permute_649: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_393, [3, 4, 1, 2, 0]);  unsqueeze_393 = None
    permute_650: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_648, [0, 4, 1, 2, 3]);  permute_648 = None
    view_582: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_650, [1, 1024, 1024]);  permute_650 = None
    permute_651: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_649, [4, 1, 2, 3, 0]);  permute_649 = None
    view_583: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_651, [1, 1024, 1024]);  permute_651 = None
    bmm_123: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_582, view_583);  view_583 = None
    view_584: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_123, [1024, 1, 1, 16, 64]);  bmm_123 = None
    permute_652: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_584, [0, 2, 3, 4, 1]);  view_584 = None
    view_585: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_652, [1024, 1, 16, 64]);  permute_652 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_167: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_573, primals_110);  primals_110 = None
    unsqueeze_394: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_167, 4);  add_167 = None
    permute_653: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_394, [1, 2, 0, 4, 3]);  unsqueeze_394 = None
    unsqueeze_395: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_577, 4);  view_577 = None
    permute_654: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_395, [1, 2, 4, 0, 3]);  unsqueeze_395 = None
    permute_655: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_653, [1, 2, 4, 0, 3]);  permute_653 = None
    view_586: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_655, [16, 512, 64]);  permute_655 = None
    permute_656: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_654, [1, 4, 0, 3, 2]);  permute_654 = None
    view_587: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_656, [16, 64, 512]);  permute_656 = None
    bmm_124: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_586, view_587)
    view_588: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_124, [16, 512, 1, 1, 512]);  bmm_124 = None
    permute_657: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_588, [3, 0, 1, 4, 2]);  view_588 = None
    view_589: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_657, [1, 16, 512, 512]);  permute_657 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_168: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_573, primals_111);  view_573 = primals_111 = None
    unsqueeze_396: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_168, 4);  add_168 = None
    permute_658: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_396, [1, 2, 0, 4, 3]);  unsqueeze_396 = None
    unsqueeze_397: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_585, 4);  view_585 = None
    permute_659: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_397, [1, 2, 4, 0, 3]);  unsqueeze_397 = None
    permute_660: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_658, [1, 2, 4, 0, 3]);  permute_658 = None
    view_590: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_660, [16, 512, 64]);  permute_660 = None
    permute_661: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_659, [1, 4, 0, 3, 2]);  permute_659 = None
    view_591: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_661, [16, 64, 1024]);  permute_661 = None
    bmm_125: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_590, view_591)
    view_592: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_125, [16, 512, 1, 1, 1024]);  bmm_125 = None
    permute_662: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_592, [3, 0, 1, 4, 2]);  view_592 = None
    view_593: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_662, [1, 16, 512, 1024]);  permute_662 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_594: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_593, [1, 16, 1024, 512]);  view_593 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_108: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_594, 0, 0, 9223372036854775807);  view_594 = None
    slice_109: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_108, 1, 0, 9223372036854775807);  slice_108 = None
    slice_110: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_109, 2, 1, 9223372036854775807);  slice_109 = None
    slice_111: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_110, 3, 0, 9223372036854775807);  slice_110 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_595: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_111, [1, 16, 512, 1023]);  slice_111 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_17: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_112: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_595, 0, 0, 9223372036854775807);  view_595 = None
    slice_113: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_112, 1, 0, 9223372036854775807);  slice_112 = None
    slice_114: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_113, 2, 0, 9223372036854775807);  slice_113 = None
    index_15: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_114, [None, None, None, iota_17]);  slice_114 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_169: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_589, index_15);  view_589 = index_15 = None
    add_170: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_169, 0);  add_169 = None
    mul_124: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_170, 0.125);  add_170 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_15: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_124, [3], True)
    sub_45: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_124, amax_15);  mul_124 = amax_15 = None
    exp_15: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_45);  sub_45 = None
    sum_16: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_15, [3], True)
    div_16: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_15, sum_16);  exp_15 = sum_16 = None
    alias_15: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_16)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_62 = torch.ops.aten.native_dropout.default(div_16, 0.1, True);  div_16 = None
    getitem_184: "f32[1, 16, 512, 512]" = native_dropout_62[0]
    getitem_185: "b8[1, 16, 512, 512]" = native_dropout_62[1];  native_dropout_62 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_398: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_184, 4);  getitem_184 = None
    permute_663: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_398, [2, 0, 1, 4, 3]);  unsqueeze_398 = None
    unsqueeze_399: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_581, 4);  view_581 = None
    permute_664: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_399, [4, 1, 2, 3, 0]);  unsqueeze_399 = None
    permute_665: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_663, [2, 0, 4, 1, 3]);  permute_663 = None
    view_596: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_665, [16, 512, 512]);  permute_665 = None
    permute_666: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_664, [2, 4, 1, 3, 0]);  permute_664 = None
    view_597: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_666, [16, 512, 64]);  permute_666 = None
    bmm_126: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_596, view_597)
    view_598: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_126, [16, 512, 1, 1, 64]);  bmm_126 = None
    permute_667: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_598, [1, 3, 0, 4, 2]);  view_598 = None
    view_599: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_667, [512, 1, 16, 64]);  permute_667 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_400: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_599, 4);  view_599 = None
    permute_668: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_400, [0, 1, 4, 3, 2]);  unsqueeze_400 = None
    unsqueeze_401: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_112, 3);  primals_112 = None
    unsqueeze_402: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_401, 4);  unsqueeze_401 = None
    permute_669: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_402, [3, 4, 0, 2, 1]);  unsqueeze_402 = None
    permute_670: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_668, [0, 3, 4, 1, 2]);  permute_668 = None
    clone_30: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_670, memory_format = torch.contiguous_format);  permute_670 = None
    view_600: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_30, [1, 512, 1024]);  clone_30 = None
    permute_671: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_669, [3, 4, 1, 2, 0]);  permute_669 = None
    clone_31: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_671, memory_format = torch.contiguous_format);  permute_671 = None
    view_601: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_31, [1, 1024, 1024]);  clone_31 = None
    bmm_127: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_600, view_601)
    view_602: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_127, [512, 1, 1, 1, 1024]);  bmm_127 = None
    permute_672: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_602, [0, 3, 4, 1, 2]);  view_602 = None
    view_603: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_672, [512, 1, 1024]);  permute_672 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_63 = torch.ops.aten.native_dropout.default(view_603, 0.1, True);  view_603 = None
    getitem_186: "f32[512, 1, 1024]" = native_dropout_63[0]
    getitem_187: "b8[512, 1, 1024]" = native_dropout_63[1];  native_dropout_63 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_171: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_186, add_166);  getitem_186 = add_166 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_30 = torch.ops.aten.var_mean.correction(add_171, [2], correction = 0, keepdim = True)
    getitem_188: "f32[512, 1, 1]" = var_mean_30[0]
    getitem_189: "f32[512, 1, 1]" = var_mean_30[1];  var_mean_30 = None
    add_172: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_188, 1e-12);  getitem_188 = None
    rsqrt_30: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_172);  add_172 = None
    sub_46: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_171, getitem_189)
    mul_125: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_46, rsqrt_30);  sub_46 = None
    mul_126: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_125, primals_290);  mul_125 = None
    add_173: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_126, primals_291);  mul_126 = primals_291 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_604: "f32[512, 1024]" = torch.ops.aten.view.default(add_173, [512, 1024])
    permute_673: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_292, [1, 0]);  primals_292 = None
    addmm_30: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_293, view_604, permute_673);  primals_293 = None
    view_605: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_30, [512, 1, 4096]);  addmm_30 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_127: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_605, 0.5)
    mul_128: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_605, 0.7071067811865476)
    erf_15: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_128);  mul_128 = None
    add_174: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_15, 1);  erf_15 = None
    mul_129: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_127, add_174);  mul_127 = add_174 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_64 = torch.ops.aten.native_dropout.default(mul_129, 0.1, True);  mul_129 = None
    getitem_190: "f32[512, 1, 4096]" = native_dropout_64[0]
    getitem_191: "b8[512, 1, 4096]" = native_dropout_64[1];  native_dropout_64 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_606: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_190, [512, 4096]);  getitem_190 = None
    permute_674: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_294, [1, 0]);  primals_294 = None
    addmm_31: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_295, view_606, permute_674);  primals_295 = None
    view_607: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_31, [512, 1, 1024]);  addmm_31 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_65 = torch.ops.aten.native_dropout.default(view_607, 0.1, True);  view_607 = None
    getitem_192: "f32[512, 1, 1024]" = native_dropout_65[0]
    getitem_193: "b8[512, 1, 1024]" = native_dropout_65[1];  native_dropout_65 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_175: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_192, add_173);  getitem_192 = add_173 = None
    var_mean_31 = torch.ops.aten.var_mean.correction(add_175, [2], correction = 0, keepdim = True)
    getitem_194: "f32[512, 1, 1]" = var_mean_31[0]
    getitem_195: "f32[512, 1, 1]" = var_mean_31[1];  var_mean_31 = None
    add_176: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_194, 1e-12);  getitem_194 = None
    rsqrt_31: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_176);  add_176 = None
    sub_47: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_175, getitem_195)
    mul_130: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_47, rsqrt_31);  sub_47 = None
    mul_131: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_130, primals_296);  mul_130 = None
    add_177: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_131, primals_297);  mul_131 = primals_297 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_403: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_177, 3)
    unsqueeze_404: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_403, 4);  unsqueeze_403 = None
    permute_675: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_404, [0, 1, 3, 4, 2]);  unsqueeze_404 = None
    unsqueeze_405: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_113, 3);  primals_113 = None
    unsqueeze_406: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_405, 4);  unsqueeze_405 = None
    permute_676: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_406, [3, 4, 1, 2, 0]);  unsqueeze_406 = None
    permute_677: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_675, [0, 4, 1, 2, 3]);  permute_675 = None
    view_608: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_677, [1, 512, 1024]);  permute_677 = None
    permute_678: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_676, [4, 1, 2, 3, 0]);  permute_676 = None
    view_609: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_678, [1, 1024, 1024]);  permute_678 = None
    bmm_128: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_608, view_609)
    view_610: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_128, [512, 1, 1, 16, 64]);  bmm_128 = None
    permute_679: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_610, [0, 2, 3, 4, 1]);  view_610 = None
    view_611: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_679, [512, 1, 16, 64]);  permute_679 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_407: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_177, 3)
    unsqueeze_408: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_407, 4);  unsqueeze_407 = None
    permute_680: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_408, [0, 1, 3, 4, 2]);  unsqueeze_408 = None
    unsqueeze_409: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_114, 3);  primals_114 = None
    unsqueeze_410: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_409, 4);  unsqueeze_409 = None
    permute_681: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_410, [3, 4, 1, 2, 0]);  unsqueeze_410 = None
    permute_682: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_680, [0, 4, 1, 2, 3]);  permute_680 = None
    view_612: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_682, [1, 512, 1024]);  permute_682 = None
    permute_683: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_681, [4, 1, 2, 3, 0]);  permute_681 = None
    view_613: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_683, [1, 1024, 1024]);  permute_683 = None
    bmm_129: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_612, view_613)
    view_614: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_129, [512, 1, 1, 16, 64]);  bmm_129 = None
    permute_684: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_614, [0, 2, 3, 4, 1]);  view_614 = None
    view_615: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_684, [512, 1, 16, 64]);  permute_684 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_411: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_177, 3)
    unsqueeze_412: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_411, 4);  unsqueeze_411 = None
    permute_685: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_412, [0, 1, 3, 4, 2]);  unsqueeze_412 = None
    unsqueeze_413: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_115, 3);  primals_115 = None
    unsqueeze_414: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_413, 4);  unsqueeze_413 = None
    permute_686: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_414, [3, 4, 1, 2, 0]);  unsqueeze_414 = None
    permute_687: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_685, [0, 4, 1, 2, 3]);  permute_685 = None
    view_616: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_687, [1, 512, 1024]);  permute_687 = None
    permute_688: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_686, [4, 1, 2, 3, 0]);  permute_686 = None
    view_617: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_688, [1, 1024, 1024]);  permute_688 = None
    bmm_130: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_616, view_617)
    view_618: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_130, [512, 1, 1, 16, 64]);  bmm_130 = None
    permute_689: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_618, [0, 2, 3, 4, 1]);  view_618 = None
    view_619: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_689, [512, 1, 16, 64]);  permute_689 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_415: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_416: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_415, 4);  unsqueeze_415 = None
    permute_690: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_416, [0, 1, 3, 4, 2]);  unsqueeze_416 = None
    unsqueeze_417: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_116, 3);  primals_116 = None
    unsqueeze_418: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_417, 4);  unsqueeze_417 = None
    permute_691: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_418, [3, 4, 1, 2, 0]);  unsqueeze_418 = None
    permute_692: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_690, [0, 4, 1, 2, 3]);  permute_690 = None
    view_620: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_692, [1, 1024, 1024]);  permute_692 = None
    permute_693: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_691, [4, 1, 2, 3, 0]);  permute_691 = None
    view_621: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_693, [1, 1024, 1024]);  permute_693 = None
    bmm_131: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_620, view_621);  view_621 = None
    view_622: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_131, [1024, 1, 1, 16, 64]);  bmm_131 = None
    permute_694: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_622, [0, 2, 3, 4, 1]);  view_622 = None
    view_623: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_694, [1024, 1, 16, 64]);  permute_694 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_178: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_611, primals_117);  primals_117 = None
    unsqueeze_419: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_178, 4);  add_178 = None
    permute_695: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_419, [1, 2, 0, 4, 3]);  unsqueeze_419 = None
    unsqueeze_420: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_615, 4);  view_615 = None
    permute_696: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_420, [1, 2, 4, 0, 3]);  unsqueeze_420 = None
    permute_697: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_695, [1, 2, 4, 0, 3]);  permute_695 = None
    view_624: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_697, [16, 512, 64]);  permute_697 = None
    permute_698: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_696, [1, 4, 0, 3, 2]);  permute_696 = None
    view_625: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_698, [16, 64, 512]);  permute_698 = None
    bmm_132: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_624, view_625)
    view_626: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_132, [16, 512, 1, 1, 512]);  bmm_132 = None
    permute_699: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_626, [3, 0, 1, 4, 2]);  view_626 = None
    view_627: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_699, [1, 16, 512, 512]);  permute_699 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_179: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_611, primals_118);  view_611 = primals_118 = None
    unsqueeze_421: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_179, 4);  add_179 = None
    permute_700: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_421, [1, 2, 0, 4, 3]);  unsqueeze_421 = None
    unsqueeze_422: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_623, 4);  view_623 = None
    permute_701: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_422, [1, 2, 4, 0, 3]);  unsqueeze_422 = None
    permute_702: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_700, [1, 2, 4, 0, 3]);  permute_700 = None
    view_628: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_702, [16, 512, 64]);  permute_702 = None
    permute_703: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_701, [1, 4, 0, 3, 2]);  permute_701 = None
    view_629: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_703, [16, 64, 1024]);  permute_703 = None
    bmm_133: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_628, view_629)
    view_630: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_133, [16, 512, 1, 1, 1024]);  bmm_133 = None
    permute_704: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_630, [3, 0, 1, 4, 2]);  view_630 = None
    view_631: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_704, [1, 16, 512, 1024]);  permute_704 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_632: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_631, [1, 16, 1024, 512]);  view_631 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_115: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_632, 0, 0, 9223372036854775807);  view_632 = None
    slice_116: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_115, 1, 0, 9223372036854775807);  slice_115 = None
    slice_117: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_116, 2, 1, 9223372036854775807);  slice_116 = None
    slice_118: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_117, 3, 0, 9223372036854775807);  slice_117 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_633: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_118, [1, 16, 512, 1023]);  slice_118 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_18: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_119: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_633, 0, 0, 9223372036854775807);  view_633 = None
    slice_120: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_119, 1, 0, 9223372036854775807);  slice_119 = None
    slice_121: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_120, 2, 0, 9223372036854775807);  slice_120 = None
    index_16: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_121, [None, None, None, iota_18]);  slice_121 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_180: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_627, index_16);  view_627 = index_16 = None
    add_181: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_180, 0);  add_180 = None
    mul_132: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_181, 0.125);  add_181 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_16: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_132, [3], True)
    sub_48: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_132, amax_16);  mul_132 = amax_16 = None
    exp_16: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_48);  sub_48 = None
    sum_17: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_16, [3], True)
    div_17: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_16, sum_17);  exp_16 = sum_17 = None
    alias_16: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_17)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_66 = torch.ops.aten.native_dropout.default(div_17, 0.1, True);  div_17 = None
    getitem_196: "f32[1, 16, 512, 512]" = native_dropout_66[0]
    getitem_197: "b8[1, 16, 512, 512]" = native_dropout_66[1];  native_dropout_66 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_423: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_196, 4);  getitem_196 = None
    permute_705: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_423, [2, 0, 1, 4, 3]);  unsqueeze_423 = None
    unsqueeze_424: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_619, 4);  view_619 = None
    permute_706: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_424, [4, 1, 2, 3, 0]);  unsqueeze_424 = None
    permute_707: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_705, [2, 0, 4, 1, 3]);  permute_705 = None
    view_634: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_707, [16, 512, 512]);  permute_707 = None
    permute_708: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_706, [2, 4, 1, 3, 0]);  permute_706 = None
    view_635: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_708, [16, 512, 64]);  permute_708 = None
    bmm_134: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_634, view_635)
    view_636: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_134, [16, 512, 1, 1, 64]);  bmm_134 = None
    permute_709: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_636, [1, 3, 0, 4, 2]);  view_636 = None
    view_637: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_709, [512, 1, 16, 64]);  permute_709 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_425: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_637, 4);  view_637 = None
    permute_710: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_425, [0, 1, 4, 3, 2]);  unsqueeze_425 = None
    unsqueeze_426: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_119, 3);  primals_119 = None
    unsqueeze_427: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_426, 4);  unsqueeze_426 = None
    permute_711: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_427, [3, 4, 0, 2, 1]);  unsqueeze_427 = None
    permute_712: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_710, [0, 3, 4, 1, 2]);  permute_710 = None
    clone_32: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_712, memory_format = torch.contiguous_format);  permute_712 = None
    view_638: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_32, [1, 512, 1024]);  clone_32 = None
    permute_713: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_711, [3, 4, 1, 2, 0]);  permute_711 = None
    clone_33: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_713, memory_format = torch.contiguous_format);  permute_713 = None
    view_639: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_33, [1, 1024, 1024]);  clone_33 = None
    bmm_135: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_638, view_639)
    view_640: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_135, [512, 1, 1, 1, 1024]);  bmm_135 = None
    permute_714: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_640, [0, 3, 4, 1, 2]);  view_640 = None
    view_641: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_714, [512, 1, 1024]);  permute_714 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_67 = torch.ops.aten.native_dropout.default(view_641, 0.1, True);  view_641 = None
    getitem_198: "f32[512, 1, 1024]" = native_dropout_67[0]
    getitem_199: "b8[512, 1, 1024]" = native_dropout_67[1];  native_dropout_67 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_182: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_198, add_177);  getitem_198 = add_177 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_32 = torch.ops.aten.var_mean.correction(add_182, [2], correction = 0, keepdim = True)
    getitem_200: "f32[512, 1, 1]" = var_mean_32[0]
    getitem_201: "f32[512, 1, 1]" = var_mean_32[1];  var_mean_32 = None
    add_183: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_200, 1e-12);  getitem_200 = None
    rsqrt_32: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_183);  add_183 = None
    sub_49: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_182, getitem_201)
    mul_133: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_49, rsqrt_32);  sub_49 = None
    mul_134: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_133, primals_298);  mul_133 = None
    add_184: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_134, primals_299);  mul_134 = primals_299 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_642: "f32[512, 1024]" = torch.ops.aten.view.default(add_184, [512, 1024])
    permute_715: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_300, [1, 0]);  primals_300 = None
    addmm_32: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_301, view_642, permute_715);  primals_301 = None
    view_643: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_32, [512, 1, 4096]);  addmm_32 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_135: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_643, 0.5)
    mul_136: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_643, 0.7071067811865476)
    erf_16: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_136);  mul_136 = None
    add_185: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_16, 1);  erf_16 = None
    mul_137: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_135, add_185);  mul_135 = add_185 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_68 = torch.ops.aten.native_dropout.default(mul_137, 0.1, True);  mul_137 = None
    getitem_202: "f32[512, 1, 4096]" = native_dropout_68[0]
    getitem_203: "b8[512, 1, 4096]" = native_dropout_68[1];  native_dropout_68 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_644: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_202, [512, 4096]);  getitem_202 = None
    permute_716: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_302, [1, 0]);  primals_302 = None
    addmm_33: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_303, view_644, permute_716);  primals_303 = None
    view_645: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_33, [512, 1, 1024]);  addmm_33 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_69 = torch.ops.aten.native_dropout.default(view_645, 0.1, True);  view_645 = None
    getitem_204: "f32[512, 1, 1024]" = native_dropout_69[0]
    getitem_205: "b8[512, 1, 1024]" = native_dropout_69[1];  native_dropout_69 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_186: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_204, add_184);  getitem_204 = add_184 = None
    var_mean_33 = torch.ops.aten.var_mean.correction(add_186, [2], correction = 0, keepdim = True)
    getitem_206: "f32[512, 1, 1]" = var_mean_33[0]
    getitem_207: "f32[512, 1, 1]" = var_mean_33[1];  var_mean_33 = None
    add_187: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_206, 1e-12);  getitem_206 = None
    rsqrt_33: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_187);  add_187 = None
    sub_50: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_186, getitem_207)
    mul_138: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_50, rsqrt_33);  sub_50 = None
    mul_139: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_138, primals_304);  mul_138 = None
    add_188: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_139, primals_305);  mul_139 = primals_305 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_428: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_188, 3)
    unsqueeze_429: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_428, 4);  unsqueeze_428 = None
    permute_717: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_429, [0, 1, 3, 4, 2]);  unsqueeze_429 = None
    unsqueeze_430: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_120, 3);  primals_120 = None
    unsqueeze_431: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_430, 4);  unsqueeze_430 = None
    permute_718: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_431, [3, 4, 1, 2, 0]);  unsqueeze_431 = None
    permute_719: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_717, [0, 4, 1, 2, 3]);  permute_717 = None
    view_646: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_719, [1, 512, 1024]);  permute_719 = None
    permute_720: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_718, [4, 1, 2, 3, 0]);  permute_718 = None
    view_647: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_720, [1, 1024, 1024]);  permute_720 = None
    bmm_136: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_646, view_647)
    view_648: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_136, [512, 1, 1, 16, 64]);  bmm_136 = None
    permute_721: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_648, [0, 2, 3, 4, 1]);  view_648 = None
    view_649: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_721, [512, 1, 16, 64]);  permute_721 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_432: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_188, 3)
    unsqueeze_433: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_432, 4);  unsqueeze_432 = None
    permute_722: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_433, [0, 1, 3, 4, 2]);  unsqueeze_433 = None
    unsqueeze_434: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_121, 3);  primals_121 = None
    unsqueeze_435: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_434, 4);  unsqueeze_434 = None
    permute_723: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_435, [3, 4, 1, 2, 0]);  unsqueeze_435 = None
    permute_724: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_722, [0, 4, 1, 2, 3]);  permute_722 = None
    view_650: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_724, [1, 512, 1024]);  permute_724 = None
    permute_725: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_723, [4, 1, 2, 3, 0]);  permute_723 = None
    view_651: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_725, [1, 1024, 1024]);  permute_725 = None
    bmm_137: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_650, view_651)
    view_652: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_137, [512, 1, 1, 16, 64]);  bmm_137 = None
    permute_726: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_652, [0, 2, 3, 4, 1]);  view_652 = None
    view_653: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_726, [512, 1, 16, 64]);  permute_726 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_436: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_188, 3)
    unsqueeze_437: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_436, 4);  unsqueeze_436 = None
    permute_727: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_437, [0, 1, 3, 4, 2]);  unsqueeze_437 = None
    unsqueeze_438: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_122, 3);  primals_122 = None
    unsqueeze_439: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_438, 4);  unsqueeze_438 = None
    permute_728: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_439, [3, 4, 1, 2, 0]);  unsqueeze_439 = None
    permute_729: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_727, [0, 4, 1, 2, 3]);  permute_727 = None
    view_654: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_729, [1, 512, 1024]);  permute_729 = None
    permute_730: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_728, [4, 1, 2, 3, 0]);  permute_728 = None
    view_655: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_730, [1, 1024, 1024]);  permute_730 = None
    bmm_138: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_654, view_655)
    view_656: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_138, [512, 1, 1, 16, 64]);  bmm_138 = None
    permute_731: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_656, [0, 2, 3, 4, 1]);  view_656 = None
    view_657: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_731, [512, 1, 16, 64]);  permute_731 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_440: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_441: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_440, 4);  unsqueeze_440 = None
    permute_732: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_441, [0, 1, 3, 4, 2]);  unsqueeze_441 = None
    unsqueeze_442: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_123, 3);  primals_123 = None
    unsqueeze_443: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_442, 4);  unsqueeze_442 = None
    permute_733: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_443, [3, 4, 1, 2, 0]);  unsqueeze_443 = None
    permute_734: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_732, [0, 4, 1, 2, 3]);  permute_732 = None
    view_658: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_734, [1, 1024, 1024]);  permute_734 = None
    permute_735: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_733, [4, 1, 2, 3, 0]);  permute_733 = None
    view_659: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_735, [1, 1024, 1024]);  permute_735 = None
    bmm_139: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_658, view_659);  view_659 = None
    view_660: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_139, [1024, 1, 1, 16, 64]);  bmm_139 = None
    permute_736: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_660, [0, 2, 3, 4, 1]);  view_660 = None
    view_661: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_736, [1024, 1, 16, 64]);  permute_736 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_189: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_649, primals_124);  primals_124 = None
    unsqueeze_444: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_189, 4);  add_189 = None
    permute_737: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_444, [1, 2, 0, 4, 3]);  unsqueeze_444 = None
    unsqueeze_445: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_653, 4);  view_653 = None
    permute_738: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_445, [1, 2, 4, 0, 3]);  unsqueeze_445 = None
    permute_739: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_737, [1, 2, 4, 0, 3]);  permute_737 = None
    view_662: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_739, [16, 512, 64]);  permute_739 = None
    permute_740: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_738, [1, 4, 0, 3, 2]);  permute_738 = None
    view_663: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_740, [16, 64, 512]);  permute_740 = None
    bmm_140: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_662, view_663)
    view_664: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_140, [16, 512, 1, 1, 512]);  bmm_140 = None
    permute_741: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_664, [3, 0, 1, 4, 2]);  view_664 = None
    view_665: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_741, [1, 16, 512, 512]);  permute_741 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_190: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_649, primals_125);  view_649 = primals_125 = None
    unsqueeze_446: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_190, 4);  add_190 = None
    permute_742: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_446, [1, 2, 0, 4, 3]);  unsqueeze_446 = None
    unsqueeze_447: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_661, 4);  view_661 = None
    permute_743: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_447, [1, 2, 4, 0, 3]);  unsqueeze_447 = None
    permute_744: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_742, [1, 2, 4, 0, 3]);  permute_742 = None
    view_666: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_744, [16, 512, 64]);  permute_744 = None
    permute_745: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_743, [1, 4, 0, 3, 2]);  permute_743 = None
    view_667: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_745, [16, 64, 1024]);  permute_745 = None
    bmm_141: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_666, view_667)
    view_668: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_141, [16, 512, 1, 1, 1024]);  bmm_141 = None
    permute_746: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_668, [3, 0, 1, 4, 2]);  view_668 = None
    view_669: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_746, [1, 16, 512, 1024]);  permute_746 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_670: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_669, [1, 16, 1024, 512]);  view_669 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_122: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_670, 0, 0, 9223372036854775807);  view_670 = None
    slice_123: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_122, 1, 0, 9223372036854775807);  slice_122 = None
    slice_124: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_123, 2, 1, 9223372036854775807);  slice_123 = None
    slice_125: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_124, 3, 0, 9223372036854775807);  slice_124 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_671: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_125, [1, 16, 512, 1023]);  slice_125 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_19: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_126: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_671, 0, 0, 9223372036854775807);  view_671 = None
    slice_127: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_126, 1, 0, 9223372036854775807);  slice_126 = None
    slice_128: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_127, 2, 0, 9223372036854775807);  slice_127 = None
    index_17: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_128, [None, None, None, iota_19]);  slice_128 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_191: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_665, index_17);  view_665 = index_17 = None
    add_192: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_191, 0);  add_191 = None
    mul_140: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_192, 0.125);  add_192 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_17: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_140, [3], True)
    sub_51: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_140, amax_17);  mul_140 = amax_17 = None
    exp_17: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_51);  sub_51 = None
    sum_18: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_17, [3], True)
    div_18: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_17, sum_18);  exp_17 = sum_18 = None
    alias_17: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_18)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_70 = torch.ops.aten.native_dropout.default(div_18, 0.1, True);  div_18 = None
    getitem_208: "f32[1, 16, 512, 512]" = native_dropout_70[0]
    getitem_209: "b8[1, 16, 512, 512]" = native_dropout_70[1];  native_dropout_70 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_448: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_208, 4);  getitem_208 = None
    permute_747: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_448, [2, 0, 1, 4, 3]);  unsqueeze_448 = None
    unsqueeze_449: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_657, 4);  view_657 = None
    permute_748: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_449, [4, 1, 2, 3, 0]);  unsqueeze_449 = None
    permute_749: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_747, [2, 0, 4, 1, 3]);  permute_747 = None
    view_672: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_749, [16, 512, 512]);  permute_749 = None
    permute_750: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_748, [2, 4, 1, 3, 0]);  permute_748 = None
    view_673: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_750, [16, 512, 64]);  permute_750 = None
    bmm_142: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_672, view_673)
    view_674: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_142, [16, 512, 1, 1, 64]);  bmm_142 = None
    permute_751: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_674, [1, 3, 0, 4, 2]);  view_674 = None
    view_675: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_751, [512, 1, 16, 64]);  permute_751 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_450: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_675, 4);  view_675 = None
    permute_752: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_450, [0, 1, 4, 3, 2]);  unsqueeze_450 = None
    unsqueeze_451: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_126, 3);  primals_126 = None
    unsqueeze_452: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_451, 4);  unsqueeze_451 = None
    permute_753: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_452, [3, 4, 0, 2, 1]);  unsqueeze_452 = None
    permute_754: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_752, [0, 3, 4, 1, 2]);  permute_752 = None
    clone_34: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_754, memory_format = torch.contiguous_format);  permute_754 = None
    view_676: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_34, [1, 512, 1024]);  clone_34 = None
    permute_755: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_753, [3, 4, 1, 2, 0]);  permute_753 = None
    clone_35: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_755, memory_format = torch.contiguous_format);  permute_755 = None
    view_677: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_35, [1, 1024, 1024]);  clone_35 = None
    bmm_143: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_676, view_677)
    view_678: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_143, [512, 1, 1, 1, 1024]);  bmm_143 = None
    permute_756: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_678, [0, 3, 4, 1, 2]);  view_678 = None
    view_679: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_756, [512, 1, 1024]);  permute_756 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_71 = torch.ops.aten.native_dropout.default(view_679, 0.1, True);  view_679 = None
    getitem_210: "f32[512, 1, 1024]" = native_dropout_71[0]
    getitem_211: "b8[512, 1, 1024]" = native_dropout_71[1];  native_dropout_71 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_193: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_210, add_188);  getitem_210 = add_188 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_34 = torch.ops.aten.var_mean.correction(add_193, [2], correction = 0, keepdim = True)
    getitem_212: "f32[512, 1, 1]" = var_mean_34[0]
    getitem_213: "f32[512, 1, 1]" = var_mean_34[1];  var_mean_34 = None
    add_194: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_212, 1e-12);  getitem_212 = None
    rsqrt_34: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_194);  add_194 = None
    sub_52: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_193, getitem_213)
    mul_141: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_52, rsqrt_34);  sub_52 = None
    mul_142: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_141, primals_306);  mul_141 = None
    add_195: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_142, primals_307);  mul_142 = primals_307 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_680: "f32[512, 1024]" = torch.ops.aten.view.default(add_195, [512, 1024])
    permute_757: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_308, [1, 0]);  primals_308 = None
    addmm_34: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_309, view_680, permute_757);  primals_309 = None
    view_681: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_34, [512, 1, 4096]);  addmm_34 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_143: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_681, 0.5)
    mul_144: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_681, 0.7071067811865476)
    erf_17: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_144);  mul_144 = None
    add_196: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_17, 1);  erf_17 = None
    mul_145: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_143, add_196);  mul_143 = add_196 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_72 = torch.ops.aten.native_dropout.default(mul_145, 0.1, True);  mul_145 = None
    getitem_214: "f32[512, 1, 4096]" = native_dropout_72[0]
    getitem_215: "b8[512, 1, 4096]" = native_dropout_72[1];  native_dropout_72 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_682: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_214, [512, 4096]);  getitem_214 = None
    permute_758: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_310, [1, 0]);  primals_310 = None
    addmm_35: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_311, view_682, permute_758);  primals_311 = None
    view_683: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_35, [512, 1, 1024]);  addmm_35 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_73 = torch.ops.aten.native_dropout.default(view_683, 0.1, True);  view_683 = None
    getitem_216: "f32[512, 1, 1024]" = native_dropout_73[0]
    getitem_217: "b8[512, 1, 1024]" = native_dropout_73[1];  native_dropout_73 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_197: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_216, add_195);  getitem_216 = add_195 = None
    var_mean_35 = torch.ops.aten.var_mean.correction(add_197, [2], correction = 0, keepdim = True)
    getitem_218: "f32[512, 1, 1]" = var_mean_35[0]
    getitem_219: "f32[512, 1, 1]" = var_mean_35[1];  var_mean_35 = None
    add_198: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_218, 1e-12);  getitem_218 = None
    rsqrt_35: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_198);  add_198 = None
    sub_53: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_197, getitem_219)
    mul_146: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_53, rsqrt_35);  sub_53 = None
    mul_147: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_146, primals_312);  mul_146 = None
    add_199: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_147, primals_313);  mul_147 = primals_313 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_453: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_199, 3)
    unsqueeze_454: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_453, 4);  unsqueeze_453 = None
    permute_759: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_454, [0, 1, 3, 4, 2]);  unsqueeze_454 = None
    unsqueeze_455: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_127, 3);  primals_127 = None
    unsqueeze_456: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_455, 4);  unsqueeze_455 = None
    permute_760: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_456, [3, 4, 1, 2, 0]);  unsqueeze_456 = None
    permute_761: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_759, [0, 4, 1, 2, 3]);  permute_759 = None
    view_684: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_761, [1, 512, 1024]);  permute_761 = None
    permute_762: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_760, [4, 1, 2, 3, 0]);  permute_760 = None
    view_685: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_762, [1, 1024, 1024]);  permute_762 = None
    bmm_144: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_684, view_685)
    view_686: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_144, [512, 1, 1, 16, 64]);  bmm_144 = None
    permute_763: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_686, [0, 2, 3, 4, 1]);  view_686 = None
    view_687: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_763, [512, 1, 16, 64]);  permute_763 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_457: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_199, 3)
    unsqueeze_458: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_457, 4);  unsqueeze_457 = None
    permute_764: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_458, [0, 1, 3, 4, 2]);  unsqueeze_458 = None
    unsqueeze_459: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_128, 3);  primals_128 = None
    unsqueeze_460: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_459, 4);  unsqueeze_459 = None
    permute_765: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_460, [3, 4, 1, 2, 0]);  unsqueeze_460 = None
    permute_766: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_764, [0, 4, 1, 2, 3]);  permute_764 = None
    view_688: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_766, [1, 512, 1024]);  permute_766 = None
    permute_767: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_765, [4, 1, 2, 3, 0]);  permute_765 = None
    view_689: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_767, [1, 1024, 1024]);  permute_767 = None
    bmm_145: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_688, view_689)
    view_690: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_145, [512, 1, 1, 16, 64]);  bmm_145 = None
    permute_768: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_690, [0, 2, 3, 4, 1]);  view_690 = None
    view_691: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_768, [512, 1, 16, 64]);  permute_768 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_461: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_199, 3)
    unsqueeze_462: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_461, 4);  unsqueeze_461 = None
    permute_769: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_462, [0, 1, 3, 4, 2]);  unsqueeze_462 = None
    unsqueeze_463: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_129, 3);  primals_129 = None
    unsqueeze_464: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_463, 4);  unsqueeze_463 = None
    permute_770: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_464, [3, 4, 1, 2, 0]);  unsqueeze_464 = None
    permute_771: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_769, [0, 4, 1, 2, 3]);  permute_769 = None
    view_692: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_771, [1, 512, 1024]);  permute_771 = None
    permute_772: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_770, [4, 1, 2, 3, 0]);  permute_770 = None
    view_693: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_772, [1, 1024, 1024]);  permute_772 = None
    bmm_146: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_692, view_693)
    view_694: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_146, [512, 1, 1, 16, 64]);  bmm_146 = None
    permute_773: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_694, [0, 2, 3, 4, 1]);  view_694 = None
    view_695: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_773, [512, 1, 16, 64]);  permute_773 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_465: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_466: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_465, 4);  unsqueeze_465 = None
    permute_774: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_466, [0, 1, 3, 4, 2]);  unsqueeze_466 = None
    unsqueeze_467: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_130, 3);  primals_130 = None
    unsqueeze_468: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_467, 4);  unsqueeze_467 = None
    permute_775: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_468, [3, 4, 1, 2, 0]);  unsqueeze_468 = None
    permute_776: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_774, [0, 4, 1, 2, 3]);  permute_774 = None
    view_696: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_776, [1, 1024, 1024]);  permute_776 = None
    permute_777: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_775, [4, 1, 2, 3, 0]);  permute_775 = None
    view_697: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_777, [1, 1024, 1024]);  permute_777 = None
    bmm_147: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_696, view_697);  view_697 = None
    view_698: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_147, [1024, 1, 1, 16, 64]);  bmm_147 = None
    permute_778: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_698, [0, 2, 3, 4, 1]);  view_698 = None
    view_699: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_778, [1024, 1, 16, 64]);  permute_778 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_200: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_687, primals_131);  primals_131 = None
    unsqueeze_469: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_200, 4);  add_200 = None
    permute_779: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_469, [1, 2, 0, 4, 3]);  unsqueeze_469 = None
    unsqueeze_470: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_691, 4);  view_691 = None
    permute_780: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_470, [1, 2, 4, 0, 3]);  unsqueeze_470 = None
    permute_781: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_779, [1, 2, 4, 0, 3]);  permute_779 = None
    view_700: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_781, [16, 512, 64]);  permute_781 = None
    permute_782: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_780, [1, 4, 0, 3, 2]);  permute_780 = None
    view_701: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_782, [16, 64, 512]);  permute_782 = None
    bmm_148: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_700, view_701)
    view_702: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_148, [16, 512, 1, 1, 512]);  bmm_148 = None
    permute_783: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_702, [3, 0, 1, 4, 2]);  view_702 = None
    view_703: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_783, [1, 16, 512, 512]);  permute_783 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_201: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_687, primals_132);  view_687 = primals_132 = None
    unsqueeze_471: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_201, 4);  add_201 = None
    permute_784: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_471, [1, 2, 0, 4, 3]);  unsqueeze_471 = None
    unsqueeze_472: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_699, 4);  view_699 = None
    permute_785: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_472, [1, 2, 4, 0, 3]);  unsqueeze_472 = None
    permute_786: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_784, [1, 2, 4, 0, 3]);  permute_784 = None
    view_704: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_786, [16, 512, 64]);  permute_786 = None
    permute_787: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_785, [1, 4, 0, 3, 2]);  permute_785 = None
    view_705: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_787, [16, 64, 1024]);  permute_787 = None
    bmm_149: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_704, view_705)
    view_706: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_149, [16, 512, 1, 1, 1024]);  bmm_149 = None
    permute_788: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_706, [3, 0, 1, 4, 2]);  view_706 = None
    view_707: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_788, [1, 16, 512, 1024]);  permute_788 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_708: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_707, [1, 16, 1024, 512]);  view_707 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_129: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_708, 0, 0, 9223372036854775807);  view_708 = None
    slice_130: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_129, 1, 0, 9223372036854775807);  slice_129 = None
    slice_131: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_130, 2, 1, 9223372036854775807);  slice_130 = None
    slice_132: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_131, 3, 0, 9223372036854775807);  slice_131 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_709: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_132, [1, 16, 512, 1023]);  slice_132 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_20: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_133: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_709, 0, 0, 9223372036854775807);  view_709 = None
    slice_134: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_133, 1, 0, 9223372036854775807);  slice_133 = None
    slice_135: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_134, 2, 0, 9223372036854775807);  slice_134 = None
    index_18: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_135, [None, None, None, iota_20]);  slice_135 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_202: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_703, index_18);  view_703 = index_18 = None
    add_203: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_202, 0);  add_202 = None
    mul_148: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_203, 0.125);  add_203 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_18: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_148, [3], True)
    sub_54: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_148, amax_18);  mul_148 = amax_18 = None
    exp_18: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_54);  sub_54 = None
    sum_19: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_18, [3], True)
    div_19: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_18, sum_19);  exp_18 = sum_19 = None
    alias_18: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_19)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_74 = torch.ops.aten.native_dropout.default(div_19, 0.1, True);  div_19 = None
    getitem_220: "f32[1, 16, 512, 512]" = native_dropout_74[0]
    getitem_221: "b8[1, 16, 512, 512]" = native_dropout_74[1];  native_dropout_74 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_473: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_220, 4);  getitem_220 = None
    permute_789: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_473, [2, 0, 1, 4, 3]);  unsqueeze_473 = None
    unsqueeze_474: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_695, 4);  view_695 = None
    permute_790: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_474, [4, 1, 2, 3, 0]);  unsqueeze_474 = None
    permute_791: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_789, [2, 0, 4, 1, 3]);  permute_789 = None
    view_710: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_791, [16, 512, 512]);  permute_791 = None
    permute_792: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_790, [2, 4, 1, 3, 0]);  permute_790 = None
    view_711: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_792, [16, 512, 64]);  permute_792 = None
    bmm_150: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_710, view_711)
    view_712: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_150, [16, 512, 1, 1, 64]);  bmm_150 = None
    permute_793: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_712, [1, 3, 0, 4, 2]);  view_712 = None
    view_713: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_793, [512, 1, 16, 64]);  permute_793 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_475: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_713, 4);  view_713 = None
    permute_794: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_475, [0, 1, 4, 3, 2]);  unsqueeze_475 = None
    unsqueeze_476: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_133, 3);  primals_133 = None
    unsqueeze_477: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_476, 4);  unsqueeze_476 = None
    permute_795: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_477, [3, 4, 0, 2, 1]);  unsqueeze_477 = None
    permute_796: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_794, [0, 3, 4, 1, 2]);  permute_794 = None
    clone_36: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_796, memory_format = torch.contiguous_format);  permute_796 = None
    view_714: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_36, [1, 512, 1024]);  clone_36 = None
    permute_797: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_795, [3, 4, 1, 2, 0]);  permute_795 = None
    clone_37: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_797, memory_format = torch.contiguous_format);  permute_797 = None
    view_715: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_37, [1, 1024, 1024]);  clone_37 = None
    bmm_151: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_714, view_715)
    view_716: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_151, [512, 1, 1, 1, 1024]);  bmm_151 = None
    permute_798: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_716, [0, 3, 4, 1, 2]);  view_716 = None
    view_717: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_798, [512, 1, 1024]);  permute_798 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_75 = torch.ops.aten.native_dropout.default(view_717, 0.1, True);  view_717 = None
    getitem_222: "f32[512, 1, 1024]" = native_dropout_75[0]
    getitem_223: "b8[512, 1, 1024]" = native_dropout_75[1];  native_dropout_75 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_204: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_222, add_199);  getitem_222 = add_199 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_36 = torch.ops.aten.var_mean.correction(add_204, [2], correction = 0, keepdim = True)
    getitem_224: "f32[512, 1, 1]" = var_mean_36[0]
    getitem_225: "f32[512, 1, 1]" = var_mean_36[1];  var_mean_36 = None
    add_205: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_224, 1e-12);  getitem_224 = None
    rsqrt_36: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_205);  add_205 = None
    sub_55: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_204, getitem_225)
    mul_149: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_55, rsqrt_36);  sub_55 = None
    mul_150: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_149, primals_314);  mul_149 = None
    add_206: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_150, primals_315);  mul_150 = primals_315 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_718: "f32[512, 1024]" = torch.ops.aten.view.default(add_206, [512, 1024])
    permute_799: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_316, [1, 0]);  primals_316 = None
    addmm_36: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_317, view_718, permute_799);  primals_317 = None
    view_719: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_36, [512, 1, 4096]);  addmm_36 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_151: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_719, 0.5)
    mul_152: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_719, 0.7071067811865476)
    erf_18: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_152);  mul_152 = None
    add_207: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_18, 1);  erf_18 = None
    mul_153: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_151, add_207);  mul_151 = add_207 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_76 = torch.ops.aten.native_dropout.default(mul_153, 0.1, True);  mul_153 = None
    getitem_226: "f32[512, 1, 4096]" = native_dropout_76[0]
    getitem_227: "b8[512, 1, 4096]" = native_dropout_76[1];  native_dropout_76 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_720: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_226, [512, 4096]);  getitem_226 = None
    permute_800: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_318, [1, 0]);  primals_318 = None
    addmm_37: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_319, view_720, permute_800);  primals_319 = None
    view_721: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_37, [512, 1, 1024]);  addmm_37 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_77 = torch.ops.aten.native_dropout.default(view_721, 0.1, True);  view_721 = None
    getitem_228: "f32[512, 1, 1024]" = native_dropout_77[0]
    getitem_229: "b8[512, 1, 1024]" = native_dropout_77[1];  native_dropout_77 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_208: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_228, add_206);  getitem_228 = add_206 = None
    var_mean_37 = torch.ops.aten.var_mean.correction(add_208, [2], correction = 0, keepdim = True)
    getitem_230: "f32[512, 1, 1]" = var_mean_37[0]
    getitem_231: "f32[512, 1, 1]" = var_mean_37[1];  var_mean_37 = None
    add_209: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_230, 1e-12);  getitem_230 = None
    rsqrt_37: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_209);  add_209 = None
    sub_56: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_208, getitem_231)
    mul_154: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_56, rsqrt_37);  sub_56 = None
    mul_155: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_154, primals_320);  mul_154 = None
    add_210: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_155, primals_321);  mul_155 = primals_321 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_478: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_210, 3)
    unsqueeze_479: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_478, 4);  unsqueeze_478 = None
    permute_801: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_479, [0, 1, 3, 4, 2]);  unsqueeze_479 = None
    unsqueeze_480: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_134, 3);  primals_134 = None
    unsqueeze_481: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_480, 4);  unsqueeze_480 = None
    permute_802: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_481, [3, 4, 1, 2, 0]);  unsqueeze_481 = None
    permute_803: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_801, [0, 4, 1, 2, 3]);  permute_801 = None
    view_722: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_803, [1, 512, 1024]);  permute_803 = None
    permute_804: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_802, [4, 1, 2, 3, 0]);  permute_802 = None
    view_723: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_804, [1, 1024, 1024]);  permute_804 = None
    bmm_152: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_722, view_723)
    view_724: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_152, [512, 1, 1, 16, 64]);  bmm_152 = None
    permute_805: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_724, [0, 2, 3, 4, 1]);  view_724 = None
    view_725: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_805, [512, 1, 16, 64]);  permute_805 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_482: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_210, 3)
    unsqueeze_483: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_482, 4);  unsqueeze_482 = None
    permute_806: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_483, [0, 1, 3, 4, 2]);  unsqueeze_483 = None
    unsqueeze_484: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_135, 3);  primals_135 = None
    unsqueeze_485: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_484, 4);  unsqueeze_484 = None
    permute_807: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_485, [3, 4, 1, 2, 0]);  unsqueeze_485 = None
    permute_808: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_806, [0, 4, 1, 2, 3]);  permute_806 = None
    view_726: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_808, [1, 512, 1024]);  permute_808 = None
    permute_809: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_807, [4, 1, 2, 3, 0]);  permute_807 = None
    view_727: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_809, [1, 1024, 1024]);  permute_809 = None
    bmm_153: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_726, view_727)
    view_728: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_153, [512, 1, 1, 16, 64]);  bmm_153 = None
    permute_810: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_728, [0, 2, 3, 4, 1]);  view_728 = None
    view_729: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_810, [512, 1, 16, 64]);  permute_810 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_486: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_210, 3)
    unsqueeze_487: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_486, 4);  unsqueeze_486 = None
    permute_811: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_487, [0, 1, 3, 4, 2]);  unsqueeze_487 = None
    unsqueeze_488: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_136, 3);  primals_136 = None
    unsqueeze_489: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_488, 4);  unsqueeze_488 = None
    permute_812: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_489, [3, 4, 1, 2, 0]);  unsqueeze_489 = None
    permute_813: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_811, [0, 4, 1, 2, 3]);  permute_811 = None
    view_730: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_813, [1, 512, 1024]);  permute_813 = None
    permute_814: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_812, [4, 1, 2, 3, 0]);  permute_812 = None
    view_731: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_814, [1, 1024, 1024]);  permute_814 = None
    bmm_154: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_730, view_731)
    view_732: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_154, [512, 1, 1, 16, 64]);  bmm_154 = None
    permute_815: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_732, [0, 2, 3, 4, 1]);  view_732 = None
    view_733: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_815, [512, 1, 16, 64]);  permute_815 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_490: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_491: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_490, 4);  unsqueeze_490 = None
    permute_816: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_491, [0, 1, 3, 4, 2]);  unsqueeze_491 = None
    unsqueeze_492: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_137, 3);  primals_137 = None
    unsqueeze_493: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_492, 4);  unsqueeze_492 = None
    permute_817: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_493, [3, 4, 1, 2, 0]);  unsqueeze_493 = None
    permute_818: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_816, [0, 4, 1, 2, 3]);  permute_816 = None
    view_734: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_818, [1, 1024, 1024]);  permute_818 = None
    permute_819: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_817, [4, 1, 2, 3, 0]);  permute_817 = None
    view_735: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_819, [1, 1024, 1024]);  permute_819 = None
    bmm_155: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_734, view_735);  view_735 = None
    view_736: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_155, [1024, 1, 1, 16, 64]);  bmm_155 = None
    permute_820: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_736, [0, 2, 3, 4, 1]);  view_736 = None
    view_737: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_820, [1024, 1, 16, 64]);  permute_820 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_211: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_725, primals_138);  primals_138 = None
    unsqueeze_494: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_211, 4);  add_211 = None
    permute_821: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_494, [1, 2, 0, 4, 3]);  unsqueeze_494 = None
    unsqueeze_495: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_729, 4);  view_729 = None
    permute_822: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_495, [1, 2, 4, 0, 3]);  unsqueeze_495 = None
    permute_823: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_821, [1, 2, 4, 0, 3]);  permute_821 = None
    view_738: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_823, [16, 512, 64]);  permute_823 = None
    permute_824: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_822, [1, 4, 0, 3, 2]);  permute_822 = None
    view_739: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_824, [16, 64, 512]);  permute_824 = None
    bmm_156: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_738, view_739)
    view_740: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_156, [16, 512, 1, 1, 512]);  bmm_156 = None
    permute_825: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_740, [3, 0, 1, 4, 2]);  view_740 = None
    view_741: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_825, [1, 16, 512, 512]);  permute_825 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_212: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_725, primals_139);  view_725 = primals_139 = None
    unsqueeze_496: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_212, 4);  add_212 = None
    permute_826: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_496, [1, 2, 0, 4, 3]);  unsqueeze_496 = None
    unsqueeze_497: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_737, 4);  view_737 = None
    permute_827: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_497, [1, 2, 4, 0, 3]);  unsqueeze_497 = None
    permute_828: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_826, [1, 2, 4, 0, 3]);  permute_826 = None
    view_742: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_828, [16, 512, 64]);  permute_828 = None
    permute_829: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_827, [1, 4, 0, 3, 2]);  permute_827 = None
    view_743: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_829, [16, 64, 1024]);  permute_829 = None
    bmm_157: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_742, view_743)
    view_744: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_157, [16, 512, 1, 1, 1024]);  bmm_157 = None
    permute_830: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_744, [3, 0, 1, 4, 2]);  view_744 = None
    view_745: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_830, [1, 16, 512, 1024]);  permute_830 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_746: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_745, [1, 16, 1024, 512]);  view_745 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_136: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_746, 0, 0, 9223372036854775807);  view_746 = None
    slice_137: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_136, 1, 0, 9223372036854775807);  slice_136 = None
    slice_138: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_137, 2, 1, 9223372036854775807);  slice_137 = None
    slice_139: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_138, 3, 0, 9223372036854775807);  slice_138 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_747: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_139, [1, 16, 512, 1023]);  slice_139 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_21: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_140: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_747, 0, 0, 9223372036854775807);  view_747 = None
    slice_141: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_140, 1, 0, 9223372036854775807);  slice_140 = None
    slice_142: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_141, 2, 0, 9223372036854775807);  slice_141 = None
    index_19: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_142, [None, None, None, iota_21]);  slice_142 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_213: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_741, index_19);  view_741 = index_19 = None
    add_214: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_213, 0);  add_213 = None
    mul_156: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_214, 0.125);  add_214 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_19: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_156, [3], True)
    sub_57: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_156, amax_19);  mul_156 = amax_19 = None
    exp_19: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_57);  sub_57 = None
    sum_20: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_19, [3], True)
    div_20: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_19, sum_20);  exp_19 = sum_20 = None
    alias_19: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_20)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_78 = torch.ops.aten.native_dropout.default(div_20, 0.1, True);  div_20 = None
    getitem_232: "f32[1, 16, 512, 512]" = native_dropout_78[0]
    getitem_233: "b8[1, 16, 512, 512]" = native_dropout_78[1];  native_dropout_78 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_498: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_232, 4);  getitem_232 = None
    permute_831: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_498, [2, 0, 1, 4, 3]);  unsqueeze_498 = None
    unsqueeze_499: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_733, 4);  view_733 = None
    permute_832: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_499, [4, 1, 2, 3, 0]);  unsqueeze_499 = None
    permute_833: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_831, [2, 0, 4, 1, 3]);  permute_831 = None
    view_748: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_833, [16, 512, 512]);  permute_833 = None
    permute_834: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_832, [2, 4, 1, 3, 0]);  permute_832 = None
    view_749: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_834, [16, 512, 64]);  permute_834 = None
    bmm_158: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_748, view_749)
    view_750: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_158, [16, 512, 1, 1, 64]);  bmm_158 = None
    permute_835: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_750, [1, 3, 0, 4, 2]);  view_750 = None
    view_751: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_835, [512, 1, 16, 64]);  permute_835 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_500: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_751, 4);  view_751 = None
    permute_836: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_500, [0, 1, 4, 3, 2]);  unsqueeze_500 = None
    unsqueeze_501: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_140, 3);  primals_140 = None
    unsqueeze_502: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_501, 4);  unsqueeze_501 = None
    permute_837: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_502, [3, 4, 0, 2, 1]);  unsqueeze_502 = None
    permute_838: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_836, [0, 3, 4, 1, 2]);  permute_836 = None
    clone_38: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_838, memory_format = torch.contiguous_format);  permute_838 = None
    view_752: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_38, [1, 512, 1024]);  clone_38 = None
    permute_839: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_837, [3, 4, 1, 2, 0]);  permute_837 = None
    clone_39: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_839, memory_format = torch.contiguous_format);  permute_839 = None
    view_753: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_39, [1, 1024, 1024]);  clone_39 = None
    bmm_159: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_752, view_753)
    view_754: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_159, [512, 1, 1, 1, 1024]);  bmm_159 = None
    permute_840: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_754, [0, 3, 4, 1, 2]);  view_754 = None
    view_755: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_840, [512, 1, 1024]);  permute_840 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_79 = torch.ops.aten.native_dropout.default(view_755, 0.1, True);  view_755 = None
    getitem_234: "f32[512, 1, 1024]" = native_dropout_79[0]
    getitem_235: "b8[512, 1, 1024]" = native_dropout_79[1];  native_dropout_79 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_215: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_234, add_210);  getitem_234 = add_210 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_38 = torch.ops.aten.var_mean.correction(add_215, [2], correction = 0, keepdim = True)
    getitem_236: "f32[512, 1, 1]" = var_mean_38[0]
    getitem_237: "f32[512, 1, 1]" = var_mean_38[1];  var_mean_38 = None
    add_216: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_236, 1e-12);  getitem_236 = None
    rsqrt_38: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_216);  add_216 = None
    sub_58: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_215, getitem_237)
    mul_157: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_58, rsqrt_38);  sub_58 = None
    mul_158: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_157, primals_322);  mul_157 = None
    add_217: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_158, primals_323);  mul_158 = primals_323 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_756: "f32[512, 1024]" = torch.ops.aten.view.default(add_217, [512, 1024])
    permute_841: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_324, [1, 0]);  primals_324 = None
    addmm_38: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_325, view_756, permute_841);  primals_325 = None
    view_757: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_38, [512, 1, 4096]);  addmm_38 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_159: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_757, 0.5)
    mul_160: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_757, 0.7071067811865476)
    erf_19: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_160);  mul_160 = None
    add_218: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_19, 1);  erf_19 = None
    mul_161: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_159, add_218);  mul_159 = add_218 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_80 = torch.ops.aten.native_dropout.default(mul_161, 0.1, True);  mul_161 = None
    getitem_238: "f32[512, 1, 4096]" = native_dropout_80[0]
    getitem_239: "b8[512, 1, 4096]" = native_dropout_80[1];  native_dropout_80 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_758: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_238, [512, 4096]);  getitem_238 = None
    permute_842: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_326, [1, 0]);  primals_326 = None
    addmm_39: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_327, view_758, permute_842);  primals_327 = None
    view_759: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_39, [512, 1, 1024]);  addmm_39 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_81 = torch.ops.aten.native_dropout.default(view_759, 0.1, True);  view_759 = None
    getitem_240: "f32[512, 1, 1024]" = native_dropout_81[0]
    getitem_241: "b8[512, 1, 1024]" = native_dropout_81[1];  native_dropout_81 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_219: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_240, add_217);  getitem_240 = add_217 = None
    var_mean_39 = torch.ops.aten.var_mean.correction(add_219, [2], correction = 0, keepdim = True)
    getitem_242: "f32[512, 1, 1]" = var_mean_39[0]
    getitem_243: "f32[512, 1, 1]" = var_mean_39[1];  var_mean_39 = None
    add_220: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_242, 1e-12);  getitem_242 = None
    rsqrt_39: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_220);  add_220 = None
    sub_59: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_219, getitem_243)
    mul_162: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_59, rsqrt_39);  sub_59 = None
    mul_163: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_162, primals_328);  mul_162 = None
    add_221: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_163, primals_329);  mul_163 = primals_329 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_503: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_221, 3)
    unsqueeze_504: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_503, 4);  unsqueeze_503 = None
    permute_843: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_504, [0, 1, 3, 4, 2]);  unsqueeze_504 = None
    unsqueeze_505: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_141, 3);  primals_141 = None
    unsqueeze_506: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_505, 4);  unsqueeze_505 = None
    permute_844: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_506, [3, 4, 1, 2, 0]);  unsqueeze_506 = None
    permute_845: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_843, [0, 4, 1, 2, 3]);  permute_843 = None
    view_760: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_845, [1, 512, 1024]);  permute_845 = None
    permute_846: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_844, [4, 1, 2, 3, 0]);  permute_844 = None
    view_761: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_846, [1, 1024, 1024]);  permute_846 = None
    bmm_160: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_760, view_761)
    view_762: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_160, [512, 1, 1, 16, 64]);  bmm_160 = None
    permute_847: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_762, [0, 2, 3, 4, 1]);  view_762 = None
    view_763: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_847, [512, 1, 16, 64]);  permute_847 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_507: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_221, 3)
    unsqueeze_508: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_507, 4);  unsqueeze_507 = None
    permute_848: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_508, [0, 1, 3, 4, 2]);  unsqueeze_508 = None
    unsqueeze_509: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_142, 3);  primals_142 = None
    unsqueeze_510: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_509, 4);  unsqueeze_509 = None
    permute_849: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_510, [3, 4, 1, 2, 0]);  unsqueeze_510 = None
    permute_850: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_848, [0, 4, 1, 2, 3]);  permute_848 = None
    view_764: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_850, [1, 512, 1024]);  permute_850 = None
    permute_851: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_849, [4, 1, 2, 3, 0]);  permute_849 = None
    view_765: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_851, [1, 1024, 1024]);  permute_851 = None
    bmm_161: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_764, view_765)
    view_766: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_161, [512, 1, 1, 16, 64]);  bmm_161 = None
    permute_852: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_766, [0, 2, 3, 4, 1]);  view_766 = None
    view_767: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_852, [512, 1, 16, 64]);  permute_852 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_511: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_221, 3)
    unsqueeze_512: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_511, 4);  unsqueeze_511 = None
    permute_853: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_512, [0, 1, 3, 4, 2]);  unsqueeze_512 = None
    unsqueeze_513: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_143, 3);  primals_143 = None
    unsqueeze_514: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_513, 4);  unsqueeze_513 = None
    permute_854: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_514, [3, 4, 1, 2, 0]);  unsqueeze_514 = None
    permute_855: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_853, [0, 4, 1, 2, 3]);  permute_853 = None
    view_768: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_855, [1, 512, 1024]);  permute_855 = None
    permute_856: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_854, [4, 1, 2, 3, 0]);  permute_854 = None
    view_769: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_856, [1, 1024, 1024]);  permute_856 = None
    bmm_162: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_768, view_769)
    view_770: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_162, [512, 1, 1, 16, 64]);  bmm_162 = None
    permute_857: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_770, [0, 2, 3, 4, 1]);  view_770 = None
    view_771: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_857, [512, 1, 16, 64]);  permute_857 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_515: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_516: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_515, 4);  unsqueeze_515 = None
    permute_858: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_516, [0, 1, 3, 4, 2]);  unsqueeze_516 = None
    unsqueeze_517: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_144, 3);  primals_144 = None
    unsqueeze_518: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_517, 4);  unsqueeze_517 = None
    permute_859: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_518, [3, 4, 1, 2, 0]);  unsqueeze_518 = None
    permute_860: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_858, [0, 4, 1, 2, 3]);  permute_858 = None
    view_772: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_860, [1, 1024, 1024]);  permute_860 = None
    permute_861: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_859, [4, 1, 2, 3, 0]);  permute_859 = None
    view_773: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_861, [1, 1024, 1024]);  permute_861 = None
    bmm_163: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_772, view_773);  view_773 = None
    view_774: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_163, [1024, 1, 1, 16, 64]);  bmm_163 = None
    permute_862: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_774, [0, 2, 3, 4, 1]);  view_774 = None
    view_775: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_862, [1024, 1, 16, 64]);  permute_862 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_222: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_763, primals_145);  primals_145 = None
    unsqueeze_519: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_222, 4);  add_222 = None
    permute_863: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_519, [1, 2, 0, 4, 3]);  unsqueeze_519 = None
    unsqueeze_520: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_767, 4);  view_767 = None
    permute_864: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_520, [1, 2, 4, 0, 3]);  unsqueeze_520 = None
    permute_865: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_863, [1, 2, 4, 0, 3]);  permute_863 = None
    view_776: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_865, [16, 512, 64]);  permute_865 = None
    permute_866: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_864, [1, 4, 0, 3, 2]);  permute_864 = None
    view_777: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_866, [16, 64, 512]);  permute_866 = None
    bmm_164: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_776, view_777)
    view_778: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_164, [16, 512, 1, 1, 512]);  bmm_164 = None
    permute_867: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_778, [3, 0, 1, 4, 2]);  view_778 = None
    view_779: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_867, [1, 16, 512, 512]);  permute_867 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_223: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_763, primals_146);  view_763 = primals_146 = None
    unsqueeze_521: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_223, 4);  add_223 = None
    permute_868: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_521, [1, 2, 0, 4, 3]);  unsqueeze_521 = None
    unsqueeze_522: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_775, 4);  view_775 = None
    permute_869: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_522, [1, 2, 4, 0, 3]);  unsqueeze_522 = None
    permute_870: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_868, [1, 2, 4, 0, 3]);  permute_868 = None
    view_780: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_870, [16, 512, 64]);  permute_870 = None
    permute_871: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_869, [1, 4, 0, 3, 2]);  permute_869 = None
    view_781: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_871, [16, 64, 1024]);  permute_871 = None
    bmm_165: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_780, view_781)
    view_782: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_165, [16, 512, 1, 1, 1024]);  bmm_165 = None
    permute_872: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_782, [3, 0, 1, 4, 2]);  view_782 = None
    view_783: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_872, [1, 16, 512, 1024]);  permute_872 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_784: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_783, [1, 16, 1024, 512]);  view_783 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_143: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_784, 0, 0, 9223372036854775807);  view_784 = None
    slice_144: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_143, 1, 0, 9223372036854775807);  slice_143 = None
    slice_145: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_144, 2, 1, 9223372036854775807);  slice_144 = None
    slice_146: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_145, 3, 0, 9223372036854775807);  slice_145 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_785: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_146, [1, 16, 512, 1023]);  slice_146 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_22: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_147: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_785, 0, 0, 9223372036854775807);  view_785 = None
    slice_148: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_147, 1, 0, 9223372036854775807);  slice_147 = None
    slice_149: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_148, 2, 0, 9223372036854775807);  slice_148 = None
    index_20: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_149, [None, None, None, iota_22]);  slice_149 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_224: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_779, index_20);  view_779 = index_20 = None
    add_225: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_224, 0);  add_224 = None
    mul_164: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_225, 0.125);  add_225 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_20: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_164, [3], True)
    sub_60: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_164, amax_20);  mul_164 = amax_20 = None
    exp_20: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_60);  sub_60 = None
    sum_21: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_20, [3], True)
    div_21: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_20, sum_21);  exp_20 = sum_21 = None
    alias_20: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_21)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_82 = torch.ops.aten.native_dropout.default(div_21, 0.1, True);  div_21 = None
    getitem_244: "f32[1, 16, 512, 512]" = native_dropout_82[0]
    getitem_245: "b8[1, 16, 512, 512]" = native_dropout_82[1];  native_dropout_82 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_523: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_244, 4);  getitem_244 = None
    permute_873: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_523, [2, 0, 1, 4, 3]);  unsqueeze_523 = None
    unsqueeze_524: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_771, 4);  view_771 = None
    permute_874: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_524, [4, 1, 2, 3, 0]);  unsqueeze_524 = None
    permute_875: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_873, [2, 0, 4, 1, 3]);  permute_873 = None
    view_786: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_875, [16, 512, 512]);  permute_875 = None
    permute_876: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_874, [2, 4, 1, 3, 0]);  permute_874 = None
    view_787: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_876, [16, 512, 64]);  permute_876 = None
    bmm_166: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_786, view_787)
    view_788: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_166, [16, 512, 1, 1, 64]);  bmm_166 = None
    permute_877: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_788, [1, 3, 0, 4, 2]);  view_788 = None
    view_789: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_877, [512, 1, 16, 64]);  permute_877 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_525: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_789, 4);  view_789 = None
    permute_878: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_525, [0, 1, 4, 3, 2]);  unsqueeze_525 = None
    unsqueeze_526: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_147, 3);  primals_147 = None
    unsqueeze_527: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_526, 4);  unsqueeze_526 = None
    permute_879: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_527, [3, 4, 0, 2, 1]);  unsqueeze_527 = None
    permute_880: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_878, [0, 3, 4, 1, 2]);  permute_878 = None
    clone_40: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_880, memory_format = torch.contiguous_format);  permute_880 = None
    view_790: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_40, [1, 512, 1024]);  clone_40 = None
    permute_881: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_879, [3, 4, 1, 2, 0]);  permute_879 = None
    clone_41: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_881, memory_format = torch.contiguous_format);  permute_881 = None
    view_791: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_41, [1, 1024, 1024]);  clone_41 = None
    bmm_167: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_790, view_791)
    view_792: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_167, [512, 1, 1, 1, 1024]);  bmm_167 = None
    permute_882: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_792, [0, 3, 4, 1, 2]);  view_792 = None
    view_793: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_882, [512, 1, 1024]);  permute_882 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_83 = torch.ops.aten.native_dropout.default(view_793, 0.1, True);  view_793 = None
    getitem_246: "f32[512, 1, 1024]" = native_dropout_83[0]
    getitem_247: "b8[512, 1, 1024]" = native_dropout_83[1];  native_dropout_83 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_226: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_246, add_221);  getitem_246 = add_221 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_40 = torch.ops.aten.var_mean.correction(add_226, [2], correction = 0, keepdim = True)
    getitem_248: "f32[512, 1, 1]" = var_mean_40[0]
    getitem_249: "f32[512, 1, 1]" = var_mean_40[1];  var_mean_40 = None
    add_227: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_248, 1e-12);  getitem_248 = None
    rsqrt_40: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_227);  add_227 = None
    sub_61: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_226, getitem_249)
    mul_165: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_61, rsqrt_40);  sub_61 = None
    mul_166: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_165, primals_330);  mul_165 = None
    add_228: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_166, primals_331);  mul_166 = primals_331 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_794: "f32[512, 1024]" = torch.ops.aten.view.default(add_228, [512, 1024])
    permute_883: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_332, [1, 0]);  primals_332 = None
    addmm_40: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_333, view_794, permute_883);  primals_333 = None
    view_795: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_40, [512, 1, 4096]);  addmm_40 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_167: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_795, 0.5)
    mul_168: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_795, 0.7071067811865476)
    erf_20: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_168);  mul_168 = None
    add_229: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_20, 1);  erf_20 = None
    mul_169: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_167, add_229);  mul_167 = add_229 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_84 = torch.ops.aten.native_dropout.default(mul_169, 0.1, True);  mul_169 = None
    getitem_250: "f32[512, 1, 4096]" = native_dropout_84[0]
    getitem_251: "b8[512, 1, 4096]" = native_dropout_84[1];  native_dropout_84 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_796: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_250, [512, 4096]);  getitem_250 = None
    permute_884: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_334, [1, 0]);  primals_334 = None
    addmm_41: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_335, view_796, permute_884);  primals_335 = None
    view_797: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_41, [512, 1, 1024]);  addmm_41 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_85 = torch.ops.aten.native_dropout.default(view_797, 0.1, True);  view_797 = None
    getitem_252: "f32[512, 1, 1024]" = native_dropout_85[0]
    getitem_253: "b8[512, 1, 1024]" = native_dropout_85[1];  native_dropout_85 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_230: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_252, add_228);  getitem_252 = add_228 = None
    var_mean_41 = torch.ops.aten.var_mean.correction(add_230, [2], correction = 0, keepdim = True)
    getitem_254: "f32[512, 1, 1]" = var_mean_41[0]
    getitem_255: "f32[512, 1, 1]" = var_mean_41[1];  var_mean_41 = None
    add_231: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_254, 1e-12);  getitem_254 = None
    rsqrt_41: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_231);  add_231 = None
    sub_62: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_230, getitem_255)
    mul_170: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_62, rsqrt_41);  sub_62 = None
    mul_171: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_170, primals_336);  mul_170 = None
    add_232: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_171, primals_337);  mul_171 = primals_337 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_528: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_232, 3)
    unsqueeze_529: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_528, 4);  unsqueeze_528 = None
    permute_885: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_529, [0, 1, 3, 4, 2]);  unsqueeze_529 = None
    unsqueeze_530: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_148, 3);  primals_148 = None
    unsqueeze_531: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_530, 4);  unsqueeze_530 = None
    permute_886: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_531, [3, 4, 1, 2, 0]);  unsqueeze_531 = None
    permute_887: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_885, [0, 4, 1, 2, 3]);  permute_885 = None
    view_798: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_887, [1, 512, 1024]);  permute_887 = None
    permute_888: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_886, [4, 1, 2, 3, 0]);  permute_886 = None
    view_799: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_888, [1, 1024, 1024]);  permute_888 = None
    bmm_168: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_798, view_799)
    view_800: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_168, [512, 1, 1, 16, 64]);  bmm_168 = None
    permute_889: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_800, [0, 2, 3, 4, 1]);  view_800 = None
    view_801: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_889, [512, 1, 16, 64]);  permute_889 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_532: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_232, 3)
    unsqueeze_533: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_532, 4);  unsqueeze_532 = None
    permute_890: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_533, [0, 1, 3, 4, 2]);  unsqueeze_533 = None
    unsqueeze_534: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_149, 3);  primals_149 = None
    unsqueeze_535: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_534, 4);  unsqueeze_534 = None
    permute_891: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_535, [3, 4, 1, 2, 0]);  unsqueeze_535 = None
    permute_892: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_890, [0, 4, 1, 2, 3]);  permute_890 = None
    view_802: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_892, [1, 512, 1024]);  permute_892 = None
    permute_893: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_891, [4, 1, 2, 3, 0]);  permute_891 = None
    view_803: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_893, [1, 1024, 1024]);  permute_893 = None
    bmm_169: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_802, view_803)
    view_804: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_169, [512, 1, 1, 16, 64]);  bmm_169 = None
    permute_894: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_804, [0, 2, 3, 4, 1]);  view_804 = None
    view_805: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_894, [512, 1, 16, 64]);  permute_894 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_536: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_232, 3)
    unsqueeze_537: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_536, 4);  unsqueeze_536 = None
    permute_895: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_537, [0, 1, 3, 4, 2]);  unsqueeze_537 = None
    unsqueeze_538: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_150, 3);  primals_150 = None
    unsqueeze_539: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_538, 4);  unsqueeze_538 = None
    permute_896: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_539, [3, 4, 1, 2, 0]);  unsqueeze_539 = None
    permute_897: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_895, [0, 4, 1, 2, 3]);  permute_895 = None
    view_806: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_897, [1, 512, 1024]);  permute_897 = None
    permute_898: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_896, [4, 1, 2, 3, 0]);  permute_896 = None
    view_807: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_898, [1, 1024, 1024]);  permute_898 = None
    bmm_170: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_806, view_807)
    view_808: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_170, [512, 1, 1, 16, 64]);  bmm_170 = None
    permute_899: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_808, [0, 2, 3, 4, 1]);  view_808 = None
    view_809: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_899, [512, 1, 16, 64]);  permute_899 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_540: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_541: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_540, 4);  unsqueeze_540 = None
    permute_900: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_541, [0, 1, 3, 4, 2]);  unsqueeze_541 = None
    unsqueeze_542: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_151, 3);  primals_151 = None
    unsqueeze_543: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_542, 4);  unsqueeze_542 = None
    permute_901: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_543, [3, 4, 1, 2, 0]);  unsqueeze_543 = None
    permute_902: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_900, [0, 4, 1, 2, 3]);  permute_900 = None
    view_810: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_902, [1, 1024, 1024]);  permute_902 = None
    permute_903: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_901, [4, 1, 2, 3, 0]);  permute_901 = None
    view_811: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_903, [1, 1024, 1024]);  permute_903 = None
    bmm_171: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_810, view_811);  view_811 = None
    view_812: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_171, [1024, 1, 1, 16, 64]);  bmm_171 = None
    permute_904: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_812, [0, 2, 3, 4, 1]);  view_812 = None
    view_813: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_904, [1024, 1, 16, 64]);  permute_904 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_233: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_801, primals_152);  primals_152 = None
    unsqueeze_544: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_233, 4);  add_233 = None
    permute_905: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_544, [1, 2, 0, 4, 3]);  unsqueeze_544 = None
    unsqueeze_545: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_805, 4);  view_805 = None
    permute_906: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_545, [1, 2, 4, 0, 3]);  unsqueeze_545 = None
    permute_907: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_905, [1, 2, 4, 0, 3]);  permute_905 = None
    view_814: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_907, [16, 512, 64]);  permute_907 = None
    permute_908: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_906, [1, 4, 0, 3, 2]);  permute_906 = None
    view_815: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_908, [16, 64, 512]);  permute_908 = None
    bmm_172: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_814, view_815)
    view_816: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_172, [16, 512, 1, 1, 512]);  bmm_172 = None
    permute_909: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_816, [3, 0, 1, 4, 2]);  view_816 = None
    view_817: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_909, [1, 16, 512, 512]);  permute_909 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_234: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_801, primals_153);  view_801 = primals_153 = None
    unsqueeze_546: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_234, 4);  add_234 = None
    permute_910: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_546, [1, 2, 0, 4, 3]);  unsqueeze_546 = None
    unsqueeze_547: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_813, 4);  view_813 = None
    permute_911: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_547, [1, 2, 4, 0, 3]);  unsqueeze_547 = None
    permute_912: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_910, [1, 2, 4, 0, 3]);  permute_910 = None
    view_818: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_912, [16, 512, 64]);  permute_912 = None
    permute_913: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_911, [1, 4, 0, 3, 2]);  permute_911 = None
    view_819: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_913, [16, 64, 1024]);  permute_913 = None
    bmm_173: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_818, view_819)
    view_820: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_173, [16, 512, 1, 1, 1024]);  bmm_173 = None
    permute_914: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_820, [3, 0, 1, 4, 2]);  view_820 = None
    view_821: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_914, [1, 16, 512, 1024]);  permute_914 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_822: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_821, [1, 16, 1024, 512]);  view_821 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_150: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_822, 0, 0, 9223372036854775807);  view_822 = None
    slice_151: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_150, 1, 0, 9223372036854775807);  slice_150 = None
    slice_152: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_151, 2, 1, 9223372036854775807);  slice_151 = None
    slice_153: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_152, 3, 0, 9223372036854775807);  slice_152 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_823: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_153, [1, 16, 512, 1023]);  slice_153 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_23: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_154: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_823, 0, 0, 9223372036854775807);  view_823 = None
    slice_155: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_154, 1, 0, 9223372036854775807);  slice_154 = None
    slice_156: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_155, 2, 0, 9223372036854775807);  slice_155 = None
    index_21: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_156, [None, None, None, iota_23]);  slice_156 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_235: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_817, index_21);  view_817 = index_21 = None
    add_236: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_235, 0);  add_235 = None
    mul_172: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_236, 0.125);  add_236 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_21: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_172, [3], True)
    sub_63: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_172, amax_21);  mul_172 = amax_21 = None
    exp_21: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_63);  sub_63 = None
    sum_22: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_21, [3], True)
    div_22: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_21, sum_22);  exp_21 = sum_22 = None
    alias_21: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_22)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_86 = torch.ops.aten.native_dropout.default(div_22, 0.1, True);  div_22 = None
    getitem_256: "f32[1, 16, 512, 512]" = native_dropout_86[0]
    getitem_257: "b8[1, 16, 512, 512]" = native_dropout_86[1];  native_dropout_86 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_548: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_256, 4);  getitem_256 = None
    permute_915: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_548, [2, 0, 1, 4, 3]);  unsqueeze_548 = None
    unsqueeze_549: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_809, 4);  view_809 = None
    permute_916: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_549, [4, 1, 2, 3, 0]);  unsqueeze_549 = None
    permute_917: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_915, [2, 0, 4, 1, 3]);  permute_915 = None
    view_824: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_917, [16, 512, 512]);  permute_917 = None
    permute_918: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_916, [2, 4, 1, 3, 0]);  permute_916 = None
    view_825: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_918, [16, 512, 64]);  permute_918 = None
    bmm_174: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_824, view_825)
    view_826: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_174, [16, 512, 1, 1, 64]);  bmm_174 = None
    permute_919: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_826, [1, 3, 0, 4, 2]);  view_826 = None
    view_827: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_919, [512, 1, 16, 64]);  permute_919 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_550: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_827, 4);  view_827 = None
    permute_920: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_550, [0, 1, 4, 3, 2]);  unsqueeze_550 = None
    unsqueeze_551: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_154, 3);  primals_154 = None
    unsqueeze_552: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_551, 4);  unsqueeze_551 = None
    permute_921: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_552, [3, 4, 0, 2, 1]);  unsqueeze_552 = None
    permute_922: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_920, [0, 3, 4, 1, 2]);  permute_920 = None
    clone_42: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_922, memory_format = torch.contiguous_format);  permute_922 = None
    view_828: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_42, [1, 512, 1024]);  clone_42 = None
    permute_923: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_921, [3, 4, 1, 2, 0]);  permute_921 = None
    clone_43: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_923, memory_format = torch.contiguous_format);  permute_923 = None
    view_829: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_43, [1, 1024, 1024]);  clone_43 = None
    bmm_175: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_828, view_829)
    view_830: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_175, [512, 1, 1, 1, 1024]);  bmm_175 = None
    permute_924: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_830, [0, 3, 4, 1, 2]);  view_830 = None
    view_831: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_924, [512, 1, 1024]);  permute_924 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_87 = torch.ops.aten.native_dropout.default(view_831, 0.1, True);  view_831 = None
    getitem_258: "f32[512, 1, 1024]" = native_dropout_87[0]
    getitem_259: "b8[512, 1, 1024]" = native_dropout_87[1];  native_dropout_87 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_237: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_258, add_232);  getitem_258 = add_232 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_42 = torch.ops.aten.var_mean.correction(add_237, [2], correction = 0, keepdim = True)
    getitem_260: "f32[512, 1, 1]" = var_mean_42[0]
    getitem_261: "f32[512, 1, 1]" = var_mean_42[1];  var_mean_42 = None
    add_238: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_260, 1e-12);  getitem_260 = None
    rsqrt_42: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_238);  add_238 = None
    sub_64: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_237, getitem_261)
    mul_173: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_64, rsqrt_42);  sub_64 = None
    mul_174: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_173, primals_338);  mul_173 = None
    add_239: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_174, primals_339);  mul_174 = primals_339 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_832: "f32[512, 1024]" = torch.ops.aten.view.default(add_239, [512, 1024])
    permute_925: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_340, [1, 0]);  primals_340 = None
    addmm_42: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_341, view_832, permute_925);  primals_341 = None
    view_833: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_42, [512, 1, 4096]);  addmm_42 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_175: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_833, 0.5)
    mul_176: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_833, 0.7071067811865476)
    erf_21: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_176);  mul_176 = None
    add_240: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_21, 1);  erf_21 = None
    mul_177: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_175, add_240);  mul_175 = add_240 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_88 = torch.ops.aten.native_dropout.default(mul_177, 0.1, True);  mul_177 = None
    getitem_262: "f32[512, 1, 4096]" = native_dropout_88[0]
    getitem_263: "b8[512, 1, 4096]" = native_dropout_88[1];  native_dropout_88 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_834: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_262, [512, 4096]);  getitem_262 = None
    permute_926: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_342, [1, 0]);  primals_342 = None
    addmm_43: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_343, view_834, permute_926);  primals_343 = None
    view_835: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_43, [512, 1, 1024]);  addmm_43 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_89 = torch.ops.aten.native_dropout.default(view_835, 0.1, True);  view_835 = None
    getitem_264: "f32[512, 1, 1024]" = native_dropout_89[0]
    getitem_265: "b8[512, 1, 1024]" = native_dropout_89[1];  native_dropout_89 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_241: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_264, add_239);  getitem_264 = add_239 = None
    var_mean_43 = torch.ops.aten.var_mean.correction(add_241, [2], correction = 0, keepdim = True)
    getitem_266: "f32[512, 1, 1]" = var_mean_43[0]
    getitem_267: "f32[512, 1, 1]" = var_mean_43[1];  var_mean_43 = None
    add_242: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_266, 1e-12);  getitem_266 = None
    rsqrt_43: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_242);  add_242 = None
    sub_65: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_241, getitem_267)
    mul_178: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_65, rsqrt_43);  sub_65 = None
    mul_179: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_178, primals_344);  mul_178 = None
    add_243: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_179, primals_345);  mul_179 = primals_345 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_553: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_243, 3)
    unsqueeze_554: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_553, 4);  unsqueeze_553 = None
    permute_927: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_554, [0, 1, 3, 4, 2]);  unsqueeze_554 = None
    unsqueeze_555: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_155, 3);  primals_155 = None
    unsqueeze_556: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_555, 4);  unsqueeze_555 = None
    permute_928: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_556, [3, 4, 1, 2, 0]);  unsqueeze_556 = None
    permute_929: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_927, [0, 4, 1, 2, 3]);  permute_927 = None
    view_836: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_929, [1, 512, 1024]);  permute_929 = None
    permute_930: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_928, [4, 1, 2, 3, 0]);  permute_928 = None
    view_837: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_930, [1, 1024, 1024]);  permute_930 = None
    bmm_176: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_836, view_837)
    view_838: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_176, [512, 1, 1, 16, 64]);  bmm_176 = None
    permute_931: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_838, [0, 2, 3, 4, 1]);  view_838 = None
    view_839: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_931, [512, 1, 16, 64]);  permute_931 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_557: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_243, 3)
    unsqueeze_558: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_557, 4);  unsqueeze_557 = None
    permute_932: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_558, [0, 1, 3, 4, 2]);  unsqueeze_558 = None
    unsqueeze_559: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_156, 3);  primals_156 = None
    unsqueeze_560: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_559, 4);  unsqueeze_559 = None
    permute_933: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_560, [3, 4, 1, 2, 0]);  unsqueeze_560 = None
    permute_934: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_932, [0, 4, 1, 2, 3]);  permute_932 = None
    view_840: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_934, [1, 512, 1024]);  permute_934 = None
    permute_935: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_933, [4, 1, 2, 3, 0]);  permute_933 = None
    view_841: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_935, [1, 1024, 1024]);  permute_935 = None
    bmm_177: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_840, view_841)
    view_842: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_177, [512, 1, 1, 16, 64]);  bmm_177 = None
    permute_936: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_842, [0, 2, 3, 4, 1]);  view_842 = None
    view_843: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_936, [512, 1, 16, 64]);  permute_936 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_561: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_243, 3)
    unsqueeze_562: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_561, 4);  unsqueeze_561 = None
    permute_937: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_562, [0, 1, 3, 4, 2]);  unsqueeze_562 = None
    unsqueeze_563: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_157, 3);  primals_157 = None
    unsqueeze_564: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_563, 4);  unsqueeze_563 = None
    permute_938: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_564, [3, 4, 1, 2, 0]);  unsqueeze_564 = None
    permute_939: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_937, [0, 4, 1, 2, 3]);  permute_937 = None
    view_844: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_939, [1, 512, 1024]);  permute_939 = None
    permute_940: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_938, [4, 1, 2, 3, 0]);  permute_938 = None
    view_845: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_940, [1, 1024, 1024]);  permute_940 = None
    bmm_178: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_844, view_845)
    view_846: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_178, [512, 1, 1, 16, 64]);  bmm_178 = None
    permute_941: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_846, [0, 2, 3, 4, 1]);  view_846 = None
    view_847: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_941, [512, 1, 16, 64]);  permute_941 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_565: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3)
    unsqueeze_566: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_565, 4);  unsqueeze_565 = None
    permute_942: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_566, [0, 1, 3, 4, 2]);  unsqueeze_566 = None
    unsqueeze_567: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_158, 3);  primals_158 = None
    unsqueeze_568: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_567, 4);  unsqueeze_567 = None
    permute_943: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_568, [3, 4, 1, 2, 0]);  unsqueeze_568 = None
    permute_944: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_942, [0, 4, 1, 2, 3]);  permute_942 = None
    view_848: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_944, [1, 1024, 1024]);  permute_944 = None
    permute_945: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_943, [4, 1, 2, 3, 0]);  permute_943 = None
    view_849: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_945, [1, 1024, 1024]);  permute_945 = None
    bmm_179: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_848, view_849);  view_849 = None
    view_850: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_179, [1024, 1, 1, 16, 64]);  bmm_179 = None
    permute_946: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_850, [0, 2, 3, 4, 1]);  view_850 = None
    view_851: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_946, [1024, 1, 16, 64]);  permute_946 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_244: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_839, primals_159);  primals_159 = None
    unsqueeze_569: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_244, 4);  add_244 = None
    permute_947: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_569, [1, 2, 0, 4, 3]);  unsqueeze_569 = None
    unsqueeze_570: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_843, 4);  view_843 = None
    permute_948: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_570, [1, 2, 4, 0, 3]);  unsqueeze_570 = None
    permute_949: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_947, [1, 2, 4, 0, 3]);  permute_947 = None
    view_852: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_949, [16, 512, 64]);  permute_949 = None
    permute_950: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_948, [1, 4, 0, 3, 2]);  permute_948 = None
    view_853: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_950, [16, 64, 512]);  permute_950 = None
    bmm_180: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_852, view_853)
    view_854: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_180, [16, 512, 1, 1, 512]);  bmm_180 = None
    permute_951: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_854, [3, 0, 1, 4, 2]);  view_854 = None
    view_855: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_951, [1, 16, 512, 512]);  permute_951 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_245: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_839, primals_160);  view_839 = primals_160 = None
    unsqueeze_571: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_245, 4);  add_245 = None
    permute_952: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_571, [1, 2, 0, 4, 3]);  unsqueeze_571 = None
    unsqueeze_572: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_851, 4);  view_851 = None
    permute_953: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_572, [1, 2, 4, 0, 3]);  unsqueeze_572 = None
    permute_954: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_952, [1, 2, 4, 0, 3]);  permute_952 = None
    view_856: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_954, [16, 512, 64]);  permute_954 = None
    permute_955: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_953, [1, 4, 0, 3, 2]);  permute_953 = None
    view_857: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_955, [16, 64, 1024]);  permute_955 = None
    bmm_181: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_856, view_857)
    view_858: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_181, [16, 512, 1, 1, 1024]);  bmm_181 = None
    permute_956: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_858, [3, 0, 1, 4, 2]);  view_858 = None
    view_859: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_956, [1, 16, 512, 1024]);  permute_956 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_860: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_859, [1, 16, 1024, 512]);  view_859 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_157: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_860, 0, 0, 9223372036854775807);  view_860 = None
    slice_158: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_157, 1, 0, 9223372036854775807);  slice_157 = None
    slice_159: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_158, 2, 1, 9223372036854775807);  slice_158 = None
    slice_160: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_159, 3, 0, 9223372036854775807);  slice_159 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_861: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_160, [1, 16, 512, 1023]);  slice_160 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_24: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_161: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_861, 0, 0, 9223372036854775807);  view_861 = None
    slice_162: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_161, 1, 0, 9223372036854775807);  slice_161 = None
    slice_163: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_162, 2, 0, 9223372036854775807);  slice_162 = None
    index_22: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_163, [None, None, None, iota_24]);  slice_163 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_246: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_855, index_22);  view_855 = index_22 = None
    add_247: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_246, 0);  add_246 = None
    mul_180: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_247, 0.125);  add_247 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_22: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_180, [3], True)
    sub_66: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_180, amax_22);  mul_180 = amax_22 = None
    exp_22: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_66);  sub_66 = None
    sum_23: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_22, [3], True)
    div_23: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_22, sum_23);  exp_22 = sum_23 = None
    alias_22: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_23)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_90 = torch.ops.aten.native_dropout.default(div_23, 0.1, True);  div_23 = None
    getitem_268: "f32[1, 16, 512, 512]" = native_dropout_90[0]
    getitem_269: "b8[1, 16, 512, 512]" = native_dropout_90[1];  native_dropout_90 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_573: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_268, 4);  getitem_268 = None
    permute_957: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_573, [2, 0, 1, 4, 3]);  unsqueeze_573 = None
    unsqueeze_574: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_847, 4);  view_847 = None
    permute_958: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_574, [4, 1, 2, 3, 0]);  unsqueeze_574 = None
    permute_959: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_957, [2, 0, 4, 1, 3]);  permute_957 = None
    view_862: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_959, [16, 512, 512]);  permute_959 = None
    permute_960: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_958, [2, 4, 1, 3, 0]);  permute_958 = None
    view_863: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_960, [16, 512, 64]);  permute_960 = None
    bmm_182: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_862, view_863)
    view_864: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_182, [16, 512, 1, 1, 64]);  bmm_182 = None
    permute_961: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_864, [1, 3, 0, 4, 2]);  view_864 = None
    view_865: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_961, [512, 1, 16, 64]);  permute_961 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_575: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_865, 4);  view_865 = None
    permute_962: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_575, [0, 1, 4, 3, 2]);  unsqueeze_575 = None
    unsqueeze_576: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_161, 3);  primals_161 = None
    unsqueeze_577: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_576, 4);  unsqueeze_576 = None
    permute_963: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_577, [3, 4, 0, 2, 1]);  unsqueeze_577 = None
    permute_964: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_962, [0, 3, 4, 1, 2]);  permute_962 = None
    clone_44: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_964, memory_format = torch.contiguous_format);  permute_964 = None
    view_866: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_44, [1, 512, 1024]);  clone_44 = None
    permute_965: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_963, [3, 4, 1, 2, 0]);  permute_963 = None
    clone_45: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_965, memory_format = torch.contiguous_format);  permute_965 = None
    view_867: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_45, [1, 1024, 1024]);  clone_45 = None
    bmm_183: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_866, view_867)
    view_868: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_183, [512, 1, 1, 1, 1024]);  bmm_183 = None
    permute_966: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_868, [0, 3, 4, 1, 2]);  view_868 = None
    view_869: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_966, [512, 1, 1024]);  permute_966 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_91 = torch.ops.aten.native_dropout.default(view_869, 0.1, True);  view_869 = None
    getitem_270: "f32[512, 1, 1024]" = native_dropout_91[0]
    getitem_271: "b8[512, 1, 1024]" = native_dropout_91[1];  native_dropout_91 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_248: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_270, add_243);  getitem_270 = add_243 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_44 = torch.ops.aten.var_mean.correction(add_248, [2], correction = 0, keepdim = True)
    getitem_272: "f32[512, 1, 1]" = var_mean_44[0]
    getitem_273: "f32[512, 1, 1]" = var_mean_44[1];  var_mean_44 = None
    add_249: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_272, 1e-12);  getitem_272 = None
    rsqrt_44: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_249);  add_249 = None
    sub_67: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_248, getitem_273)
    mul_181: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_67, rsqrt_44);  sub_67 = None
    mul_182: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_181, primals_346);  mul_181 = None
    add_250: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_182, primals_347);  mul_182 = primals_347 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_870: "f32[512, 1024]" = torch.ops.aten.view.default(add_250, [512, 1024])
    permute_967: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_348, [1, 0]);  primals_348 = None
    addmm_44: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_349, view_870, permute_967);  primals_349 = None
    view_871: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_44, [512, 1, 4096]);  addmm_44 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_183: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_871, 0.5)
    mul_184: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_871, 0.7071067811865476)
    erf_22: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_184);  mul_184 = None
    add_251: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_22, 1);  erf_22 = None
    mul_185: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_183, add_251);  mul_183 = add_251 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_92 = torch.ops.aten.native_dropout.default(mul_185, 0.1, True);  mul_185 = None
    getitem_274: "f32[512, 1, 4096]" = native_dropout_92[0]
    getitem_275: "b8[512, 1, 4096]" = native_dropout_92[1];  native_dropout_92 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_872: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_274, [512, 4096]);  getitem_274 = None
    permute_968: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_350, [1, 0]);  primals_350 = None
    addmm_45: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_351, view_872, permute_968);  primals_351 = None
    view_873: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_45, [512, 1, 1024]);  addmm_45 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_93 = torch.ops.aten.native_dropout.default(view_873, 0.1, True);  view_873 = None
    getitem_276: "f32[512, 1, 1024]" = native_dropout_93[0]
    getitem_277: "b8[512, 1, 1024]" = native_dropout_93[1];  native_dropout_93 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_252: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_276, add_250);  getitem_276 = add_250 = None
    var_mean_45 = torch.ops.aten.var_mean.correction(add_252, [2], correction = 0, keepdim = True)
    getitem_278: "f32[512, 1, 1]" = var_mean_45[0]
    getitem_279: "f32[512, 1, 1]" = var_mean_45[1];  var_mean_45 = None
    add_253: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_278, 1e-12);  getitem_278 = None
    rsqrt_45: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_253);  add_253 = None
    sub_68: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_252, getitem_279)
    mul_186: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_68, rsqrt_45);  sub_68 = None
    mul_187: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_186, primals_352);  mul_186 = None
    add_254: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_187, primals_353);  mul_187 = primals_353 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    unsqueeze_578: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_254, 3)
    unsqueeze_579: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_578, 4);  unsqueeze_578 = None
    permute_969: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_579, [0, 1, 3, 4, 2]);  unsqueeze_579 = None
    unsqueeze_580: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_162, 3);  primals_162 = None
    unsqueeze_581: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_580, 4);  unsqueeze_580 = None
    permute_970: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_581, [3, 4, 1, 2, 0]);  unsqueeze_581 = None
    permute_971: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_969, [0, 4, 1, 2, 3]);  permute_969 = None
    view_874: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_971, [1, 512, 1024]);  permute_971 = None
    permute_972: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_970, [4, 1, 2, 3, 0]);  permute_970 = None
    view_875: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_972, [1, 1024, 1024]);  permute_972 = None
    bmm_184: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_874, view_875)
    view_876: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_184, [512, 1, 1, 16, 64]);  bmm_184 = None
    permute_973: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_876, [0, 2, 3, 4, 1]);  view_876 = None
    view_877: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_973, [512, 1, 16, 64]);  permute_973 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    unsqueeze_582: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_254, 3)
    unsqueeze_583: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_582, 4);  unsqueeze_582 = None
    permute_974: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_583, [0, 1, 3, 4, 2]);  unsqueeze_583 = None
    unsqueeze_584: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_163, 3);  primals_163 = None
    unsqueeze_585: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_584, 4);  unsqueeze_584 = None
    permute_975: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_585, [3, 4, 1, 2, 0]);  unsqueeze_585 = None
    permute_976: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_974, [0, 4, 1, 2, 3]);  permute_974 = None
    view_878: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_976, [1, 512, 1024]);  permute_976 = None
    permute_977: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_975, [4, 1, 2, 3, 0]);  permute_975 = None
    view_879: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_977, [1, 1024, 1024]);  permute_977 = None
    bmm_185: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_878, view_879)
    view_880: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_185, [512, 1, 1, 16, 64]);  bmm_185 = None
    permute_978: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_880, [0, 2, 3, 4, 1]);  view_880 = None
    view_881: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_978, [512, 1, 16, 64]);  permute_978 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    unsqueeze_586: "f32[512, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(add_254, 3)
    unsqueeze_587: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_586, 4);  unsqueeze_586 = None
    permute_979: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_587, [0, 1, 3, 4, 2]);  unsqueeze_587 = None
    unsqueeze_588: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_164, 3);  primals_164 = None
    unsqueeze_589: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_588, 4);  unsqueeze_588 = None
    permute_980: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_589, [3, 4, 1, 2, 0]);  unsqueeze_589 = None
    permute_981: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_979, [0, 4, 1, 2, 3]);  permute_979 = None
    view_882: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_981, [1, 512, 1024]);  permute_981 = None
    permute_982: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_980, [4, 1, 2, 3, 0]);  permute_980 = None
    view_883: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_982, [1, 1024, 1024]);  permute_982 = None
    bmm_186: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_882, view_883)
    view_884: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_186, [512, 1, 1, 16, 64]);  bmm_186 = None
    permute_983: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_884, [0, 2, 3, 4, 1]);  view_884 = None
    view_885: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_983, [512, 1, 16, 64]);  permute_983 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    unsqueeze_590: "f32[1024, 1, 1024, 1]" = torch.ops.aten.unsqueeze.default(getitem_2, 3);  getitem_2 = None
    unsqueeze_591: "f32[1024, 1, 1024, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_590, 4);  unsqueeze_590 = None
    permute_984: "f32[1024, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(unsqueeze_591, [0, 1, 3, 4, 2]);  unsqueeze_591 = None
    unsqueeze_592: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_165, 3);  primals_165 = None
    unsqueeze_593: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_592, 4);  unsqueeze_592 = None
    permute_985: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(unsqueeze_593, [3, 4, 1, 2, 0]);  unsqueeze_593 = None
    permute_986: "f32[1024, 1024, 1, 1, 1]" = torch.ops.aten.permute.default(permute_984, [0, 4, 1, 2, 3]);  permute_984 = None
    view_886: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_986, [1, 1024, 1024]);  permute_986 = None
    permute_987: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_985, [4, 1, 2, 3, 0]);  permute_985 = None
    view_887: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_987, [1, 1024, 1024]);  permute_987 = None
    bmm_187: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(view_886, view_887);  view_887 = None
    view_888: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.view.default(bmm_187, [1024, 1, 1, 16, 64]);  bmm_187 = None
    permute_988: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_888, [0, 2, 3, 4, 1]);  view_888 = None
    view_889: "f32[1024, 1, 16, 64]" = torch.ops.aten.view.default(permute_988, [1024, 1, 16, 64]);  permute_988 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_255: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_877, primals_166);  primals_166 = None
    unsqueeze_594: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_255, 4);  add_255 = None
    permute_989: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_594, [1, 2, 0, 4, 3]);  unsqueeze_594 = None
    unsqueeze_595: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_881, 4);  view_881 = None
    permute_990: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(unsqueeze_595, [1, 2, 4, 0, 3]);  unsqueeze_595 = None
    permute_991: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_989, [1, 2, 4, 0, 3]);  permute_989 = None
    view_890: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_991, [16, 512, 64]);  permute_991 = None
    permute_992: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.permute.default(permute_990, [1, 4, 0, 3, 2]);  permute_990 = None
    view_891: "f32[16, 64, 512]" = torch.ops.aten.view.default(permute_992, [16, 64, 512]);  permute_992 = None
    bmm_188: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_890, view_891)
    view_892: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.view.default(bmm_188, [16, 512, 1, 1, 512]);  bmm_188 = None
    permute_993: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(view_892, [3, 0, 1, 4, 2]);  view_892 = None
    view_893: "f32[1, 16, 512, 512]" = torch.ops.aten.view.default(permute_993, [1, 16, 512, 512]);  permute_993 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    add_256: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(view_877, primals_167);  view_877 = primals_167 = None
    unsqueeze_596: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(add_256, 4);  add_256 = None
    permute_994: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(unsqueeze_596, [1, 2, 0, 4, 3]);  unsqueeze_596 = None
    unsqueeze_597: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_889, 4);  view_889 = None
    permute_995: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(unsqueeze_597, [1, 2, 4, 0, 3]);  unsqueeze_597 = None
    permute_996: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.permute.default(permute_994, [1, 2, 4, 0, 3]);  permute_994 = None
    view_894: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_996, [16, 512, 64]);  permute_996 = None
    permute_997: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_995, [1, 4, 0, 3, 2]);  permute_995 = None
    view_895: "f32[16, 64, 1024]" = torch.ops.aten.view.default(permute_997, [16, 64, 1024]);  permute_997 = None
    bmm_189: "f32[16, 512, 1024]" = torch.ops.aten.bmm.default(view_894, view_895)
    view_896: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_189, [16, 512, 1, 1, 1024]);  bmm_189 = None
    permute_998: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.permute.default(view_896, [3, 0, 1, 4, 2]);  view_896 = None
    view_897: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(permute_998, [1, 16, 512, 1024]);  permute_998 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_898: "f32[1, 16, 1024, 512]" = torch.ops.aten.view.default(view_897, [1, 16, 1024, 512]);  view_897 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    slice_164: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(view_898, 0, 0, 9223372036854775807);  view_898 = None
    slice_165: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice.Tensor(slice_164, 1, 0, 9223372036854775807);  slice_164 = None
    slice_166: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_165, 2, 1, 9223372036854775807);  slice_165 = None
    slice_167: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice.Tensor(slice_166, 3, 0, 9223372036854775807);  slice_166 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_899: "f32[1, 16, 512, 1023]" = torch.ops.aten.view.default(slice_167, [1, 16, 512, 1023]);  slice_167 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    iota_25: "i64[512]" = torch.ops.prims.iota.default(512, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
    slice_168: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(view_899, 0, 0, 9223372036854775807);  view_899 = None
    slice_169: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_168, 1, 0, 9223372036854775807);  slice_168 = None
    slice_170: "f32[1, 16, 512, 1023]" = torch.ops.aten.slice.Tensor(slice_169, 2, 0, 9223372036854775807);  slice_169 = None
    index_23: "f32[1, 16, 512, 512]" = torch.ops.aten.index.Tensor(slice_170, [None, None, None, iota_25]);  slice_170 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    add_257: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(view_893, index_23);  view_893 = index_23 = None
    add_258: "f32[1, 16, 512, 512]" = torch.ops.aten.add.Tensor(add_257, 0);  add_257 = None
    mul_188: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(add_258, 0.125);  add_258 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    amax_23: "f32[1, 16, 512, 1]" = torch.ops.aten.amax.default(mul_188, [3], True)
    sub_69: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_188, amax_23);  mul_188 = amax_23 = None
    exp_23: "f32[1, 16, 512, 512]" = torch.ops.aten.exp.default(sub_69);  sub_69 = None
    sum_24: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(exp_23, [3], True)
    div_24: "f32[1, 16, 512, 512]" = torch.ops.aten.div.Tensor(exp_23, sum_24);  exp_23 = sum_24 = None
    alias_23: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(div_24)
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    native_dropout_94 = torch.ops.aten.native_dropout.default(div_24, 0.1, True);  div_24 = None
    getitem_280: "f32[1, 16, 512, 512]" = native_dropout_94[0]
    getitem_281: "b8[1, 16, 512, 512]" = native_dropout_94[1];  native_dropout_94 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    unsqueeze_598: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.unsqueeze.default(getitem_280, 4);  getitem_280 = None
    permute_999: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(unsqueeze_598, [2, 0, 1, 4, 3]);  unsqueeze_598 = None
    unsqueeze_599: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_885, 4);  view_885 = None
    permute_1000: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(unsqueeze_599, [4, 1, 2, 3, 0]);  unsqueeze_599 = None
    permute_1001: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.permute.default(permute_999, [2, 0, 4, 1, 3]);  permute_999 = None
    view_900: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1001, [16, 512, 512]);  permute_1001 = None
    permute_1002: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.permute.default(permute_1000, [2, 4, 1, 3, 0]);  permute_1000 = None
    view_901: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1002, [16, 512, 64]);  permute_1002 = None
    bmm_190: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_900, view_901)
    view_902: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.view.default(bmm_190, [16, 512, 1, 1, 64]);  bmm_190 = None
    permute_1003: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(view_902, [1, 3, 0, 4, 2]);  view_902 = None
    view_903: "f32[512, 1, 16, 64]" = torch.ops.aten.view.default(permute_1003, [512, 1, 16, 64]);  permute_1003 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    unsqueeze_600: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(view_903, 4);  view_903 = None
    permute_1004: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_600, [0, 1, 4, 3, 2]);  unsqueeze_600 = None
    unsqueeze_601: "f32[1024, 16, 64, 1]" = torch.ops.aten.unsqueeze.default(primals_168, 3);  primals_168 = None
    unsqueeze_602: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.unsqueeze.default(unsqueeze_601, 4);  unsqueeze_601 = None
    permute_1005: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(unsqueeze_602, [3, 4, 0, 2, 1]);  unsqueeze_602 = None
    permute_1006: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.permute.default(permute_1004, [0, 3, 4, 1, 2]);  permute_1004 = None
    clone_46: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.clone.default(permute_1006, memory_format = torch.contiguous_format);  permute_1006 = None
    view_904: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_46, [1, 512, 1024]);  clone_46 = None
    permute_1007: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.permute.default(permute_1005, [3, 4, 1, 2, 0]);  permute_1005 = None
    clone_47: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.clone.default(permute_1007, memory_format = torch.contiguous_format);  permute_1007 = None
    view_905: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(clone_47, [1, 1024, 1024]);  clone_47 = None
    bmm_191: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_904, view_905)
    view_906: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.view.default(bmm_191, [512, 1, 1, 1, 1024]);  bmm_191 = None
    permute_1008: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(view_906, [0, 3, 4, 1, 2]);  view_906 = None
    view_907: "f32[512, 1, 1024]" = torch.ops.aten.view.default(permute_1008, [512, 1, 1024]);  permute_1008 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    native_dropout_95 = torch.ops.aten.native_dropout.default(view_907, 0.1, True);  view_907 = None
    getitem_282: "f32[512, 1, 1024]" = native_dropout_95[0]
    getitem_283: "b8[512, 1, 1024]" = native_dropout_95[1];  native_dropout_95 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:322, code: attn_out = attn_out + h
    add_259: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_282, add_254);  getitem_282 = add_254 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    var_mean_46 = torch.ops.aten.var_mean.correction(add_259, [2], correction = 0, keepdim = True)
    getitem_284: "f32[512, 1, 1]" = var_mean_46[0]
    getitem_285: "f32[512, 1, 1]" = var_mean_46[1];  var_mean_46 = None
    add_260: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_284, 1e-12);  getitem_284 = None
    rsqrt_46: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_260);  add_260 = None
    sub_70: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_259, getitem_285)
    mul_189: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_70, rsqrt_46);  sub_70 = None
    mul_190: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_189, primals_354);  mul_189 = None
    add_261: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_190, primals_355);  mul_190 = primals_355 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_908: "f32[512, 1024]" = torch.ops.aten.view.default(add_261, [512, 1024])
    permute_1009: "f32[1024, 4096]" = torch.ops.aten.permute.default(primals_356, [1, 0]);  primals_356 = None
    addmm_46: "f32[512, 4096]" = torch.ops.aten.addmm.default(primals_357, view_908, permute_1009);  primals_357 = None
    view_909: "f32[512, 1, 4096]" = torch.ops.aten.view.default(addmm_46, [512, 1, 4096]);  addmm_46 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_191: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_909, 0.5)
    mul_192: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_909, 0.7071067811865476)
    erf_23: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_192);  mul_192 = None
    add_262: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_23, 1);  erf_23 = None
    mul_193: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_191, add_262);  mul_191 = add_262 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    native_dropout_96 = torch.ops.aten.native_dropout.default(mul_193, 0.1, True);  mul_193 = None
    getitem_286: "f32[512, 1, 4096]" = native_dropout_96[0]
    getitem_287: "b8[512, 1, 4096]" = native_dropout_96[1];  native_dropout_96 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_910: "f32[512, 4096]" = torch.ops.aten.view.default(getitem_286, [512, 4096]);  getitem_286 = None
    permute_1010: "f32[4096, 1024]" = torch.ops.aten.permute.default(primals_358, [1, 0]);  primals_358 = None
    addmm_47: "f32[512, 1024]" = torch.ops.aten.addmm.default(primals_359, view_910, permute_1010);  primals_359 = None
    view_911: "f32[512, 1, 1024]" = torch.ops.aten.view.default(addmm_47, [512, 1, 1024]);  addmm_47 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    native_dropout_97 = torch.ops.aten.native_dropout.default(view_911, 0.1, True);  view_911 = None
    getitem_288: "f32[512, 1, 1024]" = native_dropout_97[0]
    getitem_289: "b8[512, 1, 1024]" = native_dropout_97[1];  native_dropout_97 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    add_263: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(getitem_288, add_261);  getitem_288 = add_261 = None
    var_mean_47 = torch.ops.aten.var_mean.correction(add_263, [2], correction = 0, keepdim = True)
    getitem_290: "f32[512, 1, 1]" = var_mean_47[0]
    getitem_291: "f32[512, 1, 1]" = var_mean_47[1];  var_mean_47 = None
    add_264: "f32[512, 1, 1]" = torch.ops.aten.add.Tensor(getitem_290, 1e-12);  getitem_290 = None
    rsqrt_47: "f32[512, 1, 1]" = torch.ops.aten.rsqrt.default(add_264);  add_264 = None
    sub_71: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_263, getitem_291)
    mul_194: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_71, rsqrt_47);  sub_71 = None
    mul_195: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_194, primals_360);  mul_194 = None
    add_265: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_195, primals_361);  mul_195 = primals_361 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1257, code: output = self.dropout(output_g if output_g is not None else output_h)
    native_dropout_98 = torch.ops.aten.native_dropout.default(add_265, 0.1, True);  add_265 = None
    getitem_292: "f32[512, 1, 1024]" = native_dropout_98[0]
    getitem_293: "b8[512, 1, 1024]" = native_dropout_98[1];  native_dropout_98 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1260, code: output = output.permute(1, 0, 2).contiguous()
    permute_1011: "f32[1, 512, 1024]" = torch.ops.aten.permute.default(getitem_292, [1, 0, 2]);  getitem_292 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1463, code: logits = self.lm_loss(transformer_outputs[0])
    view_912: "f32[512, 1024]" = torch.ops.aten.view.default(permute_1011, [512, 1024]);  permute_1011 = None
    permute_1012: "f32[1024, 32000]" = torch.ops.aten.permute.default(primals_362, [1, 0]);  primals_362 = None
    addmm_48: "f32[512, 32000]" = torch.ops.aten.addmm.default(primals_363, view_912, permute_1012);  primals_363 = None
    view_913: "f32[1, 512, 32000]" = torch.ops.aten.view.default(addmm_48, [1, 512, 32000]);  addmm_48 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1469, code: loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
    view_914: "f32[512, 32000]" = torch.ops.aten.view.default(view_913, [-1, 32000])
    view_915: "i64[512]" = torch.ops.aten.view.default(primals_365, [-1]);  primals_365 = None
    amax_24: "f32[512, 1]" = torch.ops.aten.amax.default(view_914, [1], True)
    sub_72: "f32[512, 32000]" = torch.ops.aten.sub.Tensor(view_914, amax_24);  view_914 = amax_24 = None
    exp_24: "f32[512, 32000]" = torch.ops.aten.exp.default(sub_72)
    sum_25: "f32[512, 1]" = torch.ops.aten.sum.dim_IntList(exp_24, [1], True);  exp_24 = None
    log: "f32[512, 1]" = torch.ops.aten.log.default(sum_25);  sum_25 = None
    sub_73: "f32[512, 32000]" = torch.ops.aten.sub.Tensor(sub_72, log);  sub_72 = log = None
    alias_24: "f32[512, 32000]" = torch.ops.aten.alias.default(sub_73)
    ne: "b8[512]" = torch.ops.aten.ne.Scalar(view_915, -100)
    scalar_tensor: "i64[]" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0))
    where: "i64[512]" = torch.ops.aten.where.self(ne, view_915, scalar_tensor);  ne = scalar_tensor = None
    unsqueeze_603: "i64[512, 1]" = torch.ops.aten.unsqueeze.default(where, 1);  where = None
    gather: "f32[512, 1]" = torch.ops.aten.gather.default(sub_73, 1, unsqueeze_603);  sub_73 = unsqueeze_603 = None
    squeeze: "f32[512]" = torch.ops.aten.squeeze.dim(gather, 1);  gather = None
    neg: "f32[512]" = torch.ops.aten.neg.default(squeeze);  squeeze = None
    ne_1: "b8[512]" = torch.ops.aten.ne.Scalar(view_915, -100)
    scalar_tensor_1: "f32[]" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
    where_1: "f32[512]" = torch.ops.aten.where.self(ne_1, neg, scalar_tensor_1);  ne_1 = neg = scalar_tensor_1 = None
    ne_2: "b8[512]" = torch.ops.aten.ne.Scalar(view_915, -100)
    sum_26: "i64[]" = torch.ops.aten.sum.default(ne_2);  ne_2 = None
    convert_element_type_5: "f32[]" = torch.ops.prims.convert_element_type.default(sum_26, torch.float32);  sum_26 = None
    sum_27: "f32[]" = torch.ops.aten.sum.default(where_1);  where_1 = None
    div_25: "f32[]" = torch.ops.aten.div.Tensor(sum_27, convert_element_type_5);  sum_27 = None
    div_26: "f32[]" = torch.ops.aten.div.Tensor(tangents_1, convert_element_type_5);  tangents_1 = convert_element_type_5 = None
    unsqueeze_604: "i64[512, 1]" = torch.ops.aten.unsqueeze.default(view_915, 1);  view_915 = None
    ne_3: "b8[512, 1]" = torch.ops.aten.ne.Scalar(unsqueeze_604, -100)
    scalar_tensor_2: "i64[]" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0))
    where_2: "i64[512, 1]" = torch.ops.aten.where.self(ne_3, unsqueeze_604, scalar_tensor_2);  ne_3 = scalar_tensor_2 = None
    full: "f32[512, 32000]" = torch.ops.aten.full.default([512, 32000], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    scatter: "f32[512, 32000]" = torch.ops.aten.scatter.value(full, 1, where_2, -1.0);  full = where_2 = None
    ne_4: "b8[512, 1]" = torch.ops.aten.ne.Scalar(unsqueeze_604, -100);  unsqueeze_604 = None
    scalar_tensor_3: "f32[]" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
    where_3: "f32[512, 1]" = torch.ops.aten.where.self(ne_4, div_26, scalar_tensor_3);  ne_4 = div_26 = scalar_tensor_3 = None
    mul_196: "f32[512, 32000]" = torch.ops.aten.mul.Tensor(scatter, where_3);  scatter = where_3 = None
    alias_25: "f32[512, 32000]" = torch.ops.aten.alias.default(alias_24);  alias_24 = None
    exp_25: "f32[512, 32000]" = torch.ops.aten.exp.default(alias_25);  alias_25 = None
    sum_28: "f32[512, 1]" = torch.ops.aten.sum.dim_IntList(mul_196, [1], True)
    mul_197: "f32[512, 32000]" = torch.ops.aten.mul.Tensor(exp_25, sum_28);  exp_25 = sum_28 = None
    sub_74: "f32[512, 32000]" = torch.ops.aten.sub.Tensor(mul_196, mul_197);  mul_196 = mul_197 = None
    view_916: "f32[1, 512, 32000]" = torch.ops.aten.view.default(sub_74, [1, 512, 32000]);  sub_74 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1469, code: loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
    add_266: "f32[1, 512, 32000]" = torch.ops.aten.add.Tensor(tangents_2, view_916);  tangents_2 = view_916 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1463, code: logits = self.lm_loss(transformer_outputs[0])
    view_917: "f32[512, 32000]" = torch.ops.aten.view.default(add_266, [512, 32000]);  add_266 = None
    permute_1013: "f32[32000, 1024]" = torch.ops.aten.permute.default(permute_1012, [1, 0]);  permute_1012 = None
    mm: "f32[512, 1024]" = torch.ops.aten.mm.default(view_917, permute_1013);  permute_1013 = None
    permute_1014: "f32[32000, 512]" = torch.ops.aten.permute.default(view_917, [1, 0])
    mm_1: "f32[32000, 1024]" = torch.ops.aten.mm.default(permute_1014, view_912);  permute_1014 = view_912 = None
    permute_1015: "f32[1024, 32000]" = torch.ops.aten.permute.default(mm_1, [1, 0]);  mm_1 = None
    sum_29: "f32[1, 32000]" = torch.ops.aten.sum.dim_IntList(view_917, [0], True);  view_917 = None
    view_918: "f32[32000]" = torch.ops.aten.view.default(sum_29, [32000]);  sum_29 = None
    permute_1016: "f32[32000, 1024]" = torch.ops.aten.permute.default(permute_1015, [1, 0]);  permute_1015 = None
    view_919: "f32[1, 512, 1024]" = torch.ops.aten.view.default(mm, [1, 512, 1024]);  mm = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1260, code: output = output.permute(1, 0, 2).contiguous()
    permute_1017: "f32[512, 1, 1024]" = torch.ops.aten.permute.default(view_919, [1, 0, 2]);  view_919 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1257, code: output = self.dropout(output_g if output_g is not None else output_h)
    convert_element_type_6: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_293, torch.float32);  getitem_293 = None
    mul_198: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_6, 1.1111111111111112);  convert_element_type_6 = None
    mul_199: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(permute_1017, mul_198);  permute_1017 = mul_198 = None
    clone_48: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_199, memory_format = torch.contiguous_format);  mul_199 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_75: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_263, getitem_291);  add_263 = getitem_291 = None
    mul_200: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_75, rsqrt_47);  sub_75 = None
    mul_201: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(clone_48, primals_360);  primals_360 = None
    mul_202: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_201, 1024)
    sum_30: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_201, [2], True)
    mul_203: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_201, mul_200);  mul_201 = None
    sum_31: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_203, [2], True);  mul_203 = None
    mul_204: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_200, sum_31);  sum_31 = None
    sub_76: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_202, sum_30);  mul_202 = sum_30 = None
    sub_77: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_76, mul_204);  sub_76 = mul_204 = None
    div_27: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_47, 1024);  rsqrt_47 = None
    mul_205: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_27, sub_77);  div_27 = sub_77 = None
    mul_206: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(clone_48, mul_200);  mul_200 = None
    sum_32: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_206, [0, 1]);  mul_206 = None
    sum_33: "f32[1024]" = torch.ops.aten.sum.dim_IntList(clone_48, [0, 1]);  clone_48 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_7: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_289, torch.float32);  getitem_289 = None
    mul_207: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_7, 1.1111111111111112);  convert_element_type_7 = None
    mul_208: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_205, mul_207);  mul_207 = None
    clone_49: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_208, memory_format = torch.contiguous_format);  mul_208 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_920: "f32[512, 1024]" = torch.ops.aten.view.default(clone_49, [512, 1024]);  clone_49 = None
    permute_1018: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1010, [1, 0]);  permute_1010 = None
    mm_2: "f32[512, 4096]" = torch.ops.aten.mm.default(view_920, permute_1018);  permute_1018 = None
    permute_1019: "f32[1024, 512]" = torch.ops.aten.permute.default(view_920, [1, 0])
    mm_3: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1019, view_910);  permute_1019 = view_910 = None
    permute_1020: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_3, [1, 0]);  mm_3 = None
    sum_34: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_920, [0], True);  view_920 = None
    view_921: "f32[1024]" = torch.ops.aten.view.default(sum_34, [1024]);  sum_34 = None
    permute_1021: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1020, [1, 0]);  permute_1020 = None
    view_922: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_2, [512, 1, 4096]);  mm_2 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_8: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_287, torch.float32);  getitem_287 = None
    mul_209: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_8, 1.1111111111111112);  convert_element_type_8 = None
    mul_210: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_922, mul_209);  view_922 = mul_209 = None
    clone_50: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_210, memory_format = torch.contiguous_format);  mul_210 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_211: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_909, 0.7071067811865476)
    erf_24: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_211);  mul_211 = None
    add_267: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_24, 1);  erf_24 = None
    mul_212: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_267, 0.5);  add_267 = None
    mul_213: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_909, view_909)
    mul_214: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_213, -0.5);  mul_213 = None
    exp_26: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_214);  mul_214 = None
    mul_215: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_26, 0.3989422804014327);  exp_26 = None
    mul_216: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_909, mul_215);  view_909 = mul_215 = None
    add_268: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_212, mul_216);  mul_212 = mul_216 = None
    mul_217: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_50, add_268);  clone_50 = add_268 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_923: "f32[512, 4096]" = torch.ops.aten.view.default(mul_217, [512, 4096]);  mul_217 = None
    permute_1022: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1009, [1, 0]);  permute_1009 = None
    mm_4: "f32[512, 1024]" = torch.ops.aten.mm.default(view_923, permute_1022);  permute_1022 = None
    permute_1023: "f32[4096, 512]" = torch.ops.aten.permute.default(view_923, [1, 0])
    mm_5: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1023, view_908);  permute_1023 = view_908 = None
    permute_1024: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_5, [1, 0]);  mm_5 = None
    sum_35: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_923, [0], True);  view_923 = None
    view_924: "f32[4096]" = torch.ops.aten.view.default(sum_35, [4096]);  sum_35 = None
    permute_1025: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1024, [1, 0]);  permute_1024 = None
    view_925: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_4, [512, 1, 1024]);  mm_4 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_269: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_205, view_925);  mul_205 = view_925 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_78: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_259, getitem_285);  add_259 = getitem_285 = None
    mul_218: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_78, rsqrt_46);  sub_78 = None
    mul_219: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_269, primals_354);  primals_354 = None
    mul_220: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_219, 1024)
    sum_36: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_219, [2], True)
    mul_221: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_219, mul_218);  mul_219 = None
    sum_37: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_221, [2], True);  mul_221 = None
    mul_222: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_218, sum_37);  sum_37 = None
    sub_79: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_220, sum_36);  mul_220 = sum_36 = None
    sub_80: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_79, mul_222);  sub_79 = mul_222 = None
    div_28: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_46, 1024);  rsqrt_46 = None
    mul_223: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_28, sub_80);  div_28 = sub_80 = None
    mul_224: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_269, mul_218);  mul_218 = None
    sum_38: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_224, [0, 1]);  mul_224 = None
    sum_39: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_269, [0, 1]);  add_269 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_9: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_283, torch.float32);  getitem_283 = None
    mul_225: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_9, 1.1111111111111112);  convert_element_type_9 = None
    mul_226: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_223, mul_225);  mul_225 = None
    clone_51: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_226, memory_format = torch.contiguous_format);  mul_226 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_926: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_51, [512, 1, 1024, 1, 1]);  clone_51 = None
    permute_1026: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_926, [0, 3, 4, 1, 2]);  view_926 = None
    view_927: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1026, [1, 512, 1024]);  permute_1026 = None
    permute_1027: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_904, [0, 2, 1]);  view_904 = None
    bmm_192: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1027, view_927);  permute_1027 = None
    permute_1028: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_905, [0, 2, 1]);  view_905 = None
    bmm_193: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_927, permute_1028);  view_927 = permute_1028 = None
    view_928: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_192, [64, 16, 1, 1024, 1]);  bmm_192 = None
    permute_1029: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_928, [4, 2, 3, 0, 1]);  view_928 = None
    view_929: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_193, [512, 64, 16, 1, 1]);  bmm_193 = None
    permute_1030: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_929, [0, 3, 4, 1, 2]);  view_929 = None
    permute_1031: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1029, [2, 4, 3, 0, 1]);  permute_1029 = None
    squeeze_1: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1031, 4);  permute_1031 = None
    squeeze_2: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_1, 3);  squeeze_1 = None
    permute_1032: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1030, [0, 1, 4, 3, 2]);  permute_1030 = None
    squeeze_3: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1032, 4);  permute_1032 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_930: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_3, [512, 1, 16, 64, 1]);  squeeze_3 = None
    permute_1033: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_930, [2, 0, 4, 1, 3]);  view_930 = None
    view_931: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1033, [16, 512, 64]);  permute_1033 = None
    permute_1034: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_900, [0, 2, 1]);  view_900 = None
    bmm_194: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1034, view_931);  permute_1034 = None
    permute_1035: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_901, [0, 2, 1]);  view_901 = None
    bmm_195: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_931, permute_1035);  view_931 = permute_1035 = None
    view_932: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_194, [16, 512, 1, 64, 1]);  bmm_194 = None
    permute_1036: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_932, [4, 2, 0, 3, 1]);  view_932 = None
    view_933: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_195, [16, 512, 512, 1, 1]);  bmm_195 = None
    permute_1037: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_933, [1, 3, 0, 4, 2]);  view_933 = None
    permute_1038: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1036, [4, 1, 2, 3, 0]);  permute_1036 = None
    squeeze_4: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1038, 4);  permute_1038 = None
    permute_1039: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1037, [1, 2, 0, 4, 3]);  permute_1037 = None
    squeeze_5: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1039, 4);  permute_1039 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_10: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_281, torch.float32);  getitem_281 = None
    mul_227: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_10, 1.1111111111111112);  convert_element_type_10 = None
    mul_228: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_5, mul_227);  squeeze_5 = mul_227 = None
    clone_52: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_228, memory_format = torch.contiguous_format);  mul_228 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_26: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_23);  alias_23 = None
    mul_229: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_52, alias_26);  clone_52 = None
    sum_40: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_229, [3], True)
    mul_230: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_26, sum_40);  alias_26 = sum_40 = None
    sub_81: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_229, mul_230);  mul_229 = mul_230 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_231: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_81, 0.125);  sub_81 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_1: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_1, [None, None, None, iota_25], mul_231, True);  full_1 = iota_25 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_934: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put, [1, 16, 1023, 512]);  index_put = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_2: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_2, view_934, 3, 0, 9223372036854775807);  full_2 = view_934 = None
    full_3: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_1: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_3, slice_scatter, 2, 1, 9223372036854775807);  full_3 = slice_scatter = None
    full_4: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_2: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_4, slice_scatter_1, 1, 0, 9223372036854775807);  full_4 = slice_scatter_1 = None
    full_5: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_3: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_5, slice_scatter_2, 0, 0, 9223372036854775807);  full_5 = slice_scatter_2 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_935: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_3, [1, 16, 512, 1024]);  slice_scatter_3 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_936: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_935, [1, 16, 512, 1024, 1]);  view_935 = None
    permute_1040: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_936, [1, 2, 4, 0, 3]);  view_936 = None
    view_937: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1040, [16, 512, 1024]);  permute_1040 = None
    permute_1041: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_894, [0, 2, 1]);  view_894 = None
    bmm_196: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1041, view_937);  permute_1041 = None
    permute_1042: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_895, [0, 2, 1]);  view_895 = None
    bmm_197: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_937, permute_1042);  view_937 = permute_1042 = None
    view_938: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_196, [16, 64, 1, 1024, 1]);  bmm_196 = None
    permute_1043: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_938, [2, 0, 4, 3, 1]);  view_938 = None
    view_939: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_197, [16, 512, 64, 1, 1]);  bmm_197 = None
    permute_1044: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_939, [3, 0, 1, 4, 2]);  view_939 = None
    permute_1045: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1043, [3, 0, 1, 4, 2]);  permute_1043 = None
    squeeze_6: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1045, 4);  permute_1045 = None
    permute_1046: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1044, [2, 0, 1, 4, 3]);  permute_1044 = None
    squeeze_7: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1046, 4);  permute_1046 = None
    sum_41: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_7, [0, 1], True)
    view_940: "f32[16, 64]" = torch.ops.aten.view.default(sum_41, [16, 64]);  sum_41 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_941: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_231, [1, 16, 512, 512, 1]);  mul_231 = None
    permute_1047: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_941, [1, 2, 4, 0, 3]);  view_941 = None
    view_942: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1047, [16, 512, 512]);  permute_1047 = None
    permute_1048: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_890, [0, 2, 1]);  view_890 = None
    bmm_198: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1048, view_942);  permute_1048 = None
    permute_1049: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_891, [0, 2, 1]);  view_891 = None
    bmm_199: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_942, permute_1049);  view_942 = permute_1049 = None
    view_943: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_198, [16, 64, 1, 512, 1]);  bmm_198 = None
    permute_1050: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_943, [2, 0, 4, 3, 1]);  view_943 = None
    view_944: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_199, [16, 512, 64, 1, 1]);  bmm_199 = None
    permute_1051: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_944, [3, 0, 1, 4, 2]);  view_944 = None
    permute_1052: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1050, [3, 0, 1, 4, 2]);  permute_1050 = None
    squeeze_8: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1052, 4);  permute_1052 = None
    permute_1053: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1051, [2, 0, 1, 4, 3]);  permute_1051 = None
    squeeze_9: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1053, 4);  permute_1053 = None
    sum_42: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_9, [0, 1], True)
    view_945: "f32[16, 64]" = torch.ops.aten.view.default(sum_42, [16, 64]);  sum_42 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_270: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_7, squeeze_9);  squeeze_7 = squeeze_9 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_946: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_6, [1024, 1, 16, 64, 1]);  squeeze_6 = None
    permute_1054: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_946, [0, 4, 1, 2, 3]);  view_946 = None
    view_947: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1054, [1, 1024, 1024]);  permute_1054 = None
    permute_1055: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_886, [0, 2, 1]);  view_886 = None
    bmm_200: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1055, view_947);  permute_1055 = view_947 = None
    view_948: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_200, [1024, 1, 16, 64, 1]);  bmm_200 = None
    permute_1056: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_948, [4, 1, 2, 3, 0]);  view_948 = None
    permute_1057: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1056, [4, 2, 3, 0, 1]);  permute_1056 = None
    squeeze_10: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1057, 4);  permute_1057 = None
    squeeze_11: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_10, 3);  squeeze_10 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_949: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_4, [512, 1, 16, 64, 1]);  squeeze_4 = None
    permute_1058: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_949, [0, 4, 1, 2, 3]);  view_949 = None
    clone_53: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1058, memory_format = torch.contiguous_format);  permute_1058 = None
    view_950: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_53, [1, 512, 1024]);  clone_53 = None
    permute_1059: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_882, [0, 2, 1]);  view_882 = None
    bmm_201: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1059, view_950);  permute_1059 = None
    permute_1060: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_883, [0, 2, 1]);  view_883 = None
    bmm_202: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_950, permute_1060);  view_950 = permute_1060 = None
    view_951: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_201, [1024, 1, 16, 64, 1]);  bmm_201 = None
    permute_1061: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_951, [4, 1, 2, 3, 0]);  view_951 = None
    view_952: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_202, [512, 1024, 1, 1, 1]);  bmm_202 = None
    permute_1062: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_952, [0, 2, 3, 4, 1]);  view_952 = None
    permute_1063: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1061, [4, 2, 3, 0, 1]);  permute_1061 = None
    squeeze_12: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1063, 4);  permute_1063 = None
    squeeze_13: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_12, 3);  squeeze_12 = None
    permute_1064: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1062, [0, 1, 4, 2, 3]);  permute_1062 = None
    squeeze_14: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1064, 4);  permute_1064 = None
    squeeze_15: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_14, 3);  squeeze_14 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_271: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_223, squeeze_15);  mul_223 = squeeze_15 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_953: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_8, [512, 1, 16, 64, 1]);  squeeze_8 = None
    permute_1065: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_953, [0, 4, 1, 2, 3]);  view_953 = None
    view_954: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1065, [1, 512, 1024]);  permute_1065 = None
    permute_1066: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_878, [0, 2, 1]);  view_878 = None
    bmm_203: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1066, view_954);  permute_1066 = None
    permute_1067: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_879, [0, 2, 1]);  view_879 = None
    bmm_204: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_954, permute_1067);  view_954 = permute_1067 = None
    view_955: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_203, [1024, 1, 16, 64, 1]);  bmm_203 = None
    permute_1068: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_955, [4, 1, 2, 3, 0]);  view_955 = None
    view_956: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_204, [512, 1024, 1, 1, 1]);  bmm_204 = None
    permute_1069: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_956, [0, 2, 3, 4, 1]);  view_956 = None
    permute_1070: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1068, [4, 2, 3, 0, 1]);  permute_1068 = None
    squeeze_16: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1070, 4);  permute_1070 = None
    squeeze_17: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_16, 3);  squeeze_16 = None
    permute_1071: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1069, [0, 1, 4, 2, 3]);  permute_1069 = None
    squeeze_18: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1071, 4);  permute_1071 = None
    squeeze_19: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_18, 3);  squeeze_18 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_272: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_271, squeeze_19);  add_271 = squeeze_19 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_957: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_270, [512, 1, 16, 64, 1]);  add_270 = None
    permute_1072: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_957, [0, 4, 1, 2, 3]);  view_957 = None
    clone_54: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1072, memory_format = torch.contiguous_format);  permute_1072 = None
    view_958: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_54, [1, 512, 1024]);  clone_54 = None
    permute_1073: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_874, [0, 2, 1]);  view_874 = None
    bmm_205: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1073, view_958);  permute_1073 = None
    permute_1074: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_875, [0, 2, 1]);  view_875 = None
    bmm_206: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_958, permute_1074);  view_958 = permute_1074 = None
    view_959: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_205, [1024, 1, 16, 64, 1]);  bmm_205 = None
    permute_1075: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_959, [4, 1, 2, 3, 0]);  view_959 = None
    view_960: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_206, [512, 1024, 1, 1, 1]);  bmm_206 = None
    permute_1076: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_960, [0, 2, 3, 4, 1]);  view_960 = None
    permute_1077: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1075, [4, 2, 3, 0, 1]);  permute_1075 = None
    squeeze_20: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1077, 4);  permute_1077 = None
    squeeze_21: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_20, 3);  squeeze_20 = None
    permute_1078: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1076, [0, 1, 4, 2, 3]);  permute_1076 = None
    squeeze_22: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1078, 4);  permute_1078 = None
    squeeze_23: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_22, 3);  squeeze_22 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_273: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_272, squeeze_23);  add_272 = squeeze_23 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_82: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_252, getitem_279);  add_252 = getitem_279 = None
    mul_232: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_82, rsqrt_45);  sub_82 = None
    mul_233: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_273, primals_352);  primals_352 = None
    mul_234: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_233, 1024)
    sum_43: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_233, [2], True)
    mul_235: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_233, mul_232);  mul_233 = None
    sum_44: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_235, [2], True);  mul_235 = None
    mul_236: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_232, sum_44);  sum_44 = None
    sub_83: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_234, sum_43);  mul_234 = sum_43 = None
    sub_84: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_83, mul_236);  sub_83 = mul_236 = None
    div_29: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_45, 1024);  rsqrt_45 = None
    mul_237: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_29, sub_84);  div_29 = sub_84 = None
    mul_238: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_273, mul_232);  mul_232 = None
    sum_45: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_238, [0, 1]);  mul_238 = None
    sum_46: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_273, [0, 1]);  add_273 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_11: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_277, torch.float32);  getitem_277 = None
    mul_239: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_11, 1.1111111111111112);  convert_element_type_11 = None
    mul_240: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_237, mul_239);  mul_239 = None
    clone_55: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_240, memory_format = torch.contiguous_format);  mul_240 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_961: "f32[512, 1024]" = torch.ops.aten.view.default(clone_55, [512, 1024]);  clone_55 = None
    permute_1079: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_968, [1, 0]);  permute_968 = None
    mm_6: "f32[512, 4096]" = torch.ops.aten.mm.default(view_961, permute_1079);  permute_1079 = None
    permute_1080: "f32[1024, 512]" = torch.ops.aten.permute.default(view_961, [1, 0])
    mm_7: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1080, view_872);  permute_1080 = view_872 = None
    permute_1081: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_7, [1, 0]);  mm_7 = None
    sum_47: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_961, [0], True);  view_961 = None
    view_962: "f32[1024]" = torch.ops.aten.view.default(sum_47, [1024]);  sum_47 = None
    permute_1082: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1081, [1, 0]);  permute_1081 = None
    view_963: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_6, [512, 1, 4096]);  mm_6 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_12: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_275, torch.float32);  getitem_275 = None
    mul_241: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_12, 1.1111111111111112);  convert_element_type_12 = None
    mul_242: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_963, mul_241);  view_963 = mul_241 = None
    clone_56: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_242, memory_format = torch.contiguous_format);  mul_242 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_243: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_871, 0.7071067811865476)
    erf_25: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_243);  mul_243 = None
    add_274: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_25, 1);  erf_25 = None
    mul_244: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_274, 0.5);  add_274 = None
    mul_245: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_871, view_871)
    mul_246: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_245, -0.5);  mul_245 = None
    exp_27: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_246);  mul_246 = None
    mul_247: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_27, 0.3989422804014327);  exp_27 = None
    mul_248: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_871, mul_247);  view_871 = mul_247 = None
    add_275: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_244, mul_248);  mul_244 = mul_248 = None
    mul_249: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_56, add_275);  clone_56 = add_275 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_964: "f32[512, 4096]" = torch.ops.aten.view.default(mul_249, [512, 4096]);  mul_249 = None
    permute_1083: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_967, [1, 0]);  permute_967 = None
    mm_8: "f32[512, 1024]" = torch.ops.aten.mm.default(view_964, permute_1083);  permute_1083 = None
    permute_1084: "f32[4096, 512]" = torch.ops.aten.permute.default(view_964, [1, 0])
    mm_9: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1084, view_870);  permute_1084 = view_870 = None
    permute_1085: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_9, [1, 0]);  mm_9 = None
    sum_48: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_964, [0], True);  view_964 = None
    view_965: "f32[4096]" = torch.ops.aten.view.default(sum_48, [4096]);  sum_48 = None
    permute_1086: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1085, [1, 0]);  permute_1085 = None
    view_966: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_8, [512, 1, 1024]);  mm_8 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_276: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_237, view_966);  mul_237 = view_966 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_85: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_248, getitem_273);  add_248 = getitem_273 = None
    mul_250: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_85, rsqrt_44);  sub_85 = None
    mul_251: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_276, primals_346);  primals_346 = None
    mul_252: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_251, 1024)
    sum_49: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_251, [2], True)
    mul_253: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_251, mul_250);  mul_251 = None
    sum_50: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_253, [2], True);  mul_253 = None
    mul_254: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_250, sum_50);  sum_50 = None
    sub_86: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_252, sum_49);  mul_252 = sum_49 = None
    sub_87: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_86, mul_254);  sub_86 = mul_254 = None
    div_30: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_44, 1024);  rsqrt_44 = None
    mul_255: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_30, sub_87);  div_30 = sub_87 = None
    mul_256: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_276, mul_250);  mul_250 = None
    sum_51: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_256, [0, 1]);  mul_256 = None
    sum_52: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_276, [0, 1]);  add_276 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_13: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_271, torch.float32);  getitem_271 = None
    mul_257: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_13, 1.1111111111111112);  convert_element_type_13 = None
    mul_258: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_255, mul_257);  mul_257 = None
    clone_57: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_258, memory_format = torch.contiguous_format);  mul_258 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_967: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_57, [512, 1, 1024, 1, 1]);  clone_57 = None
    permute_1087: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_967, [0, 3, 4, 1, 2]);  view_967 = None
    view_968: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1087, [1, 512, 1024]);  permute_1087 = None
    permute_1088: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_866, [0, 2, 1]);  view_866 = None
    bmm_207: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1088, view_968);  permute_1088 = None
    permute_1089: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_867, [0, 2, 1]);  view_867 = None
    bmm_208: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_968, permute_1089);  view_968 = permute_1089 = None
    view_969: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_207, [64, 16, 1, 1024, 1]);  bmm_207 = None
    permute_1090: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_969, [4, 2, 3, 0, 1]);  view_969 = None
    view_970: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_208, [512, 64, 16, 1, 1]);  bmm_208 = None
    permute_1091: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_970, [0, 3, 4, 1, 2]);  view_970 = None
    permute_1092: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1090, [2, 4, 3, 0, 1]);  permute_1090 = None
    squeeze_24: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1092, 4);  permute_1092 = None
    squeeze_25: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_24, 3);  squeeze_24 = None
    permute_1093: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1091, [0, 1, 4, 3, 2]);  permute_1091 = None
    squeeze_26: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1093, 4);  permute_1093 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_971: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_26, [512, 1, 16, 64, 1]);  squeeze_26 = None
    permute_1094: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_971, [2, 0, 4, 1, 3]);  view_971 = None
    view_972: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1094, [16, 512, 64]);  permute_1094 = None
    permute_1095: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_862, [0, 2, 1]);  view_862 = None
    bmm_209: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1095, view_972);  permute_1095 = None
    permute_1096: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_863, [0, 2, 1]);  view_863 = None
    bmm_210: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_972, permute_1096);  view_972 = permute_1096 = None
    view_973: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_209, [16, 512, 1, 64, 1]);  bmm_209 = None
    permute_1097: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_973, [4, 2, 0, 3, 1]);  view_973 = None
    view_974: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_210, [16, 512, 512, 1, 1]);  bmm_210 = None
    permute_1098: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_974, [1, 3, 0, 4, 2]);  view_974 = None
    permute_1099: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1097, [4, 1, 2, 3, 0]);  permute_1097 = None
    squeeze_27: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1099, 4);  permute_1099 = None
    permute_1100: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1098, [1, 2, 0, 4, 3]);  permute_1098 = None
    squeeze_28: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1100, 4);  permute_1100 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_14: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_269, torch.float32);  getitem_269 = None
    mul_259: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_14, 1.1111111111111112);  convert_element_type_14 = None
    mul_260: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_28, mul_259);  squeeze_28 = mul_259 = None
    clone_58: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_260, memory_format = torch.contiguous_format);  mul_260 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_27: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_22);  alias_22 = None
    mul_261: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_58, alias_27);  clone_58 = None
    sum_53: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_261, [3], True)
    mul_262: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_27, sum_53);  alias_27 = sum_53 = None
    sub_88: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_261, mul_262);  mul_261 = mul_262 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_263: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_88, 0.125);  sub_88 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_6: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_1: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_6, [None, None, None, iota_24], mul_263, True);  full_6 = iota_24 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_975: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_1, [1, 16, 1023, 512]);  index_put_1 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_7: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_4: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_7, view_975, 3, 0, 9223372036854775807);  full_7 = view_975 = None
    full_8: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_5: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_8, slice_scatter_4, 2, 1, 9223372036854775807);  full_8 = slice_scatter_4 = None
    full_9: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_6: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_9, slice_scatter_5, 1, 0, 9223372036854775807);  full_9 = slice_scatter_5 = None
    full_10: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_7: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_10, slice_scatter_6, 0, 0, 9223372036854775807);  full_10 = slice_scatter_6 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_976: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_7, [1, 16, 512, 1024]);  slice_scatter_7 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_977: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_976, [1, 16, 512, 1024, 1]);  view_976 = None
    permute_1101: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_977, [1, 2, 4, 0, 3]);  view_977 = None
    view_978: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1101, [16, 512, 1024]);  permute_1101 = None
    permute_1102: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_856, [0, 2, 1]);  view_856 = None
    bmm_211: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1102, view_978);  permute_1102 = None
    permute_1103: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_857, [0, 2, 1]);  view_857 = None
    bmm_212: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_978, permute_1103);  view_978 = permute_1103 = None
    view_979: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_211, [16, 64, 1, 1024, 1]);  bmm_211 = None
    permute_1104: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_979, [2, 0, 4, 3, 1]);  view_979 = None
    view_980: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_212, [16, 512, 64, 1, 1]);  bmm_212 = None
    permute_1105: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_980, [3, 0, 1, 4, 2]);  view_980 = None
    permute_1106: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1104, [3, 0, 1, 4, 2]);  permute_1104 = None
    squeeze_29: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1106, 4);  permute_1106 = None
    permute_1107: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1105, [2, 0, 1, 4, 3]);  permute_1105 = None
    squeeze_30: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1107, 4);  permute_1107 = None
    sum_54: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_30, [0, 1], True)
    view_981: "f32[16, 64]" = torch.ops.aten.view.default(sum_54, [16, 64]);  sum_54 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_982: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_263, [1, 16, 512, 512, 1]);  mul_263 = None
    permute_1108: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_982, [1, 2, 4, 0, 3]);  view_982 = None
    view_983: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1108, [16, 512, 512]);  permute_1108 = None
    permute_1109: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_852, [0, 2, 1]);  view_852 = None
    bmm_213: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1109, view_983);  permute_1109 = None
    permute_1110: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_853, [0, 2, 1]);  view_853 = None
    bmm_214: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_983, permute_1110);  view_983 = permute_1110 = None
    view_984: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_213, [16, 64, 1, 512, 1]);  bmm_213 = None
    permute_1111: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_984, [2, 0, 4, 3, 1]);  view_984 = None
    view_985: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_214, [16, 512, 64, 1, 1]);  bmm_214 = None
    permute_1112: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_985, [3, 0, 1, 4, 2]);  view_985 = None
    permute_1113: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1111, [3, 0, 1, 4, 2]);  permute_1111 = None
    squeeze_31: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1113, 4);  permute_1113 = None
    permute_1114: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1112, [2, 0, 1, 4, 3]);  permute_1112 = None
    squeeze_32: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1114, 4);  permute_1114 = None
    sum_55: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_32, [0, 1], True)
    view_986: "f32[16, 64]" = torch.ops.aten.view.default(sum_55, [16, 64]);  sum_55 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_277: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_30, squeeze_32);  squeeze_30 = squeeze_32 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_987: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_29, [1024, 1, 16, 64, 1]);  squeeze_29 = None
    permute_1115: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_987, [0, 4, 1, 2, 3]);  view_987 = None
    view_988: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1115, [1, 1024, 1024]);  permute_1115 = None
    permute_1116: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_848, [0, 2, 1]);  view_848 = None
    bmm_215: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1116, view_988);  permute_1116 = view_988 = None
    view_989: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_215, [1024, 1, 16, 64, 1]);  bmm_215 = None
    permute_1117: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_989, [4, 1, 2, 3, 0]);  view_989 = None
    permute_1118: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1117, [4, 2, 3, 0, 1]);  permute_1117 = None
    squeeze_33: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1118, 4);  permute_1118 = None
    squeeze_34: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_33, 3);  squeeze_33 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_990: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_27, [512, 1, 16, 64, 1]);  squeeze_27 = None
    permute_1119: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_990, [0, 4, 1, 2, 3]);  view_990 = None
    clone_59: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1119, memory_format = torch.contiguous_format);  permute_1119 = None
    view_991: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_59, [1, 512, 1024]);  clone_59 = None
    permute_1120: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_844, [0, 2, 1]);  view_844 = None
    bmm_216: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1120, view_991);  permute_1120 = None
    permute_1121: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_845, [0, 2, 1]);  view_845 = None
    bmm_217: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_991, permute_1121);  view_991 = permute_1121 = None
    view_992: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_216, [1024, 1, 16, 64, 1]);  bmm_216 = None
    permute_1122: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_992, [4, 1, 2, 3, 0]);  view_992 = None
    view_993: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_217, [512, 1024, 1, 1, 1]);  bmm_217 = None
    permute_1123: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_993, [0, 2, 3, 4, 1]);  view_993 = None
    permute_1124: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1122, [4, 2, 3, 0, 1]);  permute_1122 = None
    squeeze_35: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1124, 4);  permute_1124 = None
    squeeze_36: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_35, 3);  squeeze_35 = None
    permute_1125: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1123, [0, 1, 4, 2, 3]);  permute_1123 = None
    squeeze_37: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1125, 4);  permute_1125 = None
    squeeze_38: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_37, 3);  squeeze_37 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_278: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_255, squeeze_38);  mul_255 = squeeze_38 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_994: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_31, [512, 1, 16, 64, 1]);  squeeze_31 = None
    permute_1126: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_994, [0, 4, 1, 2, 3]);  view_994 = None
    view_995: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1126, [1, 512, 1024]);  permute_1126 = None
    permute_1127: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_840, [0, 2, 1]);  view_840 = None
    bmm_218: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1127, view_995);  permute_1127 = None
    permute_1128: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_841, [0, 2, 1]);  view_841 = None
    bmm_219: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_995, permute_1128);  view_995 = permute_1128 = None
    view_996: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_218, [1024, 1, 16, 64, 1]);  bmm_218 = None
    permute_1129: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_996, [4, 1, 2, 3, 0]);  view_996 = None
    view_997: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_219, [512, 1024, 1, 1, 1]);  bmm_219 = None
    permute_1130: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_997, [0, 2, 3, 4, 1]);  view_997 = None
    permute_1131: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1129, [4, 2, 3, 0, 1]);  permute_1129 = None
    squeeze_39: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1131, 4);  permute_1131 = None
    squeeze_40: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_39, 3);  squeeze_39 = None
    permute_1132: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1130, [0, 1, 4, 2, 3]);  permute_1130 = None
    squeeze_41: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1132, 4);  permute_1132 = None
    squeeze_42: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_41, 3);  squeeze_41 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_279: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_278, squeeze_42);  add_278 = squeeze_42 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_998: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_277, [512, 1, 16, 64, 1]);  add_277 = None
    permute_1133: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_998, [0, 4, 1, 2, 3]);  view_998 = None
    clone_60: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1133, memory_format = torch.contiguous_format);  permute_1133 = None
    view_999: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_60, [1, 512, 1024]);  clone_60 = None
    permute_1134: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_836, [0, 2, 1]);  view_836 = None
    bmm_220: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1134, view_999);  permute_1134 = None
    permute_1135: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_837, [0, 2, 1]);  view_837 = None
    bmm_221: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_999, permute_1135);  view_999 = permute_1135 = None
    view_1000: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_220, [1024, 1, 16, 64, 1]);  bmm_220 = None
    permute_1136: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1000, [4, 1, 2, 3, 0]);  view_1000 = None
    view_1001: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_221, [512, 1024, 1, 1, 1]);  bmm_221 = None
    permute_1137: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1001, [0, 2, 3, 4, 1]);  view_1001 = None
    permute_1138: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1136, [4, 2, 3, 0, 1]);  permute_1136 = None
    squeeze_43: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1138, 4);  permute_1138 = None
    squeeze_44: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_43, 3);  squeeze_43 = None
    permute_1139: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1137, [0, 1, 4, 2, 3]);  permute_1137 = None
    squeeze_45: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1139, 4);  permute_1139 = None
    squeeze_46: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_45, 3);  squeeze_45 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_280: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_279, squeeze_46);  add_279 = squeeze_46 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_89: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_241, getitem_267);  add_241 = getitem_267 = None
    mul_264: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_89, rsqrt_43);  sub_89 = None
    mul_265: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_280, primals_344);  primals_344 = None
    mul_266: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_265, 1024)
    sum_56: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_265, [2], True)
    mul_267: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_265, mul_264);  mul_265 = None
    sum_57: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_267, [2], True);  mul_267 = None
    mul_268: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_264, sum_57);  sum_57 = None
    sub_90: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_266, sum_56);  mul_266 = sum_56 = None
    sub_91: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_90, mul_268);  sub_90 = mul_268 = None
    div_31: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_43, 1024);  rsqrt_43 = None
    mul_269: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_31, sub_91);  div_31 = sub_91 = None
    mul_270: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_280, mul_264);  mul_264 = None
    sum_58: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_270, [0, 1]);  mul_270 = None
    sum_59: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_280, [0, 1]);  add_280 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_15: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_265, torch.float32);  getitem_265 = None
    mul_271: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_15, 1.1111111111111112);  convert_element_type_15 = None
    mul_272: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_269, mul_271);  mul_271 = None
    clone_61: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_272, memory_format = torch.contiguous_format);  mul_272 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1002: "f32[512, 1024]" = torch.ops.aten.view.default(clone_61, [512, 1024]);  clone_61 = None
    permute_1140: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_926, [1, 0]);  permute_926 = None
    mm_10: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1002, permute_1140);  permute_1140 = None
    permute_1141: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1002, [1, 0])
    mm_11: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1141, view_834);  permute_1141 = view_834 = None
    permute_1142: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_11, [1, 0]);  mm_11 = None
    sum_60: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1002, [0], True);  view_1002 = None
    view_1003: "f32[1024]" = torch.ops.aten.view.default(sum_60, [1024]);  sum_60 = None
    permute_1143: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1142, [1, 0]);  permute_1142 = None
    view_1004: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_10, [512, 1, 4096]);  mm_10 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_16: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_263, torch.float32);  getitem_263 = None
    mul_273: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_16, 1.1111111111111112);  convert_element_type_16 = None
    mul_274: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1004, mul_273);  view_1004 = mul_273 = None
    clone_62: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_274, memory_format = torch.contiguous_format);  mul_274 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_275: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_833, 0.7071067811865476)
    erf_26: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_275);  mul_275 = None
    add_281: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_26, 1);  erf_26 = None
    mul_276: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_281, 0.5);  add_281 = None
    mul_277: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_833, view_833)
    mul_278: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_277, -0.5);  mul_277 = None
    exp_28: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_278);  mul_278 = None
    mul_279: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_28, 0.3989422804014327);  exp_28 = None
    mul_280: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_833, mul_279);  view_833 = mul_279 = None
    add_282: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_276, mul_280);  mul_276 = mul_280 = None
    mul_281: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_62, add_282);  clone_62 = add_282 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1005: "f32[512, 4096]" = torch.ops.aten.view.default(mul_281, [512, 4096]);  mul_281 = None
    permute_1144: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_925, [1, 0]);  permute_925 = None
    mm_12: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1005, permute_1144);  permute_1144 = None
    permute_1145: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1005, [1, 0])
    mm_13: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1145, view_832);  permute_1145 = view_832 = None
    permute_1146: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_13, [1, 0]);  mm_13 = None
    sum_61: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1005, [0], True);  view_1005 = None
    view_1006: "f32[4096]" = torch.ops.aten.view.default(sum_61, [4096]);  sum_61 = None
    permute_1147: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1146, [1, 0]);  permute_1146 = None
    view_1007: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_12, [512, 1, 1024]);  mm_12 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_283: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_269, view_1007);  mul_269 = view_1007 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_92: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_237, getitem_261);  add_237 = getitem_261 = None
    mul_282: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_92, rsqrt_42);  sub_92 = None
    mul_283: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_283, primals_338);  primals_338 = None
    mul_284: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_283, 1024)
    sum_62: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_283, [2], True)
    mul_285: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_283, mul_282);  mul_283 = None
    sum_63: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_285, [2], True);  mul_285 = None
    mul_286: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_282, sum_63);  sum_63 = None
    sub_93: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_284, sum_62);  mul_284 = sum_62 = None
    sub_94: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_93, mul_286);  sub_93 = mul_286 = None
    div_32: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_42, 1024);  rsqrt_42 = None
    mul_287: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_32, sub_94);  div_32 = sub_94 = None
    mul_288: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_283, mul_282);  mul_282 = None
    sum_64: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_288, [0, 1]);  mul_288 = None
    sum_65: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_283, [0, 1]);  add_283 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_17: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_259, torch.float32);  getitem_259 = None
    mul_289: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_17, 1.1111111111111112);  convert_element_type_17 = None
    mul_290: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_287, mul_289);  mul_289 = None
    clone_63: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_290, memory_format = torch.contiguous_format);  mul_290 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1008: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_63, [512, 1, 1024, 1, 1]);  clone_63 = None
    permute_1148: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1008, [0, 3, 4, 1, 2]);  view_1008 = None
    view_1009: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1148, [1, 512, 1024]);  permute_1148 = None
    permute_1149: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_828, [0, 2, 1]);  view_828 = None
    bmm_222: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1149, view_1009);  permute_1149 = None
    permute_1150: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_829, [0, 2, 1]);  view_829 = None
    bmm_223: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1009, permute_1150);  view_1009 = permute_1150 = None
    view_1010: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_222, [64, 16, 1, 1024, 1]);  bmm_222 = None
    permute_1151: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1010, [4, 2, 3, 0, 1]);  view_1010 = None
    view_1011: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_223, [512, 64, 16, 1, 1]);  bmm_223 = None
    permute_1152: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1011, [0, 3, 4, 1, 2]);  view_1011 = None
    permute_1153: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1151, [2, 4, 3, 0, 1]);  permute_1151 = None
    squeeze_47: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1153, 4);  permute_1153 = None
    squeeze_48: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_47, 3);  squeeze_47 = None
    permute_1154: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1152, [0, 1, 4, 3, 2]);  permute_1152 = None
    squeeze_49: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1154, 4);  permute_1154 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1012: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_49, [512, 1, 16, 64, 1]);  squeeze_49 = None
    permute_1155: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1012, [2, 0, 4, 1, 3]);  view_1012 = None
    view_1013: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1155, [16, 512, 64]);  permute_1155 = None
    permute_1156: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_824, [0, 2, 1]);  view_824 = None
    bmm_224: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1156, view_1013);  permute_1156 = None
    permute_1157: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_825, [0, 2, 1]);  view_825 = None
    bmm_225: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1013, permute_1157);  view_1013 = permute_1157 = None
    view_1014: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_224, [16, 512, 1, 64, 1]);  bmm_224 = None
    permute_1158: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1014, [4, 2, 0, 3, 1]);  view_1014 = None
    view_1015: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_225, [16, 512, 512, 1, 1]);  bmm_225 = None
    permute_1159: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1015, [1, 3, 0, 4, 2]);  view_1015 = None
    permute_1160: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1158, [4, 1, 2, 3, 0]);  permute_1158 = None
    squeeze_50: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1160, 4);  permute_1160 = None
    permute_1161: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1159, [1, 2, 0, 4, 3]);  permute_1159 = None
    squeeze_51: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1161, 4);  permute_1161 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_18: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_257, torch.float32);  getitem_257 = None
    mul_291: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_18, 1.1111111111111112);  convert_element_type_18 = None
    mul_292: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_51, mul_291);  squeeze_51 = mul_291 = None
    clone_64: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_292, memory_format = torch.contiguous_format);  mul_292 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_28: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_21);  alias_21 = None
    mul_293: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_64, alias_28);  clone_64 = None
    sum_66: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_293, [3], True)
    mul_294: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_28, sum_66);  alias_28 = sum_66 = None
    sub_95: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_293, mul_294);  mul_293 = mul_294 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_295: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_95, 0.125);  sub_95 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_11: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_2: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_11, [None, None, None, iota_23], mul_295, True);  full_11 = iota_23 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1016: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_2, [1, 16, 1023, 512]);  index_put_2 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_12: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_8: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_12, view_1016, 3, 0, 9223372036854775807);  full_12 = view_1016 = None
    full_13: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_9: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_13, slice_scatter_8, 2, 1, 9223372036854775807);  full_13 = slice_scatter_8 = None
    full_14: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_10: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_14, slice_scatter_9, 1, 0, 9223372036854775807);  full_14 = slice_scatter_9 = None
    full_15: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_11: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_15, slice_scatter_10, 0, 0, 9223372036854775807);  full_15 = slice_scatter_10 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1017: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_11, [1, 16, 512, 1024]);  slice_scatter_11 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1018: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1017, [1, 16, 512, 1024, 1]);  view_1017 = None
    permute_1162: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1018, [1, 2, 4, 0, 3]);  view_1018 = None
    view_1019: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1162, [16, 512, 1024]);  permute_1162 = None
    permute_1163: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_818, [0, 2, 1]);  view_818 = None
    bmm_226: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1163, view_1019);  permute_1163 = None
    permute_1164: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_819, [0, 2, 1]);  view_819 = None
    bmm_227: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1019, permute_1164);  view_1019 = permute_1164 = None
    view_1020: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_226, [16, 64, 1, 1024, 1]);  bmm_226 = None
    permute_1165: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1020, [2, 0, 4, 3, 1]);  view_1020 = None
    view_1021: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_227, [16, 512, 64, 1, 1]);  bmm_227 = None
    permute_1166: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1021, [3, 0, 1, 4, 2]);  view_1021 = None
    permute_1167: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1165, [3, 0, 1, 4, 2]);  permute_1165 = None
    squeeze_52: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1167, 4);  permute_1167 = None
    permute_1168: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1166, [2, 0, 1, 4, 3]);  permute_1166 = None
    squeeze_53: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1168, 4);  permute_1168 = None
    sum_67: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_53, [0, 1], True)
    view_1022: "f32[16, 64]" = torch.ops.aten.view.default(sum_67, [16, 64]);  sum_67 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1023: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_295, [1, 16, 512, 512, 1]);  mul_295 = None
    permute_1169: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1023, [1, 2, 4, 0, 3]);  view_1023 = None
    view_1024: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1169, [16, 512, 512]);  permute_1169 = None
    permute_1170: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_814, [0, 2, 1]);  view_814 = None
    bmm_228: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1170, view_1024);  permute_1170 = None
    permute_1171: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_815, [0, 2, 1]);  view_815 = None
    bmm_229: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1024, permute_1171);  view_1024 = permute_1171 = None
    view_1025: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_228, [16, 64, 1, 512, 1]);  bmm_228 = None
    permute_1172: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1025, [2, 0, 4, 3, 1]);  view_1025 = None
    view_1026: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_229, [16, 512, 64, 1, 1]);  bmm_229 = None
    permute_1173: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1026, [3, 0, 1, 4, 2]);  view_1026 = None
    permute_1174: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1172, [3, 0, 1, 4, 2]);  permute_1172 = None
    squeeze_54: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1174, 4);  permute_1174 = None
    permute_1175: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1173, [2, 0, 1, 4, 3]);  permute_1173 = None
    squeeze_55: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1175, 4);  permute_1175 = None
    sum_68: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_55, [0, 1], True)
    view_1027: "f32[16, 64]" = torch.ops.aten.view.default(sum_68, [16, 64]);  sum_68 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_284: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_53, squeeze_55);  squeeze_53 = squeeze_55 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1028: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_52, [1024, 1, 16, 64, 1]);  squeeze_52 = None
    permute_1176: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1028, [0, 4, 1, 2, 3]);  view_1028 = None
    view_1029: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1176, [1, 1024, 1024]);  permute_1176 = None
    permute_1177: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_810, [0, 2, 1]);  view_810 = None
    bmm_230: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1177, view_1029);  permute_1177 = view_1029 = None
    view_1030: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_230, [1024, 1, 16, 64, 1]);  bmm_230 = None
    permute_1178: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1030, [4, 1, 2, 3, 0]);  view_1030 = None
    permute_1179: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1178, [4, 2, 3, 0, 1]);  permute_1178 = None
    squeeze_56: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1179, 4);  permute_1179 = None
    squeeze_57: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_56, 3);  squeeze_56 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1031: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_50, [512, 1, 16, 64, 1]);  squeeze_50 = None
    permute_1180: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1031, [0, 4, 1, 2, 3]);  view_1031 = None
    clone_65: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1180, memory_format = torch.contiguous_format);  permute_1180 = None
    view_1032: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_65, [1, 512, 1024]);  clone_65 = None
    permute_1181: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_806, [0, 2, 1]);  view_806 = None
    bmm_231: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1181, view_1032);  permute_1181 = None
    permute_1182: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_807, [0, 2, 1]);  view_807 = None
    bmm_232: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1032, permute_1182);  view_1032 = permute_1182 = None
    view_1033: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_231, [1024, 1, 16, 64, 1]);  bmm_231 = None
    permute_1183: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1033, [4, 1, 2, 3, 0]);  view_1033 = None
    view_1034: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_232, [512, 1024, 1, 1, 1]);  bmm_232 = None
    permute_1184: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1034, [0, 2, 3, 4, 1]);  view_1034 = None
    permute_1185: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1183, [4, 2, 3, 0, 1]);  permute_1183 = None
    squeeze_58: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1185, 4);  permute_1185 = None
    squeeze_59: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_58, 3);  squeeze_58 = None
    permute_1186: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1184, [0, 1, 4, 2, 3]);  permute_1184 = None
    squeeze_60: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1186, 4);  permute_1186 = None
    squeeze_61: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_60, 3);  squeeze_60 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_285: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_287, squeeze_61);  mul_287 = squeeze_61 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1035: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_54, [512, 1, 16, 64, 1]);  squeeze_54 = None
    permute_1187: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1035, [0, 4, 1, 2, 3]);  view_1035 = None
    view_1036: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1187, [1, 512, 1024]);  permute_1187 = None
    permute_1188: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_802, [0, 2, 1]);  view_802 = None
    bmm_233: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1188, view_1036);  permute_1188 = None
    permute_1189: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_803, [0, 2, 1]);  view_803 = None
    bmm_234: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1036, permute_1189);  view_1036 = permute_1189 = None
    view_1037: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_233, [1024, 1, 16, 64, 1]);  bmm_233 = None
    permute_1190: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1037, [4, 1, 2, 3, 0]);  view_1037 = None
    view_1038: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_234, [512, 1024, 1, 1, 1]);  bmm_234 = None
    permute_1191: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1038, [0, 2, 3, 4, 1]);  view_1038 = None
    permute_1192: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1190, [4, 2, 3, 0, 1]);  permute_1190 = None
    squeeze_62: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1192, 4);  permute_1192 = None
    squeeze_63: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_62, 3);  squeeze_62 = None
    permute_1193: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1191, [0, 1, 4, 2, 3]);  permute_1191 = None
    squeeze_64: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1193, 4);  permute_1193 = None
    squeeze_65: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_64, 3);  squeeze_64 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_286: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_285, squeeze_65);  add_285 = squeeze_65 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1039: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_284, [512, 1, 16, 64, 1]);  add_284 = None
    permute_1194: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1039, [0, 4, 1, 2, 3]);  view_1039 = None
    clone_66: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1194, memory_format = torch.contiguous_format);  permute_1194 = None
    view_1040: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_66, [1, 512, 1024]);  clone_66 = None
    permute_1195: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_798, [0, 2, 1]);  view_798 = None
    bmm_235: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1195, view_1040);  permute_1195 = None
    permute_1196: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_799, [0, 2, 1]);  view_799 = None
    bmm_236: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1040, permute_1196);  view_1040 = permute_1196 = None
    view_1041: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_235, [1024, 1, 16, 64, 1]);  bmm_235 = None
    permute_1197: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1041, [4, 1, 2, 3, 0]);  view_1041 = None
    view_1042: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_236, [512, 1024, 1, 1, 1]);  bmm_236 = None
    permute_1198: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1042, [0, 2, 3, 4, 1]);  view_1042 = None
    permute_1199: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1197, [4, 2, 3, 0, 1]);  permute_1197 = None
    squeeze_66: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1199, 4);  permute_1199 = None
    squeeze_67: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_66, 3);  squeeze_66 = None
    permute_1200: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1198, [0, 1, 4, 2, 3]);  permute_1198 = None
    squeeze_68: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1200, 4);  permute_1200 = None
    squeeze_69: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_68, 3);  squeeze_68 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_287: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_286, squeeze_69);  add_286 = squeeze_69 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_96: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_230, getitem_255);  add_230 = getitem_255 = None
    mul_296: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_96, rsqrt_41);  sub_96 = None
    mul_297: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_287, primals_336);  primals_336 = None
    mul_298: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_297, 1024)
    sum_69: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_297, [2], True)
    mul_299: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_297, mul_296);  mul_297 = None
    sum_70: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_299, [2], True);  mul_299 = None
    mul_300: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_296, sum_70);  sum_70 = None
    sub_97: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_298, sum_69);  mul_298 = sum_69 = None
    sub_98: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_97, mul_300);  sub_97 = mul_300 = None
    div_33: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_41, 1024);  rsqrt_41 = None
    mul_301: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_33, sub_98);  div_33 = sub_98 = None
    mul_302: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_287, mul_296);  mul_296 = None
    sum_71: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_302, [0, 1]);  mul_302 = None
    sum_72: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_287, [0, 1]);  add_287 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_19: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_253, torch.float32);  getitem_253 = None
    mul_303: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_19, 1.1111111111111112);  convert_element_type_19 = None
    mul_304: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_301, mul_303);  mul_303 = None
    clone_67: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_304, memory_format = torch.contiguous_format);  mul_304 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1043: "f32[512, 1024]" = torch.ops.aten.view.default(clone_67, [512, 1024]);  clone_67 = None
    permute_1201: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_884, [1, 0]);  permute_884 = None
    mm_14: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1043, permute_1201);  permute_1201 = None
    permute_1202: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1043, [1, 0])
    mm_15: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1202, view_796);  permute_1202 = view_796 = None
    permute_1203: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_15, [1, 0]);  mm_15 = None
    sum_73: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1043, [0], True);  view_1043 = None
    view_1044: "f32[1024]" = torch.ops.aten.view.default(sum_73, [1024]);  sum_73 = None
    permute_1204: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1203, [1, 0]);  permute_1203 = None
    view_1045: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_14, [512, 1, 4096]);  mm_14 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_20: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_251, torch.float32);  getitem_251 = None
    mul_305: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_20, 1.1111111111111112);  convert_element_type_20 = None
    mul_306: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1045, mul_305);  view_1045 = mul_305 = None
    clone_68: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_306, memory_format = torch.contiguous_format);  mul_306 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_307: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_795, 0.7071067811865476)
    erf_27: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_307);  mul_307 = None
    add_288: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_27, 1);  erf_27 = None
    mul_308: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_288, 0.5);  add_288 = None
    mul_309: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_795, view_795)
    mul_310: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_309, -0.5);  mul_309 = None
    exp_29: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_310);  mul_310 = None
    mul_311: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_29, 0.3989422804014327);  exp_29 = None
    mul_312: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_795, mul_311);  view_795 = mul_311 = None
    add_289: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_308, mul_312);  mul_308 = mul_312 = None
    mul_313: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_68, add_289);  clone_68 = add_289 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1046: "f32[512, 4096]" = torch.ops.aten.view.default(mul_313, [512, 4096]);  mul_313 = None
    permute_1205: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_883, [1, 0]);  permute_883 = None
    mm_16: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1046, permute_1205);  permute_1205 = None
    permute_1206: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1046, [1, 0])
    mm_17: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1206, view_794);  permute_1206 = view_794 = None
    permute_1207: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_17, [1, 0]);  mm_17 = None
    sum_74: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1046, [0], True);  view_1046 = None
    view_1047: "f32[4096]" = torch.ops.aten.view.default(sum_74, [4096]);  sum_74 = None
    permute_1208: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1207, [1, 0]);  permute_1207 = None
    view_1048: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_16, [512, 1, 1024]);  mm_16 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_290: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_301, view_1048);  mul_301 = view_1048 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_99: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_226, getitem_249);  add_226 = getitem_249 = None
    mul_314: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_99, rsqrt_40);  sub_99 = None
    mul_315: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_290, primals_330);  primals_330 = None
    mul_316: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_315, 1024)
    sum_75: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_315, [2], True)
    mul_317: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_315, mul_314);  mul_315 = None
    sum_76: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_317, [2], True);  mul_317 = None
    mul_318: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_314, sum_76);  sum_76 = None
    sub_100: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_316, sum_75);  mul_316 = sum_75 = None
    sub_101: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_100, mul_318);  sub_100 = mul_318 = None
    div_34: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_40, 1024);  rsqrt_40 = None
    mul_319: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_34, sub_101);  div_34 = sub_101 = None
    mul_320: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_290, mul_314);  mul_314 = None
    sum_77: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_320, [0, 1]);  mul_320 = None
    sum_78: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_290, [0, 1]);  add_290 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_21: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_247, torch.float32);  getitem_247 = None
    mul_321: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_21, 1.1111111111111112);  convert_element_type_21 = None
    mul_322: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_319, mul_321);  mul_321 = None
    clone_69: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_322, memory_format = torch.contiguous_format);  mul_322 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1049: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_69, [512, 1, 1024, 1, 1]);  clone_69 = None
    permute_1209: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1049, [0, 3, 4, 1, 2]);  view_1049 = None
    view_1050: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1209, [1, 512, 1024]);  permute_1209 = None
    permute_1210: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_790, [0, 2, 1]);  view_790 = None
    bmm_237: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1210, view_1050);  permute_1210 = None
    permute_1211: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_791, [0, 2, 1]);  view_791 = None
    bmm_238: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1050, permute_1211);  view_1050 = permute_1211 = None
    view_1051: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_237, [64, 16, 1, 1024, 1]);  bmm_237 = None
    permute_1212: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1051, [4, 2, 3, 0, 1]);  view_1051 = None
    view_1052: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_238, [512, 64, 16, 1, 1]);  bmm_238 = None
    permute_1213: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1052, [0, 3, 4, 1, 2]);  view_1052 = None
    permute_1214: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1212, [2, 4, 3, 0, 1]);  permute_1212 = None
    squeeze_70: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1214, 4);  permute_1214 = None
    squeeze_71: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_70, 3);  squeeze_70 = None
    permute_1215: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1213, [0, 1, 4, 3, 2]);  permute_1213 = None
    squeeze_72: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1215, 4);  permute_1215 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1053: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_72, [512, 1, 16, 64, 1]);  squeeze_72 = None
    permute_1216: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1053, [2, 0, 4, 1, 3]);  view_1053 = None
    view_1054: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1216, [16, 512, 64]);  permute_1216 = None
    permute_1217: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_786, [0, 2, 1]);  view_786 = None
    bmm_239: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1217, view_1054);  permute_1217 = None
    permute_1218: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_787, [0, 2, 1]);  view_787 = None
    bmm_240: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1054, permute_1218);  view_1054 = permute_1218 = None
    view_1055: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_239, [16, 512, 1, 64, 1]);  bmm_239 = None
    permute_1219: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1055, [4, 2, 0, 3, 1]);  view_1055 = None
    view_1056: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_240, [16, 512, 512, 1, 1]);  bmm_240 = None
    permute_1220: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1056, [1, 3, 0, 4, 2]);  view_1056 = None
    permute_1221: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1219, [4, 1, 2, 3, 0]);  permute_1219 = None
    squeeze_73: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1221, 4);  permute_1221 = None
    permute_1222: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1220, [1, 2, 0, 4, 3]);  permute_1220 = None
    squeeze_74: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1222, 4);  permute_1222 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_22: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_245, torch.float32);  getitem_245 = None
    mul_323: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_22, 1.1111111111111112);  convert_element_type_22 = None
    mul_324: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_74, mul_323);  squeeze_74 = mul_323 = None
    clone_70: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_324, memory_format = torch.contiguous_format);  mul_324 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_29: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_20);  alias_20 = None
    mul_325: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_70, alias_29);  clone_70 = None
    sum_79: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_325, [3], True)
    mul_326: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_29, sum_79);  alias_29 = sum_79 = None
    sub_102: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_325, mul_326);  mul_325 = mul_326 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_327: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_102, 0.125);  sub_102 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_16: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_3: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_16, [None, None, None, iota_22], mul_327, True);  full_16 = iota_22 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1057: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_3, [1, 16, 1023, 512]);  index_put_3 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_17: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_12: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_17, view_1057, 3, 0, 9223372036854775807);  full_17 = view_1057 = None
    full_18: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_13: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_18, slice_scatter_12, 2, 1, 9223372036854775807);  full_18 = slice_scatter_12 = None
    full_19: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_14: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_19, slice_scatter_13, 1, 0, 9223372036854775807);  full_19 = slice_scatter_13 = None
    full_20: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_15: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_20, slice_scatter_14, 0, 0, 9223372036854775807);  full_20 = slice_scatter_14 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1058: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_15, [1, 16, 512, 1024]);  slice_scatter_15 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1059: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1058, [1, 16, 512, 1024, 1]);  view_1058 = None
    permute_1223: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1059, [1, 2, 4, 0, 3]);  view_1059 = None
    view_1060: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1223, [16, 512, 1024]);  permute_1223 = None
    permute_1224: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_780, [0, 2, 1]);  view_780 = None
    bmm_241: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1224, view_1060);  permute_1224 = None
    permute_1225: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_781, [0, 2, 1]);  view_781 = None
    bmm_242: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1060, permute_1225);  view_1060 = permute_1225 = None
    view_1061: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_241, [16, 64, 1, 1024, 1]);  bmm_241 = None
    permute_1226: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1061, [2, 0, 4, 3, 1]);  view_1061 = None
    view_1062: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_242, [16, 512, 64, 1, 1]);  bmm_242 = None
    permute_1227: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1062, [3, 0, 1, 4, 2]);  view_1062 = None
    permute_1228: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1226, [3, 0, 1, 4, 2]);  permute_1226 = None
    squeeze_75: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1228, 4);  permute_1228 = None
    permute_1229: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1227, [2, 0, 1, 4, 3]);  permute_1227 = None
    squeeze_76: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1229, 4);  permute_1229 = None
    sum_80: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_76, [0, 1], True)
    view_1063: "f32[16, 64]" = torch.ops.aten.view.default(sum_80, [16, 64]);  sum_80 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1064: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_327, [1, 16, 512, 512, 1]);  mul_327 = None
    permute_1230: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1064, [1, 2, 4, 0, 3]);  view_1064 = None
    view_1065: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1230, [16, 512, 512]);  permute_1230 = None
    permute_1231: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_776, [0, 2, 1]);  view_776 = None
    bmm_243: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1231, view_1065);  permute_1231 = None
    permute_1232: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_777, [0, 2, 1]);  view_777 = None
    bmm_244: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1065, permute_1232);  view_1065 = permute_1232 = None
    view_1066: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_243, [16, 64, 1, 512, 1]);  bmm_243 = None
    permute_1233: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1066, [2, 0, 4, 3, 1]);  view_1066 = None
    view_1067: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_244, [16, 512, 64, 1, 1]);  bmm_244 = None
    permute_1234: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1067, [3, 0, 1, 4, 2]);  view_1067 = None
    permute_1235: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1233, [3, 0, 1, 4, 2]);  permute_1233 = None
    squeeze_77: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1235, 4);  permute_1235 = None
    permute_1236: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1234, [2, 0, 1, 4, 3]);  permute_1234 = None
    squeeze_78: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1236, 4);  permute_1236 = None
    sum_81: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_78, [0, 1], True)
    view_1068: "f32[16, 64]" = torch.ops.aten.view.default(sum_81, [16, 64]);  sum_81 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_291: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_76, squeeze_78);  squeeze_76 = squeeze_78 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1069: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_75, [1024, 1, 16, 64, 1]);  squeeze_75 = None
    permute_1237: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1069, [0, 4, 1, 2, 3]);  view_1069 = None
    view_1070: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1237, [1, 1024, 1024]);  permute_1237 = None
    permute_1238: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_772, [0, 2, 1]);  view_772 = None
    bmm_245: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1238, view_1070);  permute_1238 = view_1070 = None
    view_1071: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_245, [1024, 1, 16, 64, 1]);  bmm_245 = None
    permute_1239: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1071, [4, 1, 2, 3, 0]);  view_1071 = None
    permute_1240: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1239, [4, 2, 3, 0, 1]);  permute_1239 = None
    squeeze_79: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1240, 4);  permute_1240 = None
    squeeze_80: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_79, 3);  squeeze_79 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1072: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_73, [512, 1, 16, 64, 1]);  squeeze_73 = None
    permute_1241: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1072, [0, 4, 1, 2, 3]);  view_1072 = None
    clone_71: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1241, memory_format = torch.contiguous_format);  permute_1241 = None
    view_1073: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_71, [1, 512, 1024]);  clone_71 = None
    permute_1242: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_768, [0, 2, 1]);  view_768 = None
    bmm_246: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1242, view_1073);  permute_1242 = None
    permute_1243: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_769, [0, 2, 1]);  view_769 = None
    bmm_247: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1073, permute_1243);  view_1073 = permute_1243 = None
    view_1074: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_246, [1024, 1, 16, 64, 1]);  bmm_246 = None
    permute_1244: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1074, [4, 1, 2, 3, 0]);  view_1074 = None
    view_1075: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_247, [512, 1024, 1, 1, 1]);  bmm_247 = None
    permute_1245: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1075, [0, 2, 3, 4, 1]);  view_1075 = None
    permute_1246: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1244, [4, 2, 3, 0, 1]);  permute_1244 = None
    squeeze_81: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1246, 4);  permute_1246 = None
    squeeze_82: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_81, 3);  squeeze_81 = None
    permute_1247: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1245, [0, 1, 4, 2, 3]);  permute_1245 = None
    squeeze_83: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1247, 4);  permute_1247 = None
    squeeze_84: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_83, 3);  squeeze_83 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_292: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_319, squeeze_84);  mul_319 = squeeze_84 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1076: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_77, [512, 1, 16, 64, 1]);  squeeze_77 = None
    permute_1248: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1076, [0, 4, 1, 2, 3]);  view_1076 = None
    view_1077: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1248, [1, 512, 1024]);  permute_1248 = None
    permute_1249: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_764, [0, 2, 1]);  view_764 = None
    bmm_248: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1249, view_1077);  permute_1249 = None
    permute_1250: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_765, [0, 2, 1]);  view_765 = None
    bmm_249: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1077, permute_1250);  view_1077 = permute_1250 = None
    view_1078: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_248, [1024, 1, 16, 64, 1]);  bmm_248 = None
    permute_1251: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1078, [4, 1, 2, 3, 0]);  view_1078 = None
    view_1079: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_249, [512, 1024, 1, 1, 1]);  bmm_249 = None
    permute_1252: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1079, [0, 2, 3, 4, 1]);  view_1079 = None
    permute_1253: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1251, [4, 2, 3, 0, 1]);  permute_1251 = None
    squeeze_85: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1253, 4);  permute_1253 = None
    squeeze_86: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_85, 3);  squeeze_85 = None
    permute_1254: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1252, [0, 1, 4, 2, 3]);  permute_1252 = None
    squeeze_87: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1254, 4);  permute_1254 = None
    squeeze_88: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_87, 3);  squeeze_87 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_293: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_292, squeeze_88);  add_292 = squeeze_88 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1080: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_291, [512, 1, 16, 64, 1]);  add_291 = None
    permute_1255: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1080, [0, 4, 1, 2, 3]);  view_1080 = None
    clone_72: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1255, memory_format = torch.contiguous_format);  permute_1255 = None
    view_1081: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_72, [1, 512, 1024]);  clone_72 = None
    permute_1256: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_760, [0, 2, 1]);  view_760 = None
    bmm_250: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1256, view_1081);  permute_1256 = None
    permute_1257: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_761, [0, 2, 1]);  view_761 = None
    bmm_251: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1081, permute_1257);  view_1081 = permute_1257 = None
    view_1082: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_250, [1024, 1, 16, 64, 1]);  bmm_250 = None
    permute_1258: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1082, [4, 1, 2, 3, 0]);  view_1082 = None
    view_1083: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_251, [512, 1024, 1, 1, 1]);  bmm_251 = None
    permute_1259: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1083, [0, 2, 3, 4, 1]);  view_1083 = None
    permute_1260: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1258, [4, 2, 3, 0, 1]);  permute_1258 = None
    squeeze_89: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1260, 4);  permute_1260 = None
    squeeze_90: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_89, 3);  squeeze_89 = None
    permute_1261: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1259, [0, 1, 4, 2, 3]);  permute_1259 = None
    squeeze_91: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1261, 4);  permute_1261 = None
    squeeze_92: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_91, 3);  squeeze_91 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_294: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_293, squeeze_92);  add_293 = squeeze_92 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_103: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_219, getitem_243);  add_219 = getitem_243 = None
    mul_328: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_103, rsqrt_39);  sub_103 = None
    mul_329: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_294, primals_328);  primals_328 = None
    mul_330: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_329, 1024)
    sum_82: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_329, [2], True)
    mul_331: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_329, mul_328);  mul_329 = None
    sum_83: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_331, [2], True);  mul_331 = None
    mul_332: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_328, sum_83);  sum_83 = None
    sub_104: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_330, sum_82);  mul_330 = sum_82 = None
    sub_105: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_104, mul_332);  sub_104 = mul_332 = None
    div_35: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_39, 1024);  rsqrt_39 = None
    mul_333: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_35, sub_105);  div_35 = sub_105 = None
    mul_334: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_294, mul_328);  mul_328 = None
    sum_84: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_334, [0, 1]);  mul_334 = None
    sum_85: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_294, [0, 1]);  add_294 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_23: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_241, torch.float32);  getitem_241 = None
    mul_335: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_23, 1.1111111111111112);  convert_element_type_23 = None
    mul_336: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_333, mul_335);  mul_335 = None
    clone_73: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_336, memory_format = torch.contiguous_format);  mul_336 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1084: "f32[512, 1024]" = torch.ops.aten.view.default(clone_73, [512, 1024]);  clone_73 = None
    permute_1262: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_842, [1, 0]);  permute_842 = None
    mm_18: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1084, permute_1262);  permute_1262 = None
    permute_1263: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1084, [1, 0])
    mm_19: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1263, view_758);  permute_1263 = view_758 = None
    permute_1264: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_19, [1, 0]);  mm_19 = None
    sum_86: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1084, [0], True);  view_1084 = None
    view_1085: "f32[1024]" = torch.ops.aten.view.default(sum_86, [1024]);  sum_86 = None
    permute_1265: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1264, [1, 0]);  permute_1264 = None
    view_1086: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_18, [512, 1, 4096]);  mm_18 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_24: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_239, torch.float32);  getitem_239 = None
    mul_337: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_24, 1.1111111111111112);  convert_element_type_24 = None
    mul_338: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1086, mul_337);  view_1086 = mul_337 = None
    clone_74: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_338, memory_format = torch.contiguous_format);  mul_338 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_339: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_757, 0.7071067811865476)
    erf_28: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_339);  mul_339 = None
    add_295: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_28, 1);  erf_28 = None
    mul_340: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_295, 0.5);  add_295 = None
    mul_341: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_757, view_757)
    mul_342: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_341, -0.5);  mul_341 = None
    exp_30: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_342);  mul_342 = None
    mul_343: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_30, 0.3989422804014327);  exp_30 = None
    mul_344: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_757, mul_343);  view_757 = mul_343 = None
    add_296: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_340, mul_344);  mul_340 = mul_344 = None
    mul_345: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_74, add_296);  clone_74 = add_296 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1087: "f32[512, 4096]" = torch.ops.aten.view.default(mul_345, [512, 4096]);  mul_345 = None
    permute_1266: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_841, [1, 0]);  permute_841 = None
    mm_20: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1087, permute_1266);  permute_1266 = None
    permute_1267: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1087, [1, 0])
    mm_21: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1267, view_756);  permute_1267 = view_756 = None
    permute_1268: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_21, [1, 0]);  mm_21 = None
    sum_87: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1087, [0], True);  view_1087 = None
    view_1088: "f32[4096]" = torch.ops.aten.view.default(sum_87, [4096]);  sum_87 = None
    permute_1269: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1268, [1, 0]);  permute_1268 = None
    view_1089: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_20, [512, 1, 1024]);  mm_20 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_297: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_333, view_1089);  mul_333 = view_1089 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_106: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_215, getitem_237);  add_215 = getitem_237 = None
    mul_346: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_106, rsqrt_38);  sub_106 = None
    mul_347: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_297, primals_322);  primals_322 = None
    mul_348: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_347, 1024)
    sum_88: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_347, [2], True)
    mul_349: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_347, mul_346);  mul_347 = None
    sum_89: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_349, [2], True);  mul_349 = None
    mul_350: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_346, sum_89);  sum_89 = None
    sub_107: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_348, sum_88);  mul_348 = sum_88 = None
    sub_108: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_107, mul_350);  sub_107 = mul_350 = None
    div_36: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_38, 1024);  rsqrt_38 = None
    mul_351: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_36, sub_108);  div_36 = sub_108 = None
    mul_352: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_297, mul_346);  mul_346 = None
    sum_90: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_352, [0, 1]);  mul_352 = None
    sum_91: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_297, [0, 1]);  add_297 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_25: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_235, torch.float32);  getitem_235 = None
    mul_353: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_25, 1.1111111111111112);  convert_element_type_25 = None
    mul_354: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_351, mul_353);  mul_353 = None
    clone_75: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_354, memory_format = torch.contiguous_format);  mul_354 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1090: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_75, [512, 1, 1024, 1, 1]);  clone_75 = None
    permute_1270: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1090, [0, 3, 4, 1, 2]);  view_1090 = None
    view_1091: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1270, [1, 512, 1024]);  permute_1270 = None
    permute_1271: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_752, [0, 2, 1]);  view_752 = None
    bmm_252: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1271, view_1091);  permute_1271 = None
    permute_1272: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_753, [0, 2, 1]);  view_753 = None
    bmm_253: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1091, permute_1272);  view_1091 = permute_1272 = None
    view_1092: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_252, [64, 16, 1, 1024, 1]);  bmm_252 = None
    permute_1273: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1092, [4, 2, 3, 0, 1]);  view_1092 = None
    view_1093: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_253, [512, 64, 16, 1, 1]);  bmm_253 = None
    permute_1274: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1093, [0, 3, 4, 1, 2]);  view_1093 = None
    permute_1275: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1273, [2, 4, 3, 0, 1]);  permute_1273 = None
    squeeze_93: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1275, 4);  permute_1275 = None
    squeeze_94: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_93, 3);  squeeze_93 = None
    permute_1276: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1274, [0, 1, 4, 3, 2]);  permute_1274 = None
    squeeze_95: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1276, 4);  permute_1276 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1094: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_95, [512, 1, 16, 64, 1]);  squeeze_95 = None
    permute_1277: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1094, [2, 0, 4, 1, 3]);  view_1094 = None
    view_1095: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1277, [16, 512, 64]);  permute_1277 = None
    permute_1278: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_748, [0, 2, 1]);  view_748 = None
    bmm_254: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1278, view_1095);  permute_1278 = None
    permute_1279: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_749, [0, 2, 1]);  view_749 = None
    bmm_255: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1095, permute_1279);  view_1095 = permute_1279 = None
    view_1096: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_254, [16, 512, 1, 64, 1]);  bmm_254 = None
    permute_1280: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1096, [4, 2, 0, 3, 1]);  view_1096 = None
    view_1097: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_255, [16, 512, 512, 1, 1]);  bmm_255 = None
    permute_1281: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1097, [1, 3, 0, 4, 2]);  view_1097 = None
    permute_1282: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1280, [4, 1, 2, 3, 0]);  permute_1280 = None
    squeeze_96: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1282, 4);  permute_1282 = None
    permute_1283: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1281, [1, 2, 0, 4, 3]);  permute_1281 = None
    squeeze_97: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1283, 4);  permute_1283 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_26: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_233, torch.float32);  getitem_233 = None
    mul_355: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_26, 1.1111111111111112);  convert_element_type_26 = None
    mul_356: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_97, mul_355);  squeeze_97 = mul_355 = None
    clone_76: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_356, memory_format = torch.contiguous_format);  mul_356 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_30: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_19);  alias_19 = None
    mul_357: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_76, alias_30);  clone_76 = None
    sum_92: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_357, [3], True)
    mul_358: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_30, sum_92);  alias_30 = sum_92 = None
    sub_109: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_357, mul_358);  mul_357 = mul_358 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_359: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_109, 0.125);  sub_109 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_21: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_4: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_21, [None, None, None, iota_21], mul_359, True);  full_21 = iota_21 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1098: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_4, [1, 16, 1023, 512]);  index_put_4 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_22: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_16: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_22, view_1098, 3, 0, 9223372036854775807);  full_22 = view_1098 = None
    full_23: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_17: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_23, slice_scatter_16, 2, 1, 9223372036854775807);  full_23 = slice_scatter_16 = None
    full_24: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_18: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_24, slice_scatter_17, 1, 0, 9223372036854775807);  full_24 = slice_scatter_17 = None
    full_25: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_19: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_25, slice_scatter_18, 0, 0, 9223372036854775807);  full_25 = slice_scatter_18 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1099: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_19, [1, 16, 512, 1024]);  slice_scatter_19 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1100: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1099, [1, 16, 512, 1024, 1]);  view_1099 = None
    permute_1284: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1100, [1, 2, 4, 0, 3]);  view_1100 = None
    view_1101: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1284, [16, 512, 1024]);  permute_1284 = None
    permute_1285: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_742, [0, 2, 1]);  view_742 = None
    bmm_256: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1285, view_1101);  permute_1285 = None
    permute_1286: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_743, [0, 2, 1]);  view_743 = None
    bmm_257: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1101, permute_1286);  view_1101 = permute_1286 = None
    view_1102: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_256, [16, 64, 1, 1024, 1]);  bmm_256 = None
    permute_1287: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1102, [2, 0, 4, 3, 1]);  view_1102 = None
    view_1103: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_257, [16, 512, 64, 1, 1]);  bmm_257 = None
    permute_1288: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1103, [3, 0, 1, 4, 2]);  view_1103 = None
    permute_1289: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1287, [3, 0, 1, 4, 2]);  permute_1287 = None
    squeeze_98: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1289, 4);  permute_1289 = None
    permute_1290: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1288, [2, 0, 1, 4, 3]);  permute_1288 = None
    squeeze_99: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1290, 4);  permute_1290 = None
    sum_93: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_99, [0, 1], True)
    view_1104: "f32[16, 64]" = torch.ops.aten.view.default(sum_93, [16, 64]);  sum_93 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1105: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_359, [1, 16, 512, 512, 1]);  mul_359 = None
    permute_1291: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1105, [1, 2, 4, 0, 3]);  view_1105 = None
    view_1106: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1291, [16, 512, 512]);  permute_1291 = None
    permute_1292: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_738, [0, 2, 1]);  view_738 = None
    bmm_258: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1292, view_1106);  permute_1292 = None
    permute_1293: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_739, [0, 2, 1]);  view_739 = None
    bmm_259: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1106, permute_1293);  view_1106 = permute_1293 = None
    view_1107: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_258, [16, 64, 1, 512, 1]);  bmm_258 = None
    permute_1294: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1107, [2, 0, 4, 3, 1]);  view_1107 = None
    view_1108: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_259, [16, 512, 64, 1, 1]);  bmm_259 = None
    permute_1295: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1108, [3, 0, 1, 4, 2]);  view_1108 = None
    permute_1296: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1294, [3, 0, 1, 4, 2]);  permute_1294 = None
    squeeze_100: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1296, 4);  permute_1296 = None
    permute_1297: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1295, [2, 0, 1, 4, 3]);  permute_1295 = None
    squeeze_101: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1297, 4);  permute_1297 = None
    sum_94: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_101, [0, 1], True)
    view_1109: "f32[16, 64]" = torch.ops.aten.view.default(sum_94, [16, 64]);  sum_94 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_298: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_99, squeeze_101);  squeeze_99 = squeeze_101 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1110: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_98, [1024, 1, 16, 64, 1]);  squeeze_98 = None
    permute_1298: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1110, [0, 4, 1, 2, 3]);  view_1110 = None
    view_1111: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1298, [1, 1024, 1024]);  permute_1298 = None
    permute_1299: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_734, [0, 2, 1]);  view_734 = None
    bmm_260: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1299, view_1111);  permute_1299 = view_1111 = None
    view_1112: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_260, [1024, 1, 16, 64, 1]);  bmm_260 = None
    permute_1300: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1112, [4, 1, 2, 3, 0]);  view_1112 = None
    permute_1301: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1300, [4, 2, 3, 0, 1]);  permute_1300 = None
    squeeze_102: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1301, 4);  permute_1301 = None
    squeeze_103: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_102, 3);  squeeze_102 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1113: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_96, [512, 1, 16, 64, 1]);  squeeze_96 = None
    permute_1302: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1113, [0, 4, 1, 2, 3]);  view_1113 = None
    clone_77: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1302, memory_format = torch.contiguous_format);  permute_1302 = None
    view_1114: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_77, [1, 512, 1024]);  clone_77 = None
    permute_1303: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_730, [0, 2, 1]);  view_730 = None
    bmm_261: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1303, view_1114);  permute_1303 = None
    permute_1304: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_731, [0, 2, 1]);  view_731 = None
    bmm_262: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1114, permute_1304);  view_1114 = permute_1304 = None
    view_1115: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_261, [1024, 1, 16, 64, 1]);  bmm_261 = None
    permute_1305: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1115, [4, 1, 2, 3, 0]);  view_1115 = None
    view_1116: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_262, [512, 1024, 1, 1, 1]);  bmm_262 = None
    permute_1306: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1116, [0, 2, 3, 4, 1]);  view_1116 = None
    permute_1307: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1305, [4, 2, 3, 0, 1]);  permute_1305 = None
    squeeze_104: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1307, 4);  permute_1307 = None
    squeeze_105: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_104, 3);  squeeze_104 = None
    permute_1308: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1306, [0, 1, 4, 2, 3]);  permute_1306 = None
    squeeze_106: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1308, 4);  permute_1308 = None
    squeeze_107: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_106, 3);  squeeze_106 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_299: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_351, squeeze_107);  mul_351 = squeeze_107 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1117: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_100, [512, 1, 16, 64, 1]);  squeeze_100 = None
    permute_1309: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1117, [0, 4, 1, 2, 3]);  view_1117 = None
    view_1118: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1309, [1, 512, 1024]);  permute_1309 = None
    permute_1310: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_726, [0, 2, 1]);  view_726 = None
    bmm_263: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1310, view_1118);  permute_1310 = None
    permute_1311: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_727, [0, 2, 1]);  view_727 = None
    bmm_264: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1118, permute_1311);  view_1118 = permute_1311 = None
    view_1119: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_263, [1024, 1, 16, 64, 1]);  bmm_263 = None
    permute_1312: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1119, [4, 1, 2, 3, 0]);  view_1119 = None
    view_1120: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_264, [512, 1024, 1, 1, 1]);  bmm_264 = None
    permute_1313: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1120, [0, 2, 3, 4, 1]);  view_1120 = None
    permute_1314: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1312, [4, 2, 3, 0, 1]);  permute_1312 = None
    squeeze_108: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1314, 4);  permute_1314 = None
    squeeze_109: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_108, 3);  squeeze_108 = None
    permute_1315: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1313, [0, 1, 4, 2, 3]);  permute_1313 = None
    squeeze_110: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1315, 4);  permute_1315 = None
    squeeze_111: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_110, 3);  squeeze_110 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_300: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_299, squeeze_111);  add_299 = squeeze_111 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1121: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_298, [512, 1, 16, 64, 1]);  add_298 = None
    permute_1316: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1121, [0, 4, 1, 2, 3]);  view_1121 = None
    clone_78: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1316, memory_format = torch.contiguous_format);  permute_1316 = None
    view_1122: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_78, [1, 512, 1024]);  clone_78 = None
    permute_1317: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_722, [0, 2, 1]);  view_722 = None
    bmm_265: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1317, view_1122);  permute_1317 = None
    permute_1318: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_723, [0, 2, 1]);  view_723 = None
    bmm_266: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1122, permute_1318);  view_1122 = permute_1318 = None
    view_1123: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_265, [1024, 1, 16, 64, 1]);  bmm_265 = None
    permute_1319: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1123, [4, 1, 2, 3, 0]);  view_1123 = None
    view_1124: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_266, [512, 1024, 1, 1, 1]);  bmm_266 = None
    permute_1320: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1124, [0, 2, 3, 4, 1]);  view_1124 = None
    permute_1321: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1319, [4, 2, 3, 0, 1]);  permute_1319 = None
    squeeze_112: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1321, 4);  permute_1321 = None
    squeeze_113: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_112, 3);  squeeze_112 = None
    permute_1322: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1320, [0, 1, 4, 2, 3]);  permute_1320 = None
    squeeze_114: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1322, 4);  permute_1322 = None
    squeeze_115: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_114, 3);  squeeze_114 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_301: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_300, squeeze_115);  add_300 = squeeze_115 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_110: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_208, getitem_231);  add_208 = getitem_231 = None
    mul_360: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_110, rsqrt_37);  sub_110 = None
    mul_361: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_301, primals_320);  primals_320 = None
    mul_362: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_361, 1024)
    sum_95: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_361, [2], True)
    mul_363: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_361, mul_360);  mul_361 = None
    sum_96: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_363, [2], True);  mul_363 = None
    mul_364: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_360, sum_96);  sum_96 = None
    sub_111: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_362, sum_95);  mul_362 = sum_95 = None
    sub_112: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_111, mul_364);  sub_111 = mul_364 = None
    div_37: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_37, 1024);  rsqrt_37 = None
    mul_365: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_37, sub_112);  div_37 = sub_112 = None
    mul_366: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_301, mul_360);  mul_360 = None
    sum_97: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_366, [0, 1]);  mul_366 = None
    sum_98: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_301, [0, 1]);  add_301 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_27: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_229, torch.float32);  getitem_229 = None
    mul_367: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_27, 1.1111111111111112);  convert_element_type_27 = None
    mul_368: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_365, mul_367);  mul_367 = None
    clone_79: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_368, memory_format = torch.contiguous_format);  mul_368 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1125: "f32[512, 1024]" = torch.ops.aten.view.default(clone_79, [512, 1024]);  clone_79 = None
    permute_1323: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_800, [1, 0]);  permute_800 = None
    mm_22: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1125, permute_1323);  permute_1323 = None
    permute_1324: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1125, [1, 0])
    mm_23: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1324, view_720);  permute_1324 = view_720 = None
    permute_1325: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_23, [1, 0]);  mm_23 = None
    sum_99: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1125, [0], True);  view_1125 = None
    view_1126: "f32[1024]" = torch.ops.aten.view.default(sum_99, [1024]);  sum_99 = None
    permute_1326: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1325, [1, 0]);  permute_1325 = None
    view_1127: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_22, [512, 1, 4096]);  mm_22 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_28: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_227, torch.float32);  getitem_227 = None
    mul_369: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_28, 1.1111111111111112);  convert_element_type_28 = None
    mul_370: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1127, mul_369);  view_1127 = mul_369 = None
    clone_80: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_370, memory_format = torch.contiguous_format);  mul_370 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_371: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_719, 0.7071067811865476)
    erf_29: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_371);  mul_371 = None
    add_302: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_29, 1);  erf_29 = None
    mul_372: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_302, 0.5);  add_302 = None
    mul_373: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_719, view_719)
    mul_374: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_373, -0.5);  mul_373 = None
    exp_31: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_374);  mul_374 = None
    mul_375: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_31, 0.3989422804014327);  exp_31 = None
    mul_376: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_719, mul_375);  view_719 = mul_375 = None
    add_303: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_372, mul_376);  mul_372 = mul_376 = None
    mul_377: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_80, add_303);  clone_80 = add_303 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1128: "f32[512, 4096]" = torch.ops.aten.view.default(mul_377, [512, 4096]);  mul_377 = None
    permute_1327: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_799, [1, 0]);  permute_799 = None
    mm_24: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1128, permute_1327);  permute_1327 = None
    permute_1328: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1128, [1, 0])
    mm_25: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1328, view_718);  permute_1328 = view_718 = None
    permute_1329: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_25, [1, 0]);  mm_25 = None
    sum_100: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1128, [0], True);  view_1128 = None
    view_1129: "f32[4096]" = torch.ops.aten.view.default(sum_100, [4096]);  sum_100 = None
    permute_1330: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1329, [1, 0]);  permute_1329 = None
    view_1130: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_24, [512, 1, 1024]);  mm_24 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_304: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_365, view_1130);  mul_365 = view_1130 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_113: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_204, getitem_225);  add_204 = getitem_225 = None
    mul_378: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_113, rsqrt_36);  sub_113 = None
    mul_379: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_304, primals_314);  primals_314 = None
    mul_380: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_379, 1024)
    sum_101: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_379, [2], True)
    mul_381: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_379, mul_378);  mul_379 = None
    sum_102: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_381, [2], True);  mul_381 = None
    mul_382: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_378, sum_102);  sum_102 = None
    sub_114: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_380, sum_101);  mul_380 = sum_101 = None
    sub_115: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_114, mul_382);  sub_114 = mul_382 = None
    div_38: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_36, 1024);  rsqrt_36 = None
    mul_383: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_38, sub_115);  div_38 = sub_115 = None
    mul_384: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_304, mul_378);  mul_378 = None
    sum_103: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_384, [0, 1]);  mul_384 = None
    sum_104: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_304, [0, 1]);  add_304 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_29: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_223, torch.float32);  getitem_223 = None
    mul_385: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_29, 1.1111111111111112);  convert_element_type_29 = None
    mul_386: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_383, mul_385);  mul_385 = None
    clone_81: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_386, memory_format = torch.contiguous_format);  mul_386 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1131: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_81, [512, 1, 1024, 1, 1]);  clone_81 = None
    permute_1331: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1131, [0, 3, 4, 1, 2]);  view_1131 = None
    view_1132: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1331, [1, 512, 1024]);  permute_1331 = None
    permute_1332: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_714, [0, 2, 1]);  view_714 = None
    bmm_267: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1332, view_1132);  permute_1332 = None
    permute_1333: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_715, [0, 2, 1]);  view_715 = None
    bmm_268: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1132, permute_1333);  view_1132 = permute_1333 = None
    view_1133: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_267, [64, 16, 1, 1024, 1]);  bmm_267 = None
    permute_1334: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1133, [4, 2, 3, 0, 1]);  view_1133 = None
    view_1134: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_268, [512, 64, 16, 1, 1]);  bmm_268 = None
    permute_1335: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1134, [0, 3, 4, 1, 2]);  view_1134 = None
    permute_1336: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1334, [2, 4, 3, 0, 1]);  permute_1334 = None
    squeeze_116: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1336, 4);  permute_1336 = None
    squeeze_117: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_116, 3);  squeeze_116 = None
    permute_1337: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1335, [0, 1, 4, 3, 2]);  permute_1335 = None
    squeeze_118: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1337, 4);  permute_1337 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1135: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_118, [512, 1, 16, 64, 1]);  squeeze_118 = None
    permute_1338: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1135, [2, 0, 4, 1, 3]);  view_1135 = None
    view_1136: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1338, [16, 512, 64]);  permute_1338 = None
    permute_1339: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_710, [0, 2, 1]);  view_710 = None
    bmm_269: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1339, view_1136);  permute_1339 = None
    permute_1340: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_711, [0, 2, 1]);  view_711 = None
    bmm_270: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1136, permute_1340);  view_1136 = permute_1340 = None
    view_1137: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_269, [16, 512, 1, 64, 1]);  bmm_269 = None
    permute_1341: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1137, [4, 2, 0, 3, 1]);  view_1137 = None
    view_1138: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_270, [16, 512, 512, 1, 1]);  bmm_270 = None
    permute_1342: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1138, [1, 3, 0, 4, 2]);  view_1138 = None
    permute_1343: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1341, [4, 1, 2, 3, 0]);  permute_1341 = None
    squeeze_119: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1343, 4);  permute_1343 = None
    permute_1344: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1342, [1, 2, 0, 4, 3]);  permute_1342 = None
    squeeze_120: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1344, 4);  permute_1344 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_30: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_221, torch.float32);  getitem_221 = None
    mul_387: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_30, 1.1111111111111112);  convert_element_type_30 = None
    mul_388: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_120, mul_387);  squeeze_120 = mul_387 = None
    clone_82: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_388, memory_format = torch.contiguous_format);  mul_388 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_31: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_18);  alias_18 = None
    mul_389: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_82, alias_31);  clone_82 = None
    sum_105: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_389, [3], True)
    mul_390: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_31, sum_105);  alias_31 = sum_105 = None
    sub_116: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_389, mul_390);  mul_389 = mul_390 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_391: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_116, 0.125);  sub_116 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_26: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_5: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_26, [None, None, None, iota_20], mul_391, True);  full_26 = iota_20 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1139: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_5, [1, 16, 1023, 512]);  index_put_5 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_27: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_20: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_27, view_1139, 3, 0, 9223372036854775807);  full_27 = view_1139 = None
    full_28: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_21: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_28, slice_scatter_20, 2, 1, 9223372036854775807);  full_28 = slice_scatter_20 = None
    full_29: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_22: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_29, slice_scatter_21, 1, 0, 9223372036854775807);  full_29 = slice_scatter_21 = None
    full_30: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_23: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_30, slice_scatter_22, 0, 0, 9223372036854775807);  full_30 = slice_scatter_22 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1140: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_23, [1, 16, 512, 1024]);  slice_scatter_23 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1141: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1140, [1, 16, 512, 1024, 1]);  view_1140 = None
    permute_1345: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1141, [1, 2, 4, 0, 3]);  view_1141 = None
    view_1142: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1345, [16, 512, 1024]);  permute_1345 = None
    permute_1346: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_704, [0, 2, 1]);  view_704 = None
    bmm_271: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1346, view_1142);  permute_1346 = None
    permute_1347: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_705, [0, 2, 1]);  view_705 = None
    bmm_272: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1142, permute_1347);  view_1142 = permute_1347 = None
    view_1143: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_271, [16, 64, 1, 1024, 1]);  bmm_271 = None
    permute_1348: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1143, [2, 0, 4, 3, 1]);  view_1143 = None
    view_1144: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_272, [16, 512, 64, 1, 1]);  bmm_272 = None
    permute_1349: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1144, [3, 0, 1, 4, 2]);  view_1144 = None
    permute_1350: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1348, [3, 0, 1, 4, 2]);  permute_1348 = None
    squeeze_121: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1350, 4);  permute_1350 = None
    permute_1351: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1349, [2, 0, 1, 4, 3]);  permute_1349 = None
    squeeze_122: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1351, 4);  permute_1351 = None
    sum_106: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_122, [0, 1], True)
    view_1145: "f32[16, 64]" = torch.ops.aten.view.default(sum_106, [16, 64]);  sum_106 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1146: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_391, [1, 16, 512, 512, 1]);  mul_391 = None
    permute_1352: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1146, [1, 2, 4, 0, 3]);  view_1146 = None
    view_1147: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1352, [16, 512, 512]);  permute_1352 = None
    permute_1353: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_700, [0, 2, 1]);  view_700 = None
    bmm_273: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1353, view_1147);  permute_1353 = None
    permute_1354: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_701, [0, 2, 1]);  view_701 = None
    bmm_274: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1147, permute_1354);  view_1147 = permute_1354 = None
    view_1148: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_273, [16, 64, 1, 512, 1]);  bmm_273 = None
    permute_1355: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1148, [2, 0, 4, 3, 1]);  view_1148 = None
    view_1149: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_274, [16, 512, 64, 1, 1]);  bmm_274 = None
    permute_1356: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1149, [3, 0, 1, 4, 2]);  view_1149 = None
    permute_1357: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1355, [3, 0, 1, 4, 2]);  permute_1355 = None
    squeeze_123: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1357, 4);  permute_1357 = None
    permute_1358: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1356, [2, 0, 1, 4, 3]);  permute_1356 = None
    squeeze_124: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1358, 4);  permute_1358 = None
    sum_107: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_124, [0, 1], True)
    view_1150: "f32[16, 64]" = torch.ops.aten.view.default(sum_107, [16, 64]);  sum_107 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_305: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_122, squeeze_124);  squeeze_122 = squeeze_124 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1151: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_121, [1024, 1, 16, 64, 1]);  squeeze_121 = None
    permute_1359: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1151, [0, 4, 1, 2, 3]);  view_1151 = None
    view_1152: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1359, [1, 1024, 1024]);  permute_1359 = None
    permute_1360: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_696, [0, 2, 1]);  view_696 = None
    bmm_275: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1360, view_1152);  permute_1360 = view_1152 = None
    view_1153: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_275, [1024, 1, 16, 64, 1]);  bmm_275 = None
    permute_1361: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1153, [4, 1, 2, 3, 0]);  view_1153 = None
    permute_1362: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1361, [4, 2, 3, 0, 1]);  permute_1361 = None
    squeeze_125: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1362, 4);  permute_1362 = None
    squeeze_126: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_125, 3);  squeeze_125 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1154: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_119, [512, 1, 16, 64, 1]);  squeeze_119 = None
    permute_1363: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1154, [0, 4, 1, 2, 3]);  view_1154 = None
    clone_83: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1363, memory_format = torch.contiguous_format);  permute_1363 = None
    view_1155: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_83, [1, 512, 1024]);  clone_83 = None
    permute_1364: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_692, [0, 2, 1]);  view_692 = None
    bmm_276: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1364, view_1155);  permute_1364 = None
    permute_1365: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_693, [0, 2, 1]);  view_693 = None
    bmm_277: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1155, permute_1365);  view_1155 = permute_1365 = None
    view_1156: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_276, [1024, 1, 16, 64, 1]);  bmm_276 = None
    permute_1366: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1156, [4, 1, 2, 3, 0]);  view_1156 = None
    view_1157: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_277, [512, 1024, 1, 1, 1]);  bmm_277 = None
    permute_1367: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1157, [0, 2, 3, 4, 1]);  view_1157 = None
    permute_1368: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1366, [4, 2, 3, 0, 1]);  permute_1366 = None
    squeeze_127: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1368, 4);  permute_1368 = None
    squeeze_128: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_127, 3);  squeeze_127 = None
    permute_1369: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1367, [0, 1, 4, 2, 3]);  permute_1367 = None
    squeeze_129: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1369, 4);  permute_1369 = None
    squeeze_130: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_129, 3);  squeeze_129 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_306: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_383, squeeze_130);  mul_383 = squeeze_130 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1158: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_123, [512, 1, 16, 64, 1]);  squeeze_123 = None
    permute_1370: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1158, [0, 4, 1, 2, 3]);  view_1158 = None
    view_1159: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1370, [1, 512, 1024]);  permute_1370 = None
    permute_1371: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_688, [0, 2, 1]);  view_688 = None
    bmm_278: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1371, view_1159);  permute_1371 = None
    permute_1372: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_689, [0, 2, 1]);  view_689 = None
    bmm_279: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1159, permute_1372);  view_1159 = permute_1372 = None
    view_1160: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_278, [1024, 1, 16, 64, 1]);  bmm_278 = None
    permute_1373: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1160, [4, 1, 2, 3, 0]);  view_1160 = None
    view_1161: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_279, [512, 1024, 1, 1, 1]);  bmm_279 = None
    permute_1374: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1161, [0, 2, 3, 4, 1]);  view_1161 = None
    permute_1375: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1373, [4, 2, 3, 0, 1]);  permute_1373 = None
    squeeze_131: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1375, 4);  permute_1375 = None
    squeeze_132: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_131, 3);  squeeze_131 = None
    permute_1376: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1374, [0, 1, 4, 2, 3]);  permute_1374 = None
    squeeze_133: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1376, 4);  permute_1376 = None
    squeeze_134: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_133, 3);  squeeze_133 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_307: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_306, squeeze_134);  add_306 = squeeze_134 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1162: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_305, [512, 1, 16, 64, 1]);  add_305 = None
    permute_1377: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1162, [0, 4, 1, 2, 3]);  view_1162 = None
    clone_84: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1377, memory_format = torch.contiguous_format);  permute_1377 = None
    view_1163: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_84, [1, 512, 1024]);  clone_84 = None
    permute_1378: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_684, [0, 2, 1]);  view_684 = None
    bmm_280: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1378, view_1163);  permute_1378 = None
    permute_1379: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_685, [0, 2, 1]);  view_685 = None
    bmm_281: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1163, permute_1379);  view_1163 = permute_1379 = None
    view_1164: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_280, [1024, 1, 16, 64, 1]);  bmm_280 = None
    permute_1380: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1164, [4, 1, 2, 3, 0]);  view_1164 = None
    view_1165: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_281, [512, 1024, 1, 1, 1]);  bmm_281 = None
    permute_1381: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1165, [0, 2, 3, 4, 1]);  view_1165 = None
    permute_1382: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1380, [4, 2, 3, 0, 1]);  permute_1380 = None
    squeeze_135: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1382, 4);  permute_1382 = None
    squeeze_136: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_135, 3);  squeeze_135 = None
    permute_1383: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1381, [0, 1, 4, 2, 3]);  permute_1381 = None
    squeeze_137: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1383, 4);  permute_1383 = None
    squeeze_138: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_137, 3);  squeeze_137 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_308: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_307, squeeze_138);  add_307 = squeeze_138 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_117: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_197, getitem_219);  add_197 = getitem_219 = None
    mul_392: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_117, rsqrt_35);  sub_117 = None
    mul_393: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_308, primals_312);  primals_312 = None
    mul_394: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_393, 1024)
    sum_108: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_393, [2], True)
    mul_395: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_393, mul_392);  mul_393 = None
    sum_109: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_395, [2], True);  mul_395 = None
    mul_396: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_392, sum_109);  sum_109 = None
    sub_118: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_394, sum_108);  mul_394 = sum_108 = None
    sub_119: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_118, mul_396);  sub_118 = mul_396 = None
    div_39: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_35, 1024);  rsqrt_35 = None
    mul_397: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_39, sub_119);  div_39 = sub_119 = None
    mul_398: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_308, mul_392);  mul_392 = None
    sum_110: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_398, [0, 1]);  mul_398 = None
    sum_111: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_308, [0, 1]);  add_308 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_31: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_217, torch.float32);  getitem_217 = None
    mul_399: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_31, 1.1111111111111112);  convert_element_type_31 = None
    mul_400: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_397, mul_399);  mul_399 = None
    clone_85: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_400, memory_format = torch.contiguous_format);  mul_400 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1166: "f32[512, 1024]" = torch.ops.aten.view.default(clone_85, [512, 1024]);  clone_85 = None
    permute_1384: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_758, [1, 0]);  permute_758 = None
    mm_26: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1166, permute_1384);  permute_1384 = None
    permute_1385: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1166, [1, 0])
    mm_27: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1385, view_682);  permute_1385 = view_682 = None
    permute_1386: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_27, [1, 0]);  mm_27 = None
    sum_112: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1166, [0], True);  view_1166 = None
    view_1167: "f32[1024]" = torch.ops.aten.view.default(sum_112, [1024]);  sum_112 = None
    permute_1387: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1386, [1, 0]);  permute_1386 = None
    view_1168: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_26, [512, 1, 4096]);  mm_26 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_32: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_215, torch.float32);  getitem_215 = None
    mul_401: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_32, 1.1111111111111112);  convert_element_type_32 = None
    mul_402: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1168, mul_401);  view_1168 = mul_401 = None
    clone_86: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_402, memory_format = torch.contiguous_format);  mul_402 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_403: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_681, 0.7071067811865476)
    erf_30: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_403);  mul_403 = None
    add_309: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_30, 1);  erf_30 = None
    mul_404: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_309, 0.5);  add_309 = None
    mul_405: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_681, view_681)
    mul_406: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_405, -0.5);  mul_405 = None
    exp_32: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_406);  mul_406 = None
    mul_407: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_32, 0.3989422804014327);  exp_32 = None
    mul_408: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_681, mul_407);  view_681 = mul_407 = None
    add_310: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_404, mul_408);  mul_404 = mul_408 = None
    mul_409: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_86, add_310);  clone_86 = add_310 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1169: "f32[512, 4096]" = torch.ops.aten.view.default(mul_409, [512, 4096]);  mul_409 = None
    permute_1388: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_757, [1, 0]);  permute_757 = None
    mm_28: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1169, permute_1388);  permute_1388 = None
    permute_1389: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1169, [1, 0])
    mm_29: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1389, view_680);  permute_1389 = view_680 = None
    permute_1390: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_29, [1, 0]);  mm_29 = None
    sum_113: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1169, [0], True);  view_1169 = None
    view_1170: "f32[4096]" = torch.ops.aten.view.default(sum_113, [4096]);  sum_113 = None
    permute_1391: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1390, [1, 0]);  permute_1390 = None
    view_1171: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_28, [512, 1, 1024]);  mm_28 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_311: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_397, view_1171);  mul_397 = view_1171 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_120: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_193, getitem_213);  add_193 = getitem_213 = None
    mul_410: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_120, rsqrt_34);  sub_120 = None
    mul_411: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_311, primals_306);  primals_306 = None
    mul_412: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_411, 1024)
    sum_114: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_411, [2], True)
    mul_413: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_411, mul_410);  mul_411 = None
    sum_115: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_413, [2], True);  mul_413 = None
    mul_414: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_410, sum_115);  sum_115 = None
    sub_121: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_412, sum_114);  mul_412 = sum_114 = None
    sub_122: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_121, mul_414);  sub_121 = mul_414 = None
    div_40: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_34, 1024);  rsqrt_34 = None
    mul_415: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_40, sub_122);  div_40 = sub_122 = None
    mul_416: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_311, mul_410);  mul_410 = None
    sum_116: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_416, [0, 1]);  mul_416 = None
    sum_117: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_311, [0, 1]);  add_311 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_33: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_211, torch.float32);  getitem_211 = None
    mul_417: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_33, 1.1111111111111112);  convert_element_type_33 = None
    mul_418: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_415, mul_417);  mul_417 = None
    clone_87: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_418, memory_format = torch.contiguous_format);  mul_418 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1172: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_87, [512, 1, 1024, 1, 1]);  clone_87 = None
    permute_1392: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1172, [0, 3, 4, 1, 2]);  view_1172 = None
    view_1173: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1392, [1, 512, 1024]);  permute_1392 = None
    permute_1393: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_676, [0, 2, 1]);  view_676 = None
    bmm_282: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1393, view_1173);  permute_1393 = None
    permute_1394: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_677, [0, 2, 1]);  view_677 = None
    bmm_283: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1173, permute_1394);  view_1173 = permute_1394 = None
    view_1174: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_282, [64, 16, 1, 1024, 1]);  bmm_282 = None
    permute_1395: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1174, [4, 2, 3, 0, 1]);  view_1174 = None
    view_1175: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_283, [512, 64, 16, 1, 1]);  bmm_283 = None
    permute_1396: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1175, [0, 3, 4, 1, 2]);  view_1175 = None
    permute_1397: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1395, [2, 4, 3, 0, 1]);  permute_1395 = None
    squeeze_139: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1397, 4);  permute_1397 = None
    squeeze_140: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_139, 3);  squeeze_139 = None
    permute_1398: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1396, [0, 1, 4, 3, 2]);  permute_1396 = None
    squeeze_141: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1398, 4);  permute_1398 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1176: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_141, [512, 1, 16, 64, 1]);  squeeze_141 = None
    permute_1399: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1176, [2, 0, 4, 1, 3]);  view_1176 = None
    view_1177: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1399, [16, 512, 64]);  permute_1399 = None
    permute_1400: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_672, [0, 2, 1]);  view_672 = None
    bmm_284: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1400, view_1177);  permute_1400 = None
    permute_1401: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_673, [0, 2, 1]);  view_673 = None
    bmm_285: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1177, permute_1401);  view_1177 = permute_1401 = None
    view_1178: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_284, [16, 512, 1, 64, 1]);  bmm_284 = None
    permute_1402: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1178, [4, 2, 0, 3, 1]);  view_1178 = None
    view_1179: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_285, [16, 512, 512, 1, 1]);  bmm_285 = None
    permute_1403: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1179, [1, 3, 0, 4, 2]);  view_1179 = None
    permute_1404: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1402, [4, 1, 2, 3, 0]);  permute_1402 = None
    squeeze_142: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1404, 4);  permute_1404 = None
    permute_1405: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1403, [1, 2, 0, 4, 3]);  permute_1403 = None
    squeeze_143: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1405, 4);  permute_1405 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_34: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_209, torch.float32);  getitem_209 = None
    mul_419: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_34, 1.1111111111111112);  convert_element_type_34 = None
    mul_420: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_143, mul_419);  squeeze_143 = mul_419 = None
    clone_88: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_420, memory_format = torch.contiguous_format);  mul_420 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_32: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_17);  alias_17 = None
    mul_421: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_88, alias_32);  clone_88 = None
    sum_118: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_421, [3], True)
    mul_422: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_32, sum_118);  alias_32 = sum_118 = None
    sub_123: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_421, mul_422);  mul_421 = mul_422 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_423: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_123, 0.125);  sub_123 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_31: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_6: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_31, [None, None, None, iota_19], mul_423, True);  full_31 = iota_19 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1180: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_6, [1, 16, 1023, 512]);  index_put_6 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_32: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_24: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_32, view_1180, 3, 0, 9223372036854775807);  full_32 = view_1180 = None
    full_33: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_25: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_33, slice_scatter_24, 2, 1, 9223372036854775807);  full_33 = slice_scatter_24 = None
    full_34: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_26: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_34, slice_scatter_25, 1, 0, 9223372036854775807);  full_34 = slice_scatter_25 = None
    full_35: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_27: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_35, slice_scatter_26, 0, 0, 9223372036854775807);  full_35 = slice_scatter_26 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1181: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_27, [1, 16, 512, 1024]);  slice_scatter_27 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1182: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1181, [1, 16, 512, 1024, 1]);  view_1181 = None
    permute_1406: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1182, [1, 2, 4, 0, 3]);  view_1182 = None
    view_1183: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1406, [16, 512, 1024]);  permute_1406 = None
    permute_1407: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_666, [0, 2, 1]);  view_666 = None
    bmm_286: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1407, view_1183);  permute_1407 = None
    permute_1408: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_667, [0, 2, 1]);  view_667 = None
    bmm_287: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1183, permute_1408);  view_1183 = permute_1408 = None
    view_1184: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_286, [16, 64, 1, 1024, 1]);  bmm_286 = None
    permute_1409: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1184, [2, 0, 4, 3, 1]);  view_1184 = None
    view_1185: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_287, [16, 512, 64, 1, 1]);  bmm_287 = None
    permute_1410: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1185, [3, 0, 1, 4, 2]);  view_1185 = None
    permute_1411: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1409, [3, 0, 1, 4, 2]);  permute_1409 = None
    squeeze_144: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1411, 4);  permute_1411 = None
    permute_1412: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1410, [2, 0, 1, 4, 3]);  permute_1410 = None
    squeeze_145: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1412, 4);  permute_1412 = None
    sum_119: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_145, [0, 1], True)
    view_1186: "f32[16, 64]" = torch.ops.aten.view.default(sum_119, [16, 64]);  sum_119 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1187: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_423, [1, 16, 512, 512, 1]);  mul_423 = None
    permute_1413: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1187, [1, 2, 4, 0, 3]);  view_1187 = None
    view_1188: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1413, [16, 512, 512]);  permute_1413 = None
    permute_1414: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_662, [0, 2, 1]);  view_662 = None
    bmm_288: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1414, view_1188);  permute_1414 = None
    permute_1415: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_663, [0, 2, 1]);  view_663 = None
    bmm_289: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1188, permute_1415);  view_1188 = permute_1415 = None
    view_1189: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_288, [16, 64, 1, 512, 1]);  bmm_288 = None
    permute_1416: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1189, [2, 0, 4, 3, 1]);  view_1189 = None
    view_1190: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_289, [16, 512, 64, 1, 1]);  bmm_289 = None
    permute_1417: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1190, [3, 0, 1, 4, 2]);  view_1190 = None
    permute_1418: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1416, [3, 0, 1, 4, 2]);  permute_1416 = None
    squeeze_146: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1418, 4);  permute_1418 = None
    permute_1419: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1417, [2, 0, 1, 4, 3]);  permute_1417 = None
    squeeze_147: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1419, 4);  permute_1419 = None
    sum_120: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_147, [0, 1], True)
    view_1191: "f32[16, 64]" = torch.ops.aten.view.default(sum_120, [16, 64]);  sum_120 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_312: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_145, squeeze_147);  squeeze_145 = squeeze_147 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1192: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_144, [1024, 1, 16, 64, 1]);  squeeze_144 = None
    permute_1420: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1192, [0, 4, 1, 2, 3]);  view_1192 = None
    view_1193: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1420, [1, 1024, 1024]);  permute_1420 = None
    permute_1421: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_658, [0, 2, 1]);  view_658 = None
    bmm_290: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1421, view_1193);  permute_1421 = view_1193 = None
    view_1194: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_290, [1024, 1, 16, 64, 1]);  bmm_290 = None
    permute_1422: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1194, [4, 1, 2, 3, 0]);  view_1194 = None
    permute_1423: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1422, [4, 2, 3, 0, 1]);  permute_1422 = None
    squeeze_148: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1423, 4);  permute_1423 = None
    squeeze_149: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_148, 3);  squeeze_148 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1195: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_142, [512, 1, 16, 64, 1]);  squeeze_142 = None
    permute_1424: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1195, [0, 4, 1, 2, 3]);  view_1195 = None
    clone_89: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1424, memory_format = torch.contiguous_format);  permute_1424 = None
    view_1196: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_89, [1, 512, 1024]);  clone_89 = None
    permute_1425: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_654, [0, 2, 1]);  view_654 = None
    bmm_291: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1425, view_1196);  permute_1425 = None
    permute_1426: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_655, [0, 2, 1]);  view_655 = None
    bmm_292: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1196, permute_1426);  view_1196 = permute_1426 = None
    view_1197: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_291, [1024, 1, 16, 64, 1]);  bmm_291 = None
    permute_1427: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1197, [4, 1, 2, 3, 0]);  view_1197 = None
    view_1198: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_292, [512, 1024, 1, 1, 1]);  bmm_292 = None
    permute_1428: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1198, [0, 2, 3, 4, 1]);  view_1198 = None
    permute_1429: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1427, [4, 2, 3, 0, 1]);  permute_1427 = None
    squeeze_150: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1429, 4);  permute_1429 = None
    squeeze_151: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_150, 3);  squeeze_150 = None
    permute_1430: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1428, [0, 1, 4, 2, 3]);  permute_1428 = None
    squeeze_152: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1430, 4);  permute_1430 = None
    squeeze_153: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_152, 3);  squeeze_152 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_313: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_415, squeeze_153);  mul_415 = squeeze_153 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1199: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_146, [512, 1, 16, 64, 1]);  squeeze_146 = None
    permute_1431: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1199, [0, 4, 1, 2, 3]);  view_1199 = None
    view_1200: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1431, [1, 512, 1024]);  permute_1431 = None
    permute_1432: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_650, [0, 2, 1]);  view_650 = None
    bmm_293: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1432, view_1200);  permute_1432 = None
    permute_1433: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_651, [0, 2, 1]);  view_651 = None
    bmm_294: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1200, permute_1433);  view_1200 = permute_1433 = None
    view_1201: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_293, [1024, 1, 16, 64, 1]);  bmm_293 = None
    permute_1434: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1201, [4, 1, 2, 3, 0]);  view_1201 = None
    view_1202: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_294, [512, 1024, 1, 1, 1]);  bmm_294 = None
    permute_1435: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1202, [0, 2, 3, 4, 1]);  view_1202 = None
    permute_1436: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1434, [4, 2, 3, 0, 1]);  permute_1434 = None
    squeeze_154: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1436, 4);  permute_1436 = None
    squeeze_155: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_154, 3);  squeeze_154 = None
    permute_1437: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1435, [0, 1, 4, 2, 3]);  permute_1435 = None
    squeeze_156: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1437, 4);  permute_1437 = None
    squeeze_157: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_156, 3);  squeeze_156 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_314: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_313, squeeze_157);  add_313 = squeeze_157 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1203: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_312, [512, 1, 16, 64, 1]);  add_312 = None
    permute_1438: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1203, [0, 4, 1, 2, 3]);  view_1203 = None
    clone_90: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1438, memory_format = torch.contiguous_format);  permute_1438 = None
    view_1204: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_90, [1, 512, 1024]);  clone_90 = None
    permute_1439: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_646, [0, 2, 1]);  view_646 = None
    bmm_295: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1439, view_1204);  permute_1439 = None
    permute_1440: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_647, [0, 2, 1]);  view_647 = None
    bmm_296: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1204, permute_1440);  view_1204 = permute_1440 = None
    view_1205: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_295, [1024, 1, 16, 64, 1]);  bmm_295 = None
    permute_1441: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1205, [4, 1, 2, 3, 0]);  view_1205 = None
    view_1206: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_296, [512, 1024, 1, 1, 1]);  bmm_296 = None
    permute_1442: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1206, [0, 2, 3, 4, 1]);  view_1206 = None
    permute_1443: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1441, [4, 2, 3, 0, 1]);  permute_1441 = None
    squeeze_158: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1443, 4);  permute_1443 = None
    squeeze_159: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_158, 3);  squeeze_158 = None
    permute_1444: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1442, [0, 1, 4, 2, 3]);  permute_1442 = None
    squeeze_160: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1444, 4);  permute_1444 = None
    squeeze_161: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_160, 3);  squeeze_160 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_315: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_314, squeeze_161);  add_314 = squeeze_161 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_124: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_186, getitem_207);  add_186 = getitem_207 = None
    mul_424: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_124, rsqrt_33);  sub_124 = None
    mul_425: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_315, primals_304);  primals_304 = None
    mul_426: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_425, 1024)
    sum_121: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_425, [2], True)
    mul_427: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_425, mul_424);  mul_425 = None
    sum_122: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_427, [2], True);  mul_427 = None
    mul_428: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_424, sum_122);  sum_122 = None
    sub_125: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_426, sum_121);  mul_426 = sum_121 = None
    sub_126: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_125, mul_428);  sub_125 = mul_428 = None
    div_41: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_33, 1024);  rsqrt_33 = None
    mul_429: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_41, sub_126);  div_41 = sub_126 = None
    mul_430: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_315, mul_424);  mul_424 = None
    sum_123: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_430, [0, 1]);  mul_430 = None
    sum_124: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_315, [0, 1]);  add_315 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_35: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_205, torch.float32);  getitem_205 = None
    mul_431: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_35, 1.1111111111111112);  convert_element_type_35 = None
    mul_432: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_429, mul_431);  mul_431 = None
    clone_91: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_432, memory_format = torch.contiguous_format);  mul_432 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1207: "f32[512, 1024]" = torch.ops.aten.view.default(clone_91, [512, 1024]);  clone_91 = None
    permute_1445: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_716, [1, 0]);  permute_716 = None
    mm_30: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1207, permute_1445);  permute_1445 = None
    permute_1446: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1207, [1, 0])
    mm_31: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1446, view_644);  permute_1446 = view_644 = None
    permute_1447: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_31, [1, 0]);  mm_31 = None
    sum_125: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1207, [0], True);  view_1207 = None
    view_1208: "f32[1024]" = torch.ops.aten.view.default(sum_125, [1024]);  sum_125 = None
    permute_1448: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1447, [1, 0]);  permute_1447 = None
    view_1209: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_30, [512, 1, 4096]);  mm_30 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_36: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_203, torch.float32);  getitem_203 = None
    mul_433: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_36, 1.1111111111111112);  convert_element_type_36 = None
    mul_434: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1209, mul_433);  view_1209 = mul_433 = None
    clone_92: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_434, memory_format = torch.contiguous_format);  mul_434 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_435: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_643, 0.7071067811865476)
    erf_31: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_435);  mul_435 = None
    add_316: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_31, 1);  erf_31 = None
    mul_436: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_316, 0.5);  add_316 = None
    mul_437: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_643, view_643)
    mul_438: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_437, -0.5);  mul_437 = None
    exp_33: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_438);  mul_438 = None
    mul_439: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_33, 0.3989422804014327);  exp_33 = None
    mul_440: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_643, mul_439);  view_643 = mul_439 = None
    add_317: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_436, mul_440);  mul_436 = mul_440 = None
    mul_441: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_92, add_317);  clone_92 = add_317 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1210: "f32[512, 4096]" = torch.ops.aten.view.default(mul_441, [512, 4096]);  mul_441 = None
    permute_1449: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_715, [1, 0]);  permute_715 = None
    mm_32: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1210, permute_1449);  permute_1449 = None
    permute_1450: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1210, [1, 0])
    mm_33: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1450, view_642);  permute_1450 = view_642 = None
    permute_1451: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_33, [1, 0]);  mm_33 = None
    sum_126: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1210, [0], True);  view_1210 = None
    view_1211: "f32[4096]" = torch.ops.aten.view.default(sum_126, [4096]);  sum_126 = None
    permute_1452: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1451, [1, 0]);  permute_1451 = None
    view_1212: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_32, [512, 1, 1024]);  mm_32 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_318: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_429, view_1212);  mul_429 = view_1212 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_127: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_182, getitem_201);  add_182 = getitem_201 = None
    mul_442: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_127, rsqrt_32);  sub_127 = None
    mul_443: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_318, primals_298);  primals_298 = None
    mul_444: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_443, 1024)
    sum_127: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_443, [2], True)
    mul_445: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_443, mul_442);  mul_443 = None
    sum_128: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_445, [2], True);  mul_445 = None
    mul_446: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_442, sum_128);  sum_128 = None
    sub_128: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_444, sum_127);  mul_444 = sum_127 = None
    sub_129: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_128, mul_446);  sub_128 = mul_446 = None
    div_42: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_32, 1024);  rsqrt_32 = None
    mul_447: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_42, sub_129);  div_42 = sub_129 = None
    mul_448: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_318, mul_442);  mul_442 = None
    sum_129: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_448, [0, 1]);  mul_448 = None
    sum_130: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_318, [0, 1]);  add_318 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_37: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_199, torch.float32);  getitem_199 = None
    mul_449: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_37, 1.1111111111111112);  convert_element_type_37 = None
    mul_450: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_447, mul_449);  mul_449 = None
    clone_93: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_450, memory_format = torch.contiguous_format);  mul_450 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1213: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_93, [512, 1, 1024, 1, 1]);  clone_93 = None
    permute_1453: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1213, [0, 3, 4, 1, 2]);  view_1213 = None
    view_1214: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1453, [1, 512, 1024]);  permute_1453 = None
    permute_1454: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_638, [0, 2, 1]);  view_638 = None
    bmm_297: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1454, view_1214);  permute_1454 = None
    permute_1455: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_639, [0, 2, 1]);  view_639 = None
    bmm_298: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1214, permute_1455);  view_1214 = permute_1455 = None
    view_1215: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_297, [64, 16, 1, 1024, 1]);  bmm_297 = None
    permute_1456: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1215, [4, 2, 3, 0, 1]);  view_1215 = None
    view_1216: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_298, [512, 64, 16, 1, 1]);  bmm_298 = None
    permute_1457: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1216, [0, 3, 4, 1, 2]);  view_1216 = None
    permute_1458: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1456, [2, 4, 3, 0, 1]);  permute_1456 = None
    squeeze_162: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1458, 4);  permute_1458 = None
    squeeze_163: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_162, 3);  squeeze_162 = None
    permute_1459: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1457, [0, 1, 4, 3, 2]);  permute_1457 = None
    squeeze_164: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1459, 4);  permute_1459 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1217: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_164, [512, 1, 16, 64, 1]);  squeeze_164 = None
    permute_1460: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1217, [2, 0, 4, 1, 3]);  view_1217 = None
    view_1218: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1460, [16, 512, 64]);  permute_1460 = None
    permute_1461: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_634, [0, 2, 1]);  view_634 = None
    bmm_299: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1461, view_1218);  permute_1461 = None
    permute_1462: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_635, [0, 2, 1]);  view_635 = None
    bmm_300: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1218, permute_1462);  view_1218 = permute_1462 = None
    view_1219: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_299, [16, 512, 1, 64, 1]);  bmm_299 = None
    permute_1463: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1219, [4, 2, 0, 3, 1]);  view_1219 = None
    view_1220: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_300, [16, 512, 512, 1, 1]);  bmm_300 = None
    permute_1464: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1220, [1, 3, 0, 4, 2]);  view_1220 = None
    permute_1465: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1463, [4, 1, 2, 3, 0]);  permute_1463 = None
    squeeze_165: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1465, 4);  permute_1465 = None
    permute_1466: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1464, [1, 2, 0, 4, 3]);  permute_1464 = None
    squeeze_166: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1466, 4);  permute_1466 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_38: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_197, torch.float32);  getitem_197 = None
    mul_451: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_38, 1.1111111111111112);  convert_element_type_38 = None
    mul_452: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_166, mul_451);  squeeze_166 = mul_451 = None
    clone_94: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_452, memory_format = torch.contiguous_format);  mul_452 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_33: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_16);  alias_16 = None
    mul_453: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_94, alias_33);  clone_94 = None
    sum_131: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_453, [3], True)
    mul_454: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_33, sum_131);  alias_33 = sum_131 = None
    sub_130: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_453, mul_454);  mul_453 = mul_454 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_455: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_130, 0.125);  sub_130 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_36: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_7: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_36, [None, None, None, iota_18], mul_455, True);  full_36 = iota_18 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1221: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_7, [1, 16, 1023, 512]);  index_put_7 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_37: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_28: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_37, view_1221, 3, 0, 9223372036854775807);  full_37 = view_1221 = None
    full_38: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_29: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_38, slice_scatter_28, 2, 1, 9223372036854775807);  full_38 = slice_scatter_28 = None
    full_39: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_30: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_39, slice_scatter_29, 1, 0, 9223372036854775807);  full_39 = slice_scatter_29 = None
    full_40: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_31: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_40, slice_scatter_30, 0, 0, 9223372036854775807);  full_40 = slice_scatter_30 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1222: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_31, [1, 16, 512, 1024]);  slice_scatter_31 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1223: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1222, [1, 16, 512, 1024, 1]);  view_1222 = None
    permute_1467: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1223, [1, 2, 4, 0, 3]);  view_1223 = None
    view_1224: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1467, [16, 512, 1024]);  permute_1467 = None
    permute_1468: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_628, [0, 2, 1]);  view_628 = None
    bmm_301: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1468, view_1224);  permute_1468 = None
    permute_1469: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_629, [0, 2, 1]);  view_629 = None
    bmm_302: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1224, permute_1469);  view_1224 = permute_1469 = None
    view_1225: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_301, [16, 64, 1, 1024, 1]);  bmm_301 = None
    permute_1470: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1225, [2, 0, 4, 3, 1]);  view_1225 = None
    view_1226: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_302, [16, 512, 64, 1, 1]);  bmm_302 = None
    permute_1471: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1226, [3, 0, 1, 4, 2]);  view_1226 = None
    permute_1472: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1470, [3, 0, 1, 4, 2]);  permute_1470 = None
    squeeze_167: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1472, 4);  permute_1472 = None
    permute_1473: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1471, [2, 0, 1, 4, 3]);  permute_1471 = None
    squeeze_168: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1473, 4);  permute_1473 = None
    sum_132: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_168, [0, 1], True)
    view_1227: "f32[16, 64]" = torch.ops.aten.view.default(sum_132, [16, 64]);  sum_132 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1228: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_455, [1, 16, 512, 512, 1]);  mul_455 = None
    permute_1474: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1228, [1, 2, 4, 0, 3]);  view_1228 = None
    view_1229: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1474, [16, 512, 512]);  permute_1474 = None
    permute_1475: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_624, [0, 2, 1]);  view_624 = None
    bmm_303: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1475, view_1229);  permute_1475 = None
    permute_1476: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_625, [0, 2, 1]);  view_625 = None
    bmm_304: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1229, permute_1476);  view_1229 = permute_1476 = None
    view_1230: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_303, [16, 64, 1, 512, 1]);  bmm_303 = None
    permute_1477: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1230, [2, 0, 4, 3, 1]);  view_1230 = None
    view_1231: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_304, [16, 512, 64, 1, 1]);  bmm_304 = None
    permute_1478: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1231, [3, 0, 1, 4, 2]);  view_1231 = None
    permute_1479: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1477, [3, 0, 1, 4, 2]);  permute_1477 = None
    squeeze_169: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1479, 4);  permute_1479 = None
    permute_1480: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1478, [2, 0, 1, 4, 3]);  permute_1478 = None
    squeeze_170: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1480, 4);  permute_1480 = None
    sum_133: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_170, [0, 1], True)
    view_1232: "f32[16, 64]" = torch.ops.aten.view.default(sum_133, [16, 64]);  sum_133 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_319: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_168, squeeze_170);  squeeze_168 = squeeze_170 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1233: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_167, [1024, 1, 16, 64, 1]);  squeeze_167 = None
    permute_1481: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1233, [0, 4, 1, 2, 3]);  view_1233 = None
    view_1234: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1481, [1, 1024, 1024]);  permute_1481 = None
    permute_1482: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_620, [0, 2, 1]);  view_620 = None
    bmm_305: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1482, view_1234);  permute_1482 = view_1234 = None
    view_1235: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_305, [1024, 1, 16, 64, 1]);  bmm_305 = None
    permute_1483: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1235, [4, 1, 2, 3, 0]);  view_1235 = None
    permute_1484: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1483, [4, 2, 3, 0, 1]);  permute_1483 = None
    squeeze_171: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1484, 4);  permute_1484 = None
    squeeze_172: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_171, 3);  squeeze_171 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1236: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_165, [512, 1, 16, 64, 1]);  squeeze_165 = None
    permute_1485: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1236, [0, 4, 1, 2, 3]);  view_1236 = None
    clone_95: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1485, memory_format = torch.contiguous_format);  permute_1485 = None
    view_1237: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_95, [1, 512, 1024]);  clone_95 = None
    permute_1486: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_616, [0, 2, 1]);  view_616 = None
    bmm_306: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1486, view_1237);  permute_1486 = None
    permute_1487: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_617, [0, 2, 1]);  view_617 = None
    bmm_307: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1237, permute_1487);  view_1237 = permute_1487 = None
    view_1238: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_306, [1024, 1, 16, 64, 1]);  bmm_306 = None
    permute_1488: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1238, [4, 1, 2, 3, 0]);  view_1238 = None
    view_1239: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_307, [512, 1024, 1, 1, 1]);  bmm_307 = None
    permute_1489: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1239, [0, 2, 3, 4, 1]);  view_1239 = None
    permute_1490: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1488, [4, 2, 3, 0, 1]);  permute_1488 = None
    squeeze_173: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1490, 4);  permute_1490 = None
    squeeze_174: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_173, 3);  squeeze_173 = None
    permute_1491: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1489, [0, 1, 4, 2, 3]);  permute_1489 = None
    squeeze_175: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1491, 4);  permute_1491 = None
    squeeze_176: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_175, 3);  squeeze_175 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_320: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_447, squeeze_176);  mul_447 = squeeze_176 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1240: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_169, [512, 1, 16, 64, 1]);  squeeze_169 = None
    permute_1492: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1240, [0, 4, 1, 2, 3]);  view_1240 = None
    view_1241: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1492, [1, 512, 1024]);  permute_1492 = None
    permute_1493: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_612, [0, 2, 1]);  view_612 = None
    bmm_308: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1493, view_1241);  permute_1493 = None
    permute_1494: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_613, [0, 2, 1]);  view_613 = None
    bmm_309: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1241, permute_1494);  view_1241 = permute_1494 = None
    view_1242: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_308, [1024, 1, 16, 64, 1]);  bmm_308 = None
    permute_1495: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1242, [4, 1, 2, 3, 0]);  view_1242 = None
    view_1243: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_309, [512, 1024, 1, 1, 1]);  bmm_309 = None
    permute_1496: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1243, [0, 2, 3, 4, 1]);  view_1243 = None
    permute_1497: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1495, [4, 2, 3, 0, 1]);  permute_1495 = None
    squeeze_177: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1497, 4);  permute_1497 = None
    squeeze_178: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_177, 3);  squeeze_177 = None
    permute_1498: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1496, [0, 1, 4, 2, 3]);  permute_1496 = None
    squeeze_179: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1498, 4);  permute_1498 = None
    squeeze_180: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_179, 3);  squeeze_179 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_321: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_320, squeeze_180);  add_320 = squeeze_180 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1244: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_319, [512, 1, 16, 64, 1]);  add_319 = None
    permute_1499: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1244, [0, 4, 1, 2, 3]);  view_1244 = None
    clone_96: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1499, memory_format = torch.contiguous_format);  permute_1499 = None
    view_1245: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_96, [1, 512, 1024]);  clone_96 = None
    permute_1500: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_608, [0, 2, 1]);  view_608 = None
    bmm_310: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1500, view_1245);  permute_1500 = None
    permute_1501: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_609, [0, 2, 1]);  view_609 = None
    bmm_311: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1245, permute_1501);  view_1245 = permute_1501 = None
    view_1246: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_310, [1024, 1, 16, 64, 1]);  bmm_310 = None
    permute_1502: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1246, [4, 1, 2, 3, 0]);  view_1246 = None
    view_1247: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_311, [512, 1024, 1, 1, 1]);  bmm_311 = None
    permute_1503: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1247, [0, 2, 3, 4, 1]);  view_1247 = None
    permute_1504: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1502, [4, 2, 3, 0, 1]);  permute_1502 = None
    squeeze_181: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1504, 4);  permute_1504 = None
    squeeze_182: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_181, 3);  squeeze_181 = None
    permute_1505: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1503, [0, 1, 4, 2, 3]);  permute_1503 = None
    squeeze_183: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1505, 4);  permute_1505 = None
    squeeze_184: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_183, 3);  squeeze_183 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_322: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_321, squeeze_184);  add_321 = squeeze_184 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_131: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_175, getitem_195);  add_175 = getitem_195 = None
    mul_456: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_131, rsqrt_31);  sub_131 = None
    mul_457: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_322, primals_296);  primals_296 = None
    mul_458: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_457, 1024)
    sum_134: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_457, [2], True)
    mul_459: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_457, mul_456);  mul_457 = None
    sum_135: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_459, [2], True);  mul_459 = None
    mul_460: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_456, sum_135);  sum_135 = None
    sub_132: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_458, sum_134);  mul_458 = sum_134 = None
    sub_133: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_132, mul_460);  sub_132 = mul_460 = None
    div_43: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_31, 1024);  rsqrt_31 = None
    mul_461: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_43, sub_133);  div_43 = sub_133 = None
    mul_462: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_322, mul_456);  mul_456 = None
    sum_136: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_462, [0, 1]);  mul_462 = None
    sum_137: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_322, [0, 1]);  add_322 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_39: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_193, torch.float32);  getitem_193 = None
    mul_463: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_39, 1.1111111111111112);  convert_element_type_39 = None
    mul_464: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_461, mul_463);  mul_463 = None
    clone_97: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_464, memory_format = torch.contiguous_format);  mul_464 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1248: "f32[512, 1024]" = torch.ops.aten.view.default(clone_97, [512, 1024]);  clone_97 = None
    permute_1506: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_674, [1, 0]);  permute_674 = None
    mm_34: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1248, permute_1506);  permute_1506 = None
    permute_1507: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1248, [1, 0])
    mm_35: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1507, view_606);  permute_1507 = view_606 = None
    permute_1508: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_35, [1, 0]);  mm_35 = None
    sum_138: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1248, [0], True);  view_1248 = None
    view_1249: "f32[1024]" = torch.ops.aten.view.default(sum_138, [1024]);  sum_138 = None
    permute_1509: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1508, [1, 0]);  permute_1508 = None
    view_1250: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_34, [512, 1, 4096]);  mm_34 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_40: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_191, torch.float32);  getitem_191 = None
    mul_465: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_40, 1.1111111111111112);  convert_element_type_40 = None
    mul_466: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1250, mul_465);  view_1250 = mul_465 = None
    clone_98: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_466, memory_format = torch.contiguous_format);  mul_466 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_467: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_605, 0.7071067811865476)
    erf_32: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_467);  mul_467 = None
    add_323: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_32, 1);  erf_32 = None
    mul_468: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_323, 0.5);  add_323 = None
    mul_469: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_605, view_605)
    mul_470: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_469, -0.5);  mul_469 = None
    exp_34: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_470);  mul_470 = None
    mul_471: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_34, 0.3989422804014327);  exp_34 = None
    mul_472: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_605, mul_471);  view_605 = mul_471 = None
    add_324: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_468, mul_472);  mul_468 = mul_472 = None
    mul_473: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_98, add_324);  clone_98 = add_324 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1251: "f32[512, 4096]" = torch.ops.aten.view.default(mul_473, [512, 4096]);  mul_473 = None
    permute_1510: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_673, [1, 0]);  permute_673 = None
    mm_36: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1251, permute_1510);  permute_1510 = None
    permute_1511: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1251, [1, 0])
    mm_37: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1511, view_604);  permute_1511 = view_604 = None
    permute_1512: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_37, [1, 0]);  mm_37 = None
    sum_139: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1251, [0], True);  view_1251 = None
    view_1252: "f32[4096]" = torch.ops.aten.view.default(sum_139, [4096]);  sum_139 = None
    permute_1513: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1512, [1, 0]);  permute_1512 = None
    view_1253: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_36, [512, 1, 1024]);  mm_36 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_325: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_461, view_1253);  mul_461 = view_1253 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_134: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_171, getitem_189);  add_171 = getitem_189 = None
    mul_474: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_134, rsqrt_30);  sub_134 = None
    mul_475: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_325, primals_290);  primals_290 = None
    mul_476: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_475, 1024)
    sum_140: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_475, [2], True)
    mul_477: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_475, mul_474);  mul_475 = None
    sum_141: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_477, [2], True);  mul_477 = None
    mul_478: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_474, sum_141);  sum_141 = None
    sub_135: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_476, sum_140);  mul_476 = sum_140 = None
    sub_136: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_135, mul_478);  sub_135 = mul_478 = None
    div_44: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_30, 1024);  rsqrt_30 = None
    mul_479: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_44, sub_136);  div_44 = sub_136 = None
    mul_480: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_325, mul_474);  mul_474 = None
    sum_142: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_480, [0, 1]);  mul_480 = None
    sum_143: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_325, [0, 1]);  add_325 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_41: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_187, torch.float32);  getitem_187 = None
    mul_481: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_41, 1.1111111111111112);  convert_element_type_41 = None
    mul_482: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_479, mul_481);  mul_481 = None
    clone_99: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_482, memory_format = torch.contiguous_format);  mul_482 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1254: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_99, [512, 1, 1024, 1, 1]);  clone_99 = None
    permute_1514: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1254, [0, 3, 4, 1, 2]);  view_1254 = None
    view_1255: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1514, [1, 512, 1024]);  permute_1514 = None
    permute_1515: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_600, [0, 2, 1]);  view_600 = None
    bmm_312: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1515, view_1255);  permute_1515 = None
    permute_1516: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_601, [0, 2, 1]);  view_601 = None
    bmm_313: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1255, permute_1516);  view_1255 = permute_1516 = None
    view_1256: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_312, [64, 16, 1, 1024, 1]);  bmm_312 = None
    permute_1517: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1256, [4, 2, 3, 0, 1]);  view_1256 = None
    view_1257: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_313, [512, 64, 16, 1, 1]);  bmm_313 = None
    permute_1518: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1257, [0, 3, 4, 1, 2]);  view_1257 = None
    permute_1519: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1517, [2, 4, 3, 0, 1]);  permute_1517 = None
    squeeze_185: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1519, 4);  permute_1519 = None
    squeeze_186: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_185, 3);  squeeze_185 = None
    permute_1520: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1518, [0, 1, 4, 3, 2]);  permute_1518 = None
    squeeze_187: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1520, 4);  permute_1520 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1258: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_187, [512, 1, 16, 64, 1]);  squeeze_187 = None
    permute_1521: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1258, [2, 0, 4, 1, 3]);  view_1258 = None
    view_1259: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1521, [16, 512, 64]);  permute_1521 = None
    permute_1522: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_596, [0, 2, 1]);  view_596 = None
    bmm_314: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1522, view_1259);  permute_1522 = None
    permute_1523: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_597, [0, 2, 1]);  view_597 = None
    bmm_315: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1259, permute_1523);  view_1259 = permute_1523 = None
    view_1260: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_314, [16, 512, 1, 64, 1]);  bmm_314 = None
    permute_1524: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1260, [4, 2, 0, 3, 1]);  view_1260 = None
    view_1261: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_315, [16, 512, 512, 1, 1]);  bmm_315 = None
    permute_1525: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1261, [1, 3, 0, 4, 2]);  view_1261 = None
    permute_1526: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1524, [4, 1, 2, 3, 0]);  permute_1524 = None
    squeeze_188: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1526, 4);  permute_1526 = None
    permute_1527: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1525, [1, 2, 0, 4, 3]);  permute_1525 = None
    squeeze_189: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1527, 4);  permute_1527 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_42: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_185, torch.float32);  getitem_185 = None
    mul_483: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_42, 1.1111111111111112);  convert_element_type_42 = None
    mul_484: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_189, mul_483);  squeeze_189 = mul_483 = None
    clone_100: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_484, memory_format = torch.contiguous_format);  mul_484 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_34: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_15);  alias_15 = None
    mul_485: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_100, alias_34);  clone_100 = None
    sum_144: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_485, [3], True)
    mul_486: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_34, sum_144);  alias_34 = sum_144 = None
    sub_137: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_485, mul_486);  mul_485 = mul_486 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_487: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_137, 0.125);  sub_137 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_41: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_8: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_41, [None, None, None, iota_17], mul_487, True);  full_41 = iota_17 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1262: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_8, [1, 16, 1023, 512]);  index_put_8 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_42: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_32: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_42, view_1262, 3, 0, 9223372036854775807);  full_42 = view_1262 = None
    full_43: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_33: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_43, slice_scatter_32, 2, 1, 9223372036854775807);  full_43 = slice_scatter_32 = None
    full_44: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_34: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_44, slice_scatter_33, 1, 0, 9223372036854775807);  full_44 = slice_scatter_33 = None
    full_45: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_35: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_45, slice_scatter_34, 0, 0, 9223372036854775807);  full_45 = slice_scatter_34 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1263: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_35, [1, 16, 512, 1024]);  slice_scatter_35 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1264: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1263, [1, 16, 512, 1024, 1]);  view_1263 = None
    permute_1528: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1264, [1, 2, 4, 0, 3]);  view_1264 = None
    view_1265: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1528, [16, 512, 1024]);  permute_1528 = None
    permute_1529: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_590, [0, 2, 1]);  view_590 = None
    bmm_316: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1529, view_1265);  permute_1529 = None
    permute_1530: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_591, [0, 2, 1]);  view_591 = None
    bmm_317: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1265, permute_1530);  view_1265 = permute_1530 = None
    view_1266: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_316, [16, 64, 1, 1024, 1]);  bmm_316 = None
    permute_1531: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1266, [2, 0, 4, 3, 1]);  view_1266 = None
    view_1267: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_317, [16, 512, 64, 1, 1]);  bmm_317 = None
    permute_1532: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1267, [3, 0, 1, 4, 2]);  view_1267 = None
    permute_1533: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1531, [3, 0, 1, 4, 2]);  permute_1531 = None
    squeeze_190: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1533, 4);  permute_1533 = None
    permute_1534: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1532, [2, 0, 1, 4, 3]);  permute_1532 = None
    squeeze_191: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1534, 4);  permute_1534 = None
    sum_145: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_191, [0, 1], True)
    view_1268: "f32[16, 64]" = torch.ops.aten.view.default(sum_145, [16, 64]);  sum_145 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1269: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_487, [1, 16, 512, 512, 1]);  mul_487 = None
    permute_1535: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1269, [1, 2, 4, 0, 3]);  view_1269 = None
    view_1270: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1535, [16, 512, 512]);  permute_1535 = None
    permute_1536: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_586, [0, 2, 1]);  view_586 = None
    bmm_318: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1536, view_1270);  permute_1536 = None
    permute_1537: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_587, [0, 2, 1]);  view_587 = None
    bmm_319: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1270, permute_1537);  view_1270 = permute_1537 = None
    view_1271: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_318, [16, 64, 1, 512, 1]);  bmm_318 = None
    permute_1538: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1271, [2, 0, 4, 3, 1]);  view_1271 = None
    view_1272: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_319, [16, 512, 64, 1, 1]);  bmm_319 = None
    permute_1539: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1272, [3, 0, 1, 4, 2]);  view_1272 = None
    permute_1540: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1538, [3, 0, 1, 4, 2]);  permute_1538 = None
    squeeze_192: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1540, 4);  permute_1540 = None
    permute_1541: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1539, [2, 0, 1, 4, 3]);  permute_1539 = None
    squeeze_193: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1541, 4);  permute_1541 = None
    sum_146: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_193, [0, 1], True)
    view_1273: "f32[16, 64]" = torch.ops.aten.view.default(sum_146, [16, 64]);  sum_146 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_326: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_191, squeeze_193);  squeeze_191 = squeeze_193 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1274: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_190, [1024, 1, 16, 64, 1]);  squeeze_190 = None
    permute_1542: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1274, [0, 4, 1, 2, 3]);  view_1274 = None
    view_1275: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1542, [1, 1024, 1024]);  permute_1542 = None
    permute_1543: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_582, [0, 2, 1]);  view_582 = None
    bmm_320: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1543, view_1275);  permute_1543 = view_1275 = None
    view_1276: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_320, [1024, 1, 16, 64, 1]);  bmm_320 = None
    permute_1544: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1276, [4, 1, 2, 3, 0]);  view_1276 = None
    permute_1545: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1544, [4, 2, 3, 0, 1]);  permute_1544 = None
    squeeze_194: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1545, 4);  permute_1545 = None
    squeeze_195: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_194, 3);  squeeze_194 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1277: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_188, [512, 1, 16, 64, 1]);  squeeze_188 = None
    permute_1546: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1277, [0, 4, 1, 2, 3]);  view_1277 = None
    clone_101: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1546, memory_format = torch.contiguous_format);  permute_1546 = None
    view_1278: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_101, [1, 512, 1024]);  clone_101 = None
    permute_1547: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_578, [0, 2, 1]);  view_578 = None
    bmm_321: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1547, view_1278);  permute_1547 = None
    permute_1548: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_579, [0, 2, 1]);  view_579 = None
    bmm_322: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1278, permute_1548);  view_1278 = permute_1548 = None
    view_1279: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_321, [1024, 1, 16, 64, 1]);  bmm_321 = None
    permute_1549: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1279, [4, 1, 2, 3, 0]);  view_1279 = None
    view_1280: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_322, [512, 1024, 1, 1, 1]);  bmm_322 = None
    permute_1550: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1280, [0, 2, 3, 4, 1]);  view_1280 = None
    permute_1551: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1549, [4, 2, 3, 0, 1]);  permute_1549 = None
    squeeze_196: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1551, 4);  permute_1551 = None
    squeeze_197: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_196, 3);  squeeze_196 = None
    permute_1552: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1550, [0, 1, 4, 2, 3]);  permute_1550 = None
    squeeze_198: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1552, 4);  permute_1552 = None
    squeeze_199: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_198, 3);  squeeze_198 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_327: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_479, squeeze_199);  mul_479 = squeeze_199 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1281: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_192, [512, 1, 16, 64, 1]);  squeeze_192 = None
    permute_1553: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1281, [0, 4, 1, 2, 3]);  view_1281 = None
    view_1282: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1553, [1, 512, 1024]);  permute_1553 = None
    permute_1554: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_574, [0, 2, 1]);  view_574 = None
    bmm_323: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1554, view_1282);  permute_1554 = None
    permute_1555: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_575, [0, 2, 1]);  view_575 = None
    bmm_324: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1282, permute_1555);  view_1282 = permute_1555 = None
    view_1283: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_323, [1024, 1, 16, 64, 1]);  bmm_323 = None
    permute_1556: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1283, [4, 1, 2, 3, 0]);  view_1283 = None
    view_1284: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_324, [512, 1024, 1, 1, 1]);  bmm_324 = None
    permute_1557: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1284, [0, 2, 3, 4, 1]);  view_1284 = None
    permute_1558: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1556, [4, 2, 3, 0, 1]);  permute_1556 = None
    squeeze_200: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1558, 4);  permute_1558 = None
    squeeze_201: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_200, 3);  squeeze_200 = None
    permute_1559: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1557, [0, 1, 4, 2, 3]);  permute_1557 = None
    squeeze_202: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1559, 4);  permute_1559 = None
    squeeze_203: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_202, 3);  squeeze_202 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_328: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_327, squeeze_203);  add_327 = squeeze_203 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1285: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_326, [512, 1, 16, 64, 1]);  add_326 = None
    permute_1560: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1285, [0, 4, 1, 2, 3]);  view_1285 = None
    clone_102: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1560, memory_format = torch.contiguous_format);  permute_1560 = None
    view_1286: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_102, [1, 512, 1024]);  clone_102 = None
    permute_1561: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_570, [0, 2, 1]);  view_570 = None
    bmm_325: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1561, view_1286);  permute_1561 = None
    permute_1562: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_571, [0, 2, 1]);  view_571 = None
    bmm_326: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1286, permute_1562);  view_1286 = permute_1562 = None
    view_1287: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_325, [1024, 1, 16, 64, 1]);  bmm_325 = None
    permute_1563: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1287, [4, 1, 2, 3, 0]);  view_1287 = None
    view_1288: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_326, [512, 1024, 1, 1, 1]);  bmm_326 = None
    permute_1564: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1288, [0, 2, 3, 4, 1]);  view_1288 = None
    permute_1565: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1563, [4, 2, 3, 0, 1]);  permute_1563 = None
    squeeze_204: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1565, 4);  permute_1565 = None
    squeeze_205: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_204, 3);  squeeze_204 = None
    permute_1566: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1564, [0, 1, 4, 2, 3]);  permute_1564 = None
    squeeze_206: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1566, 4);  permute_1566 = None
    squeeze_207: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_206, 3);  squeeze_206 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_329: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_328, squeeze_207);  add_328 = squeeze_207 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_138: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_164, getitem_183);  add_164 = getitem_183 = None
    mul_488: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_138, rsqrt_29);  sub_138 = None
    mul_489: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_329, primals_288);  primals_288 = None
    mul_490: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_489, 1024)
    sum_147: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_489, [2], True)
    mul_491: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_489, mul_488);  mul_489 = None
    sum_148: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_491, [2], True);  mul_491 = None
    mul_492: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_488, sum_148);  sum_148 = None
    sub_139: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_490, sum_147);  mul_490 = sum_147 = None
    sub_140: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_139, mul_492);  sub_139 = mul_492 = None
    div_45: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_29, 1024);  rsqrt_29 = None
    mul_493: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_45, sub_140);  div_45 = sub_140 = None
    mul_494: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_329, mul_488);  mul_488 = None
    sum_149: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_494, [0, 1]);  mul_494 = None
    sum_150: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_329, [0, 1]);  add_329 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_43: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_181, torch.float32);  getitem_181 = None
    mul_495: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_43, 1.1111111111111112);  convert_element_type_43 = None
    mul_496: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_493, mul_495);  mul_495 = None
    clone_103: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_496, memory_format = torch.contiguous_format);  mul_496 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1289: "f32[512, 1024]" = torch.ops.aten.view.default(clone_103, [512, 1024]);  clone_103 = None
    permute_1567: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_632, [1, 0]);  permute_632 = None
    mm_38: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1289, permute_1567);  permute_1567 = None
    permute_1568: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1289, [1, 0])
    mm_39: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1568, view_568);  permute_1568 = view_568 = None
    permute_1569: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_39, [1, 0]);  mm_39 = None
    sum_151: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1289, [0], True);  view_1289 = None
    view_1290: "f32[1024]" = torch.ops.aten.view.default(sum_151, [1024]);  sum_151 = None
    permute_1570: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1569, [1, 0]);  permute_1569 = None
    view_1291: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_38, [512, 1, 4096]);  mm_38 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_44: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_179, torch.float32);  getitem_179 = None
    mul_497: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_44, 1.1111111111111112);  convert_element_type_44 = None
    mul_498: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1291, mul_497);  view_1291 = mul_497 = None
    clone_104: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_498, memory_format = torch.contiguous_format);  mul_498 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_499: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_567, 0.7071067811865476)
    erf_33: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_499);  mul_499 = None
    add_330: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_33, 1);  erf_33 = None
    mul_500: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_330, 0.5);  add_330 = None
    mul_501: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_567, view_567)
    mul_502: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_501, -0.5);  mul_501 = None
    exp_35: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_502);  mul_502 = None
    mul_503: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_35, 0.3989422804014327);  exp_35 = None
    mul_504: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_567, mul_503);  view_567 = mul_503 = None
    add_331: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_500, mul_504);  mul_500 = mul_504 = None
    mul_505: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_104, add_331);  clone_104 = add_331 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1292: "f32[512, 4096]" = torch.ops.aten.view.default(mul_505, [512, 4096]);  mul_505 = None
    permute_1571: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_631, [1, 0]);  permute_631 = None
    mm_40: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1292, permute_1571);  permute_1571 = None
    permute_1572: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1292, [1, 0])
    mm_41: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1572, view_566);  permute_1572 = view_566 = None
    permute_1573: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_41, [1, 0]);  mm_41 = None
    sum_152: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1292, [0], True);  view_1292 = None
    view_1293: "f32[4096]" = torch.ops.aten.view.default(sum_152, [4096]);  sum_152 = None
    permute_1574: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1573, [1, 0]);  permute_1573 = None
    view_1294: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_40, [512, 1, 1024]);  mm_40 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_332: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_493, view_1294);  mul_493 = view_1294 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_141: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_160, getitem_177);  add_160 = getitem_177 = None
    mul_506: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_141, rsqrt_28);  sub_141 = None
    mul_507: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_332, primals_282);  primals_282 = None
    mul_508: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_507, 1024)
    sum_153: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_507, [2], True)
    mul_509: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_507, mul_506);  mul_507 = None
    sum_154: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_509, [2], True);  mul_509 = None
    mul_510: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_506, sum_154);  sum_154 = None
    sub_142: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_508, sum_153);  mul_508 = sum_153 = None
    sub_143: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_142, mul_510);  sub_142 = mul_510 = None
    div_46: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_28, 1024);  rsqrt_28 = None
    mul_511: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_46, sub_143);  div_46 = sub_143 = None
    mul_512: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_332, mul_506);  mul_506 = None
    sum_155: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_512, [0, 1]);  mul_512 = None
    sum_156: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_332, [0, 1]);  add_332 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_45: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_175, torch.float32);  getitem_175 = None
    mul_513: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_45, 1.1111111111111112);  convert_element_type_45 = None
    mul_514: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_511, mul_513);  mul_513 = None
    clone_105: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_514, memory_format = torch.contiguous_format);  mul_514 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1295: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_105, [512, 1, 1024, 1, 1]);  clone_105 = None
    permute_1575: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1295, [0, 3, 4, 1, 2]);  view_1295 = None
    view_1296: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1575, [1, 512, 1024]);  permute_1575 = None
    permute_1576: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_562, [0, 2, 1]);  view_562 = None
    bmm_327: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1576, view_1296);  permute_1576 = None
    permute_1577: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_563, [0, 2, 1]);  view_563 = None
    bmm_328: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1296, permute_1577);  view_1296 = permute_1577 = None
    view_1297: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_327, [64, 16, 1, 1024, 1]);  bmm_327 = None
    permute_1578: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1297, [4, 2, 3, 0, 1]);  view_1297 = None
    view_1298: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_328, [512, 64, 16, 1, 1]);  bmm_328 = None
    permute_1579: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1298, [0, 3, 4, 1, 2]);  view_1298 = None
    permute_1580: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1578, [2, 4, 3, 0, 1]);  permute_1578 = None
    squeeze_208: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1580, 4);  permute_1580 = None
    squeeze_209: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_208, 3);  squeeze_208 = None
    permute_1581: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1579, [0, 1, 4, 3, 2]);  permute_1579 = None
    squeeze_210: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1581, 4);  permute_1581 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1299: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_210, [512, 1, 16, 64, 1]);  squeeze_210 = None
    permute_1582: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1299, [2, 0, 4, 1, 3]);  view_1299 = None
    view_1300: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1582, [16, 512, 64]);  permute_1582 = None
    permute_1583: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_558, [0, 2, 1]);  view_558 = None
    bmm_329: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1583, view_1300);  permute_1583 = None
    permute_1584: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_559, [0, 2, 1]);  view_559 = None
    bmm_330: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1300, permute_1584);  view_1300 = permute_1584 = None
    view_1301: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_329, [16, 512, 1, 64, 1]);  bmm_329 = None
    permute_1585: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1301, [4, 2, 0, 3, 1]);  view_1301 = None
    view_1302: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_330, [16, 512, 512, 1, 1]);  bmm_330 = None
    permute_1586: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1302, [1, 3, 0, 4, 2]);  view_1302 = None
    permute_1587: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1585, [4, 1, 2, 3, 0]);  permute_1585 = None
    squeeze_211: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1587, 4);  permute_1587 = None
    permute_1588: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1586, [1, 2, 0, 4, 3]);  permute_1586 = None
    squeeze_212: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1588, 4);  permute_1588 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_46: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_173, torch.float32);  getitem_173 = None
    mul_515: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_46, 1.1111111111111112);  convert_element_type_46 = None
    mul_516: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_212, mul_515);  squeeze_212 = mul_515 = None
    clone_106: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_516, memory_format = torch.contiguous_format);  mul_516 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_35: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_14);  alias_14 = None
    mul_517: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_106, alias_35);  clone_106 = None
    sum_157: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_517, [3], True)
    mul_518: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_35, sum_157);  alias_35 = sum_157 = None
    sub_144: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_517, mul_518);  mul_517 = mul_518 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_519: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_144, 0.125);  sub_144 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_46: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_9: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_46, [None, None, None, iota_16], mul_519, True);  full_46 = iota_16 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1303: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_9, [1, 16, 1023, 512]);  index_put_9 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_47: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_36: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_47, view_1303, 3, 0, 9223372036854775807);  full_47 = view_1303 = None
    full_48: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_37: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_48, slice_scatter_36, 2, 1, 9223372036854775807);  full_48 = slice_scatter_36 = None
    full_49: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_38: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_49, slice_scatter_37, 1, 0, 9223372036854775807);  full_49 = slice_scatter_37 = None
    full_50: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_39: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_50, slice_scatter_38, 0, 0, 9223372036854775807);  full_50 = slice_scatter_38 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1304: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_39, [1, 16, 512, 1024]);  slice_scatter_39 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1305: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1304, [1, 16, 512, 1024, 1]);  view_1304 = None
    permute_1589: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1305, [1, 2, 4, 0, 3]);  view_1305 = None
    view_1306: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1589, [16, 512, 1024]);  permute_1589 = None
    permute_1590: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_552, [0, 2, 1]);  view_552 = None
    bmm_331: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1590, view_1306);  permute_1590 = None
    permute_1591: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_553, [0, 2, 1]);  view_553 = None
    bmm_332: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1306, permute_1591);  view_1306 = permute_1591 = None
    view_1307: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_331, [16, 64, 1, 1024, 1]);  bmm_331 = None
    permute_1592: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1307, [2, 0, 4, 3, 1]);  view_1307 = None
    view_1308: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_332, [16, 512, 64, 1, 1]);  bmm_332 = None
    permute_1593: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1308, [3, 0, 1, 4, 2]);  view_1308 = None
    permute_1594: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1592, [3, 0, 1, 4, 2]);  permute_1592 = None
    squeeze_213: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1594, 4);  permute_1594 = None
    permute_1595: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1593, [2, 0, 1, 4, 3]);  permute_1593 = None
    squeeze_214: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1595, 4);  permute_1595 = None
    sum_158: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_214, [0, 1], True)
    view_1309: "f32[16, 64]" = torch.ops.aten.view.default(sum_158, [16, 64]);  sum_158 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1310: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_519, [1, 16, 512, 512, 1]);  mul_519 = None
    permute_1596: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1310, [1, 2, 4, 0, 3]);  view_1310 = None
    view_1311: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1596, [16, 512, 512]);  permute_1596 = None
    permute_1597: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_548, [0, 2, 1]);  view_548 = None
    bmm_333: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1597, view_1311);  permute_1597 = None
    permute_1598: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_549, [0, 2, 1]);  view_549 = None
    bmm_334: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1311, permute_1598);  view_1311 = permute_1598 = None
    view_1312: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_333, [16, 64, 1, 512, 1]);  bmm_333 = None
    permute_1599: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1312, [2, 0, 4, 3, 1]);  view_1312 = None
    view_1313: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_334, [16, 512, 64, 1, 1]);  bmm_334 = None
    permute_1600: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1313, [3, 0, 1, 4, 2]);  view_1313 = None
    permute_1601: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1599, [3, 0, 1, 4, 2]);  permute_1599 = None
    squeeze_215: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1601, 4);  permute_1601 = None
    permute_1602: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1600, [2, 0, 1, 4, 3]);  permute_1600 = None
    squeeze_216: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1602, 4);  permute_1602 = None
    sum_159: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_216, [0, 1], True)
    view_1314: "f32[16, 64]" = torch.ops.aten.view.default(sum_159, [16, 64]);  sum_159 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_333: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_214, squeeze_216);  squeeze_214 = squeeze_216 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1315: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_213, [1024, 1, 16, 64, 1]);  squeeze_213 = None
    permute_1603: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1315, [0, 4, 1, 2, 3]);  view_1315 = None
    view_1316: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1603, [1, 1024, 1024]);  permute_1603 = None
    permute_1604: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_544, [0, 2, 1]);  view_544 = None
    bmm_335: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1604, view_1316);  permute_1604 = view_1316 = None
    view_1317: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_335, [1024, 1, 16, 64, 1]);  bmm_335 = None
    permute_1605: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1317, [4, 1, 2, 3, 0]);  view_1317 = None
    permute_1606: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1605, [4, 2, 3, 0, 1]);  permute_1605 = None
    squeeze_217: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1606, 4);  permute_1606 = None
    squeeze_218: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_217, 3);  squeeze_217 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1318: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_211, [512, 1, 16, 64, 1]);  squeeze_211 = None
    permute_1607: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1318, [0, 4, 1, 2, 3]);  view_1318 = None
    clone_107: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1607, memory_format = torch.contiguous_format);  permute_1607 = None
    view_1319: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_107, [1, 512, 1024]);  clone_107 = None
    permute_1608: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_540, [0, 2, 1]);  view_540 = None
    bmm_336: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1608, view_1319);  permute_1608 = None
    permute_1609: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_541, [0, 2, 1]);  view_541 = None
    bmm_337: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1319, permute_1609);  view_1319 = permute_1609 = None
    view_1320: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_336, [1024, 1, 16, 64, 1]);  bmm_336 = None
    permute_1610: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1320, [4, 1, 2, 3, 0]);  view_1320 = None
    view_1321: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_337, [512, 1024, 1, 1, 1]);  bmm_337 = None
    permute_1611: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1321, [0, 2, 3, 4, 1]);  view_1321 = None
    permute_1612: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1610, [4, 2, 3, 0, 1]);  permute_1610 = None
    squeeze_219: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1612, 4);  permute_1612 = None
    squeeze_220: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_219, 3);  squeeze_219 = None
    permute_1613: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1611, [0, 1, 4, 2, 3]);  permute_1611 = None
    squeeze_221: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1613, 4);  permute_1613 = None
    squeeze_222: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_221, 3);  squeeze_221 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_334: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_511, squeeze_222);  mul_511 = squeeze_222 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1322: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_215, [512, 1, 16, 64, 1]);  squeeze_215 = None
    permute_1614: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1322, [0, 4, 1, 2, 3]);  view_1322 = None
    view_1323: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1614, [1, 512, 1024]);  permute_1614 = None
    permute_1615: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_536, [0, 2, 1]);  view_536 = None
    bmm_338: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1615, view_1323);  permute_1615 = None
    permute_1616: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_537, [0, 2, 1]);  view_537 = None
    bmm_339: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1323, permute_1616);  view_1323 = permute_1616 = None
    view_1324: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_338, [1024, 1, 16, 64, 1]);  bmm_338 = None
    permute_1617: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1324, [4, 1, 2, 3, 0]);  view_1324 = None
    view_1325: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_339, [512, 1024, 1, 1, 1]);  bmm_339 = None
    permute_1618: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1325, [0, 2, 3, 4, 1]);  view_1325 = None
    permute_1619: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1617, [4, 2, 3, 0, 1]);  permute_1617 = None
    squeeze_223: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1619, 4);  permute_1619 = None
    squeeze_224: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_223, 3);  squeeze_223 = None
    permute_1620: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1618, [0, 1, 4, 2, 3]);  permute_1618 = None
    squeeze_225: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1620, 4);  permute_1620 = None
    squeeze_226: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_225, 3);  squeeze_225 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_335: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_334, squeeze_226);  add_334 = squeeze_226 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1326: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_333, [512, 1, 16, 64, 1]);  add_333 = None
    permute_1621: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1326, [0, 4, 1, 2, 3]);  view_1326 = None
    clone_108: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1621, memory_format = torch.contiguous_format);  permute_1621 = None
    view_1327: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_108, [1, 512, 1024]);  clone_108 = None
    permute_1622: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_532, [0, 2, 1]);  view_532 = None
    bmm_340: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1622, view_1327);  permute_1622 = None
    permute_1623: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_533, [0, 2, 1]);  view_533 = None
    bmm_341: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1327, permute_1623);  view_1327 = permute_1623 = None
    view_1328: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_340, [1024, 1, 16, 64, 1]);  bmm_340 = None
    permute_1624: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1328, [4, 1, 2, 3, 0]);  view_1328 = None
    view_1329: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_341, [512, 1024, 1, 1, 1]);  bmm_341 = None
    permute_1625: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1329, [0, 2, 3, 4, 1]);  view_1329 = None
    permute_1626: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1624, [4, 2, 3, 0, 1]);  permute_1624 = None
    squeeze_227: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1626, 4);  permute_1626 = None
    squeeze_228: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_227, 3);  squeeze_227 = None
    permute_1627: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1625, [0, 1, 4, 2, 3]);  permute_1625 = None
    squeeze_229: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1627, 4);  permute_1627 = None
    squeeze_230: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_229, 3);  squeeze_229 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_336: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_335, squeeze_230);  add_335 = squeeze_230 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_145: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_153, getitem_171);  add_153 = getitem_171 = None
    mul_520: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_145, rsqrt_27);  sub_145 = None
    mul_521: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_336, primals_280);  primals_280 = None
    mul_522: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_521, 1024)
    sum_160: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_521, [2], True)
    mul_523: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_521, mul_520);  mul_521 = None
    sum_161: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_523, [2], True);  mul_523 = None
    mul_524: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_520, sum_161);  sum_161 = None
    sub_146: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_522, sum_160);  mul_522 = sum_160 = None
    sub_147: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_146, mul_524);  sub_146 = mul_524 = None
    div_47: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_27, 1024);  rsqrt_27 = None
    mul_525: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_47, sub_147);  div_47 = sub_147 = None
    mul_526: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_336, mul_520);  mul_520 = None
    sum_162: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_526, [0, 1]);  mul_526 = None
    sum_163: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_336, [0, 1]);  add_336 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_47: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_169, torch.float32);  getitem_169 = None
    mul_527: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_47, 1.1111111111111112);  convert_element_type_47 = None
    mul_528: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_525, mul_527);  mul_527 = None
    clone_109: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_528, memory_format = torch.contiguous_format);  mul_528 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1330: "f32[512, 1024]" = torch.ops.aten.view.default(clone_109, [512, 1024]);  clone_109 = None
    permute_1628: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_590, [1, 0]);  permute_590 = None
    mm_42: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1330, permute_1628);  permute_1628 = None
    permute_1629: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1330, [1, 0])
    mm_43: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1629, view_530);  permute_1629 = view_530 = None
    permute_1630: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_43, [1, 0]);  mm_43 = None
    sum_164: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1330, [0], True);  view_1330 = None
    view_1331: "f32[1024]" = torch.ops.aten.view.default(sum_164, [1024]);  sum_164 = None
    permute_1631: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1630, [1, 0]);  permute_1630 = None
    view_1332: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_42, [512, 1, 4096]);  mm_42 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_48: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_167, torch.float32);  getitem_167 = None
    mul_529: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_48, 1.1111111111111112);  convert_element_type_48 = None
    mul_530: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1332, mul_529);  view_1332 = mul_529 = None
    clone_110: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_530, memory_format = torch.contiguous_format);  mul_530 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_531: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_529, 0.7071067811865476)
    erf_34: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_531);  mul_531 = None
    add_337: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_34, 1);  erf_34 = None
    mul_532: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_337, 0.5);  add_337 = None
    mul_533: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_529, view_529)
    mul_534: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_533, -0.5);  mul_533 = None
    exp_36: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_534);  mul_534 = None
    mul_535: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_36, 0.3989422804014327);  exp_36 = None
    mul_536: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_529, mul_535);  view_529 = mul_535 = None
    add_338: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_532, mul_536);  mul_532 = mul_536 = None
    mul_537: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_110, add_338);  clone_110 = add_338 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1333: "f32[512, 4096]" = torch.ops.aten.view.default(mul_537, [512, 4096]);  mul_537 = None
    permute_1632: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_589, [1, 0]);  permute_589 = None
    mm_44: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1333, permute_1632);  permute_1632 = None
    permute_1633: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1333, [1, 0])
    mm_45: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1633, view_528);  permute_1633 = view_528 = None
    permute_1634: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_45, [1, 0]);  mm_45 = None
    sum_165: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1333, [0], True);  view_1333 = None
    view_1334: "f32[4096]" = torch.ops.aten.view.default(sum_165, [4096]);  sum_165 = None
    permute_1635: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1634, [1, 0]);  permute_1634 = None
    view_1335: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_44, [512, 1, 1024]);  mm_44 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_339: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_525, view_1335);  mul_525 = view_1335 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_148: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_149, getitem_165);  add_149 = getitem_165 = None
    mul_538: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_148, rsqrt_26);  sub_148 = None
    mul_539: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_339, primals_274);  primals_274 = None
    mul_540: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_539, 1024)
    sum_166: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_539, [2], True)
    mul_541: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_539, mul_538);  mul_539 = None
    sum_167: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_541, [2], True);  mul_541 = None
    mul_542: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_538, sum_167);  sum_167 = None
    sub_149: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_540, sum_166);  mul_540 = sum_166 = None
    sub_150: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_149, mul_542);  sub_149 = mul_542 = None
    div_48: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_26, 1024);  rsqrt_26 = None
    mul_543: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_48, sub_150);  div_48 = sub_150 = None
    mul_544: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_339, mul_538);  mul_538 = None
    sum_168: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_544, [0, 1]);  mul_544 = None
    sum_169: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_339, [0, 1]);  add_339 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_49: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_163, torch.float32);  getitem_163 = None
    mul_545: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_49, 1.1111111111111112);  convert_element_type_49 = None
    mul_546: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_543, mul_545);  mul_545 = None
    clone_111: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_546, memory_format = torch.contiguous_format);  mul_546 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1336: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_111, [512, 1, 1024, 1, 1]);  clone_111 = None
    permute_1636: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1336, [0, 3, 4, 1, 2]);  view_1336 = None
    view_1337: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1636, [1, 512, 1024]);  permute_1636 = None
    permute_1637: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_524, [0, 2, 1]);  view_524 = None
    bmm_342: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1637, view_1337);  permute_1637 = None
    permute_1638: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_525, [0, 2, 1]);  view_525 = None
    bmm_343: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1337, permute_1638);  view_1337 = permute_1638 = None
    view_1338: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_342, [64, 16, 1, 1024, 1]);  bmm_342 = None
    permute_1639: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1338, [4, 2, 3, 0, 1]);  view_1338 = None
    view_1339: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_343, [512, 64, 16, 1, 1]);  bmm_343 = None
    permute_1640: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1339, [0, 3, 4, 1, 2]);  view_1339 = None
    permute_1641: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1639, [2, 4, 3, 0, 1]);  permute_1639 = None
    squeeze_231: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1641, 4);  permute_1641 = None
    squeeze_232: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_231, 3);  squeeze_231 = None
    permute_1642: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1640, [0, 1, 4, 3, 2]);  permute_1640 = None
    squeeze_233: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1642, 4);  permute_1642 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1340: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_233, [512, 1, 16, 64, 1]);  squeeze_233 = None
    permute_1643: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1340, [2, 0, 4, 1, 3]);  view_1340 = None
    view_1341: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1643, [16, 512, 64]);  permute_1643 = None
    permute_1644: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_520, [0, 2, 1]);  view_520 = None
    bmm_344: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1644, view_1341);  permute_1644 = None
    permute_1645: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_521, [0, 2, 1]);  view_521 = None
    bmm_345: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1341, permute_1645);  view_1341 = permute_1645 = None
    view_1342: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_344, [16, 512, 1, 64, 1]);  bmm_344 = None
    permute_1646: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1342, [4, 2, 0, 3, 1]);  view_1342 = None
    view_1343: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_345, [16, 512, 512, 1, 1]);  bmm_345 = None
    permute_1647: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1343, [1, 3, 0, 4, 2]);  view_1343 = None
    permute_1648: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1646, [4, 1, 2, 3, 0]);  permute_1646 = None
    squeeze_234: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1648, 4);  permute_1648 = None
    permute_1649: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1647, [1, 2, 0, 4, 3]);  permute_1647 = None
    squeeze_235: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1649, 4);  permute_1649 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_50: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_161, torch.float32);  getitem_161 = None
    mul_547: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_50, 1.1111111111111112);  convert_element_type_50 = None
    mul_548: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_235, mul_547);  squeeze_235 = mul_547 = None
    clone_112: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_548, memory_format = torch.contiguous_format);  mul_548 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_36: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_13);  alias_13 = None
    mul_549: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_112, alias_36);  clone_112 = None
    sum_170: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_549, [3], True)
    mul_550: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_36, sum_170);  alias_36 = sum_170 = None
    sub_151: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_549, mul_550);  mul_549 = mul_550 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_551: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_151, 0.125);  sub_151 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_51: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_10: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_51, [None, None, None, iota_15], mul_551, True);  full_51 = iota_15 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1344: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_10, [1, 16, 1023, 512]);  index_put_10 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_52: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_40: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_52, view_1344, 3, 0, 9223372036854775807);  full_52 = view_1344 = None
    full_53: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_41: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_53, slice_scatter_40, 2, 1, 9223372036854775807);  full_53 = slice_scatter_40 = None
    full_54: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_42: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_54, slice_scatter_41, 1, 0, 9223372036854775807);  full_54 = slice_scatter_41 = None
    full_55: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_43: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_55, slice_scatter_42, 0, 0, 9223372036854775807);  full_55 = slice_scatter_42 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1345: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_43, [1, 16, 512, 1024]);  slice_scatter_43 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1346: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1345, [1, 16, 512, 1024, 1]);  view_1345 = None
    permute_1650: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1346, [1, 2, 4, 0, 3]);  view_1346 = None
    view_1347: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1650, [16, 512, 1024]);  permute_1650 = None
    permute_1651: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_514, [0, 2, 1]);  view_514 = None
    bmm_346: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1651, view_1347);  permute_1651 = None
    permute_1652: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_515, [0, 2, 1]);  view_515 = None
    bmm_347: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1347, permute_1652);  view_1347 = permute_1652 = None
    view_1348: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_346, [16, 64, 1, 1024, 1]);  bmm_346 = None
    permute_1653: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1348, [2, 0, 4, 3, 1]);  view_1348 = None
    view_1349: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_347, [16, 512, 64, 1, 1]);  bmm_347 = None
    permute_1654: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1349, [3, 0, 1, 4, 2]);  view_1349 = None
    permute_1655: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1653, [3, 0, 1, 4, 2]);  permute_1653 = None
    squeeze_236: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1655, 4);  permute_1655 = None
    permute_1656: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1654, [2, 0, 1, 4, 3]);  permute_1654 = None
    squeeze_237: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1656, 4);  permute_1656 = None
    sum_171: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_237, [0, 1], True)
    view_1350: "f32[16, 64]" = torch.ops.aten.view.default(sum_171, [16, 64]);  sum_171 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1351: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_551, [1, 16, 512, 512, 1]);  mul_551 = None
    permute_1657: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1351, [1, 2, 4, 0, 3]);  view_1351 = None
    view_1352: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1657, [16, 512, 512]);  permute_1657 = None
    permute_1658: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_510, [0, 2, 1]);  view_510 = None
    bmm_348: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1658, view_1352);  permute_1658 = None
    permute_1659: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_511, [0, 2, 1]);  view_511 = None
    bmm_349: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1352, permute_1659);  view_1352 = permute_1659 = None
    view_1353: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_348, [16, 64, 1, 512, 1]);  bmm_348 = None
    permute_1660: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1353, [2, 0, 4, 3, 1]);  view_1353 = None
    view_1354: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_349, [16, 512, 64, 1, 1]);  bmm_349 = None
    permute_1661: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1354, [3, 0, 1, 4, 2]);  view_1354 = None
    permute_1662: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1660, [3, 0, 1, 4, 2]);  permute_1660 = None
    squeeze_238: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1662, 4);  permute_1662 = None
    permute_1663: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1661, [2, 0, 1, 4, 3]);  permute_1661 = None
    squeeze_239: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1663, 4);  permute_1663 = None
    sum_172: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_239, [0, 1], True)
    view_1355: "f32[16, 64]" = torch.ops.aten.view.default(sum_172, [16, 64]);  sum_172 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_340: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_237, squeeze_239);  squeeze_237 = squeeze_239 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1356: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_236, [1024, 1, 16, 64, 1]);  squeeze_236 = None
    permute_1664: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1356, [0, 4, 1, 2, 3]);  view_1356 = None
    view_1357: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1664, [1, 1024, 1024]);  permute_1664 = None
    permute_1665: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_506, [0, 2, 1]);  view_506 = None
    bmm_350: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1665, view_1357);  permute_1665 = view_1357 = None
    view_1358: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_350, [1024, 1, 16, 64, 1]);  bmm_350 = None
    permute_1666: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1358, [4, 1, 2, 3, 0]);  view_1358 = None
    permute_1667: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1666, [4, 2, 3, 0, 1]);  permute_1666 = None
    squeeze_240: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1667, 4);  permute_1667 = None
    squeeze_241: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_240, 3);  squeeze_240 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1359: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_234, [512, 1, 16, 64, 1]);  squeeze_234 = None
    permute_1668: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1359, [0, 4, 1, 2, 3]);  view_1359 = None
    clone_113: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1668, memory_format = torch.contiguous_format);  permute_1668 = None
    view_1360: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_113, [1, 512, 1024]);  clone_113 = None
    permute_1669: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_502, [0, 2, 1]);  view_502 = None
    bmm_351: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1669, view_1360);  permute_1669 = None
    permute_1670: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_503, [0, 2, 1]);  view_503 = None
    bmm_352: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1360, permute_1670);  view_1360 = permute_1670 = None
    view_1361: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_351, [1024, 1, 16, 64, 1]);  bmm_351 = None
    permute_1671: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1361, [4, 1, 2, 3, 0]);  view_1361 = None
    view_1362: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_352, [512, 1024, 1, 1, 1]);  bmm_352 = None
    permute_1672: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1362, [0, 2, 3, 4, 1]);  view_1362 = None
    permute_1673: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1671, [4, 2, 3, 0, 1]);  permute_1671 = None
    squeeze_242: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1673, 4);  permute_1673 = None
    squeeze_243: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_242, 3);  squeeze_242 = None
    permute_1674: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1672, [0, 1, 4, 2, 3]);  permute_1672 = None
    squeeze_244: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1674, 4);  permute_1674 = None
    squeeze_245: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_244, 3);  squeeze_244 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_341: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_543, squeeze_245);  mul_543 = squeeze_245 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1363: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_238, [512, 1, 16, 64, 1]);  squeeze_238 = None
    permute_1675: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1363, [0, 4, 1, 2, 3]);  view_1363 = None
    view_1364: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1675, [1, 512, 1024]);  permute_1675 = None
    permute_1676: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_498, [0, 2, 1]);  view_498 = None
    bmm_353: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1676, view_1364);  permute_1676 = None
    permute_1677: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_499, [0, 2, 1]);  view_499 = None
    bmm_354: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1364, permute_1677);  view_1364 = permute_1677 = None
    view_1365: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_353, [1024, 1, 16, 64, 1]);  bmm_353 = None
    permute_1678: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1365, [4, 1, 2, 3, 0]);  view_1365 = None
    view_1366: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_354, [512, 1024, 1, 1, 1]);  bmm_354 = None
    permute_1679: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1366, [0, 2, 3, 4, 1]);  view_1366 = None
    permute_1680: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1678, [4, 2, 3, 0, 1]);  permute_1678 = None
    squeeze_246: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1680, 4);  permute_1680 = None
    squeeze_247: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_246, 3);  squeeze_246 = None
    permute_1681: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1679, [0, 1, 4, 2, 3]);  permute_1679 = None
    squeeze_248: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1681, 4);  permute_1681 = None
    squeeze_249: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_248, 3);  squeeze_248 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_342: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_341, squeeze_249);  add_341 = squeeze_249 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1367: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_340, [512, 1, 16, 64, 1]);  add_340 = None
    permute_1682: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1367, [0, 4, 1, 2, 3]);  view_1367 = None
    clone_114: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1682, memory_format = torch.contiguous_format);  permute_1682 = None
    view_1368: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_114, [1, 512, 1024]);  clone_114 = None
    permute_1683: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_494, [0, 2, 1]);  view_494 = None
    bmm_355: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1683, view_1368);  permute_1683 = None
    permute_1684: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_495, [0, 2, 1]);  view_495 = None
    bmm_356: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1368, permute_1684);  view_1368 = permute_1684 = None
    view_1369: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_355, [1024, 1, 16, 64, 1]);  bmm_355 = None
    permute_1685: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1369, [4, 1, 2, 3, 0]);  view_1369 = None
    view_1370: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_356, [512, 1024, 1, 1, 1]);  bmm_356 = None
    permute_1686: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1370, [0, 2, 3, 4, 1]);  view_1370 = None
    permute_1687: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1685, [4, 2, 3, 0, 1]);  permute_1685 = None
    squeeze_250: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1687, 4);  permute_1687 = None
    squeeze_251: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_250, 3);  squeeze_250 = None
    permute_1688: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1686, [0, 1, 4, 2, 3]);  permute_1686 = None
    squeeze_252: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1688, 4);  permute_1688 = None
    squeeze_253: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_252, 3);  squeeze_252 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_343: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_342, squeeze_253);  add_342 = squeeze_253 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_152: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_142, getitem_159);  add_142 = getitem_159 = None
    mul_552: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_152, rsqrt_25);  sub_152 = None
    mul_553: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_343, primals_272);  primals_272 = None
    mul_554: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_553, 1024)
    sum_173: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_553, [2], True)
    mul_555: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_553, mul_552);  mul_553 = None
    sum_174: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_555, [2], True);  mul_555 = None
    mul_556: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_552, sum_174);  sum_174 = None
    sub_153: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_554, sum_173);  mul_554 = sum_173 = None
    sub_154: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_153, mul_556);  sub_153 = mul_556 = None
    div_49: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_25, 1024);  rsqrt_25 = None
    mul_557: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_49, sub_154);  div_49 = sub_154 = None
    mul_558: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_343, mul_552);  mul_552 = None
    sum_175: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_558, [0, 1]);  mul_558 = None
    sum_176: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_343, [0, 1]);  add_343 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_51: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_157, torch.float32);  getitem_157 = None
    mul_559: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_51, 1.1111111111111112);  convert_element_type_51 = None
    mul_560: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_557, mul_559);  mul_559 = None
    clone_115: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_560, memory_format = torch.contiguous_format);  mul_560 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1371: "f32[512, 1024]" = torch.ops.aten.view.default(clone_115, [512, 1024]);  clone_115 = None
    permute_1689: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_548, [1, 0]);  permute_548 = None
    mm_46: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1371, permute_1689);  permute_1689 = None
    permute_1690: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1371, [1, 0])
    mm_47: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1690, view_492);  permute_1690 = view_492 = None
    permute_1691: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_47, [1, 0]);  mm_47 = None
    sum_177: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1371, [0], True);  view_1371 = None
    view_1372: "f32[1024]" = torch.ops.aten.view.default(sum_177, [1024]);  sum_177 = None
    permute_1692: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1691, [1, 0]);  permute_1691 = None
    view_1373: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_46, [512, 1, 4096]);  mm_46 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_52: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_155, torch.float32);  getitem_155 = None
    mul_561: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_52, 1.1111111111111112);  convert_element_type_52 = None
    mul_562: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1373, mul_561);  view_1373 = mul_561 = None
    clone_116: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_562, memory_format = torch.contiguous_format);  mul_562 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_563: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_491, 0.7071067811865476)
    erf_35: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_563);  mul_563 = None
    add_344: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_35, 1);  erf_35 = None
    mul_564: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_344, 0.5);  add_344 = None
    mul_565: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_491, view_491)
    mul_566: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_565, -0.5);  mul_565 = None
    exp_37: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_566);  mul_566 = None
    mul_567: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_37, 0.3989422804014327);  exp_37 = None
    mul_568: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_491, mul_567);  view_491 = mul_567 = None
    add_345: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_564, mul_568);  mul_564 = mul_568 = None
    mul_569: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_116, add_345);  clone_116 = add_345 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1374: "f32[512, 4096]" = torch.ops.aten.view.default(mul_569, [512, 4096]);  mul_569 = None
    permute_1693: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_547, [1, 0]);  permute_547 = None
    mm_48: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1374, permute_1693);  permute_1693 = None
    permute_1694: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1374, [1, 0])
    mm_49: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1694, view_490);  permute_1694 = view_490 = None
    permute_1695: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_49, [1, 0]);  mm_49 = None
    sum_178: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1374, [0], True);  view_1374 = None
    view_1375: "f32[4096]" = torch.ops.aten.view.default(sum_178, [4096]);  sum_178 = None
    permute_1696: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1695, [1, 0]);  permute_1695 = None
    view_1376: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_48, [512, 1, 1024]);  mm_48 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_346: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_557, view_1376);  mul_557 = view_1376 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_155: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_138, getitem_153);  add_138 = getitem_153 = None
    mul_570: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_155, rsqrt_24);  sub_155 = None
    mul_571: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_346, primals_266);  primals_266 = None
    mul_572: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_571, 1024)
    sum_179: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_571, [2], True)
    mul_573: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_571, mul_570);  mul_571 = None
    sum_180: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_573, [2], True);  mul_573 = None
    mul_574: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_570, sum_180);  sum_180 = None
    sub_156: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_572, sum_179);  mul_572 = sum_179 = None
    sub_157: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_156, mul_574);  sub_156 = mul_574 = None
    div_50: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_24, 1024);  rsqrt_24 = None
    mul_575: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_50, sub_157);  div_50 = sub_157 = None
    mul_576: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_346, mul_570);  mul_570 = None
    sum_181: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_576, [0, 1]);  mul_576 = None
    sum_182: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_346, [0, 1]);  add_346 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_53: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_151, torch.float32);  getitem_151 = None
    mul_577: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_53, 1.1111111111111112);  convert_element_type_53 = None
    mul_578: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_575, mul_577);  mul_577 = None
    clone_117: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_578, memory_format = torch.contiguous_format);  mul_578 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1377: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_117, [512, 1, 1024, 1, 1]);  clone_117 = None
    permute_1697: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1377, [0, 3, 4, 1, 2]);  view_1377 = None
    view_1378: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1697, [1, 512, 1024]);  permute_1697 = None
    permute_1698: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_486, [0, 2, 1]);  view_486 = None
    bmm_357: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1698, view_1378);  permute_1698 = None
    permute_1699: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_487, [0, 2, 1]);  view_487 = None
    bmm_358: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1378, permute_1699);  view_1378 = permute_1699 = None
    view_1379: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_357, [64, 16, 1, 1024, 1]);  bmm_357 = None
    permute_1700: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1379, [4, 2, 3, 0, 1]);  view_1379 = None
    view_1380: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_358, [512, 64, 16, 1, 1]);  bmm_358 = None
    permute_1701: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1380, [0, 3, 4, 1, 2]);  view_1380 = None
    permute_1702: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1700, [2, 4, 3, 0, 1]);  permute_1700 = None
    squeeze_254: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1702, 4);  permute_1702 = None
    squeeze_255: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_254, 3);  squeeze_254 = None
    permute_1703: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1701, [0, 1, 4, 3, 2]);  permute_1701 = None
    squeeze_256: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1703, 4);  permute_1703 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1381: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_256, [512, 1, 16, 64, 1]);  squeeze_256 = None
    permute_1704: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1381, [2, 0, 4, 1, 3]);  view_1381 = None
    view_1382: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1704, [16, 512, 64]);  permute_1704 = None
    permute_1705: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_482, [0, 2, 1]);  view_482 = None
    bmm_359: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1705, view_1382);  permute_1705 = None
    permute_1706: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_483, [0, 2, 1]);  view_483 = None
    bmm_360: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1382, permute_1706);  view_1382 = permute_1706 = None
    view_1383: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_359, [16, 512, 1, 64, 1]);  bmm_359 = None
    permute_1707: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1383, [4, 2, 0, 3, 1]);  view_1383 = None
    view_1384: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_360, [16, 512, 512, 1, 1]);  bmm_360 = None
    permute_1708: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1384, [1, 3, 0, 4, 2]);  view_1384 = None
    permute_1709: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1707, [4, 1, 2, 3, 0]);  permute_1707 = None
    squeeze_257: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1709, 4);  permute_1709 = None
    permute_1710: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1708, [1, 2, 0, 4, 3]);  permute_1708 = None
    squeeze_258: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1710, 4);  permute_1710 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_54: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_149, torch.float32);  getitem_149 = None
    mul_579: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_54, 1.1111111111111112);  convert_element_type_54 = None
    mul_580: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_258, mul_579);  squeeze_258 = mul_579 = None
    clone_118: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_580, memory_format = torch.contiguous_format);  mul_580 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_37: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_12);  alias_12 = None
    mul_581: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_118, alias_37);  clone_118 = None
    sum_183: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_581, [3], True)
    mul_582: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_37, sum_183);  alias_37 = sum_183 = None
    sub_158: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_581, mul_582);  mul_581 = mul_582 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_583: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_158, 0.125);  sub_158 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_56: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_11: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_56, [None, None, None, iota_14], mul_583, True);  full_56 = iota_14 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1385: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_11, [1, 16, 1023, 512]);  index_put_11 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_57: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_44: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_57, view_1385, 3, 0, 9223372036854775807);  full_57 = view_1385 = None
    full_58: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_45: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_58, slice_scatter_44, 2, 1, 9223372036854775807);  full_58 = slice_scatter_44 = None
    full_59: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_46: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_59, slice_scatter_45, 1, 0, 9223372036854775807);  full_59 = slice_scatter_45 = None
    full_60: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_47: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_60, slice_scatter_46, 0, 0, 9223372036854775807);  full_60 = slice_scatter_46 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1386: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_47, [1, 16, 512, 1024]);  slice_scatter_47 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1387: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1386, [1, 16, 512, 1024, 1]);  view_1386 = None
    permute_1711: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1387, [1, 2, 4, 0, 3]);  view_1387 = None
    view_1388: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1711, [16, 512, 1024]);  permute_1711 = None
    permute_1712: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_476, [0, 2, 1]);  view_476 = None
    bmm_361: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1712, view_1388);  permute_1712 = None
    permute_1713: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_477, [0, 2, 1]);  view_477 = None
    bmm_362: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1388, permute_1713);  view_1388 = permute_1713 = None
    view_1389: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_361, [16, 64, 1, 1024, 1]);  bmm_361 = None
    permute_1714: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1389, [2, 0, 4, 3, 1]);  view_1389 = None
    view_1390: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_362, [16, 512, 64, 1, 1]);  bmm_362 = None
    permute_1715: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1390, [3, 0, 1, 4, 2]);  view_1390 = None
    permute_1716: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1714, [3, 0, 1, 4, 2]);  permute_1714 = None
    squeeze_259: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1716, 4);  permute_1716 = None
    permute_1717: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1715, [2, 0, 1, 4, 3]);  permute_1715 = None
    squeeze_260: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1717, 4);  permute_1717 = None
    sum_184: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_260, [0, 1], True)
    view_1391: "f32[16, 64]" = torch.ops.aten.view.default(sum_184, [16, 64]);  sum_184 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1392: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_583, [1, 16, 512, 512, 1]);  mul_583 = None
    permute_1718: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1392, [1, 2, 4, 0, 3]);  view_1392 = None
    view_1393: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1718, [16, 512, 512]);  permute_1718 = None
    permute_1719: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_472, [0, 2, 1]);  view_472 = None
    bmm_363: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1719, view_1393);  permute_1719 = None
    permute_1720: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_473, [0, 2, 1]);  view_473 = None
    bmm_364: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1393, permute_1720);  view_1393 = permute_1720 = None
    view_1394: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_363, [16, 64, 1, 512, 1]);  bmm_363 = None
    permute_1721: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1394, [2, 0, 4, 3, 1]);  view_1394 = None
    view_1395: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_364, [16, 512, 64, 1, 1]);  bmm_364 = None
    permute_1722: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1395, [3, 0, 1, 4, 2]);  view_1395 = None
    permute_1723: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1721, [3, 0, 1, 4, 2]);  permute_1721 = None
    squeeze_261: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1723, 4);  permute_1723 = None
    permute_1724: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1722, [2, 0, 1, 4, 3]);  permute_1722 = None
    squeeze_262: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1724, 4);  permute_1724 = None
    sum_185: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_262, [0, 1], True)
    view_1396: "f32[16, 64]" = torch.ops.aten.view.default(sum_185, [16, 64]);  sum_185 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_347: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_260, squeeze_262);  squeeze_260 = squeeze_262 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1397: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_259, [1024, 1, 16, 64, 1]);  squeeze_259 = None
    permute_1725: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1397, [0, 4, 1, 2, 3]);  view_1397 = None
    view_1398: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1725, [1, 1024, 1024]);  permute_1725 = None
    permute_1726: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_468, [0, 2, 1]);  view_468 = None
    bmm_365: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1726, view_1398);  permute_1726 = view_1398 = None
    view_1399: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_365, [1024, 1, 16, 64, 1]);  bmm_365 = None
    permute_1727: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1399, [4, 1, 2, 3, 0]);  view_1399 = None
    permute_1728: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1727, [4, 2, 3, 0, 1]);  permute_1727 = None
    squeeze_263: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1728, 4);  permute_1728 = None
    squeeze_264: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_263, 3);  squeeze_263 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1400: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_257, [512, 1, 16, 64, 1]);  squeeze_257 = None
    permute_1729: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1400, [0, 4, 1, 2, 3]);  view_1400 = None
    clone_119: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1729, memory_format = torch.contiguous_format);  permute_1729 = None
    view_1401: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_119, [1, 512, 1024]);  clone_119 = None
    permute_1730: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_464, [0, 2, 1]);  view_464 = None
    bmm_366: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1730, view_1401);  permute_1730 = None
    permute_1731: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_465, [0, 2, 1]);  view_465 = None
    bmm_367: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1401, permute_1731);  view_1401 = permute_1731 = None
    view_1402: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_366, [1024, 1, 16, 64, 1]);  bmm_366 = None
    permute_1732: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1402, [4, 1, 2, 3, 0]);  view_1402 = None
    view_1403: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_367, [512, 1024, 1, 1, 1]);  bmm_367 = None
    permute_1733: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1403, [0, 2, 3, 4, 1]);  view_1403 = None
    permute_1734: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1732, [4, 2, 3, 0, 1]);  permute_1732 = None
    squeeze_265: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1734, 4);  permute_1734 = None
    squeeze_266: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_265, 3);  squeeze_265 = None
    permute_1735: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1733, [0, 1, 4, 2, 3]);  permute_1733 = None
    squeeze_267: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1735, 4);  permute_1735 = None
    squeeze_268: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_267, 3);  squeeze_267 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_348: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_575, squeeze_268);  mul_575 = squeeze_268 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1404: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_261, [512, 1, 16, 64, 1]);  squeeze_261 = None
    permute_1736: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1404, [0, 4, 1, 2, 3]);  view_1404 = None
    view_1405: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1736, [1, 512, 1024]);  permute_1736 = None
    permute_1737: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_460, [0, 2, 1]);  view_460 = None
    bmm_368: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1737, view_1405);  permute_1737 = None
    permute_1738: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_461, [0, 2, 1]);  view_461 = None
    bmm_369: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1405, permute_1738);  view_1405 = permute_1738 = None
    view_1406: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_368, [1024, 1, 16, 64, 1]);  bmm_368 = None
    permute_1739: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1406, [4, 1, 2, 3, 0]);  view_1406 = None
    view_1407: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_369, [512, 1024, 1, 1, 1]);  bmm_369 = None
    permute_1740: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1407, [0, 2, 3, 4, 1]);  view_1407 = None
    permute_1741: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1739, [4, 2, 3, 0, 1]);  permute_1739 = None
    squeeze_269: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1741, 4);  permute_1741 = None
    squeeze_270: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_269, 3);  squeeze_269 = None
    permute_1742: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1740, [0, 1, 4, 2, 3]);  permute_1740 = None
    squeeze_271: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1742, 4);  permute_1742 = None
    squeeze_272: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_271, 3);  squeeze_271 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_349: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_348, squeeze_272);  add_348 = squeeze_272 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1408: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_347, [512, 1, 16, 64, 1]);  add_347 = None
    permute_1743: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1408, [0, 4, 1, 2, 3]);  view_1408 = None
    clone_120: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1743, memory_format = torch.contiguous_format);  permute_1743 = None
    view_1409: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_120, [1, 512, 1024]);  clone_120 = None
    permute_1744: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_456, [0, 2, 1]);  view_456 = None
    bmm_370: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1744, view_1409);  permute_1744 = None
    permute_1745: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_457, [0, 2, 1]);  view_457 = None
    bmm_371: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1409, permute_1745);  view_1409 = permute_1745 = None
    view_1410: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_370, [1024, 1, 16, 64, 1]);  bmm_370 = None
    permute_1746: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1410, [4, 1, 2, 3, 0]);  view_1410 = None
    view_1411: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_371, [512, 1024, 1, 1, 1]);  bmm_371 = None
    permute_1747: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1411, [0, 2, 3, 4, 1]);  view_1411 = None
    permute_1748: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1746, [4, 2, 3, 0, 1]);  permute_1746 = None
    squeeze_273: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1748, 4);  permute_1748 = None
    squeeze_274: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_273, 3);  squeeze_273 = None
    permute_1749: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1747, [0, 1, 4, 2, 3]);  permute_1747 = None
    squeeze_275: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1749, 4);  permute_1749 = None
    squeeze_276: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_275, 3);  squeeze_275 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_350: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_349, squeeze_276);  add_349 = squeeze_276 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_159: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_131, getitem_147);  add_131 = getitem_147 = None
    mul_584: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_159, rsqrt_23);  sub_159 = None
    mul_585: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_350, primals_264);  primals_264 = None
    mul_586: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_585, 1024)
    sum_186: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_585, [2], True)
    mul_587: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_585, mul_584);  mul_585 = None
    sum_187: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_587, [2], True);  mul_587 = None
    mul_588: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_584, sum_187);  sum_187 = None
    sub_160: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_586, sum_186);  mul_586 = sum_186 = None
    sub_161: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_160, mul_588);  sub_160 = mul_588 = None
    div_51: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_23, 1024);  rsqrt_23 = None
    mul_589: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_51, sub_161);  div_51 = sub_161 = None
    mul_590: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_350, mul_584);  mul_584 = None
    sum_188: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_590, [0, 1]);  mul_590 = None
    sum_189: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_350, [0, 1]);  add_350 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_55: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_145, torch.float32);  getitem_145 = None
    mul_591: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_55, 1.1111111111111112);  convert_element_type_55 = None
    mul_592: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_589, mul_591);  mul_591 = None
    clone_121: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_592, memory_format = torch.contiguous_format);  mul_592 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1412: "f32[512, 1024]" = torch.ops.aten.view.default(clone_121, [512, 1024]);  clone_121 = None
    permute_1750: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_506, [1, 0]);  permute_506 = None
    mm_50: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1412, permute_1750);  permute_1750 = None
    permute_1751: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1412, [1, 0])
    mm_51: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1751, view_454);  permute_1751 = view_454 = None
    permute_1752: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_51, [1, 0]);  mm_51 = None
    sum_190: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1412, [0], True);  view_1412 = None
    view_1413: "f32[1024]" = torch.ops.aten.view.default(sum_190, [1024]);  sum_190 = None
    permute_1753: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1752, [1, 0]);  permute_1752 = None
    view_1414: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_50, [512, 1, 4096]);  mm_50 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_56: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_143, torch.float32);  getitem_143 = None
    mul_593: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_56, 1.1111111111111112);  convert_element_type_56 = None
    mul_594: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1414, mul_593);  view_1414 = mul_593 = None
    clone_122: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_594, memory_format = torch.contiguous_format);  mul_594 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_595: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_453, 0.7071067811865476)
    erf_36: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_595);  mul_595 = None
    add_351: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_36, 1);  erf_36 = None
    mul_596: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_351, 0.5);  add_351 = None
    mul_597: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_453, view_453)
    mul_598: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_597, -0.5);  mul_597 = None
    exp_38: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_598);  mul_598 = None
    mul_599: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_38, 0.3989422804014327);  exp_38 = None
    mul_600: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_453, mul_599);  view_453 = mul_599 = None
    add_352: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_596, mul_600);  mul_596 = mul_600 = None
    mul_601: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_122, add_352);  clone_122 = add_352 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1415: "f32[512, 4096]" = torch.ops.aten.view.default(mul_601, [512, 4096]);  mul_601 = None
    permute_1754: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_505, [1, 0]);  permute_505 = None
    mm_52: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1415, permute_1754);  permute_1754 = None
    permute_1755: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1415, [1, 0])
    mm_53: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1755, view_452);  permute_1755 = view_452 = None
    permute_1756: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_53, [1, 0]);  mm_53 = None
    sum_191: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1415, [0], True);  view_1415 = None
    view_1416: "f32[4096]" = torch.ops.aten.view.default(sum_191, [4096]);  sum_191 = None
    permute_1757: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1756, [1, 0]);  permute_1756 = None
    view_1417: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_52, [512, 1, 1024]);  mm_52 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_353: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_589, view_1417);  mul_589 = view_1417 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_162: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_127, getitem_141);  add_127 = getitem_141 = None
    mul_602: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_162, rsqrt_22);  sub_162 = None
    mul_603: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_353, primals_258);  primals_258 = None
    mul_604: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_603, 1024)
    sum_192: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_603, [2], True)
    mul_605: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_603, mul_602);  mul_603 = None
    sum_193: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_605, [2], True);  mul_605 = None
    mul_606: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_602, sum_193);  sum_193 = None
    sub_163: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_604, sum_192);  mul_604 = sum_192 = None
    sub_164: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_163, mul_606);  sub_163 = mul_606 = None
    div_52: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_22, 1024);  rsqrt_22 = None
    mul_607: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_52, sub_164);  div_52 = sub_164 = None
    mul_608: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_353, mul_602);  mul_602 = None
    sum_194: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_608, [0, 1]);  mul_608 = None
    sum_195: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_353, [0, 1]);  add_353 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_57: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_139, torch.float32);  getitem_139 = None
    mul_609: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_57, 1.1111111111111112);  convert_element_type_57 = None
    mul_610: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_607, mul_609);  mul_609 = None
    clone_123: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_610, memory_format = torch.contiguous_format);  mul_610 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1418: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_123, [512, 1, 1024, 1, 1]);  clone_123 = None
    permute_1758: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1418, [0, 3, 4, 1, 2]);  view_1418 = None
    view_1419: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1758, [1, 512, 1024]);  permute_1758 = None
    permute_1759: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_448, [0, 2, 1]);  view_448 = None
    bmm_372: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1759, view_1419);  permute_1759 = None
    permute_1760: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_449, [0, 2, 1]);  view_449 = None
    bmm_373: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1419, permute_1760);  view_1419 = permute_1760 = None
    view_1420: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_372, [64, 16, 1, 1024, 1]);  bmm_372 = None
    permute_1761: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1420, [4, 2, 3, 0, 1]);  view_1420 = None
    view_1421: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_373, [512, 64, 16, 1, 1]);  bmm_373 = None
    permute_1762: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1421, [0, 3, 4, 1, 2]);  view_1421 = None
    permute_1763: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1761, [2, 4, 3, 0, 1]);  permute_1761 = None
    squeeze_277: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1763, 4);  permute_1763 = None
    squeeze_278: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_277, 3);  squeeze_277 = None
    permute_1764: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1762, [0, 1, 4, 3, 2]);  permute_1762 = None
    squeeze_279: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1764, 4);  permute_1764 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1422: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_279, [512, 1, 16, 64, 1]);  squeeze_279 = None
    permute_1765: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1422, [2, 0, 4, 1, 3]);  view_1422 = None
    view_1423: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1765, [16, 512, 64]);  permute_1765 = None
    permute_1766: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_444, [0, 2, 1]);  view_444 = None
    bmm_374: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1766, view_1423);  permute_1766 = None
    permute_1767: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_445, [0, 2, 1]);  view_445 = None
    bmm_375: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1423, permute_1767);  view_1423 = permute_1767 = None
    view_1424: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_374, [16, 512, 1, 64, 1]);  bmm_374 = None
    permute_1768: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1424, [4, 2, 0, 3, 1]);  view_1424 = None
    view_1425: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_375, [16, 512, 512, 1, 1]);  bmm_375 = None
    permute_1769: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1425, [1, 3, 0, 4, 2]);  view_1425 = None
    permute_1770: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1768, [4, 1, 2, 3, 0]);  permute_1768 = None
    squeeze_280: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1770, 4);  permute_1770 = None
    permute_1771: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1769, [1, 2, 0, 4, 3]);  permute_1769 = None
    squeeze_281: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1771, 4);  permute_1771 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_58: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_137, torch.float32);  getitem_137 = None
    mul_611: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_58, 1.1111111111111112);  convert_element_type_58 = None
    mul_612: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_281, mul_611);  squeeze_281 = mul_611 = None
    clone_124: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_612, memory_format = torch.contiguous_format);  mul_612 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_38: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_11);  alias_11 = None
    mul_613: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_124, alias_38);  clone_124 = None
    sum_196: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_613, [3], True)
    mul_614: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_38, sum_196);  alias_38 = sum_196 = None
    sub_165: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_613, mul_614);  mul_613 = mul_614 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_615: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_165, 0.125);  sub_165 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_61: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_12: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_61, [None, None, None, iota_13], mul_615, True);  full_61 = iota_13 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1426: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_12, [1, 16, 1023, 512]);  index_put_12 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_62: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_48: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_62, view_1426, 3, 0, 9223372036854775807);  full_62 = view_1426 = None
    full_63: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_49: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_63, slice_scatter_48, 2, 1, 9223372036854775807);  full_63 = slice_scatter_48 = None
    full_64: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_50: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_64, slice_scatter_49, 1, 0, 9223372036854775807);  full_64 = slice_scatter_49 = None
    full_65: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_51: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_65, slice_scatter_50, 0, 0, 9223372036854775807);  full_65 = slice_scatter_50 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1427: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_51, [1, 16, 512, 1024]);  slice_scatter_51 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1428: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1427, [1, 16, 512, 1024, 1]);  view_1427 = None
    permute_1772: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1428, [1, 2, 4, 0, 3]);  view_1428 = None
    view_1429: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1772, [16, 512, 1024]);  permute_1772 = None
    permute_1773: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_438, [0, 2, 1]);  view_438 = None
    bmm_376: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1773, view_1429);  permute_1773 = None
    permute_1774: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_439, [0, 2, 1]);  view_439 = None
    bmm_377: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1429, permute_1774);  view_1429 = permute_1774 = None
    view_1430: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_376, [16, 64, 1, 1024, 1]);  bmm_376 = None
    permute_1775: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1430, [2, 0, 4, 3, 1]);  view_1430 = None
    view_1431: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_377, [16, 512, 64, 1, 1]);  bmm_377 = None
    permute_1776: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1431, [3, 0, 1, 4, 2]);  view_1431 = None
    permute_1777: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1775, [3, 0, 1, 4, 2]);  permute_1775 = None
    squeeze_282: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1777, 4);  permute_1777 = None
    permute_1778: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1776, [2, 0, 1, 4, 3]);  permute_1776 = None
    squeeze_283: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1778, 4);  permute_1778 = None
    sum_197: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_283, [0, 1], True)
    view_1432: "f32[16, 64]" = torch.ops.aten.view.default(sum_197, [16, 64]);  sum_197 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1433: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_615, [1, 16, 512, 512, 1]);  mul_615 = None
    permute_1779: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1433, [1, 2, 4, 0, 3]);  view_1433 = None
    view_1434: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1779, [16, 512, 512]);  permute_1779 = None
    permute_1780: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_434, [0, 2, 1]);  view_434 = None
    bmm_378: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1780, view_1434);  permute_1780 = None
    permute_1781: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_435, [0, 2, 1]);  view_435 = None
    bmm_379: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1434, permute_1781);  view_1434 = permute_1781 = None
    view_1435: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_378, [16, 64, 1, 512, 1]);  bmm_378 = None
    permute_1782: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1435, [2, 0, 4, 3, 1]);  view_1435 = None
    view_1436: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_379, [16, 512, 64, 1, 1]);  bmm_379 = None
    permute_1783: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1436, [3, 0, 1, 4, 2]);  view_1436 = None
    permute_1784: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1782, [3, 0, 1, 4, 2]);  permute_1782 = None
    squeeze_284: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1784, 4);  permute_1784 = None
    permute_1785: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1783, [2, 0, 1, 4, 3]);  permute_1783 = None
    squeeze_285: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1785, 4);  permute_1785 = None
    sum_198: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_285, [0, 1], True)
    view_1437: "f32[16, 64]" = torch.ops.aten.view.default(sum_198, [16, 64]);  sum_198 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_354: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_283, squeeze_285);  squeeze_283 = squeeze_285 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1438: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_282, [1024, 1, 16, 64, 1]);  squeeze_282 = None
    permute_1786: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1438, [0, 4, 1, 2, 3]);  view_1438 = None
    view_1439: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1786, [1, 1024, 1024]);  permute_1786 = None
    permute_1787: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_430, [0, 2, 1]);  view_430 = None
    bmm_380: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1787, view_1439);  permute_1787 = view_1439 = None
    view_1440: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_380, [1024, 1, 16, 64, 1]);  bmm_380 = None
    permute_1788: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1440, [4, 1, 2, 3, 0]);  view_1440 = None
    permute_1789: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1788, [4, 2, 3, 0, 1]);  permute_1788 = None
    squeeze_286: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1789, 4);  permute_1789 = None
    squeeze_287: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_286, 3);  squeeze_286 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1441: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_280, [512, 1, 16, 64, 1]);  squeeze_280 = None
    permute_1790: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1441, [0, 4, 1, 2, 3]);  view_1441 = None
    clone_125: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1790, memory_format = torch.contiguous_format);  permute_1790 = None
    view_1442: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_125, [1, 512, 1024]);  clone_125 = None
    permute_1791: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_426, [0, 2, 1]);  view_426 = None
    bmm_381: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1791, view_1442);  permute_1791 = None
    permute_1792: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_427, [0, 2, 1]);  view_427 = None
    bmm_382: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1442, permute_1792);  view_1442 = permute_1792 = None
    view_1443: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_381, [1024, 1, 16, 64, 1]);  bmm_381 = None
    permute_1793: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1443, [4, 1, 2, 3, 0]);  view_1443 = None
    view_1444: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_382, [512, 1024, 1, 1, 1]);  bmm_382 = None
    permute_1794: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1444, [0, 2, 3, 4, 1]);  view_1444 = None
    permute_1795: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1793, [4, 2, 3, 0, 1]);  permute_1793 = None
    squeeze_288: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1795, 4);  permute_1795 = None
    squeeze_289: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_288, 3);  squeeze_288 = None
    permute_1796: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1794, [0, 1, 4, 2, 3]);  permute_1794 = None
    squeeze_290: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1796, 4);  permute_1796 = None
    squeeze_291: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_290, 3);  squeeze_290 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_355: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_607, squeeze_291);  mul_607 = squeeze_291 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1445: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_284, [512, 1, 16, 64, 1]);  squeeze_284 = None
    permute_1797: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1445, [0, 4, 1, 2, 3]);  view_1445 = None
    view_1446: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1797, [1, 512, 1024]);  permute_1797 = None
    permute_1798: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_422, [0, 2, 1]);  view_422 = None
    bmm_383: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1798, view_1446);  permute_1798 = None
    permute_1799: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_423, [0, 2, 1]);  view_423 = None
    bmm_384: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1446, permute_1799);  view_1446 = permute_1799 = None
    view_1447: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_383, [1024, 1, 16, 64, 1]);  bmm_383 = None
    permute_1800: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1447, [4, 1, 2, 3, 0]);  view_1447 = None
    view_1448: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_384, [512, 1024, 1, 1, 1]);  bmm_384 = None
    permute_1801: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1448, [0, 2, 3, 4, 1]);  view_1448 = None
    permute_1802: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1800, [4, 2, 3, 0, 1]);  permute_1800 = None
    squeeze_292: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1802, 4);  permute_1802 = None
    squeeze_293: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_292, 3);  squeeze_292 = None
    permute_1803: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1801, [0, 1, 4, 2, 3]);  permute_1801 = None
    squeeze_294: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1803, 4);  permute_1803 = None
    squeeze_295: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_294, 3);  squeeze_294 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_356: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_355, squeeze_295);  add_355 = squeeze_295 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1449: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_354, [512, 1, 16, 64, 1]);  add_354 = None
    permute_1804: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1449, [0, 4, 1, 2, 3]);  view_1449 = None
    clone_126: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1804, memory_format = torch.contiguous_format);  permute_1804 = None
    view_1450: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_126, [1, 512, 1024]);  clone_126 = None
    permute_1805: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_418, [0, 2, 1]);  view_418 = None
    bmm_385: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1805, view_1450);  permute_1805 = None
    permute_1806: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_419, [0, 2, 1]);  view_419 = None
    bmm_386: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1450, permute_1806);  view_1450 = permute_1806 = None
    view_1451: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_385, [1024, 1, 16, 64, 1]);  bmm_385 = None
    permute_1807: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1451, [4, 1, 2, 3, 0]);  view_1451 = None
    view_1452: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_386, [512, 1024, 1, 1, 1]);  bmm_386 = None
    permute_1808: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1452, [0, 2, 3, 4, 1]);  view_1452 = None
    permute_1809: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1807, [4, 2, 3, 0, 1]);  permute_1807 = None
    squeeze_296: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1809, 4);  permute_1809 = None
    squeeze_297: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_296, 3);  squeeze_296 = None
    permute_1810: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1808, [0, 1, 4, 2, 3]);  permute_1808 = None
    squeeze_298: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1810, 4);  permute_1810 = None
    squeeze_299: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_298, 3);  squeeze_298 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_357: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_356, squeeze_299);  add_356 = squeeze_299 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_166: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_120, getitem_135);  add_120 = getitem_135 = None
    mul_616: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_166, rsqrt_21);  sub_166 = None
    mul_617: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_357, primals_256);  primals_256 = None
    mul_618: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_617, 1024)
    sum_199: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_617, [2], True)
    mul_619: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_617, mul_616);  mul_617 = None
    sum_200: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_619, [2], True);  mul_619 = None
    mul_620: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_616, sum_200);  sum_200 = None
    sub_167: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_618, sum_199);  mul_618 = sum_199 = None
    sub_168: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_167, mul_620);  sub_167 = mul_620 = None
    div_53: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_21, 1024);  rsqrt_21 = None
    mul_621: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_53, sub_168);  div_53 = sub_168 = None
    mul_622: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_357, mul_616);  mul_616 = None
    sum_201: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_622, [0, 1]);  mul_622 = None
    sum_202: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_357, [0, 1]);  add_357 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_59: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_133, torch.float32);  getitem_133 = None
    mul_623: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_59, 1.1111111111111112);  convert_element_type_59 = None
    mul_624: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_621, mul_623);  mul_623 = None
    clone_127: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_624, memory_format = torch.contiguous_format);  mul_624 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1453: "f32[512, 1024]" = torch.ops.aten.view.default(clone_127, [512, 1024]);  clone_127 = None
    permute_1811: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_464, [1, 0]);  permute_464 = None
    mm_54: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1453, permute_1811);  permute_1811 = None
    permute_1812: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1453, [1, 0])
    mm_55: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1812, view_416);  permute_1812 = view_416 = None
    permute_1813: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_55, [1, 0]);  mm_55 = None
    sum_203: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1453, [0], True);  view_1453 = None
    view_1454: "f32[1024]" = torch.ops.aten.view.default(sum_203, [1024]);  sum_203 = None
    permute_1814: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1813, [1, 0]);  permute_1813 = None
    view_1455: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_54, [512, 1, 4096]);  mm_54 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_60: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_131, torch.float32);  getitem_131 = None
    mul_625: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_60, 1.1111111111111112);  convert_element_type_60 = None
    mul_626: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1455, mul_625);  view_1455 = mul_625 = None
    clone_128: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_626, memory_format = torch.contiguous_format);  mul_626 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_627: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_415, 0.7071067811865476)
    erf_37: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_627);  mul_627 = None
    add_358: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_37, 1);  erf_37 = None
    mul_628: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_358, 0.5);  add_358 = None
    mul_629: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_415, view_415)
    mul_630: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_629, -0.5);  mul_629 = None
    exp_39: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_630);  mul_630 = None
    mul_631: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_39, 0.3989422804014327);  exp_39 = None
    mul_632: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_415, mul_631);  view_415 = mul_631 = None
    add_359: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_628, mul_632);  mul_628 = mul_632 = None
    mul_633: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_128, add_359);  clone_128 = add_359 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1456: "f32[512, 4096]" = torch.ops.aten.view.default(mul_633, [512, 4096]);  mul_633 = None
    permute_1815: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_463, [1, 0]);  permute_463 = None
    mm_56: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1456, permute_1815);  permute_1815 = None
    permute_1816: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1456, [1, 0])
    mm_57: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1816, view_414);  permute_1816 = view_414 = None
    permute_1817: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_57, [1, 0]);  mm_57 = None
    sum_204: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1456, [0], True);  view_1456 = None
    view_1457: "f32[4096]" = torch.ops.aten.view.default(sum_204, [4096]);  sum_204 = None
    permute_1818: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1817, [1, 0]);  permute_1817 = None
    view_1458: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_56, [512, 1, 1024]);  mm_56 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_360: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_621, view_1458);  mul_621 = view_1458 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_169: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_116, getitem_129);  add_116 = getitem_129 = None
    mul_634: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_169, rsqrt_20);  sub_169 = None
    mul_635: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_360, primals_250);  primals_250 = None
    mul_636: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_635, 1024)
    sum_205: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_635, [2], True)
    mul_637: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_635, mul_634);  mul_635 = None
    sum_206: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_637, [2], True);  mul_637 = None
    mul_638: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_634, sum_206);  sum_206 = None
    sub_170: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_636, sum_205);  mul_636 = sum_205 = None
    sub_171: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_170, mul_638);  sub_170 = mul_638 = None
    div_54: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_20, 1024);  rsqrt_20 = None
    mul_639: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_54, sub_171);  div_54 = sub_171 = None
    mul_640: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_360, mul_634);  mul_634 = None
    sum_207: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_640, [0, 1]);  mul_640 = None
    sum_208: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_360, [0, 1]);  add_360 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_61: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_127, torch.float32);  getitem_127 = None
    mul_641: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_61, 1.1111111111111112);  convert_element_type_61 = None
    mul_642: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_639, mul_641);  mul_641 = None
    clone_129: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_642, memory_format = torch.contiguous_format);  mul_642 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1459: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_129, [512, 1, 1024, 1, 1]);  clone_129 = None
    permute_1819: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1459, [0, 3, 4, 1, 2]);  view_1459 = None
    view_1460: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1819, [1, 512, 1024]);  permute_1819 = None
    permute_1820: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_410, [0, 2, 1]);  view_410 = None
    bmm_387: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1820, view_1460);  permute_1820 = None
    permute_1821: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_411, [0, 2, 1]);  view_411 = None
    bmm_388: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1460, permute_1821);  view_1460 = permute_1821 = None
    view_1461: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_387, [64, 16, 1, 1024, 1]);  bmm_387 = None
    permute_1822: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1461, [4, 2, 3, 0, 1]);  view_1461 = None
    view_1462: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_388, [512, 64, 16, 1, 1]);  bmm_388 = None
    permute_1823: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1462, [0, 3, 4, 1, 2]);  view_1462 = None
    permute_1824: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1822, [2, 4, 3, 0, 1]);  permute_1822 = None
    squeeze_300: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1824, 4);  permute_1824 = None
    squeeze_301: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_300, 3);  squeeze_300 = None
    permute_1825: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1823, [0, 1, 4, 3, 2]);  permute_1823 = None
    squeeze_302: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1825, 4);  permute_1825 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1463: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_302, [512, 1, 16, 64, 1]);  squeeze_302 = None
    permute_1826: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1463, [2, 0, 4, 1, 3]);  view_1463 = None
    view_1464: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1826, [16, 512, 64]);  permute_1826 = None
    permute_1827: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_406, [0, 2, 1]);  view_406 = None
    bmm_389: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1827, view_1464);  permute_1827 = None
    permute_1828: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_407, [0, 2, 1]);  view_407 = None
    bmm_390: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1464, permute_1828);  view_1464 = permute_1828 = None
    view_1465: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_389, [16, 512, 1, 64, 1]);  bmm_389 = None
    permute_1829: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1465, [4, 2, 0, 3, 1]);  view_1465 = None
    view_1466: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_390, [16, 512, 512, 1, 1]);  bmm_390 = None
    permute_1830: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1466, [1, 3, 0, 4, 2]);  view_1466 = None
    permute_1831: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1829, [4, 1, 2, 3, 0]);  permute_1829 = None
    squeeze_303: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1831, 4);  permute_1831 = None
    permute_1832: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1830, [1, 2, 0, 4, 3]);  permute_1830 = None
    squeeze_304: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1832, 4);  permute_1832 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_62: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_125, torch.float32);  getitem_125 = None
    mul_643: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_62, 1.1111111111111112);  convert_element_type_62 = None
    mul_644: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_304, mul_643);  squeeze_304 = mul_643 = None
    clone_130: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_644, memory_format = torch.contiguous_format);  mul_644 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_39: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_10);  alias_10 = None
    mul_645: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_130, alias_39);  clone_130 = None
    sum_209: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_645, [3], True)
    mul_646: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_39, sum_209);  alias_39 = sum_209 = None
    sub_172: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_645, mul_646);  mul_645 = mul_646 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_647: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_172, 0.125);  sub_172 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_66: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_13: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_66, [None, None, None, iota_12], mul_647, True);  full_66 = iota_12 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1467: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_13, [1, 16, 1023, 512]);  index_put_13 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_67: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_52: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_67, view_1467, 3, 0, 9223372036854775807);  full_67 = view_1467 = None
    full_68: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_53: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_68, slice_scatter_52, 2, 1, 9223372036854775807);  full_68 = slice_scatter_52 = None
    full_69: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_54: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_69, slice_scatter_53, 1, 0, 9223372036854775807);  full_69 = slice_scatter_53 = None
    full_70: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_55: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_70, slice_scatter_54, 0, 0, 9223372036854775807);  full_70 = slice_scatter_54 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1468: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_55, [1, 16, 512, 1024]);  slice_scatter_55 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1469: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1468, [1, 16, 512, 1024, 1]);  view_1468 = None
    permute_1833: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1469, [1, 2, 4, 0, 3]);  view_1469 = None
    view_1470: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1833, [16, 512, 1024]);  permute_1833 = None
    permute_1834: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_400, [0, 2, 1]);  view_400 = None
    bmm_391: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1834, view_1470);  permute_1834 = None
    permute_1835: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_401, [0, 2, 1]);  view_401 = None
    bmm_392: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1470, permute_1835);  view_1470 = permute_1835 = None
    view_1471: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_391, [16, 64, 1, 1024, 1]);  bmm_391 = None
    permute_1836: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1471, [2, 0, 4, 3, 1]);  view_1471 = None
    view_1472: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_392, [16, 512, 64, 1, 1]);  bmm_392 = None
    permute_1837: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1472, [3, 0, 1, 4, 2]);  view_1472 = None
    permute_1838: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1836, [3, 0, 1, 4, 2]);  permute_1836 = None
    squeeze_305: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1838, 4);  permute_1838 = None
    permute_1839: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1837, [2, 0, 1, 4, 3]);  permute_1837 = None
    squeeze_306: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1839, 4);  permute_1839 = None
    sum_210: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_306, [0, 1], True)
    view_1473: "f32[16, 64]" = torch.ops.aten.view.default(sum_210, [16, 64]);  sum_210 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1474: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_647, [1, 16, 512, 512, 1]);  mul_647 = None
    permute_1840: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1474, [1, 2, 4, 0, 3]);  view_1474 = None
    view_1475: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1840, [16, 512, 512]);  permute_1840 = None
    permute_1841: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_396, [0, 2, 1]);  view_396 = None
    bmm_393: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1841, view_1475);  permute_1841 = None
    permute_1842: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_397, [0, 2, 1]);  view_397 = None
    bmm_394: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1475, permute_1842);  view_1475 = permute_1842 = None
    view_1476: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_393, [16, 64, 1, 512, 1]);  bmm_393 = None
    permute_1843: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1476, [2, 0, 4, 3, 1]);  view_1476 = None
    view_1477: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_394, [16, 512, 64, 1, 1]);  bmm_394 = None
    permute_1844: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1477, [3, 0, 1, 4, 2]);  view_1477 = None
    permute_1845: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1843, [3, 0, 1, 4, 2]);  permute_1843 = None
    squeeze_307: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1845, 4);  permute_1845 = None
    permute_1846: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1844, [2, 0, 1, 4, 3]);  permute_1844 = None
    squeeze_308: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1846, 4);  permute_1846 = None
    sum_211: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_308, [0, 1], True)
    view_1478: "f32[16, 64]" = torch.ops.aten.view.default(sum_211, [16, 64]);  sum_211 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_361: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_306, squeeze_308);  squeeze_306 = squeeze_308 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1479: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_305, [1024, 1, 16, 64, 1]);  squeeze_305 = None
    permute_1847: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1479, [0, 4, 1, 2, 3]);  view_1479 = None
    view_1480: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1847, [1, 1024, 1024]);  permute_1847 = None
    permute_1848: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_392, [0, 2, 1]);  view_392 = None
    bmm_395: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1848, view_1480);  permute_1848 = view_1480 = None
    view_1481: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_395, [1024, 1, 16, 64, 1]);  bmm_395 = None
    permute_1849: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1481, [4, 1, 2, 3, 0]);  view_1481 = None
    permute_1850: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1849, [4, 2, 3, 0, 1]);  permute_1849 = None
    squeeze_309: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1850, 4);  permute_1850 = None
    squeeze_310: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_309, 3);  squeeze_309 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1482: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_303, [512, 1, 16, 64, 1]);  squeeze_303 = None
    permute_1851: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1482, [0, 4, 1, 2, 3]);  view_1482 = None
    clone_131: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1851, memory_format = torch.contiguous_format);  permute_1851 = None
    view_1483: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_131, [1, 512, 1024]);  clone_131 = None
    permute_1852: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_388, [0, 2, 1]);  view_388 = None
    bmm_396: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1852, view_1483);  permute_1852 = None
    permute_1853: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_389, [0, 2, 1]);  view_389 = None
    bmm_397: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1483, permute_1853);  view_1483 = permute_1853 = None
    view_1484: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_396, [1024, 1, 16, 64, 1]);  bmm_396 = None
    permute_1854: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1484, [4, 1, 2, 3, 0]);  view_1484 = None
    view_1485: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_397, [512, 1024, 1, 1, 1]);  bmm_397 = None
    permute_1855: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1485, [0, 2, 3, 4, 1]);  view_1485 = None
    permute_1856: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1854, [4, 2, 3, 0, 1]);  permute_1854 = None
    squeeze_311: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1856, 4);  permute_1856 = None
    squeeze_312: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_311, 3);  squeeze_311 = None
    permute_1857: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1855, [0, 1, 4, 2, 3]);  permute_1855 = None
    squeeze_313: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1857, 4);  permute_1857 = None
    squeeze_314: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_313, 3);  squeeze_313 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_362: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_639, squeeze_314);  mul_639 = squeeze_314 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1486: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_307, [512, 1, 16, 64, 1]);  squeeze_307 = None
    permute_1858: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1486, [0, 4, 1, 2, 3]);  view_1486 = None
    view_1487: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1858, [1, 512, 1024]);  permute_1858 = None
    permute_1859: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_384, [0, 2, 1]);  view_384 = None
    bmm_398: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1859, view_1487);  permute_1859 = None
    permute_1860: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_385, [0, 2, 1]);  view_385 = None
    bmm_399: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1487, permute_1860);  view_1487 = permute_1860 = None
    view_1488: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_398, [1024, 1, 16, 64, 1]);  bmm_398 = None
    permute_1861: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1488, [4, 1, 2, 3, 0]);  view_1488 = None
    view_1489: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_399, [512, 1024, 1, 1, 1]);  bmm_399 = None
    permute_1862: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1489, [0, 2, 3, 4, 1]);  view_1489 = None
    permute_1863: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1861, [4, 2, 3, 0, 1]);  permute_1861 = None
    squeeze_315: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1863, 4);  permute_1863 = None
    squeeze_316: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_315, 3);  squeeze_315 = None
    permute_1864: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1862, [0, 1, 4, 2, 3]);  permute_1862 = None
    squeeze_317: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1864, 4);  permute_1864 = None
    squeeze_318: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_317, 3);  squeeze_317 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_363: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_362, squeeze_318);  add_362 = squeeze_318 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1490: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_361, [512, 1, 16, 64, 1]);  add_361 = None
    permute_1865: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1490, [0, 4, 1, 2, 3]);  view_1490 = None
    clone_132: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1865, memory_format = torch.contiguous_format);  permute_1865 = None
    view_1491: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_132, [1, 512, 1024]);  clone_132 = None
    permute_1866: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_380, [0, 2, 1]);  view_380 = None
    bmm_400: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1866, view_1491);  permute_1866 = None
    permute_1867: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_381, [0, 2, 1]);  view_381 = None
    bmm_401: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1491, permute_1867);  view_1491 = permute_1867 = None
    view_1492: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_400, [1024, 1, 16, 64, 1]);  bmm_400 = None
    permute_1868: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1492, [4, 1, 2, 3, 0]);  view_1492 = None
    view_1493: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_401, [512, 1024, 1, 1, 1]);  bmm_401 = None
    permute_1869: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1493, [0, 2, 3, 4, 1]);  view_1493 = None
    permute_1870: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1868, [4, 2, 3, 0, 1]);  permute_1868 = None
    squeeze_319: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1870, 4);  permute_1870 = None
    squeeze_320: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_319, 3);  squeeze_319 = None
    permute_1871: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1869, [0, 1, 4, 2, 3]);  permute_1869 = None
    squeeze_321: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1871, 4);  permute_1871 = None
    squeeze_322: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_321, 3);  squeeze_321 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_364: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_363, squeeze_322);  add_363 = squeeze_322 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_173: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_109, getitem_123);  add_109 = getitem_123 = None
    mul_648: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_173, rsqrt_19);  sub_173 = None
    mul_649: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_364, primals_248);  primals_248 = None
    mul_650: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_649, 1024)
    sum_212: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_649, [2], True)
    mul_651: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_649, mul_648);  mul_649 = None
    sum_213: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_651, [2], True);  mul_651 = None
    mul_652: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_648, sum_213);  sum_213 = None
    sub_174: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_650, sum_212);  mul_650 = sum_212 = None
    sub_175: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_174, mul_652);  sub_174 = mul_652 = None
    div_55: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_19, 1024);  rsqrt_19 = None
    mul_653: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_55, sub_175);  div_55 = sub_175 = None
    mul_654: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_364, mul_648);  mul_648 = None
    sum_214: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_654, [0, 1]);  mul_654 = None
    sum_215: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_364, [0, 1]);  add_364 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_63: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_121, torch.float32);  getitem_121 = None
    mul_655: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_63, 1.1111111111111112);  convert_element_type_63 = None
    mul_656: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_653, mul_655);  mul_655 = None
    clone_133: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_656, memory_format = torch.contiguous_format);  mul_656 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1494: "f32[512, 1024]" = torch.ops.aten.view.default(clone_133, [512, 1024]);  clone_133 = None
    permute_1872: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_422, [1, 0]);  permute_422 = None
    mm_58: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1494, permute_1872);  permute_1872 = None
    permute_1873: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1494, [1, 0])
    mm_59: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1873, view_378);  permute_1873 = view_378 = None
    permute_1874: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_59, [1, 0]);  mm_59 = None
    sum_216: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1494, [0], True);  view_1494 = None
    view_1495: "f32[1024]" = torch.ops.aten.view.default(sum_216, [1024]);  sum_216 = None
    permute_1875: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1874, [1, 0]);  permute_1874 = None
    view_1496: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_58, [512, 1, 4096]);  mm_58 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_64: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_119, torch.float32);  getitem_119 = None
    mul_657: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_64, 1.1111111111111112);  convert_element_type_64 = None
    mul_658: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1496, mul_657);  view_1496 = mul_657 = None
    clone_134: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_658, memory_format = torch.contiguous_format);  mul_658 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_659: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_377, 0.7071067811865476)
    erf_38: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_659);  mul_659 = None
    add_365: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_38, 1);  erf_38 = None
    mul_660: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_365, 0.5);  add_365 = None
    mul_661: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_377, view_377)
    mul_662: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_661, -0.5);  mul_661 = None
    exp_40: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_662);  mul_662 = None
    mul_663: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_40, 0.3989422804014327);  exp_40 = None
    mul_664: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_377, mul_663);  view_377 = mul_663 = None
    add_366: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_660, mul_664);  mul_660 = mul_664 = None
    mul_665: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_134, add_366);  clone_134 = add_366 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1497: "f32[512, 4096]" = torch.ops.aten.view.default(mul_665, [512, 4096]);  mul_665 = None
    permute_1876: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_421, [1, 0]);  permute_421 = None
    mm_60: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1497, permute_1876);  permute_1876 = None
    permute_1877: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1497, [1, 0])
    mm_61: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1877, view_376);  permute_1877 = view_376 = None
    permute_1878: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_61, [1, 0]);  mm_61 = None
    sum_217: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1497, [0], True);  view_1497 = None
    view_1498: "f32[4096]" = torch.ops.aten.view.default(sum_217, [4096]);  sum_217 = None
    permute_1879: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1878, [1, 0]);  permute_1878 = None
    view_1499: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_60, [512, 1, 1024]);  mm_60 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_367: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_653, view_1499);  mul_653 = view_1499 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_176: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_105, getitem_117);  add_105 = getitem_117 = None
    mul_666: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_176, rsqrt_18);  sub_176 = None
    mul_667: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_367, primals_242);  primals_242 = None
    mul_668: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_667, 1024)
    sum_218: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_667, [2], True)
    mul_669: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_667, mul_666);  mul_667 = None
    sum_219: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_669, [2], True);  mul_669 = None
    mul_670: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_666, sum_219);  sum_219 = None
    sub_177: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_668, sum_218);  mul_668 = sum_218 = None
    sub_178: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_177, mul_670);  sub_177 = mul_670 = None
    div_56: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_18, 1024);  rsqrt_18 = None
    mul_671: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_56, sub_178);  div_56 = sub_178 = None
    mul_672: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_367, mul_666);  mul_666 = None
    sum_220: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_672, [0, 1]);  mul_672 = None
    sum_221: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_367, [0, 1]);  add_367 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_65: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_115, torch.float32);  getitem_115 = None
    mul_673: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_65, 1.1111111111111112);  convert_element_type_65 = None
    mul_674: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_671, mul_673);  mul_673 = None
    clone_135: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_674, memory_format = torch.contiguous_format);  mul_674 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1500: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_135, [512, 1, 1024, 1, 1]);  clone_135 = None
    permute_1880: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1500, [0, 3, 4, 1, 2]);  view_1500 = None
    view_1501: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1880, [1, 512, 1024]);  permute_1880 = None
    permute_1881: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_372, [0, 2, 1]);  view_372 = None
    bmm_402: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1881, view_1501);  permute_1881 = None
    permute_1882: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_373, [0, 2, 1]);  view_373 = None
    bmm_403: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1501, permute_1882);  view_1501 = permute_1882 = None
    view_1502: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_402, [64, 16, 1, 1024, 1]);  bmm_402 = None
    permute_1883: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1502, [4, 2, 3, 0, 1]);  view_1502 = None
    view_1503: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_403, [512, 64, 16, 1, 1]);  bmm_403 = None
    permute_1884: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1503, [0, 3, 4, 1, 2]);  view_1503 = None
    permute_1885: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1883, [2, 4, 3, 0, 1]);  permute_1883 = None
    squeeze_323: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1885, 4);  permute_1885 = None
    squeeze_324: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_323, 3);  squeeze_323 = None
    permute_1886: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1884, [0, 1, 4, 3, 2]);  permute_1884 = None
    squeeze_325: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1886, 4);  permute_1886 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1504: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_325, [512, 1, 16, 64, 1]);  squeeze_325 = None
    permute_1887: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1504, [2, 0, 4, 1, 3]);  view_1504 = None
    view_1505: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1887, [16, 512, 64]);  permute_1887 = None
    permute_1888: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_368, [0, 2, 1]);  view_368 = None
    bmm_404: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1888, view_1505);  permute_1888 = None
    permute_1889: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_369, [0, 2, 1]);  view_369 = None
    bmm_405: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1505, permute_1889);  view_1505 = permute_1889 = None
    view_1506: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_404, [16, 512, 1, 64, 1]);  bmm_404 = None
    permute_1890: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1506, [4, 2, 0, 3, 1]);  view_1506 = None
    view_1507: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_405, [16, 512, 512, 1, 1]);  bmm_405 = None
    permute_1891: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1507, [1, 3, 0, 4, 2]);  view_1507 = None
    permute_1892: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1890, [4, 1, 2, 3, 0]);  permute_1890 = None
    squeeze_326: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1892, 4);  permute_1892 = None
    permute_1893: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1891, [1, 2, 0, 4, 3]);  permute_1891 = None
    squeeze_327: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1893, 4);  permute_1893 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_66: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_113, torch.float32);  getitem_113 = None
    mul_675: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_66, 1.1111111111111112);  convert_element_type_66 = None
    mul_676: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_327, mul_675);  squeeze_327 = mul_675 = None
    clone_136: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_676, memory_format = torch.contiguous_format);  mul_676 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_40: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_9);  alias_9 = None
    mul_677: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_136, alias_40);  clone_136 = None
    sum_222: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_677, [3], True)
    mul_678: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_40, sum_222);  alias_40 = sum_222 = None
    sub_179: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_677, mul_678);  mul_677 = mul_678 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_679: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_179, 0.125);  sub_179 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_71: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_14: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_71, [None, None, None, iota_11], mul_679, True);  full_71 = iota_11 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1508: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_14, [1, 16, 1023, 512]);  index_put_14 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_72: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_56: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_72, view_1508, 3, 0, 9223372036854775807);  full_72 = view_1508 = None
    full_73: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_57: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_73, slice_scatter_56, 2, 1, 9223372036854775807);  full_73 = slice_scatter_56 = None
    full_74: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_58: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_74, slice_scatter_57, 1, 0, 9223372036854775807);  full_74 = slice_scatter_57 = None
    full_75: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_59: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_75, slice_scatter_58, 0, 0, 9223372036854775807);  full_75 = slice_scatter_58 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1509: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_59, [1, 16, 512, 1024]);  slice_scatter_59 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1510: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1509, [1, 16, 512, 1024, 1]);  view_1509 = None
    permute_1894: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1510, [1, 2, 4, 0, 3]);  view_1510 = None
    view_1511: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1894, [16, 512, 1024]);  permute_1894 = None
    permute_1895: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_362, [0, 2, 1]);  view_362 = None
    bmm_406: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1895, view_1511);  permute_1895 = None
    permute_1896: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_363, [0, 2, 1]);  view_363 = None
    bmm_407: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1511, permute_1896);  view_1511 = permute_1896 = None
    view_1512: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_406, [16, 64, 1, 1024, 1]);  bmm_406 = None
    permute_1897: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1512, [2, 0, 4, 3, 1]);  view_1512 = None
    view_1513: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_407, [16, 512, 64, 1, 1]);  bmm_407 = None
    permute_1898: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1513, [3, 0, 1, 4, 2]);  view_1513 = None
    permute_1899: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1897, [3, 0, 1, 4, 2]);  permute_1897 = None
    squeeze_328: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1899, 4);  permute_1899 = None
    permute_1900: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1898, [2, 0, 1, 4, 3]);  permute_1898 = None
    squeeze_329: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1900, 4);  permute_1900 = None
    sum_223: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_329, [0, 1], True)
    view_1514: "f32[16, 64]" = torch.ops.aten.view.default(sum_223, [16, 64]);  sum_223 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1515: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_679, [1, 16, 512, 512, 1]);  mul_679 = None
    permute_1901: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1515, [1, 2, 4, 0, 3]);  view_1515 = None
    view_1516: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1901, [16, 512, 512]);  permute_1901 = None
    permute_1902: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_358, [0, 2, 1]);  view_358 = None
    bmm_408: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1902, view_1516);  permute_1902 = None
    permute_1903: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_359, [0, 2, 1]);  view_359 = None
    bmm_409: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1516, permute_1903);  view_1516 = permute_1903 = None
    view_1517: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_408, [16, 64, 1, 512, 1]);  bmm_408 = None
    permute_1904: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1517, [2, 0, 4, 3, 1]);  view_1517 = None
    view_1518: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_409, [16, 512, 64, 1, 1]);  bmm_409 = None
    permute_1905: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1518, [3, 0, 1, 4, 2]);  view_1518 = None
    permute_1906: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1904, [3, 0, 1, 4, 2]);  permute_1904 = None
    squeeze_330: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1906, 4);  permute_1906 = None
    permute_1907: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1905, [2, 0, 1, 4, 3]);  permute_1905 = None
    squeeze_331: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1907, 4);  permute_1907 = None
    sum_224: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_331, [0, 1], True)
    view_1519: "f32[16, 64]" = torch.ops.aten.view.default(sum_224, [16, 64]);  sum_224 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_368: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_329, squeeze_331);  squeeze_329 = squeeze_331 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1520: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_328, [1024, 1, 16, 64, 1]);  squeeze_328 = None
    permute_1908: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1520, [0, 4, 1, 2, 3]);  view_1520 = None
    view_1521: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1908, [1, 1024, 1024]);  permute_1908 = None
    permute_1909: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_354, [0, 2, 1]);  view_354 = None
    bmm_410: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1909, view_1521);  permute_1909 = view_1521 = None
    view_1522: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_410, [1024, 1, 16, 64, 1]);  bmm_410 = None
    permute_1910: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1522, [4, 1, 2, 3, 0]);  view_1522 = None
    permute_1911: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1910, [4, 2, 3, 0, 1]);  permute_1910 = None
    squeeze_332: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1911, 4);  permute_1911 = None
    squeeze_333: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_332, 3);  squeeze_332 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1523: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_326, [512, 1, 16, 64, 1]);  squeeze_326 = None
    permute_1912: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1523, [0, 4, 1, 2, 3]);  view_1523 = None
    clone_137: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1912, memory_format = torch.contiguous_format);  permute_1912 = None
    view_1524: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_137, [1, 512, 1024]);  clone_137 = None
    permute_1913: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_350, [0, 2, 1]);  view_350 = None
    bmm_411: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1913, view_1524);  permute_1913 = None
    permute_1914: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_351, [0, 2, 1]);  view_351 = None
    bmm_412: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1524, permute_1914);  view_1524 = permute_1914 = None
    view_1525: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_411, [1024, 1, 16, 64, 1]);  bmm_411 = None
    permute_1915: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1525, [4, 1, 2, 3, 0]);  view_1525 = None
    view_1526: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_412, [512, 1024, 1, 1, 1]);  bmm_412 = None
    permute_1916: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1526, [0, 2, 3, 4, 1]);  view_1526 = None
    permute_1917: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1915, [4, 2, 3, 0, 1]);  permute_1915 = None
    squeeze_334: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1917, 4);  permute_1917 = None
    squeeze_335: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_334, 3);  squeeze_334 = None
    permute_1918: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1916, [0, 1, 4, 2, 3]);  permute_1916 = None
    squeeze_336: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1918, 4);  permute_1918 = None
    squeeze_337: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_336, 3);  squeeze_336 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_369: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_671, squeeze_337);  mul_671 = squeeze_337 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1527: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_330, [512, 1, 16, 64, 1]);  squeeze_330 = None
    permute_1919: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1527, [0, 4, 1, 2, 3]);  view_1527 = None
    view_1528: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1919, [1, 512, 1024]);  permute_1919 = None
    permute_1920: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_346, [0, 2, 1]);  view_346 = None
    bmm_413: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1920, view_1528);  permute_1920 = None
    permute_1921: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_347, [0, 2, 1]);  view_347 = None
    bmm_414: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1528, permute_1921);  view_1528 = permute_1921 = None
    view_1529: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_413, [1024, 1, 16, 64, 1]);  bmm_413 = None
    permute_1922: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1529, [4, 1, 2, 3, 0]);  view_1529 = None
    view_1530: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_414, [512, 1024, 1, 1, 1]);  bmm_414 = None
    permute_1923: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1530, [0, 2, 3, 4, 1]);  view_1530 = None
    permute_1924: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1922, [4, 2, 3, 0, 1]);  permute_1922 = None
    squeeze_338: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1924, 4);  permute_1924 = None
    squeeze_339: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_338, 3);  squeeze_338 = None
    permute_1925: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1923, [0, 1, 4, 2, 3]);  permute_1923 = None
    squeeze_340: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1925, 4);  permute_1925 = None
    squeeze_341: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_340, 3);  squeeze_340 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_370: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_369, squeeze_341);  add_369 = squeeze_341 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1531: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_368, [512, 1, 16, 64, 1]);  add_368 = None
    permute_1926: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1531, [0, 4, 1, 2, 3]);  view_1531 = None
    clone_138: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1926, memory_format = torch.contiguous_format);  permute_1926 = None
    view_1532: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_138, [1, 512, 1024]);  clone_138 = None
    permute_1927: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_342, [0, 2, 1]);  view_342 = None
    bmm_415: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1927, view_1532);  permute_1927 = None
    permute_1928: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_343, [0, 2, 1]);  view_343 = None
    bmm_416: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1532, permute_1928);  view_1532 = permute_1928 = None
    view_1533: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_415, [1024, 1, 16, 64, 1]);  bmm_415 = None
    permute_1929: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1533, [4, 1, 2, 3, 0]);  view_1533 = None
    view_1534: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_416, [512, 1024, 1, 1, 1]);  bmm_416 = None
    permute_1930: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1534, [0, 2, 3, 4, 1]);  view_1534 = None
    permute_1931: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1929, [4, 2, 3, 0, 1]);  permute_1929 = None
    squeeze_342: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1931, 4);  permute_1931 = None
    squeeze_343: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_342, 3);  squeeze_342 = None
    permute_1932: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1930, [0, 1, 4, 2, 3]);  permute_1930 = None
    squeeze_344: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1932, 4);  permute_1932 = None
    squeeze_345: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_344, 3);  squeeze_344 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_371: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_370, squeeze_345);  add_370 = squeeze_345 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_180: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_98, getitem_111);  add_98 = getitem_111 = None
    mul_680: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_180, rsqrt_17);  sub_180 = None
    mul_681: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_371, primals_240);  primals_240 = None
    mul_682: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_681, 1024)
    sum_225: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_681, [2], True)
    mul_683: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_681, mul_680);  mul_681 = None
    sum_226: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_683, [2], True);  mul_683 = None
    mul_684: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_680, sum_226);  sum_226 = None
    sub_181: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_682, sum_225);  mul_682 = sum_225 = None
    sub_182: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_181, mul_684);  sub_181 = mul_684 = None
    div_57: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_17, 1024);  rsqrt_17 = None
    mul_685: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_57, sub_182);  div_57 = sub_182 = None
    mul_686: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_371, mul_680);  mul_680 = None
    sum_227: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_686, [0, 1]);  mul_686 = None
    sum_228: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_371, [0, 1]);  add_371 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_67: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_109, torch.float32);  getitem_109 = None
    mul_687: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_67, 1.1111111111111112);  convert_element_type_67 = None
    mul_688: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_685, mul_687);  mul_687 = None
    clone_139: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_688, memory_format = torch.contiguous_format);  mul_688 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1535: "f32[512, 1024]" = torch.ops.aten.view.default(clone_139, [512, 1024]);  clone_139 = None
    permute_1933: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_380, [1, 0]);  permute_380 = None
    mm_62: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1535, permute_1933);  permute_1933 = None
    permute_1934: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1535, [1, 0])
    mm_63: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1934, view_340);  permute_1934 = view_340 = None
    permute_1935: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_63, [1, 0]);  mm_63 = None
    sum_229: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1535, [0], True);  view_1535 = None
    view_1536: "f32[1024]" = torch.ops.aten.view.default(sum_229, [1024]);  sum_229 = None
    permute_1936: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1935, [1, 0]);  permute_1935 = None
    view_1537: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_62, [512, 1, 4096]);  mm_62 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_68: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_107, torch.float32);  getitem_107 = None
    mul_689: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_68, 1.1111111111111112);  convert_element_type_68 = None
    mul_690: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1537, mul_689);  view_1537 = mul_689 = None
    clone_140: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_690, memory_format = torch.contiguous_format);  mul_690 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_691: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_339, 0.7071067811865476)
    erf_39: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_691);  mul_691 = None
    add_372: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_39, 1);  erf_39 = None
    mul_692: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_372, 0.5);  add_372 = None
    mul_693: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_339, view_339)
    mul_694: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_693, -0.5);  mul_693 = None
    exp_41: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_694);  mul_694 = None
    mul_695: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_41, 0.3989422804014327);  exp_41 = None
    mul_696: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_339, mul_695);  view_339 = mul_695 = None
    add_373: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_692, mul_696);  mul_692 = mul_696 = None
    mul_697: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_140, add_373);  clone_140 = add_373 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1538: "f32[512, 4096]" = torch.ops.aten.view.default(mul_697, [512, 4096]);  mul_697 = None
    permute_1937: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_379, [1, 0]);  permute_379 = None
    mm_64: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1538, permute_1937);  permute_1937 = None
    permute_1938: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1538, [1, 0])
    mm_65: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1938, view_338);  permute_1938 = view_338 = None
    permute_1939: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_65, [1, 0]);  mm_65 = None
    sum_230: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1538, [0], True);  view_1538 = None
    view_1539: "f32[4096]" = torch.ops.aten.view.default(sum_230, [4096]);  sum_230 = None
    permute_1940: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_1939, [1, 0]);  permute_1939 = None
    view_1540: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_64, [512, 1, 1024]);  mm_64 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_374: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_685, view_1540);  mul_685 = view_1540 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_183: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_94, getitem_105);  add_94 = getitem_105 = None
    mul_698: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_183, rsqrt_16);  sub_183 = None
    mul_699: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_374, primals_234);  primals_234 = None
    mul_700: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_699, 1024)
    sum_231: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_699, [2], True)
    mul_701: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_699, mul_698);  mul_699 = None
    sum_232: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_701, [2], True);  mul_701 = None
    mul_702: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_698, sum_232);  sum_232 = None
    sub_184: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_700, sum_231);  mul_700 = sum_231 = None
    sub_185: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_184, mul_702);  sub_184 = mul_702 = None
    div_58: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_16, 1024);  rsqrt_16 = None
    mul_703: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_58, sub_185);  div_58 = sub_185 = None
    mul_704: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_374, mul_698);  mul_698 = None
    sum_233: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_704, [0, 1]);  mul_704 = None
    sum_234: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_374, [0, 1]);  add_374 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_69: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_103, torch.float32);  getitem_103 = None
    mul_705: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_69, 1.1111111111111112);  convert_element_type_69 = None
    mul_706: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_703, mul_705);  mul_705 = None
    clone_141: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_706, memory_format = torch.contiguous_format);  mul_706 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1541: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_141, [512, 1, 1024, 1, 1]);  clone_141 = None
    permute_1941: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1541, [0, 3, 4, 1, 2]);  view_1541 = None
    view_1542: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1941, [1, 512, 1024]);  permute_1941 = None
    permute_1942: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_334, [0, 2, 1]);  view_334 = None
    bmm_417: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1942, view_1542);  permute_1942 = None
    permute_1943: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_335, [0, 2, 1]);  view_335 = None
    bmm_418: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1542, permute_1943);  view_1542 = permute_1943 = None
    view_1543: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_417, [64, 16, 1, 1024, 1]);  bmm_417 = None
    permute_1944: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1543, [4, 2, 3, 0, 1]);  view_1543 = None
    view_1544: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_418, [512, 64, 16, 1, 1]);  bmm_418 = None
    permute_1945: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1544, [0, 3, 4, 1, 2]);  view_1544 = None
    permute_1946: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1944, [2, 4, 3, 0, 1]);  permute_1944 = None
    squeeze_346: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1946, 4);  permute_1946 = None
    squeeze_347: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_346, 3);  squeeze_346 = None
    permute_1947: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1945, [0, 1, 4, 3, 2]);  permute_1945 = None
    squeeze_348: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1947, 4);  permute_1947 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1545: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_348, [512, 1, 16, 64, 1]);  squeeze_348 = None
    permute_1948: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1545, [2, 0, 4, 1, 3]);  view_1545 = None
    view_1546: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_1948, [16, 512, 64]);  permute_1948 = None
    permute_1949: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_330, [0, 2, 1]);  view_330 = None
    bmm_419: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_1949, view_1546);  permute_1949 = None
    permute_1950: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_331, [0, 2, 1]);  view_331 = None
    bmm_420: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1546, permute_1950);  view_1546 = permute_1950 = None
    view_1547: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_419, [16, 512, 1, 64, 1]);  bmm_419 = None
    permute_1951: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1547, [4, 2, 0, 3, 1]);  view_1547 = None
    view_1548: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_420, [16, 512, 512, 1, 1]);  bmm_420 = None
    permute_1952: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1548, [1, 3, 0, 4, 2]);  view_1548 = None
    permute_1953: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1951, [4, 1, 2, 3, 0]);  permute_1951 = None
    squeeze_349: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1953, 4);  permute_1953 = None
    permute_1954: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_1952, [1, 2, 0, 4, 3]);  permute_1952 = None
    squeeze_350: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_1954, 4);  permute_1954 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_70: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_101, torch.float32);  getitem_101 = None
    mul_707: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_70, 1.1111111111111112);  convert_element_type_70 = None
    mul_708: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_350, mul_707);  squeeze_350 = mul_707 = None
    clone_142: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_708, memory_format = torch.contiguous_format);  mul_708 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_41: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_8);  alias_8 = None
    mul_709: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_142, alias_41);  clone_142 = None
    sum_235: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_709, [3], True)
    mul_710: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_41, sum_235);  alias_41 = sum_235 = None
    sub_186: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_709, mul_710);  mul_709 = mul_710 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_711: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_186, 0.125);  sub_186 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_76: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_15: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_76, [None, None, None, iota_10], mul_711, True);  full_76 = iota_10 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1549: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_15, [1, 16, 1023, 512]);  index_put_15 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_77: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_60: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_77, view_1549, 3, 0, 9223372036854775807);  full_77 = view_1549 = None
    full_78: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_61: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_78, slice_scatter_60, 2, 1, 9223372036854775807);  full_78 = slice_scatter_60 = None
    full_79: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_62: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_79, slice_scatter_61, 1, 0, 9223372036854775807);  full_79 = slice_scatter_61 = None
    full_80: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_63: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_80, slice_scatter_62, 0, 0, 9223372036854775807);  full_80 = slice_scatter_62 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1550: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_63, [1, 16, 512, 1024]);  slice_scatter_63 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1551: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1550, [1, 16, 512, 1024, 1]);  view_1550 = None
    permute_1955: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1551, [1, 2, 4, 0, 3]);  view_1551 = None
    view_1552: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_1955, [16, 512, 1024]);  permute_1955 = None
    permute_1956: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_324, [0, 2, 1]);  view_324 = None
    bmm_421: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_1956, view_1552);  permute_1956 = None
    permute_1957: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_325, [0, 2, 1]);  view_325 = None
    bmm_422: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1552, permute_1957);  view_1552 = permute_1957 = None
    view_1553: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_421, [16, 64, 1, 1024, 1]);  bmm_421 = None
    permute_1958: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1553, [2, 0, 4, 3, 1]);  view_1553 = None
    view_1554: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_422, [16, 512, 64, 1, 1]);  bmm_422 = None
    permute_1959: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1554, [3, 0, 1, 4, 2]);  view_1554 = None
    permute_1960: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1958, [3, 0, 1, 4, 2]);  permute_1958 = None
    squeeze_351: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1960, 4);  permute_1960 = None
    permute_1961: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1959, [2, 0, 1, 4, 3]);  permute_1959 = None
    squeeze_352: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1961, 4);  permute_1961 = None
    sum_236: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_352, [0, 1], True)
    view_1555: "f32[16, 64]" = torch.ops.aten.view.default(sum_236, [16, 64]);  sum_236 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1556: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_711, [1, 16, 512, 512, 1]);  mul_711 = None
    permute_1962: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1556, [1, 2, 4, 0, 3]);  view_1556 = None
    view_1557: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_1962, [16, 512, 512]);  permute_1962 = None
    permute_1963: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_320, [0, 2, 1]);  view_320 = None
    bmm_423: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_1963, view_1557);  permute_1963 = None
    permute_1964: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_321, [0, 2, 1]);  view_321 = None
    bmm_424: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1557, permute_1964);  view_1557 = permute_1964 = None
    view_1558: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_423, [16, 64, 1, 512, 1]);  bmm_423 = None
    permute_1965: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1558, [2, 0, 4, 3, 1]);  view_1558 = None
    view_1559: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_424, [16, 512, 64, 1, 1]);  bmm_424 = None
    permute_1966: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1559, [3, 0, 1, 4, 2]);  view_1559 = None
    permute_1967: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1965, [3, 0, 1, 4, 2]);  permute_1965 = None
    squeeze_353: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1967, 4);  permute_1967 = None
    permute_1968: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_1966, [2, 0, 1, 4, 3]);  permute_1966 = None
    squeeze_354: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_1968, 4);  permute_1968 = None
    sum_237: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_354, [0, 1], True)
    view_1560: "f32[16, 64]" = torch.ops.aten.view.default(sum_237, [16, 64]);  sum_237 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_375: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_352, squeeze_354);  squeeze_352 = squeeze_354 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1561: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_351, [1024, 1, 16, 64, 1]);  squeeze_351 = None
    permute_1969: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1561, [0, 4, 1, 2, 3]);  view_1561 = None
    view_1562: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_1969, [1, 1024, 1024]);  permute_1969 = None
    permute_1970: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_316, [0, 2, 1]);  view_316 = None
    bmm_425: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1970, view_1562);  permute_1970 = view_1562 = None
    view_1563: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_425, [1024, 1, 16, 64, 1]);  bmm_425 = None
    permute_1971: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1563, [4, 1, 2, 3, 0]);  view_1563 = None
    permute_1972: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1971, [4, 2, 3, 0, 1]);  permute_1971 = None
    squeeze_355: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1972, 4);  permute_1972 = None
    squeeze_356: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_355, 3);  squeeze_355 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1564: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_349, [512, 1, 16, 64, 1]);  squeeze_349 = None
    permute_1973: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1564, [0, 4, 1, 2, 3]);  view_1564 = None
    clone_143: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1973, memory_format = torch.contiguous_format);  permute_1973 = None
    view_1565: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_143, [1, 512, 1024]);  clone_143 = None
    permute_1974: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_312, [0, 2, 1]);  view_312 = None
    bmm_426: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1974, view_1565);  permute_1974 = None
    permute_1975: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_313, [0, 2, 1]);  view_313 = None
    bmm_427: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1565, permute_1975);  view_1565 = permute_1975 = None
    view_1566: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_426, [1024, 1, 16, 64, 1]);  bmm_426 = None
    permute_1976: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1566, [4, 1, 2, 3, 0]);  view_1566 = None
    view_1567: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_427, [512, 1024, 1, 1, 1]);  bmm_427 = None
    permute_1977: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1567, [0, 2, 3, 4, 1]);  view_1567 = None
    permute_1978: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1976, [4, 2, 3, 0, 1]);  permute_1976 = None
    squeeze_357: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1978, 4);  permute_1978 = None
    squeeze_358: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_357, 3);  squeeze_357 = None
    permute_1979: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1977, [0, 1, 4, 2, 3]);  permute_1977 = None
    squeeze_359: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1979, 4);  permute_1979 = None
    squeeze_360: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_359, 3);  squeeze_359 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_376: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_703, squeeze_360);  mul_703 = squeeze_360 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1568: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_353, [512, 1, 16, 64, 1]);  squeeze_353 = None
    permute_1980: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1568, [0, 4, 1, 2, 3]);  view_1568 = None
    view_1569: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_1980, [1, 512, 1024]);  permute_1980 = None
    permute_1981: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_308, [0, 2, 1]);  view_308 = None
    bmm_428: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1981, view_1569);  permute_1981 = None
    permute_1982: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_309, [0, 2, 1]);  view_309 = None
    bmm_429: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1569, permute_1982);  view_1569 = permute_1982 = None
    view_1570: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_428, [1024, 1, 16, 64, 1]);  bmm_428 = None
    permute_1983: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1570, [4, 1, 2, 3, 0]);  view_1570 = None
    view_1571: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_429, [512, 1024, 1, 1, 1]);  bmm_429 = None
    permute_1984: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1571, [0, 2, 3, 4, 1]);  view_1571 = None
    permute_1985: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1983, [4, 2, 3, 0, 1]);  permute_1983 = None
    squeeze_361: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1985, 4);  permute_1985 = None
    squeeze_362: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_361, 3);  squeeze_361 = None
    permute_1986: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1984, [0, 1, 4, 2, 3]);  permute_1984 = None
    squeeze_363: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1986, 4);  permute_1986 = None
    squeeze_364: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_363, 3);  squeeze_363 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_377: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_376, squeeze_364);  add_376 = squeeze_364 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1572: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_375, [512, 1, 16, 64, 1]);  add_375 = None
    permute_1987: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1572, [0, 4, 1, 2, 3]);  view_1572 = None
    clone_144: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_1987, memory_format = torch.contiguous_format);  permute_1987 = None
    view_1573: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_144, [1, 512, 1024]);  clone_144 = None
    permute_1988: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_304, [0, 2, 1]);  view_304 = None
    bmm_430: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_1988, view_1573);  permute_1988 = None
    permute_1989: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_305, [0, 2, 1]);  view_305 = None
    bmm_431: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1573, permute_1989);  view_1573 = permute_1989 = None
    view_1574: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_430, [1024, 1, 16, 64, 1]);  bmm_430 = None
    permute_1990: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1574, [4, 1, 2, 3, 0]);  view_1574 = None
    view_1575: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_431, [512, 1024, 1, 1, 1]);  bmm_431 = None
    permute_1991: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1575, [0, 2, 3, 4, 1]);  view_1575 = None
    permute_1992: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_1990, [4, 2, 3, 0, 1]);  permute_1990 = None
    squeeze_365: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_1992, 4);  permute_1992 = None
    squeeze_366: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_365, 3);  squeeze_365 = None
    permute_1993: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_1991, [0, 1, 4, 2, 3]);  permute_1991 = None
    squeeze_367: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_1993, 4);  permute_1993 = None
    squeeze_368: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_367, 3);  squeeze_367 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_378: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_377, squeeze_368);  add_377 = squeeze_368 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_187: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_87, getitem_99);  add_87 = getitem_99 = None
    mul_712: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_187, rsqrt_15);  sub_187 = None
    mul_713: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_378, primals_232);  primals_232 = None
    mul_714: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_713, 1024)
    sum_238: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_713, [2], True)
    mul_715: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_713, mul_712);  mul_713 = None
    sum_239: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_715, [2], True);  mul_715 = None
    mul_716: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_712, sum_239);  sum_239 = None
    sub_188: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_714, sum_238);  mul_714 = sum_238 = None
    sub_189: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_188, mul_716);  sub_188 = mul_716 = None
    div_59: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_15, 1024);  rsqrt_15 = None
    mul_717: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_59, sub_189);  div_59 = sub_189 = None
    mul_718: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_378, mul_712);  mul_712 = None
    sum_240: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_718, [0, 1]);  mul_718 = None
    sum_241: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_378, [0, 1]);  add_378 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_71: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_97, torch.float32);  getitem_97 = None
    mul_719: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_71, 1.1111111111111112);  convert_element_type_71 = None
    mul_720: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_717, mul_719);  mul_719 = None
    clone_145: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_720, memory_format = torch.contiguous_format);  mul_720 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1576: "f32[512, 1024]" = torch.ops.aten.view.default(clone_145, [512, 1024]);  clone_145 = None
    permute_1994: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_338, [1, 0]);  permute_338 = None
    mm_66: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1576, permute_1994);  permute_1994 = None
    permute_1995: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1576, [1, 0])
    mm_67: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_1995, view_302);  permute_1995 = view_302 = None
    permute_1996: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_67, [1, 0]);  mm_67 = None
    sum_242: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1576, [0], True);  view_1576 = None
    view_1577: "f32[1024]" = torch.ops.aten.view.default(sum_242, [1024]);  sum_242 = None
    permute_1997: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_1996, [1, 0]);  permute_1996 = None
    view_1578: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_66, [512, 1, 4096]);  mm_66 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_72: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_95, torch.float32);  getitem_95 = None
    mul_721: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_72, 1.1111111111111112);  convert_element_type_72 = None
    mul_722: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1578, mul_721);  view_1578 = mul_721 = None
    clone_146: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_722, memory_format = torch.contiguous_format);  mul_722 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_723: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_301, 0.7071067811865476)
    erf_40: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_723);  mul_723 = None
    add_379: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_40, 1);  erf_40 = None
    mul_724: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_379, 0.5);  add_379 = None
    mul_725: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_301, view_301)
    mul_726: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_725, -0.5);  mul_725 = None
    exp_42: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_726);  mul_726 = None
    mul_727: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_42, 0.3989422804014327);  exp_42 = None
    mul_728: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_301, mul_727);  view_301 = mul_727 = None
    add_380: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_724, mul_728);  mul_724 = mul_728 = None
    mul_729: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_146, add_380);  clone_146 = add_380 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1579: "f32[512, 4096]" = torch.ops.aten.view.default(mul_729, [512, 4096]);  mul_729 = None
    permute_1998: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_337, [1, 0]);  permute_337 = None
    mm_68: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1579, permute_1998);  permute_1998 = None
    permute_1999: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1579, [1, 0])
    mm_69: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_1999, view_300);  permute_1999 = view_300 = None
    permute_2000: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_69, [1, 0]);  mm_69 = None
    sum_243: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1579, [0], True);  view_1579 = None
    view_1580: "f32[4096]" = torch.ops.aten.view.default(sum_243, [4096]);  sum_243 = None
    permute_2001: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_2000, [1, 0]);  permute_2000 = None
    view_1581: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_68, [512, 1, 1024]);  mm_68 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_381: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_717, view_1581);  mul_717 = view_1581 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_190: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_83, getitem_93);  add_83 = getitem_93 = None
    mul_730: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_190, rsqrt_14);  sub_190 = None
    mul_731: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_381, primals_226);  primals_226 = None
    mul_732: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_731, 1024)
    sum_244: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_731, [2], True)
    mul_733: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_731, mul_730);  mul_731 = None
    sum_245: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_733, [2], True);  mul_733 = None
    mul_734: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_730, sum_245);  sum_245 = None
    sub_191: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_732, sum_244);  mul_732 = sum_244 = None
    sub_192: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_191, mul_734);  sub_191 = mul_734 = None
    div_60: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_14, 1024);  rsqrt_14 = None
    mul_735: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_60, sub_192);  div_60 = sub_192 = None
    mul_736: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_381, mul_730);  mul_730 = None
    sum_246: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_736, [0, 1]);  mul_736 = None
    sum_247: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_381, [0, 1]);  add_381 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_73: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_91, torch.float32);  getitem_91 = None
    mul_737: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_73, 1.1111111111111112);  convert_element_type_73 = None
    mul_738: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_735, mul_737);  mul_737 = None
    clone_147: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_738, memory_format = torch.contiguous_format);  mul_738 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1582: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_147, [512, 1, 1024, 1, 1]);  clone_147 = None
    permute_2002: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1582, [0, 3, 4, 1, 2]);  view_1582 = None
    view_1583: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2002, [1, 512, 1024]);  permute_2002 = None
    permute_2003: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_296, [0, 2, 1]);  view_296 = None
    bmm_432: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2003, view_1583);  permute_2003 = None
    permute_2004: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_297, [0, 2, 1]);  view_297 = None
    bmm_433: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1583, permute_2004);  view_1583 = permute_2004 = None
    view_1584: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_432, [64, 16, 1, 1024, 1]);  bmm_432 = None
    permute_2005: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1584, [4, 2, 3, 0, 1]);  view_1584 = None
    view_1585: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_433, [512, 64, 16, 1, 1]);  bmm_433 = None
    permute_2006: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1585, [0, 3, 4, 1, 2]);  view_1585 = None
    permute_2007: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2005, [2, 4, 3, 0, 1]);  permute_2005 = None
    squeeze_369: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2007, 4);  permute_2007 = None
    squeeze_370: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_369, 3);  squeeze_369 = None
    permute_2008: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2006, [0, 1, 4, 3, 2]);  permute_2006 = None
    squeeze_371: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2008, 4);  permute_2008 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1586: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_371, [512, 1, 16, 64, 1]);  squeeze_371 = None
    permute_2009: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1586, [2, 0, 4, 1, 3]);  view_1586 = None
    view_1587: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_2009, [16, 512, 64]);  permute_2009 = None
    permute_2010: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_292, [0, 2, 1]);  view_292 = None
    bmm_434: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_2010, view_1587);  permute_2010 = None
    permute_2011: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_293, [0, 2, 1]);  view_293 = None
    bmm_435: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1587, permute_2011);  view_1587 = permute_2011 = None
    view_1588: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_434, [16, 512, 1, 64, 1]);  bmm_434 = None
    permute_2012: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1588, [4, 2, 0, 3, 1]);  view_1588 = None
    view_1589: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_435, [16, 512, 512, 1, 1]);  bmm_435 = None
    permute_2013: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1589, [1, 3, 0, 4, 2]);  view_1589 = None
    permute_2014: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2012, [4, 1, 2, 3, 0]);  permute_2012 = None
    squeeze_372: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2014, 4);  permute_2014 = None
    permute_2015: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_2013, [1, 2, 0, 4, 3]);  permute_2013 = None
    squeeze_373: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_2015, 4);  permute_2015 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_74: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_89, torch.float32);  getitem_89 = None
    mul_739: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_74, 1.1111111111111112);  convert_element_type_74 = None
    mul_740: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_373, mul_739);  squeeze_373 = mul_739 = None
    clone_148: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_740, memory_format = torch.contiguous_format);  mul_740 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_42: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_7);  alias_7 = None
    mul_741: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_148, alias_42);  clone_148 = None
    sum_248: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_741, [3], True)
    mul_742: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_42, sum_248);  alias_42 = sum_248 = None
    sub_193: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_741, mul_742);  mul_741 = mul_742 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_743: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_193, 0.125);  sub_193 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_81: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_16: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_81, [None, None, None, iota_9], mul_743, True);  full_81 = iota_9 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1590: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_16, [1, 16, 1023, 512]);  index_put_16 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_82: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_64: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_82, view_1590, 3, 0, 9223372036854775807);  full_82 = view_1590 = None
    full_83: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_65: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_83, slice_scatter_64, 2, 1, 9223372036854775807);  full_83 = slice_scatter_64 = None
    full_84: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_66: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_84, slice_scatter_65, 1, 0, 9223372036854775807);  full_84 = slice_scatter_65 = None
    full_85: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_67: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_85, slice_scatter_66, 0, 0, 9223372036854775807);  full_85 = slice_scatter_66 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1591: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_67, [1, 16, 512, 1024]);  slice_scatter_67 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1592: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1591, [1, 16, 512, 1024, 1]);  view_1591 = None
    permute_2016: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1592, [1, 2, 4, 0, 3]);  view_1592 = None
    view_1593: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_2016, [16, 512, 1024]);  permute_2016 = None
    permute_2017: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_286, [0, 2, 1]);  view_286 = None
    bmm_436: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_2017, view_1593);  permute_2017 = None
    permute_2018: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_287, [0, 2, 1]);  view_287 = None
    bmm_437: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1593, permute_2018);  view_1593 = permute_2018 = None
    view_1594: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_436, [16, 64, 1, 1024, 1]);  bmm_436 = None
    permute_2019: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1594, [2, 0, 4, 3, 1]);  view_1594 = None
    view_1595: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_437, [16, 512, 64, 1, 1]);  bmm_437 = None
    permute_2020: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1595, [3, 0, 1, 4, 2]);  view_1595 = None
    permute_2021: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2019, [3, 0, 1, 4, 2]);  permute_2019 = None
    squeeze_374: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2021, 4);  permute_2021 = None
    permute_2022: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2020, [2, 0, 1, 4, 3]);  permute_2020 = None
    squeeze_375: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2022, 4);  permute_2022 = None
    sum_249: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_375, [0, 1], True)
    view_1596: "f32[16, 64]" = torch.ops.aten.view.default(sum_249, [16, 64]);  sum_249 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1597: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_743, [1, 16, 512, 512, 1]);  mul_743 = None
    permute_2023: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1597, [1, 2, 4, 0, 3]);  view_1597 = None
    view_1598: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_2023, [16, 512, 512]);  permute_2023 = None
    permute_2024: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_282, [0, 2, 1]);  view_282 = None
    bmm_438: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_2024, view_1598);  permute_2024 = None
    permute_2025: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_283, [0, 2, 1]);  view_283 = None
    bmm_439: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1598, permute_2025);  view_1598 = permute_2025 = None
    view_1599: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_438, [16, 64, 1, 512, 1]);  bmm_438 = None
    permute_2026: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1599, [2, 0, 4, 3, 1]);  view_1599 = None
    view_1600: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_439, [16, 512, 64, 1, 1]);  bmm_439 = None
    permute_2027: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1600, [3, 0, 1, 4, 2]);  view_1600 = None
    permute_2028: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2026, [3, 0, 1, 4, 2]);  permute_2026 = None
    squeeze_376: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2028, 4);  permute_2028 = None
    permute_2029: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2027, [2, 0, 1, 4, 3]);  permute_2027 = None
    squeeze_377: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2029, 4);  permute_2029 = None
    sum_250: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_377, [0, 1], True)
    view_1601: "f32[16, 64]" = torch.ops.aten.view.default(sum_250, [16, 64]);  sum_250 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_382: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_375, squeeze_377);  squeeze_375 = squeeze_377 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1602: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_374, [1024, 1, 16, 64, 1]);  squeeze_374 = None
    permute_2030: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1602, [0, 4, 1, 2, 3]);  view_1602 = None
    view_1603: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_2030, [1, 1024, 1024]);  permute_2030 = None
    permute_2031: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_278, [0, 2, 1]);  view_278 = None
    bmm_440: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2031, view_1603);  permute_2031 = view_1603 = None
    view_1604: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_440, [1024, 1, 16, 64, 1]);  bmm_440 = None
    permute_2032: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1604, [4, 1, 2, 3, 0]);  view_1604 = None
    permute_2033: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2032, [4, 2, 3, 0, 1]);  permute_2032 = None
    squeeze_378: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2033, 4);  permute_2033 = None
    squeeze_379: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_378, 3);  squeeze_378 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1605: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_372, [512, 1, 16, 64, 1]);  squeeze_372 = None
    permute_2034: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1605, [0, 4, 1, 2, 3]);  view_1605 = None
    clone_149: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2034, memory_format = torch.contiguous_format);  permute_2034 = None
    view_1606: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_149, [1, 512, 1024]);  clone_149 = None
    permute_2035: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_274, [0, 2, 1]);  view_274 = None
    bmm_441: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2035, view_1606);  permute_2035 = None
    permute_2036: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_275, [0, 2, 1]);  view_275 = None
    bmm_442: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1606, permute_2036);  view_1606 = permute_2036 = None
    view_1607: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_441, [1024, 1, 16, 64, 1]);  bmm_441 = None
    permute_2037: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1607, [4, 1, 2, 3, 0]);  view_1607 = None
    view_1608: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_442, [512, 1024, 1, 1, 1]);  bmm_442 = None
    permute_2038: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1608, [0, 2, 3, 4, 1]);  view_1608 = None
    permute_2039: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2037, [4, 2, 3, 0, 1]);  permute_2037 = None
    squeeze_380: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2039, 4);  permute_2039 = None
    squeeze_381: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_380, 3);  squeeze_380 = None
    permute_2040: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2038, [0, 1, 4, 2, 3]);  permute_2038 = None
    squeeze_382: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2040, 4);  permute_2040 = None
    squeeze_383: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_382, 3);  squeeze_382 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_383: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_735, squeeze_383);  mul_735 = squeeze_383 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1609: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_376, [512, 1, 16, 64, 1]);  squeeze_376 = None
    permute_2041: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1609, [0, 4, 1, 2, 3]);  view_1609 = None
    view_1610: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2041, [1, 512, 1024]);  permute_2041 = None
    permute_2042: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_270, [0, 2, 1]);  view_270 = None
    bmm_443: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2042, view_1610);  permute_2042 = None
    permute_2043: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_271, [0, 2, 1]);  view_271 = None
    bmm_444: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1610, permute_2043);  view_1610 = permute_2043 = None
    view_1611: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_443, [1024, 1, 16, 64, 1]);  bmm_443 = None
    permute_2044: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1611, [4, 1, 2, 3, 0]);  view_1611 = None
    view_1612: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_444, [512, 1024, 1, 1, 1]);  bmm_444 = None
    permute_2045: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1612, [0, 2, 3, 4, 1]);  view_1612 = None
    permute_2046: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2044, [4, 2, 3, 0, 1]);  permute_2044 = None
    squeeze_384: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2046, 4);  permute_2046 = None
    squeeze_385: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_384, 3);  squeeze_384 = None
    permute_2047: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2045, [0, 1, 4, 2, 3]);  permute_2045 = None
    squeeze_386: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2047, 4);  permute_2047 = None
    squeeze_387: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_386, 3);  squeeze_386 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_384: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_383, squeeze_387);  add_383 = squeeze_387 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1613: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_382, [512, 1, 16, 64, 1]);  add_382 = None
    permute_2048: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1613, [0, 4, 1, 2, 3]);  view_1613 = None
    clone_150: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2048, memory_format = torch.contiguous_format);  permute_2048 = None
    view_1614: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_150, [1, 512, 1024]);  clone_150 = None
    permute_2049: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_266, [0, 2, 1]);  view_266 = None
    bmm_445: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2049, view_1614);  permute_2049 = None
    permute_2050: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_267, [0, 2, 1]);  view_267 = None
    bmm_446: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1614, permute_2050);  view_1614 = permute_2050 = None
    view_1615: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_445, [1024, 1, 16, 64, 1]);  bmm_445 = None
    permute_2051: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1615, [4, 1, 2, 3, 0]);  view_1615 = None
    view_1616: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_446, [512, 1024, 1, 1, 1]);  bmm_446 = None
    permute_2052: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1616, [0, 2, 3, 4, 1]);  view_1616 = None
    permute_2053: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2051, [4, 2, 3, 0, 1]);  permute_2051 = None
    squeeze_388: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2053, 4);  permute_2053 = None
    squeeze_389: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_388, 3);  squeeze_388 = None
    permute_2054: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2052, [0, 1, 4, 2, 3]);  permute_2052 = None
    squeeze_390: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2054, 4);  permute_2054 = None
    squeeze_391: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_390, 3);  squeeze_390 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_385: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_384, squeeze_391);  add_384 = squeeze_391 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_194: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_76, getitem_87);  add_76 = getitem_87 = None
    mul_744: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_194, rsqrt_13);  sub_194 = None
    mul_745: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_385, primals_224);  primals_224 = None
    mul_746: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_745, 1024)
    sum_251: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_745, [2], True)
    mul_747: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_745, mul_744);  mul_745 = None
    sum_252: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_747, [2], True);  mul_747 = None
    mul_748: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_744, sum_252);  sum_252 = None
    sub_195: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_746, sum_251);  mul_746 = sum_251 = None
    sub_196: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_195, mul_748);  sub_195 = mul_748 = None
    div_61: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_13, 1024);  rsqrt_13 = None
    mul_749: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_61, sub_196);  div_61 = sub_196 = None
    mul_750: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_385, mul_744);  mul_744 = None
    sum_253: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_750, [0, 1]);  mul_750 = None
    sum_254: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_385, [0, 1]);  add_385 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_75: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_85, torch.float32);  getitem_85 = None
    mul_751: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_75, 1.1111111111111112);  convert_element_type_75 = None
    mul_752: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_749, mul_751);  mul_751 = None
    clone_151: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_752, memory_format = torch.contiguous_format);  mul_752 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1617: "f32[512, 1024]" = torch.ops.aten.view.default(clone_151, [512, 1024]);  clone_151 = None
    permute_2055: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_296, [1, 0]);  permute_296 = None
    mm_70: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1617, permute_2055);  permute_2055 = None
    permute_2056: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1617, [1, 0])
    mm_71: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_2056, view_264);  permute_2056 = view_264 = None
    permute_2057: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_71, [1, 0]);  mm_71 = None
    sum_255: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1617, [0], True);  view_1617 = None
    view_1618: "f32[1024]" = torch.ops.aten.view.default(sum_255, [1024]);  sum_255 = None
    permute_2058: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_2057, [1, 0]);  permute_2057 = None
    view_1619: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_70, [512, 1, 4096]);  mm_70 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_76: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_83, torch.float32);  getitem_83 = None
    mul_753: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_76, 1.1111111111111112);  convert_element_type_76 = None
    mul_754: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1619, mul_753);  view_1619 = mul_753 = None
    clone_152: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_754, memory_format = torch.contiguous_format);  mul_754 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_755: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_263, 0.7071067811865476)
    erf_41: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_755);  mul_755 = None
    add_386: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_41, 1);  erf_41 = None
    mul_756: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_386, 0.5);  add_386 = None
    mul_757: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_263, view_263)
    mul_758: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_757, -0.5);  mul_757 = None
    exp_43: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_758);  mul_758 = None
    mul_759: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_43, 0.3989422804014327);  exp_43 = None
    mul_760: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_263, mul_759);  view_263 = mul_759 = None
    add_387: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_756, mul_760);  mul_756 = mul_760 = None
    mul_761: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_152, add_387);  clone_152 = add_387 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1620: "f32[512, 4096]" = torch.ops.aten.view.default(mul_761, [512, 4096]);  mul_761 = None
    permute_2059: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_295, [1, 0]);  permute_295 = None
    mm_72: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1620, permute_2059);  permute_2059 = None
    permute_2060: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1620, [1, 0])
    mm_73: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_2060, view_262);  permute_2060 = view_262 = None
    permute_2061: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_73, [1, 0]);  mm_73 = None
    sum_256: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1620, [0], True);  view_1620 = None
    view_1621: "f32[4096]" = torch.ops.aten.view.default(sum_256, [4096]);  sum_256 = None
    permute_2062: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_2061, [1, 0]);  permute_2061 = None
    view_1622: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_72, [512, 1, 1024]);  mm_72 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_388: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_749, view_1622);  mul_749 = view_1622 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_197: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_72, getitem_81);  add_72 = getitem_81 = None
    mul_762: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_197, rsqrt_12);  sub_197 = None
    mul_763: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_388, primals_218);  primals_218 = None
    mul_764: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_763, 1024)
    sum_257: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_763, [2], True)
    mul_765: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_763, mul_762);  mul_763 = None
    sum_258: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_765, [2], True);  mul_765 = None
    mul_766: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_762, sum_258);  sum_258 = None
    sub_198: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_764, sum_257);  mul_764 = sum_257 = None
    sub_199: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_198, mul_766);  sub_198 = mul_766 = None
    div_62: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_12, 1024);  rsqrt_12 = None
    mul_767: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_62, sub_199);  div_62 = sub_199 = None
    mul_768: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_388, mul_762);  mul_762 = None
    sum_259: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_768, [0, 1]);  mul_768 = None
    sum_260: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_388, [0, 1]);  add_388 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_77: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_79, torch.float32);  getitem_79 = None
    mul_769: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_77, 1.1111111111111112);  convert_element_type_77 = None
    mul_770: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_767, mul_769);  mul_769 = None
    clone_153: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_770, memory_format = torch.contiguous_format);  mul_770 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1623: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_153, [512, 1, 1024, 1, 1]);  clone_153 = None
    permute_2063: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1623, [0, 3, 4, 1, 2]);  view_1623 = None
    view_1624: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2063, [1, 512, 1024]);  permute_2063 = None
    permute_2064: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_258, [0, 2, 1]);  view_258 = None
    bmm_447: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2064, view_1624);  permute_2064 = None
    permute_2065: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_259, [0, 2, 1]);  view_259 = None
    bmm_448: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1624, permute_2065);  view_1624 = permute_2065 = None
    view_1625: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_447, [64, 16, 1, 1024, 1]);  bmm_447 = None
    permute_2066: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1625, [4, 2, 3, 0, 1]);  view_1625 = None
    view_1626: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_448, [512, 64, 16, 1, 1]);  bmm_448 = None
    permute_2067: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1626, [0, 3, 4, 1, 2]);  view_1626 = None
    permute_2068: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2066, [2, 4, 3, 0, 1]);  permute_2066 = None
    squeeze_392: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2068, 4);  permute_2068 = None
    squeeze_393: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_392, 3);  squeeze_392 = None
    permute_2069: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2067, [0, 1, 4, 3, 2]);  permute_2067 = None
    squeeze_394: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2069, 4);  permute_2069 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1627: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_394, [512, 1, 16, 64, 1]);  squeeze_394 = None
    permute_2070: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1627, [2, 0, 4, 1, 3]);  view_1627 = None
    view_1628: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_2070, [16, 512, 64]);  permute_2070 = None
    permute_2071: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_254, [0, 2, 1]);  view_254 = None
    bmm_449: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_2071, view_1628);  permute_2071 = None
    permute_2072: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_255, [0, 2, 1]);  view_255 = None
    bmm_450: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1628, permute_2072);  view_1628 = permute_2072 = None
    view_1629: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_449, [16, 512, 1, 64, 1]);  bmm_449 = None
    permute_2073: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1629, [4, 2, 0, 3, 1]);  view_1629 = None
    view_1630: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_450, [16, 512, 512, 1, 1]);  bmm_450 = None
    permute_2074: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1630, [1, 3, 0, 4, 2]);  view_1630 = None
    permute_2075: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2073, [4, 1, 2, 3, 0]);  permute_2073 = None
    squeeze_395: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2075, 4);  permute_2075 = None
    permute_2076: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_2074, [1, 2, 0, 4, 3]);  permute_2074 = None
    squeeze_396: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_2076, 4);  permute_2076 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_78: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_77, torch.float32);  getitem_77 = None
    mul_771: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_78, 1.1111111111111112);  convert_element_type_78 = None
    mul_772: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_396, mul_771);  squeeze_396 = mul_771 = None
    clone_154: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_772, memory_format = torch.contiguous_format);  mul_772 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_43: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_6);  alias_6 = None
    mul_773: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_154, alias_43);  clone_154 = None
    sum_261: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_773, [3], True)
    mul_774: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_43, sum_261);  alias_43 = sum_261 = None
    sub_200: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_773, mul_774);  mul_773 = mul_774 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_775: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_200, 0.125);  sub_200 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_86: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_17: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_86, [None, None, None, iota_8], mul_775, True);  full_86 = iota_8 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1631: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_17, [1, 16, 1023, 512]);  index_put_17 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_87: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_68: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_87, view_1631, 3, 0, 9223372036854775807);  full_87 = view_1631 = None
    full_88: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_69: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_88, slice_scatter_68, 2, 1, 9223372036854775807);  full_88 = slice_scatter_68 = None
    full_89: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_70: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_89, slice_scatter_69, 1, 0, 9223372036854775807);  full_89 = slice_scatter_69 = None
    full_90: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_71: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_90, slice_scatter_70, 0, 0, 9223372036854775807);  full_90 = slice_scatter_70 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1632: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_71, [1, 16, 512, 1024]);  slice_scatter_71 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1633: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1632, [1, 16, 512, 1024, 1]);  view_1632 = None
    permute_2077: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1633, [1, 2, 4, 0, 3]);  view_1633 = None
    view_1634: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_2077, [16, 512, 1024]);  permute_2077 = None
    permute_2078: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_248, [0, 2, 1]);  view_248 = None
    bmm_451: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_2078, view_1634);  permute_2078 = None
    permute_2079: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_249, [0, 2, 1]);  view_249 = None
    bmm_452: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1634, permute_2079);  view_1634 = permute_2079 = None
    view_1635: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_451, [16, 64, 1, 1024, 1]);  bmm_451 = None
    permute_2080: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1635, [2, 0, 4, 3, 1]);  view_1635 = None
    view_1636: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_452, [16, 512, 64, 1, 1]);  bmm_452 = None
    permute_2081: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1636, [3, 0, 1, 4, 2]);  view_1636 = None
    permute_2082: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2080, [3, 0, 1, 4, 2]);  permute_2080 = None
    squeeze_397: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2082, 4);  permute_2082 = None
    permute_2083: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2081, [2, 0, 1, 4, 3]);  permute_2081 = None
    squeeze_398: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2083, 4);  permute_2083 = None
    sum_262: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_398, [0, 1], True)
    view_1637: "f32[16, 64]" = torch.ops.aten.view.default(sum_262, [16, 64]);  sum_262 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1638: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_775, [1, 16, 512, 512, 1]);  mul_775 = None
    permute_2084: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1638, [1, 2, 4, 0, 3]);  view_1638 = None
    view_1639: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_2084, [16, 512, 512]);  permute_2084 = None
    permute_2085: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_244, [0, 2, 1]);  view_244 = None
    bmm_453: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_2085, view_1639);  permute_2085 = None
    permute_2086: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_245, [0, 2, 1]);  view_245 = None
    bmm_454: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1639, permute_2086);  view_1639 = permute_2086 = None
    view_1640: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_453, [16, 64, 1, 512, 1]);  bmm_453 = None
    permute_2087: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1640, [2, 0, 4, 3, 1]);  view_1640 = None
    view_1641: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_454, [16, 512, 64, 1, 1]);  bmm_454 = None
    permute_2088: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1641, [3, 0, 1, 4, 2]);  view_1641 = None
    permute_2089: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2087, [3, 0, 1, 4, 2]);  permute_2087 = None
    squeeze_399: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2089, 4);  permute_2089 = None
    permute_2090: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2088, [2, 0, 1, 4, 3]);  permute_2088 = None
    squeeze_400: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2090, 4);  permute_2090 = None
    sum_263: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_400, [0, 1], True)
    view_1642: "f32[16, 64]" = torch.ops.aten.view.default(sum_263, [16, 64]);  sum_263 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_389: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_398, squeeze_400);  squeeze_398 = squeeze_400 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1643: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_397, [1024, 1, 16, 64, 1]);  squeeze_397 = None
    permute_2091: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1643, [0, 4, 1, 2, 3]);  view_1643 = None
    view_1644: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_2091, [1, 1024, 1024]);  permute_2091 = None
    permute_2092: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_240, [0, 2, 1]);  view_240 = None
    bmm_455: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2092, view_1644);  permute_2092 = view_1644 = None
    view_1645: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_455, [1024, 1, 16, 64, 1]);  bmm_455 = None
    permute_2093: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1645, [4, 1, 2, 3, 0]);  view_1645 = None
    permute_2094: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2093, [4, 2, 3, 0, 1]);  permute_2093 = None
    squeeze_401: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2094, 4);  permute_2094 = None
    squeeze_402: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_401, 3);  squeeze_401 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1646: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_395, [512, 1, 16, 64, 1]);  squeeze_395 = None
    permute_2095: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1646, [0, 4, 1, 2, 3]);  view_1646 = None
    clone_155: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2095, memory_format = torch.contiguous_format);  permute_2095 = None
    view_1647: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_155, [1, 512, 1024]);  clone_155 = None
    permute_2096: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_236, [0, 2, 1]);  view_236 = None
    bmm_456: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2096, view_1647);  permute_2096 = None
    permute_2097: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_237, [0, 2, 1]);  view_237 = None
    bmm_457: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1647, permute_2097);  view_1647 = permute_2097 = None
    view_1648: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_456, [1024, 1, 16, 64, 1]);  bmm_456 = None
    permute_2098: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1648, [4, 1, 2, 3, 0]);  view_1648 = None
    view_1649: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_457, [512, 1024, 1, 1, 1]);  bmm_457 = None
    permute_2099: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1649, [0, 2, 3, 4, 1]);  view_1649 = None
    permute_2100: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2098, [4, 2, 3, 0, 1]);  permute_2098 = None
    squeeze_403: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2100, 4);  permute_2100 = None
    squeeze_404: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_403, 3);  squeeze_403 = None
    permute_2101: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2099, [0, 1, 4, 2, 3]);  permute_2099 = None
    squeeze_405: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2101, 4);  permute_2101 = None
    squeeze_406: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_405, 3);  squeeze_405 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_390: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_767, squeeze_406);  mul_767 = squeeze_406 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1650: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_399, [512, 1, 16, 64, 1]);  squeeze_399 = None
    permute_2102: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1650, [0, 4, 1, 2, 3]);  view_1650 = None
    view_1651: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2102, [1, 512, 1024]);  permute_2102 = None
    permute_2103: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_232, [0, 2, 1]);  view_232 = None
    bmm_458: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2103, view_1651);  permute_2103 = None
    permute_2104: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_233, [0, 2, 1]);  view_233 = None
    bmm_459: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1651, permute_2104);  view_1651 = permute_2104 = None
    view_1652: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_458, [1024, 1, 16, 64, 1]);  bmm_458 = None
    permute_2105: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1652, [4, 1, 2, 3, 0]);  view_1652 = None
    view_1653: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_459, [512, 1024, 1, 1, 1]);  bmm_459 = None
    permute_2106: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1653, [0, 2, 3, 4, 1]);  view_1653 = None
    permute_2107: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2105, [4, 2, 3, 0, 1]);  permute_2105 = None
    squeeze_407: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2107, 4);  permute_2107 = None
    squeeze_408: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_407, 3);  squeeze_407 = None
    permute_2108: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2106, [0, 1, 4, 2, 3]);  permute_2106 = None
    squeeze_409: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2108, 4);  permute_2108 = None
    squeeze_410: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_409, 3);  squeeze_409 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_391: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_390, squeeze_410);  add_390 = squeeze_410 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1654: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_389, [512, 1, 16, 64, 1]);  add_389 = None
    permute_2109: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1654, [0, 4, 1, 2, 3]);  view_1654 = None
    clone_156: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2109, memory_format = torch.contiguous_format);  permute_2109 = None
    view_1655: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_156, [1, 512, 1024]);  clone_156 = None
    permute_2110: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_228, [0, 2, 1]);  view_228 = None
    bmm_460: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2110, view_1655);  permute_2110 = None
    permute_2111: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_229, [0, 2, 1]);  view_229 = None
    bmm_461: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1655, permute_2111);  view_1655 = permute_2111 = None
    view_1656: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_460, [1024, 1, 16, 64, 1]);  bmm_460 = None
    permute_2112: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1656, [4, 1, 2, 3, 0]);  view_1656 = None
    view_1657: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_461, [512, 1024, 1, 1, 1]);  bmm_461 = None
    permute_2113: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1657, [0, 2, 3, 4, 1]);  view_1657 = None
    permute_2114: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2112, [4, 2, 3, 0, 1]);  permute_2112 = None
    squeeze_411: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2114, 4);  permute_2114 = None
    squeeze_412: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_411, 3);  squeeze_411 = None
    permute_2115: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2113, [0, 1, 4, 2, 3]);  permute_2113 = None
    squeeze_413: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2115, 4);  permute_2115 = None
    squeeze_414: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_413, 3);  squeeze_413 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_392: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_391, squeeze_414);  add_391 = squeeze_414 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_201: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_65, getitem_75);  add_65 = getitem_75 = None
    mul_776: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_201, rsqrt_11);  sub_201 = None
    mul_777: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_392, primals_216);  primals_216 = None
    mul_778: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_777, 1024)
    sum_264: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_777, [2], True)
    mul_779: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_777, mul_776);  mul_777 = None
    sum_265: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_779, [2], True);  mul_779 = None
    mul_780: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_776, sum_265);  sum_265 = None
    sub_202: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_778, sum_264);  mul_778 = sum_264 = None
    sub_203: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_202, mul_780);  sub_202 = mul_780 = None
    div_63: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_11, 1024);  rsqrt_11 = None
    mul_781: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_63, sub_203);  div_63 = sub_203 = None
    mul_782: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_392, mul_776);  mul_776 = None
    sum_266: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_782, [0, 1]);  mul_782 = None
    sum_267: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_392, [0, 1]);  add_392 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_79: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_73, torch.float32);  getitem_73 = None
    mul_783: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_79, 1.1111111111111112);  convert_element_type_79 = None
    mul_784: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_781, mul_783);  mul_783 = None
    clone_157: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_784, memory_format = torch.contiguous_format);  mul_784 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1658: "f32[512, 1024]" = torch.ops.aten.view.default(clone_157, [512, 1024]);  clone_157 = None
    permute_2116: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_254, [1, 0]);  permute_254 = None
    mm_74: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1658, permute_2116);  permute_2116 = None
    permute_2117: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1658, [1, 0])
    mm_75: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_2117, view_226);  permute_2117 = view_226 = None
    permute_2118: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_75, [1, 0]);  mm_75 = None
    sum_268: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1658, [0], True);  view_1658 = None
    view_1659: "f32[1024]" = torch.ops.aten.view.default(sum_268, [1024]);  sum_268 = None
    permute_2119: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_2118, [1, 0]);  permute_2118 = None
    view_1660: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_74, [512, 1, 4096]);  mm_74 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_80: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_71, torch.float32);  getitem_71 = None
    mul_785: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_80, 1.1111111111111112);  convert_element_type_80 = None
    mul_786: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1660, mul_785);  view_1660 = mul_785 = None
    clone_158: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_786, memory_format = torch.contiguous_format);  mul_786 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_787: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_225, 0.7071067811865476)
    erf_42: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_787);  mul_787 = None
    add_393: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_42, 1);  erf_42 = None
    mul_788: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_393, 0.5);  add_393 = None
    mul_789: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_225, view_225)
    mul_790: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_789, -0.5);  mul_789 = None
    exp_44: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_790);  mul_790 = None
    mul_791: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_44, 0.3989422804014327);  exp_44 = None
    mul_792: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_225, mul_791);  view_225 = mul_791 = None
    add_394: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_788, mul_792);  mul_788 = mul_792 = None
    mul_793: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_158, add_394);  clone_158 = add_394 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1661: "f32[512, 4096]" = torch.ops.aten.view.default(mul_793, [512, 4096]);  mul_793 = None
    permute_2120: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_253, [1, 0]);  permute_253 = None
    mm_76: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1661, permute_2120);  permute_2120 = None
    permute_2121: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1661, [1, 0])
    mm_77: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_2121, view_224);  permute_2121 = view_224 = None
    permute_2122: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_77, [1, 0]);  mm_77 = None
    sum_269: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1661, [0], True);  view_1661 = None
    view_1662: "f32[4096]" = torch.ops.aten.view.default(sum_269, [4096]);  sum_269 = None
    permute_2123: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_2122, [1, 0]);  permute_2122 = None
    view_1663: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_76, [512, 1, 1024]);  mm_76 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_395: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_781, view_1663);  mul_781 = view_1663 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_204: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_61, getitem_69);  add_61 = getitem_69 = None
    mul_794: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_204, rsqrt_10);  sub_204 = None
    mul_795: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_395, primals_210);  primals_210 = None
    mul_796: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_795, 1024)
    sum_270: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_795, [2], True)
    mul_797: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_795, mul_794);  mul_795 = None
    sum_271: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_797, [2], True);  mul_797 = None
    mul_798: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_794, sum_271);  sum_271 = None
    sub_205: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_796, sum_270);  mul_796 = sum_270 = None
    sub_206: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_205, mul_798);  sub_205 = mul_798 = None
    div_64: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_10, 1024);  rsqrt_10 = None
    mul_799: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_64, sub_206);  div_64 = sub_206 = None
    mul_800: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_395, mul_794);  mul_794 = None
    sum_272: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_800, [0, 1]);  mul_800 = None
    sum_273: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_395, [0, 1]);  add_395 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_81: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_67, torch.float32);  getitem_67 = None
    mul_801: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_81, 1.1111111111111112);  convert_element_type_81 = None
    mul_802: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_799, mul_801);  mul_801 = None
    clone_159: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_802, memory_format = torch.contiguous_format);  mul_802 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1664: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_159, [512, 1, 1024, 1, 1]);  clone_159 = None
    permute_2124: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1664, [0, 3, 4, 1, 2]);  view_1664 = None
    view_1665: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2124, [1, 512, 1024]);  permute_2124 = None
    permute_2125: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_220, [0, 2, 1]);  view_220 = None
    bmm_462: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2125, view_1665);  permute_2125 = None
    permute_2126: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_221, [0, 2, 1]);  view_221 = None
    bmm_463: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1665, permute_2126);  view_1665 = permute_2126 = None
    view_1666: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_462, [64, 16, 1, 1024, 1]);  bmm_462 = None
    permute_2127: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1666, [4, 2, 3, 0, 1]);  view_1666 = None
    view_1667: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_463, [512, 64, 16, 1, 1]);  bmm_463 = None
    permute_2128: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1667, [0, 3, 4, 1, 2]);  view_1667 = None
    permute_2129: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2127, [2, 4, 3, 0, 1]);  permute_2127 = None
    squeeze_415: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2129, 4);  permute_2129 = None
    squeeze_416: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_415, 3);  squeeze_415 = None
    permute_2130: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2128, [0, 1, 4, 3, 2]);  permute_2128 = None
    squeeze_417: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2130, 4);  permute_2130 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1668: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_417, [512, 1, 16, 64, 1]);  squeeze_417 = None
    permute_2131: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1668, [2, 0, 4, 1, 3]);  view_1668 = None
    view_1669: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_2131, [16, 512, 64]);  permute_2131 = None
    permute_2132: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_216, [0, 2, 1]);  view_216 = None
    bmm_464: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_2132, view_1669);  permute_2132 = None
    permute_2133: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_217, [0, 2, 1]);  view_217 = None
    bmm_465: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1669, permute_2133);  view_1669 = permute_2133 = None
    view_1670: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_464, [16, 512, 1, 64, 1]);  bmm_464 = None
    permute_2134: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1670, [4, 2, 0, 3, 1]);  view_1670 = None
    view_1671: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_465, [16, 512, 512, 1, 1]);  bmm_465 = None
    permute_2135: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1671, [1, 3, 0, 4, 2]);  view_1671 = None
    permute_2136: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2134, [4, 1, 2, 3, 0]);  permute_2134 = None
    squeeze_418: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2136, 4);  permute_2136 = None
    permute_2137: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_2135, [1, 2, 0, 4, 3]);  permute_2135 = None
    squeeze_419: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_2137, 4);  permute_2137 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_82: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_65, torch.float32);  getitem_65 = None
    mul_803: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_82, 1.1111111111111112);  convert_element_type_82 = None
    mul_804: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_419, mul_803);  squeeze_419 = mul_803 = None
    clone_160: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_804, memory_format = torch.contiguous_format);  mul_804 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_44: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_5);  alias_5 = None
    mul_805: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_160, alias_44);  clone_160 = None
    sum_274: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_805, [3], True)
    mul_806: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_44, sum_274);  alias_44 = sum_274 = None
    sub_207: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_805, mul_806);  mul_805 = mul_806 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_807: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_207, 0.125);  sub_207 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_91: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_18: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_91, [None, None, None, iota_7], mul_807, True);  full_91 = iota_7 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1672: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_18, [1, 16, 1023, 512]);  index_put_18 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_92: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_72: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_92, view_1672, 3, 0, 9223372036854775807);  full_92 = view_1672 = None
    full_93: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_73: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_93, slice_scatter_72, 2, 1, 9223372036854775807);  full_93 = slice_scatter_72 = None
    full_94: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_74: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_94, slice_scatter_73, 1, 0, 9223372036854775807);  full_94 = slice_scatter_73 = None
    full_95: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_75: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_95, slice_scatter_74, 0, 0, 9223372036854775807);  full_95 = slice_scatter_74 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1673: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_75, [1, 16, 512, 1024]);  slice_scatter_75 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1674: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1673, [1, 16, 512, 1024, 1]);  view_1673 = None
    permute_2138: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1674, [1, 2, 4, 0, 3]);  view_1674 = None
    view_1675: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_2138, [16, 512, 1024]);  permute_2138 = None
    permute_2139: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_210, [0, 2, 1]);  view_210 = None
    bmm_466: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_2139, view_1675);  permute_2139 = None
    permute_2140: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_211, [0, 2, 1]);  view_211 = None
    bmm_467: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1675, permute_2140);  view_1675 = permute_2140 = None
    view_1676: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_466, [16, 64, 1, 1024, 1]);  bmm_466 = None
    permute_2141: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1676, [2, 0, 4, 3, 1]);  view_1676 = None
    view_1677: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_467, [16, 512, 64, 1, 1]);  bmm_467 = None
    permute_2142: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1677, [3, 0, 1, 4, 2]);  view_1677 = None
    permute_2143: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2141, [3, 0, 1, 4, 2]);  permute_2141 = None
    squeeze_420: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2143, 4);  permute_2143 = None
    permute_2144: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2142, [2, 0, 1, 4, 3]);  permute_2142 = None
    squeeze_421: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2144, 4);  permute_2144 = None
    sum_275: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_421, [0, 1], True)
    view_1678: "f32[16, 64]" = torch.ops.aten.view.default(sum_275, [16, 64]);  sum_275 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1679: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_807, [1, 16, 512, 512, 1]);  mul_807 = None
    permute_2145: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1679, [1, 2, 4, 0, 3]);  view_1679 = None
    view_1680: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_2145, [16, 512, 512]);  permute_2145 = None
    permute_2146: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_206, [0, 2, 1]);  view_206 = None
    bmm_468: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_2146, view_1680);  permute_2146 = None
    permute_2147: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_207, [0, 2, 1]);  view_207 = None
    bmm_469: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1680, permute_2147);  view_1680 = permute_2147 = None
    view_1681: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_468, [16, 64, 1, 512, 1]);  bmm_468 = None
    permute_2148: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1681, [2, 0, 4, 3, 1]);  view_1681 = None
    view_1682: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_469, [16, 512, 64, 1, 1]);  bmm_469 = None
    permute_2149: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1682, [3, 0, 1, 4, 2]);  view_1682 = None
    permute_2150: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2148, [3, 0, 1, 4, 2]);  permute_2148 = None
    squeeze_422: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2150, 4);  permute_2150 = None
    permute_2151: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2149, [2, 0, 1, 4, 3]);  permute_2149 = None
    squeeze_423: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2151, 4);  permute_2151 = None
    sum_276: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_423, [0, 1], True)
    view_1683: "f32[16, 64]" = torch.ops.aten.view.default(sum_276, [16, 64]);  sum_276 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_396: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_421, squeeze_423);  squeeze_421 = squeeze_423 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1684: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_420, [1024, 1, 16, 64, 1]);  squeeze_420 = None
    permute_2152: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1684, [0, 4, 1, 2, 3]);  view_1684 = None
    view_1685: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_2152, [1, 1024, 1024]);  permute_2152 = None
    permute_2153: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_202, [0, 2, 1]);  view_202 = None
    bmm_470: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2153, view_1685);  permute_2153 = view_1685 = None
    view_1686: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_470, [1024, 1, 16, 64, 1]);  bmm_470 = None
    permute_2154: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1686, [4, 1, 2, 3, 0]);  view_1686 = None
    permute_2155: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2154, [4, 2, 3, 0, 1]);  permute_2154 = None
    squeeze_424: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2155, 4);  permute_2155 = None
    squeeze_425: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_424, 3);  squeeze_424 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1687: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_418, [512, 1, 16, 64, 1]);  squeeze_418 = None
    permute_2156: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1687, [0, 4, 1, 2, 3]);  view_1687 = None
    clone_161: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2156, memory_format = torch.contiguous_format);  permute_2156 = None
    view_1688: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_161, [1, 512, 1024]);  clone_161 = None
    permute_2157: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_198, [0, 2, 1]);  view_198 = None
    bmm_471: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2157, view_1688);  permute_2157 = None
    permute_2158: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_199, [0, 2, 1]);  view_199 = None
    bmm_472: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1688, permute_2158);  view_1688 = permute_2158 = None
    view_1689: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_471, [1024, 1, 16, 64, 1]);  bmm_471 = None
    permute_2159: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1689, [4, 1, 2, 3, 0]);  view_1689 = None
    view_1690: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_472, [512, 1024, 1, 1, 1]);  bmm_472 = None
    permute_2160: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1690, [0, 2, 3, 4, 1]);  view_1690 = None
    permute_2161: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2159, [4, 2, 3, 0, 1]);  permute_2159 = None
    squeeze_426: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2161, 4);  permute_2161 = None
    squeeze_427: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_426, 3);  squeeze_426 = None
    permute_2162: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2160, [0, 1, 4, 2, 3]);  permute_2160 = None
    squeeze_428: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2162, 4);  permute_2162 = None
    squeeze_429: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_428, 3);  squeeze_428 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_397: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_799, squeeze_429);  mul_799 = squeeze_429 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1691: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_422, [512, 1, 16, 64, 1]);  squeeze_422 = None
    permute_2163: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1691, [0, 4, 1, 2, 3]);  view_1691 = None
    view_1692: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2163, [1, 512, 1024]);  permute_2163 = None
    permute_2164: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_194, [0, 2, 1]);  view_194 = None
    bmm_473: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2164, view_1692);  permute_2164 = None
    permute_2165: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_195, [0, 2, 1]);  view_195 = None
    bmm_474: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1692, permute_2165);  view_1692 = permute_2165 = None
    view_1693: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_473, [1024, 1, 16, 64, 1]);  bmm_473 = None
    permute_2166: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1693, [4, 1, 2, 3, 0]);  view_1693 = None
    view_1694: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_474, [512, 1024, 1, 1, 1]);  bmm_474 = None
    permute_2167: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1694, [0, 2, 3, 4, 1]);  view_1694 = None
    permute_2168: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2166, [4, 2, 3, 0, 1]);  permute_2166 = None
    squeeze_430: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2168, 4);  permute_2168 = None
    squeeze_431: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_430, 3);  squeeze_430 = None
    permute_2169: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2167, [0, 1, 4, 2, 3]);  permute_2167 = None
    squeeze_432: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2169, 4);  permute_2169 = None
    squeeze_433: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_432, 3);  squeeze_432 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_398: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_397, squeeze_433);  add_397 = squeeze_433 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1695: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_396, [512, 1, 16, 64, 1]);  add_396 = None
    permute_2170: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1695, [0, 4, 1, 2, 3]);  view_1695 = None
    clone_162: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2170, memory_format = torch.contiguous_format);  permute_2170 = None
    view_1696: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_162, [1, 512, 1024]);  clone_162 = None
    permute_2171: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_190, [0, 2, 1]);  view_190 = None
    bmm_475: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2171, view_1696);  permute_2171 = None
    permute_2172: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_191, [0, 2, 1]);  view_191 = None
    bmm_476: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1696, permute_2172);  view_1696 = permute_2172 = None
    view_1697: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_475, [1024, 1, 16, 64, 1]);  bmm_475 = None
    permute_2173: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1697, [4, 1, 2, 3, 0]);  view_1697 = None
    view_1698: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_476, [512, 1024, 1, 1, 1]);  bmm_476 = None
    permute_2174: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1698, [0, 2, 3, 4, 1]);  view_1698 = None
    permute_2175: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2173, [4, 2, 3, 0, 1]);  permute_2173 = None
    squeeze_434: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2175, 4);  permute_2175 = None
    squeeze_435: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_434, 3);  squeeze_434 = None
    permute_2176: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2174, [0, 1, 4, 2, 3]);  permute_2174 = None
    squeeze_436: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2176, 4);  permute_2176 = None
    squeeze_437: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_436, 3);  squeeze_436 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_399: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_398, squeeze_437);  add_398 = squeeze_437 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_208: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_54, getitem_63);  add_54 = getitem_63 = None
    mul_808: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_208, rsqrt_9);  sub_208 = None
    mul_809: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_399, primals_208);  primals_208 = None
    mul_810: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_809, 1024)
    sum_277: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_809, [2], True)
    mul_811: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_809, mul_808);  mul_809 = None
    sum_278: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_811, [2], True);  mul_811 = None
    mul_812: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_808, sum_278);  sum_278 = None
    sub_209: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_810, sum_277);  mul_810 = sum_277 = None
    sub_210: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_209, mul_812);  sub_209 = mul_812 = None
    div_65: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_9, 1024);  rsqrt_9 = None
    mul_813: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_65, sub_210);  div_65 = sub_210 = None
    mul_814: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_399, mul_808);  mul_808 = None
    sum_279: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_814, [0, 1]);  mul_814 = None
    sum_280: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_399, [0, 1]);  add_399 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_83: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_61, torch.float32);  getitem_61 = None
    mul_815: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_83, 1.1111111111111112);  convert_element_type_83 = None
    mul_816: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_813, mul_815);  mul_815 = None
    clone_163: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_816, memory_format = torch.contiguous_format);  mul_816 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1699: "f32[512, 1024]" = torch.ops.aten.view.default(clone_163, [512, 1024]);  clone_163 = None
    permute_2177: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_212, [1, 0]);  permute_212 = None
    mm_78: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1699, permute_2177);  permute_2177 = None
    permute_2178: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1699, [1, 0])
    mm_79: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_2178, view_188);  permute_2178 = view_188 = None
    permute_2179: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_79, [1, 0]);  mm_79 = None
    sum_281: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1699, [0], True);  view_1699 = None
    view_1700: "f32[1024]" = torch.ops.aten.view.default(sum_281, [1024]);  sum_281 = None
    permute_2180: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_2179, [1, 0]);  permute_2179 = None
    view_1701: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_78, [512, 1, 4096]);  mm_78 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_84: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_59, torch.float32);  getitem_59 = None
    mul_817: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_84, 1.1111111111111112);  convert_element_type_84 = None
    mul_818: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1701, mul_817);  view_1701 = mul_817 = None
    clone_164: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_818, memory_format = torch.contiguous_format);  mul_818 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_819: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_187, 0.7071067811865476)
    erf_43: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_819);  mul_819 = None
    add_400: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_43, 1);  erf_43 = None
    mul_820: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_400, 0.5);  add_400 = None
    mul_821: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_187, view_187)
    mul_822: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_821, -0.5);  mul_821 = None
    exp_45: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_822);  mul_822 = None
    mul_823: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_45, 0.3989422804014327);  exp_45 = None
    mul_824: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_187, mul_823);  view_187 = mul_823 = None
    add_401: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_820, mul_824);  mul_820 = mul_824 = None
    mul_825: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_164, add_401);  clone_164 = add_401 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1702: "f32[512, 4096]" = torch.ops.aten.view.default(mul_825, [512, 4096]);  mul_825 = None
    permute_2181: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_211, [1, 0]);  permute_211 = None
    mm_80: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1702, permute_2181);  permute_2181 = None
    permute_2182: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1702, [1, 0])
    mm_81: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_2182, view_186);  permute_2182 = view_186 = None
    permute_2183: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_81, [1, 0]);  mm_81 = None
    sum_282: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1702, [0], True);  view_1702 = None
    view_1703: "f32[4096]" = torch.ops.aten.view.default(sum_282, [4096]);  sum_282 = None
    permute_2184: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_2183, [1, 0]);  permute_2183 = None
    view_1704: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_80, [512, 1, 1024]);  mm_80 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_402: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_813, view_1704);  mul_813 = view_1704 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_211: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_50, getitem_57);  add_50 = getitem_57 = None
    mul_826: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_211, rsqrt_8);  sub_211 = None
    mul_827: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_402, primals_202);  primals_202 = None
    mul_828: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_827, 1024)
    sum_283: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_827, [2], True)
    mul_829: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_827, mul_826);  mul_827 = None
    sum_284: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_829, [2], True);  mul_829 = None
    mul_830: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_826, sum_284);  sum_284 = None
    sub_212: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_828, sum_283);  mul_828 = sum_283 = None
    sub_213: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_212, mul_830);  sub_212 = mul_830 = None
    div_66: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_8, 1024);  rsqrt_8 = None
    mul_831: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_66, sub_213);  div_66 = sub_213 = None
    mul_832: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_402, mul_826);  mul_826 = None
    sum_285: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_832, [0, 1]);  mul_832 = None
    sum_286: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_402, [0, 1]);  add_402 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_85: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_55, torch.float32);  getitem_55 = None
    mul_833: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_85, 1.1111111111111112);  convert_element_type_85 = None
    mul_834: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_831, mul_833);  mul_833 = None
    clone_165: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_834, memory_format = torch.contiguous_format);  mul_834 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1705: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_165, [512, 1, 1024, 1, 1]);  clone_165 = None
    permute_2185: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1705, [0, 3, 4, 1, 2]);  view_1705 = None
    view_1706: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2185, [1, 512, 1024]);  permute_2185 = None
    permute_2186: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_182, [0, 2, 1]);  view_182 = None
    bmm_477: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2186, view_1706);  permute_2186 = None
    permute_2187: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_183, [0, 2, 1]);  view_183 = None
    bmm_478: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1706, permute_2187);  view_1706 = permute_2187 = None
    view_1707: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_477, [64, 16, 1, 1024, 1]);  bmm_477 = None
    permute_2188: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1707, [4, 2, 3, 0, 1]);  view_1707 = None
    view_1708: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_478, [512, 64, 16, 1, 1]);  bmm_478 = None
    permute_2189: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1708, [0, 3, 4, 1, 2]);  view_1708 = None
    permute_2190: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2188, [2, 4, 3, 0, 1]);  permute_2188 = None
    squeeze_438: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2190, 4);  permute_2190 = None
    squeeze_439: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_438, 3);  squeeze_438 = None
    permute_2191: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2189, [0, 1, 4, 3, 2]);  permute_2189 = None
    squeeze_440: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2191, 4);  permute_2191 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1709: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_440, [512, 1, 16, 64, 1]);  squeeze_440 = None
    permute_2192: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1709, [2, 0, 4, 1, 3]);  view_1709 = None
    view_1710: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_2192, [16, 512, 64]);  permute_2192 = None
    permute_2193: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_178, [0, 2, 1]);  view_178 = None
    bmm_479: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_2193, view_1710);  permute_2193 = None
    permute_2194: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_179, [0, 2, 1]);  view_179 = None
    bmm_480: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1710, permute_2194);  view_1710 = permute_2194 = None
    view_1711: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_479, [16, 512, 1, 64, 1]);  bmm_479 = None
    permute_2195: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1711, [4, 2, 0, 3, 1]);  view_1711 = None
    view_1712: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_480, [16, 512, 512, 1, 1]);  bmm_480 = None
    permute_2196: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1712, [1, 3, 0, 4, 2]);  view_1712 = None
    permute_2197: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2195, [4, 1, 2, 3, 0]);  permute_2195 = None
    squeeze_441: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2197, 4);  permute_2197 = None
    permute_2198: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_2196, [1, 2, 0, 4, 3]);  permute_2196 = None
    squeeze_442: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_2198, 4);  permute_2198 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_86: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_53, torch.float32);  getitem_53 = None
    mul_835: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_86, 1.1111111111111112);  convert_element_type_86 = None
    mul_836: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_442, mul_835);  squeeze_442 = mul_835 = None
    clone_166: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_836, memory_format = torch.contiguous_format);  mul_836 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_45: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_4);  alias_4 = None
    mul_837: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_166, alias_45);  clone_166 = None
    sum_287: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_837, [3], True)
    mul_838: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_45, sum_287);  alias_45 = sum_287 = None
    sub_214: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_837, mul_838);  mul_837 = mul_838 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_839: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_214, 0.125);  sub_214 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_96: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_19: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_96, [None, None, None, iota_6], mul_839, True);  full_96 = iota_6 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1713: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_19, [1, 16, 1023, 512]);  index_put_19 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_97: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_76: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_97, view_1713, 3, 0, 9223372036854775807);  full_97 = view_1713 = None
    full_98: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_77: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_98, slice_scatter_76, 2, 1, 9223372036854775807);  full_98 = slice_scatter_76 = None
    full_99: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_78: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_99, slice_scatter_77, 1, 0, 9223372036854775807);  full_99 = slice_scatter_77 = None
    full_100: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_79: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_100, slice_scatter_78, 0, 0, 9223372036854775807);  full_100 = slice_scatter_78 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1714: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_79, [1, 16, 512, 1024]);  slice_scatter_79 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1715: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1714, [1, 16, 512, 1024, 1]);  view_1714 = None
    permute_2199: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1715, [1, 2, 4, 0, 3]);  view_1715 = None
    view_1716: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_2199, [16, 512, 1024]);  permute_2199 = None
    permute_2200: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_172, [0, 2, 1]);  view_172 = None
    bmm_481: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_2200, view_1716);  permute_2200 = None
    permute_2201: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_173, [0, 2, 1]);  view_173 = None
    bmm_482: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1716, permute_2201);  view_1716 = permute_2201 = None
    view_1717: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_481, [16, 64, 1, 1024, 1]);  bmm_481 = None
    permute_2202: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1717, [2, 0, 4, 3, 1]);  view_1717 = None
    view_1718: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_482, [16, 512, 64, 1, 1]);  bmm_482 = None
    permute_2203: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1718, [3, 0, 1, 4, 2]);  view_1718 = None
    permute_2204: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2202, [3, 0, 1, 4, 2]);  permute_2202 = None
    squeeze_443: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2204, 4);  permute_2204 = None
    permute_2205: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2203, [2, 0, 1, 4, 3]);  permute_2203 = None
    squeeze_444: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2205, 4);  permute_2205 = None
    sum_288: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_444, [0, 1], True)
    view_1719: "f32[16, 64]" = torch.ops.aten.view.default(sum_288, [16, 64]);  sum_288 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1720: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_839, [1, 16, 512, 512, 1]);  mul_839 = None
    permute_2206: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1720, [1, 2, 4, 0, 3]);  view_1720 = None
    view_1721: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_2206, [16, 512, 512]);  permute_2206 = None
    permute_2207: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_168, [0, 2, 1]);  view_168 = None
    bmm_483: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_2207, view_1721);  permute_2207 = None
    permute_2208: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_169, [0, 2, 1]);  view_169 = None
    bmm_484: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1721, permute_2208);  view_1721 = permute_2208 = None
    view_1722: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_483, [16, 64, 1, 512, 1]);  bmm_483 = None
    permute_2209: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1722, [2, 0, 4, 3, 1]);  view_1722 = None
    view_1723: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_484, [16, 512, 64, 1, 1]);  bmm_484 = None
    permute_2210: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1723, [3, 0, 1, 4, 2]);  view_1723 = None
    permute_2211: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2209, [3, 0, 1, 4, 2]);  permute_2209 = None
    squeeze_445: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2211, 4);  permute_2211 = None
    permute_2212: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2210, [2, 0, 1, 4, 3]);  permute_2210 = None
    squeeze_446: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2212, 4);  permute_2212 = None
    sum_289: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_446, [0, 1], True)
    view_1724: "f32[16, 64]" = torch.ops.aten.view.default(sum_289, [16, 64]);  sum_289 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_403: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_444, squeeze_446);  squeeze_444 = squeeze_446 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1725: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_443, [1024, 1, 16, 64, 1]);  squeeze_443 = None
    permute_2213: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1725, [0, 4, 1, 2, 3]);  view_1725 = None
    view_1726: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_2213, [1, 1024, 1024]);  permute_2213 = None
    permute_2214: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_164, [0, 2, 1]);  view_164 = None
    bmm_485: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2214, view_1726);  permute_2214 = view_1726 = None
    view_1727: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_485, [1024, 1, 16, 64, 1]);  bmm_485 = None
    permute_2215: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1727, [4, 1, 2, 3, 0]);  view_1727 = None
    permute_2216: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2215, [4, 2, 3, 0, 1]);  permute_2215 = None
    squeeze_447: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2216, 4);  permute_2216 = None
    squeeze_448: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_447, 3);  squeeze_447 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1728: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_441, [512, 1, 16, 64, 1]);  squeeze_441 = None
    permute_2217: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1728, [0, 4, 1, 2, 3]);  view_1728 = None
    clone_167: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2217, memory_format = torch.contiguous_format);  permute_2217 = None
    view_1729: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_167, [1, 512, 1024]);  clone_167 = None
    permute_2218: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_160, [0, 2, 1]);  view_160 = None
    bmm_486: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2218, view_1729);  permute_2218 = None
    permute_2219: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_161, [0, 2, 1]);  view_161 = None
    bmm_487: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1729, permute_2219);  view_1729 = permute_2219 = None
    view_1730: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_486, [1024, 1, 16, 64, 1]);  bmm_486 = None
    permute_2220: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1730, [4, 1, 2, 3, 0]);  view_1730 = None
    view_1731: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_487, [512, 1024, 1, 1, 1]);  bmm_487 = None
    permute_2221: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1731, [0, 2, 3, 4, 1]);  view_1731 = None
    permute_2222: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2220, [4, 2, 3, 0, 1]);  permute_2220 = None
    squeeze_449: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2222, 4);  permute_2222 = None
    squeeze_450: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_449, 3);  squeeze_449 = None
    permute_2223: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2221, [0, 1, 4, 2, 3]);  permute_2221 = None
    squeeze_451: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2223, 4);  permute_2223 = None
    squeeze_452: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_451, 3);  squeeze_451 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_404: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_831, squeeze_452);  mul_831 = squeeze_452 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1732: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_445, [512, 1, 16, 64, 1]);  squeeze_445 = None
    permute_2224: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1732, [0, 4, 1, 2, 3]);  view_1732 = None
    view_1733: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2224, [1, 512, 1024]);  permute_2224 = None
    permute_2225: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_156, [0, 2, 1]);  view_156 = None
    bmm_488: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2225, view_1733);  permute_2225 = None
    permute_2226: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_157, [0, 2, 1]);  view_157 = None
    bmm_489: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1733, permute_2226);  view_1733 = permute_2226 = None
    view_1734: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_488, [1024, 1, 16, 64, 1]);  bmm_488 = None
    permute_2227: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1734, [4, 1, 2, 3, 0]);  view_1734 = None
    view_1735: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_489, [512, 1024, 1, 1, 1]);  bmm_489 = None
    permute_2228: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1735, [0, 2, 3, 4, 1]);  view_1735 = None
    permute_2229: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2227, [4, 2, 3, 0, 1]);  permute_2227 = None
    squeeze_453: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2229, 4);  permute_2229 = None
    squeeze_454: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_453, 3);  squeeze_453 = None
    permute_2230: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2228, [0, 1, 4, 2, 3]);  permute_2228 = None
    squeeze_455: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2230, 4);  permute_2230 = None
    squeeze_456: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_455, 3);  squeeze_455 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_405: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_404, squeeze_456);  add_404 = squeeze_456 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1736: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_403, [512, 1, 16, 64, 1]);  add_403 = None
    permute_2231: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1736, [0, 4, 1, 2, 3]);  view_1736 = None
    clone_168: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2231, memory_format = torch.contiguous_format);  permute_2231 = None
    view_1737: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_168, [1, 512, 1024]);  clone_168 = None
    permute_2232: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_152, [0, 2, 1]);  view_152 = None
    bmm_490: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2232, view_1737);  permute_2232 = None
    permute_2233: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_153, [0, 2, 1]);  view_153 = None
    bmm_491: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1737, permute_2233);  view_1737 = permute_2233 = None
    view_1738: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_490, [1024, 1, 16, 64, 1]);  bmm_490 = None
    permute_2234: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1738, [4, 1, 2, 3, 0]);  view_1738 = None
    view_1739: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_491, [512, 1024, 1, 1, 1]);  bmm_491 = None
    permute_2235: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1739, [0, 2, 3, 4, 1]);  view_1739 = None
    permute_2236: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2234, [4, 2, 3, 0, 1]);  permute_2234 = None
    squeeze_457: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2236, 4);  permute_2236 = None
    squeeze_458: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_457, 3);  squeeze_457 = None
    permute_2237: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2235, [0, 1, 4, 2, 3]);  permute_2235 = None
    squeeze_459: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2237, 4);  permute_2237 = None
    squeeze_460: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_459, 3);  squeeze_459 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_406: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_405, squeeze_460);  add_405 = squeeze_460 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_215: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_43, getitem_51);  add_43 = getitem_51 = None
    mul_840: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_215, rsqrt_7);  sub_215 = None
    mul_841: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_406, primals_200);  primals_200 = None
    mul_842: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_841, 1024)
    sum_290: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_841, [2], True)
    mul_843: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_841, mul_840);  mul_841 = None
    sum_291: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_843, [2], True);  mul_843 = None
    mul_844: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_840, sum_291);  sum_291 = None
    sub_216: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_842, sum_290);  mul_842 = sum_290 = None
    sub_217: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_216, mul_844);  sub_216 = mul_844 = None
    div_67: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_7, 1024);  rsqrt_7 = None
    mul_845: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_67, sub_217);  div_67 = sub_217 = None
    mul_846: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_406, mul_840);  mul_840 = None
    sum_292: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_846, [0, 1]);  mul_846 = None
    sum_293: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_406, [0, 1]);  add_406 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_87: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_49, torch.float32);  getitem_49 = None
    mul_847: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_87, 1.1111111111111112);  convert_element_type_87 = None
    mul_848: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_845, mul_847);  mul_847 = None
    clone_169: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_848, memory_format = torch.contiguous_format);  mul_848 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1740: "f32[512, 1024]" = torch.ops.aten.view.default(clone_169, [512, 1024]);  clone_169 = None
    permute_2238: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_170, [1, 0]);  permute_170 = None
    mm_82: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1740, permute_2238);  permute_2238 = None
    permute_2239: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1740, [1, 0])
    mm_83: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_2239, view_150);  permute_2239 = view_150 = None
    permute_2240: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_83, [1, 0]);  mm_83 = None
    sum_294: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1740, [0], True);  view_1740 = None
    view_1741: "f32[1024]" = torch.ops.aten.view.default(sum_294, [1024]);  sum_294 = None
    permute_2241: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_2240, [1, 0]);  permute_2240 = None
    view_1742: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_82, [512, 1, 4096]);  mm_82 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_88: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_47, torch.float32);  getitem_47 = None
    mul_849: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_88, 1.1111111111111112);  convert_element_type_88 = None
    mul_850: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1742, mul_849);  view_1742 = mul_849 = None
    clone_170: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_850, memory_format = torch.contiguous_format);  mul_850 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_851: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_149, 0.7071067811865476)
    erf_44: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_851);  mul_851 = None
    add_407: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_44, 1);  erf_44 = None
    mul_852: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_407, 0.5);  add_407 = None
    mul_853: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_149, view_149)
    mul_854: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_853, -0.5);  mul_853 = None
    exp_46: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_854);  mul_854 = None
    mul_855: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_46, 0.3989422804014327);  exp_46 = None
    mul_856: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_149, mul_855);  view_149 = mul_855 = None
    add_408: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_852, mul_856);  mul_852 = mul_856 = None
    mul_857: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_170, add_408);  clone_170 = add_408 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1743: "f32[512, 4096]" = torch.ops.aten.view.default(mul_857, [512, 4096]);  mul_857 = None
    permute_2242: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_169, [1, 0]);  permute_169 = None
    mm_84: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1743, permute_2242);  permute_2242 = None
    permute_2243: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1743, [1, 0])
    mm_85: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_2243, view_148);  permute_2243 = view_148 = None
    permute_2244: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_85, [1, 0]);  mm_85 = None
    sum_295: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1743, [0], True);  view_1743 = None
    view_1744: "f32[4096]" = torch.ops.aten.view.default(sum_295, [4096]);  sum_295 = None
    permute_2245: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_2244, [1, 0]);  permute_2244 = None
    view_1745: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_84, [512, 1, 1024]);  mm_84 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_409: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_845, view_1745);  mul_845 = view_1745 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_218: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_39, getitem_45);  add_39 = getitem_45 = None
    mul_858: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_218, rsqrt_6);  sub_218 = None
    mul_859: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_409, primals_194);  primals_194 = None
    mul_860: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_859, 1024)
    sum_296: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_859, [2], True)
    mul_861: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_859, mul_858);  mul_859 = None
    sum_297: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_861, [2], True);  mul_861 = None
    mul_862: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_858, sum_297);  sum_297 = None
    sub_219: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_860, sum_296);  mul_860 = sum_296 = None
    sub_220: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_219, mul_862);  sub_219 = mul_862 = None
    div_68: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_6, 1024);  rsqrt_6 = None
    mul_863: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_68, sub_220);  div_68 = sub_220 = None
    mul_864: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_409, mul_858);  mul_858 = None
    sum_298: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_864, [0, 1]);  mul_864 = None
    sum_299: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_409, [0, 1]);  add_409 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_89: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_43, torch.float32);  getitem_43 = None
    mul_865: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_89, 1.1111111111111112);  convert_element_type_89 = None
    mul_866: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_863, mul_865);  mul_865 = None
    clone_171: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_866, memory_format = torch.contiguous_format);  mul_866 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1746: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_171, [512, 1, 1024, 1, 1]);  clone_171 = None
    permute_2246: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1746, [0, 3, 4, 1, 2]);  view_1746 = None
    view_1747: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2246, [1, 512, 1024]);  permute_2246 = None
    permute_2247: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_144, [0, 2, 1]);  view_144 = None
    bmm_492: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2247, view_1747);  permute_2247 = None
    permute_2248: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_145, [0, 2, 1]);  view_145 = None
    bmm_493: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1747, permute_2248);  view_1747 = permute_2248 = None
    view_1748: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_492, [64, 16, 1, 1024, 1]);  bmm_492 = None
    permute_2249: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1748, [4, 2, 3, 0, 1]);  view_1748 = None
    view_1749: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_493, [512, 64, 16, 1, 1]);  bmm_493 = None
    permute_2250: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1749, [0, 3, 4, 1, 2]);  view_1749 = None
    permute_2251: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2249, [2, 4, 3, 0, 1]);  permute_2249 = None
    squeeze_461: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2251, 4);  permute_2251 = None
    squeeze_462: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_461, 3);  squeeze_461 = None
    permute_2252: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2250, [0, 1, 4, 3, 2]);  permute_2250 = None
    squeeze_463: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2252, 4);  permute_2252 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1750: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_463, [512, 1, 16, 64, 1]);  squeeze_463 = None
    permute_2253: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1750, [2, 0, 4, 1, 3]);  view_1750 = None
    view_1751: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_2253, [16, 512, 64]);  permute_2253 = None
    permute_2254: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_140, [0, 2, 1]);  view_140 = None
    bmm_494: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_2254, view_1751);  permute_2254 = None
    permute_2255: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_141, [0, 2, 1]);  view_141 = None
    bmm_495: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1751, permute_2255);  view_1751 = permute_2255 = None
    view_1752: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_494, [16, 512, 1, 64, 1]);  bmm_494 = None
    permute_2256: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1752, [4, 2, 0, 3, 1]);  view_1752 = None
    view_1753: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_495, [16, 512, 512, 1, 1]);  bmm_495 = None
    permute_2257: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1753, [1, 3, 0, 4, 2]);  view_1753 = None
    permute_2258: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2256, [4, 1, 2, 3, 0]);  permute_2256 = None
    squeeze_464: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2258, 4);  permute_2258 = None
    permute_2259: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_2257, [1, 2, 0, 4, 3]);  permute_2257 = None
    squeeze_465: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_2259, 4);  permute_2259 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_90: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_41, torch.float32);  getitem_41 = None
    mul_867: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_90, 1.1111111111111112);  convert_element_type_90 = None
    mul_868: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_465, mul_867);  squeeze_465 = mul_867 = None
    clone_172: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_868, memory_format = torch.contiguous_format);  mul_868 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_46: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_3);  alias_3 = None
    mul_869: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_172, alias_46);  clone_172 = None
    sum_300: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_869, [3], True)
    mul_870: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_46, sum_300);  alias_46 = sum_300 = None
    sub_221: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_869, mul_870);  mul_869 = mul_870 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_871: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_221, 0.125);  sub_221 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_101: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_20: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_101, [None, None, None, iota_5], mul_871, True);  full_101 = iota_5 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1754: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_20, [1, 16, 1023, 512]);  index_put_20 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_102: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_80: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_102, view_1754, 3, 0, 9223372036854775807);  full_102 = view_1754 = None
    full_103: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_81: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_103, slice_scatter_80, 2, 1, 9223372036854775807);  full_103 = slice_scatter_80 = None
    full_104: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_82: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_104, slice_scatter_81, 1, 0, 9223372036854775807);  full_104 = slice_scatter_81 = None
    full_105: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_83: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_105, slice_scatter_82, 0, 0, 9223372036854775807);  full_105 = slice_scatter_82 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1755: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_83, [1, 16, 512, 1024]);  slice_scatter_83 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1756: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1755, [1, 16, 512, 1024, 1]);  view_1755 = None
    permute_2260: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1756, [1, 2, 4, 0, 3]);  view_1756 = None
    view_1757: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_2260, [16, 512, 1024]);  permute_2260 = None
    permute_2261: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_134, [0, 2, 1]);  view_134 = None
    bmm_496: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_2261, view_1757);  permute_2261 = None
    permute_2262: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_135, [0, 2, 1]);  view_135 = None
    bmm_497: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1757, permute_2262);  view_1757 = permute_2262 = None
    view_1758: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_496, [16, 64, 1, 1024, 1]);  bmm_496 = None
    permute_2263: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1758, [2, 0, 4, 3, 1]);  view_1758 = None
    view_1759: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_497, [16, 512, 64, 1, 1]);  bmm_497 = None
    permute_2264: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1759, [3, 0, 1, 4, 2]);  view_1759 = None
    permute_2265: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2263, [3, 0, 1, 4, 2]);  permute_2263 = None
    squeeze_466: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2265, 4);  permute_2265 = None
    permute_2266: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2264, [2, 0, 1, 4, 3]);  permute_2264 = None
    squeeze_467: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2266, 4);  permute_2266 = None
    sum_301: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_467, [0, 1], True)
    view_1760: "f32[16, 64]" = torch.ops.aten.view.default(sum_301, [16, 64]);  sum_301 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1761: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_871, [1, 16, 512, 512, 1]);  mul_871 = None
    permute_2267: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1761, [1, 2, 4, 0, 3]);  view_1761 = None
    view_1762: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_2267, [16, 512, 512]);  permute_2267 = None
    permute_2268: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_130, [0, 2, 1]);  view_130 = None
    bmm_498: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_2268, view_1762);  permute_2268 = None
    permute_2269: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_131, [0, 2, 1]);  view_131 = None
    bmm_499: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1762, permute_2269);  view_1762 = permute_2269 = None
    view_1763: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_498, [16, 64, 1, 512, 1]);  bmm_498 = None
    permute_2270: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1763, [2, 0, 4, 3, 1]);  view_1763 = None
    view_1764: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_499, [16, 512, 64, 1, 1]);  bmm_499 = None
    permute_2271: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1764, [3, 0, 1, 4, 2]);  view_1764 = None
    permute_2272: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2270, [3, 0, 1, 4, 2]);  permute_2270 = None
    squeeze_468: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2272, 4);  permute_2272 = None
    permute_2273: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2271, [2, 0, 1, 4, 3]);  permute_2271 = None
    squeeze_469: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2273, 4);  permute_2273 = None
    sum_302: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_469, [0, 1], True)
    view_1765: "f32[16, 64]" = torch.ops.aten.view.default(sum_302, [16, 64]);  sum_302 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_410: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_467, squeeze_469);  squeeze_467 = squeeze_469 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1766: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_466, [1024, 1, 16, 64, 1]);  squeeze_466 = None
    permute_2274: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1766, [0, 4, 1, 2, 3]);  view_1766 = None
    view_1767: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_2274, [1, 1024, 1024]);  permute_2274 = None
    permute_2275: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_126, [0, 2, 1]);  view_126 = None
    bmm_500: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2275, view_1767);  permute_2275 = view_1767 = None
    view_1768: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_500, [1024, 1, 16, 64, 1]);  bmm_500 = None
    permute_2276: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1768, [4, 1, 2, 3, 0]);  view_1768 = None
    permute_2277: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2276, [4, 2, 3, 0, 1]);  permute_2276 = None
    squeeze_470: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2277, 4);  permute_2277 = None
    squeeze_471: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_470, 3);  squeeze_470 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1769: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_464, [512, 1, 16, 64, 1]);  squeeze_464 = None
    permute_2278: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1769, [0, 4, 1, 2, 3]);  view_1769 = None
    clone_173: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2278, memory_format = torch.contiguous_format);  permute_2278 = None
    view_1770: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_173, [1, 512, 1024]);  clone_173 = None
    permute_2279: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_122, [0, 2, 1]);  view_122 = None
    bmm_501: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2279, view_1770);  permute_2279 = None
    permute_2280: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_123, [0, 2, 1]);  view_123 = None
    bmm_502: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1770, permute_2280);  view_1770 = permute_2280 = None
    view_1771: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_501, [1024, 1, 16, 64, 1]);  bmm_501 = None
    permute_2281: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1771, [4, 1, 2, 3, 0]);  view_1771 = None
    view_1772: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_502, [512, 1024, 1, 1, 1]);  bmm_502 = None
    permute_2282: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1772, [0, 2, 3, 4, 1]);  view_1772 = None
    permute_2283: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2281, [4, 2, 3, 0, 1]);  permute_2281 = None
    squeeze_472: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2283, 4);  permute_2283 = None
    squeeze_473: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_472, 3);  squeeze_472 = None
    permute_2284: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2282, [0, 1, 4, 2, 3]);  permute_2282 = None
    squeeze_474: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2284, 4);  permute_2284 = None
    squeeze_475: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_474, 3);  squeeze_474 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_411: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_863, squeeze_475);  mul_863 = squeeze_475 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1773: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_468, [512, 1, 16, 64, 1]);  squeeze_468 = None
    permute_2285: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1773, [0, 4, 1, 2, 3]);  view_1773 = None
    view_1774: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2285, [1, 512, 1024]);  permute_2285 = None
    permute_2286: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_118, [0, 2, 1]);  view_118 = None
    bmm_503: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2286, view_1774);  permute_2286 = None
    permute_2287: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_119, [0, 2, 1]);  view_119 = None
    bmm_504: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1774, permute_2287);  view_1774 = permute_2287 = None
    view_1775: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_503, [1024, 1, 16, 64, 1]);  bmm_503 = None
    permute_2288: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1775, [4, 1, 2, 3, 0]);  view_1775 = None
    view_1776: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_504, [512, 1024, 1, 1, 1]);  bmm_504 = None
    permute_2289: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1776, [0, 2, 3, 4, 1]);  view_1776 = None
    permute_2290: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2288, [4, 2, 3, 0, 1]);  permute_2288 = None
    squeeze_476: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2290, 4);  permute_2290 = None
    squeeze_477: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_476, 3);  squeeze_476 = None
    permute_2291: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2289, [0, 1, 4, 2, 3]);  permute_2289 = None
    squeeze_478: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2291, 4);  permute_2291 = None
    squeeze_479: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_478, 3);  squeeze_478 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_412: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_411, squeeze_479);  add_411 = squeeze_479 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1777: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_410, [512, 1, 16, 64, 1]);  add_410 = None
    permute_2292: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1777, [0, 4, 1, 2, 3]);  view_1777 = None
    clone_174: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2292, memory_format = torch.contiguous_format);  permute_2292 = None
    view_1778: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_174, [1, 512, 1024]);  clone_174 = None
    permute_2293: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_114, [0, 2, 1]);  view_114 = None
    bmm_505: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2293, view_1778);  permute_2293 = None
    permute_2294: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_115, [0, 2, 1]);  view_115 = None
    bmm_506: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1778, permute_2294);  view_1778 = permute_2294 = None
    view_1779: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_505, [1024, 1, 16, 64, 1]);  bmm_505 = None
    permute_2295: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1779, [4, 1, 2, 3, 0]);  view_1779 = None
    view_1780: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_506, [512, 1024, 1, 1, 1]);  bmm_506 = None
    permute_2296: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1780, [0, 2, 3, 4, 1]);  view_1780 = None
    permute_2297: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2295, [4, 2, 3, 0, 1]);  permute_2295 = None
    squeeze_480: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2297, 4);  permute_2297 = None
    squeeze_481: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_480, 3);  squeeze_480 = None
    permute_2298: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2296, [0, 1, 4, 2, 3]);  permute_2296 = None
    squeeze_482: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2298, 4);  permute_2298 = None
    squeeze_483: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_482, 3);  squeeze_482 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_413: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_412, squeeze_483);  add_412 = squeeze_483 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_222: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_32, getitem_39);  add_32 = getitem_39 = None
    mul_872: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_222, rsqrt_5);  sub_222 = None
    mul_873: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_413, primals_192);  primals_192 = None
    mul_874: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_873, 1024)
    sum_303: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_873, [2], True)
    mul_875: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_873, mul_872);  mul_873 = None
    sum_304: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_875, [2], True);  mul_875 = None
    mul_876: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_872, sum_304);  sum_304 = None
    sub_223: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_874, sum_303);  mul_874 = sum_303 = None
    sub_224: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_223, mul_876);  sub_223 = mul_876 = None
    div_69: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_5, 1024);  rsqrt_5 = None
    mul_877: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_69, sub_224);  div_69 = sub_224 = None
    mul_878: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_413, mul_872);  mul_872 = None
    sum_305: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_878, [0, 1]);  mul_878 = None
    sum_306: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_413, [0, 1]);  add_413 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_91: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_37, torch.float32);  getitem_37 = None
    mul_879: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_91, 1.1111111111111112);  convert_element_type_91 = None
    mul_880: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_877, mul_879);  mul_879 = None
    clone_175: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_880, memory_format = torch.contiguous_format);  mul_880 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1781: "f32[512, 1024]" = torch.ops.aten.view.default(clone_175, [512, 1024]);  clone_175 = None
    permute_2299: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_128, [1, 0]);  permute_128 = None
    mm_86: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1781, permute_2299);  permute_2299 = None
    permute_2300: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1781, [1, 0])
    mm_87: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_2300, view_112);  permute_2300 = view_112 = None
    permute_2301: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_87, [1, 0]);  mm_87 = None
    sum_307: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1781, [0], True);  view_1781 = None
    view_1782: "f32[1024]" = torch.ops.aten.view.default(sum_307, [1024]);  sum_307 = None
    permute_2302: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_2301, [1, 0]);  permute_2301 = None
    view_1783: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_86, [512, 1, 4096]);  mm_86 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_92: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_35, torch.float32);  getitem_35 = None
    mul_881: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_92, 1.1111111111111112);  convert_element_type_92 = None
    mul_882: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1783, mul_881);  view_1783 = mul_881 = None
    clone_176: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_882, memory_format = torch.contiguous_format);  mul_882 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_883: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_111, 0.7071067811865476)
    erf_45: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_883);  mul_883 = None
    add_414: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_45, 1);  erf_45 = None
    mul_884: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_414, 0.5);  add_414 = None
    mul_885: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_111, view_111)
    mul_886: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_885, -0.5);  mul_885 = None
    exp_47: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_886);  mul_886 = None
    mul_887: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_47, 0.3989422804014327);  exp_47 = None
    mul_888: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_111, mul_887);  view_111 = mul_887 = None
    add_415: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_884, mul_888);  mul_884 = mul_888 = None
    mul_889: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_176, add_415);  clone_176 = add_415 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1784: "f32[512, 4096]" = torch.ops.aten.view.default(mul_889, [512, 4096]);  mul_889 = None
    permute_2303: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_127, [1, 0]);  permute_127 = None
    mm_88: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1784, permute_2303);  permute_2303 = None
    permute_2304: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1784, [1, 0])
    mm_89: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_2304, view_110);  permute_2304 = view_110 = None
    permute_2305: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_89, [1, 0]);  mm_89 = None
    sum_308: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1784, [0], True);  view_1784 = None
    view_1785: "f32[4096]" = torch.ops.aten.view.default(sum_308, [4096]);  sum_308 = None
    permute_2306: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_2305, [1, 0]);  permute_2305 = None
    view_1786: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_88, [512, 1, 1024]);  mm_88 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_416: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_877, view_1786);  mul_877 = view_1786 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_225: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_28, getitem_33);  add_28 = getitem_33 = None
    mul_890: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_225, rsqrt_4);  sub_225 = None
    mul_891: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_416, primals_186);  primals_186 = None
    mul_892: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_891, 1024)
    sum_309: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_891, [2], True)
    mul_893: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_891, mul_890);  mul_891 = None
    sum_310: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_893, [2], True);  mul_893 = None
    mul_894: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_890, sum_310);  sum_310 = None
    sub_226: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_892, sum_309);  mul_892 = sum_309 = None
    sub_227: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_226, mul_894);  sub_226 = mul_894 = None
    div_70: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_4, 1024);  rsqrt_4 = None
    mul_895: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_70, sub_227);  div_70 = sub_227 = None
    mul_896: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_416, mul_890);  mul_890 = None
    sum_311: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_896, [0, 1]);  mul_896 = None
    sum_312: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_416, [0, 1]);  add_416 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_93: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_31, torch.float32);  getitem_31 = None
    mul_897: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_93, 1.1111111111111112);  convert_element_type_93 = None
    mul_898: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_895, mul_897);  mul_897 = None
    clone_177: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_898, memory_format = torch.contiguous_format);  mul_898 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1787: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_177, [512, 1, 1024, 1, 1]);  clone_177 = None
    permute_2307: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1787, [0, 3, 4, 1, 2]);  view_1787 = None
    view_1788: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2307, [1, 512, 1024]);  permute_2307 = None
    permute_2308: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_106, [0, 2, 1]);  view_106 = None
    bmm_507: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2308, view_1788);  permute_2308 = None
    permute_2309: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_107, [0, 2, 1]);  view_107 = None
    bmm_508: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1788, permute_2309);  view_1788 = permute_2309 = None
    view_1789: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_507, [64, 16, 1, 1024, 1]);  bmm_507 = None
    permute_2310: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1789, [4, 2, 3, 0, 1]);  view_1789 = None
    view_1790: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_508, [512, 64, 16, 1, 1]);  bmm_508 = None
    permute_2311: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1790, [0, 3, 4, 1, 2]);  view_1790 = None
    permute_2312: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2310, [2, 4, 3, 0, 1]);  permute_2310 = None
    squeeze_484: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2312, 4);  permute_2312 = None
    squeeze_485: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_484, 3);  squeeze_484 = None
    permute_2313: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2311, [0, 1, 4, 3, 2]);  permute_2311 = None
    squeeze_486: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2313, 4);  permute_2313 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1791: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_486, [512, 1, 16, 64, 1]);  squeeze_486 = None
    permute_2314: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1791, [2, 0, 4, 1, 3]);  view_1791 = None
    view_1792: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_2314, [16, 512, 64]);  permute_2314 = None
    permute_2315: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_102, [0, 2, 1]);  view_102 = None
    bmm_509: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_2315, view_1792);  permute_2315 = None
    permute_2316: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_103, [0, 2, 1]);  view_103 = None
    bmm_510: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1792, permute_2316);  view_1792 = permute_2316 = None
    view_1793: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_509, [16, 512, 1, 64, 1]);  bmm_509 = None
    permute_2317: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1793, [4, 2, 0, 3, 1]);  view_1793 = None
    view_1794: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_510, [16, 512, 512, 1, 1]);  bmm_510 = None
    permute_2318: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1794, [1, 3, 0, 4, 2]);  view_1794 = None
    permute_2319: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2317, [4, 1, 2, 3, 0]);  permute_2317 = None
    squeeze_487: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2319, 4);  permute_2319 = None
    permute_2320: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_2318, [1, 2, 0, 4, 3]);  permute_2318 = None
    squeeze_488: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_2320, 4);  permute_2320 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_94: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_29, torch.float32);  getitem_29 = None
    mul_899: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_94, 1.1111111111111112);  convert_element_type_94 = None
    mul_900: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_488, mul_899);  squeeze_488 = mul_899 = None
    clone_178: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_900, memory_format = torch.contiguous_format);  mul_900 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_47: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
    mul_901: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_178, alias_47);  clone_178 = None
    sum_313: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_901, [3], True)
    mul_902: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_47, sum_313);  alias_47 = sum_313 = None
    sub_228: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_901, mul_902);  mul_901 = mul_902 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_903: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_228, 0.125);  sub_228 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_106: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_21: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_106, [None, None, None, iota_4], mul_903, True);  full_106 = iota_4 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1795: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_21, [1, 16, 1023, 512]);  index_put_21 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_107: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_84: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_107, view_1795, 3, 0, 9223372036854775807);  full_107 = view_1795 = None
    full_108: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_85: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_108, slice_scatter_84, 2, 1, 9223372036854775807);  full_108 = slice_scatter_84 = None
    full_109: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_86: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_109, slice_scatter_85, 1, 0, 9223372036854775807);  full_109 = slice_scatter_85 = None
    full_110: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_87: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_110, slice_scatter_86, 0, 0, 9223372036854775807);  full_110 = slice_scatter_86 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1796: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_87, [1, 16, 512, 1024]);  slice_scatter_87 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1797: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1796, [1, 16, 512, 1024, 1]);  view_1796 = None
    permute_2321: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1797, [1, 2, 4, 0, 3]);  view_1797 = None
    view_1798: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_2321, [16, 512, 1024]);  permute_2321 = None
    permute_2322: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_96, [0, 2, 1]);  view_96 = None
    bmm_511: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_2322, view_1798);  permute_2322 = None
    permute_2323: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_97, [0, 2, 1]);  view_97 = None
    bmm_512: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1798, permute_2323);  view_1798 = permute_2323 = None
    view_1799: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_511, [16, 64, 1, 1024, 1]);  bmm_511 = None
    permute_2324: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1799, [2, 0, 4, 3, 1]);  view_1799 = None
    view_1800: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_512, [16, 512, 64, 1, 1]);  bmm_512 = None
    permute_2325: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1800, [3, 0, 1, 4, 2]);  view_1800 = None
    permute_2326: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2324, [3, 0, 1, 4, 2]);  permute_2324 = None
    squeeze_489: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2326, 4);  permute_2326 = None
    permute_2327: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2325, [2, 0, 1, 4, 3]);  permute_2325 = None
    squeeze_490: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2327, 4);  permute_2327 = None
    sum_314: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_490, [0, 1], True)
    view_1801: "f32[16, 64]" = torch.ops.aten.view.default(sum_314, [16, 64]);  sum_314 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1802: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_903, [1, 16, 512, 512, 1]);  mul_903 = None
    permute_2328: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1802, [1, 2, 4, 0, 3]);  view_1802 = None
    view_1803: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_2328, [16, 512, 512]);  permute_2328 = None
    permute_2329: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_92, [0, 2, 1]);  view_92 = None
    bmm_513: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_2329, view_1803);  permute_2329 = None
    permute_2330: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_93, [0, 2, 1]);  view_93 = None
    bmm_514: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1803, permute_2330);  view_1803 = permute_2330 = None
    view_1804: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_513, [16, 64, 1, 512, 1]);  bmm_513 = None
    permute_2331: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1804, [2, 0, 4, 3, 1]);  view_1804 = None
    view_1805: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_514, [16, 512, 64, 1, 1]);  bmm_514 = None
    permute_2332: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1805, [3, 0, 1, 4, 2]);  view_1805 = None
    permute_2333: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2331, [3, 0, 1, 4, 2]);  permute_2331 = None
    squeeze_491: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2333, 4);  permute_2333 = None
    permute_2334: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2332, [2, 0, 1, 4, 3]);  permute_2332 = None
    squeeze_492: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2334, 4);  permute_2334 = None
    sum_315: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_492, [0, 1], True)
    view_1806: "f32[16, 64]" = torch.ops.aten.view.default(sum_315, [16, 64]);  sum_315 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_417: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_490, squeeze_492);  squeeze_490 = squeeze_492 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1807: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_489, [1024, 1, 16, 64, 1]);  squeeze_489 = None
    permute_2335: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1807, [0, 4, 1, 2, 3]);  view_1807 = None
    view_1808: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_2335, [1, 1024, 1024]);  permute_2335 = None
    permute_2336: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_88, [0, 2, 1]);  view_88 = None
    bmm_515: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2336, view_1808);  permute_2336 = view_1808 = None
    view_1809: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_515, [1024, 1, 16, 64, 1]);  bmm_515 = None
    permute_2337: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1809, [4, 1, 2, 3, 0]);  view_1809 = None
    permute_2338: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2337, [4, 2, 3, 0, 1]);  permute_2337 = None
    squeeze_493: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2338, 4);  permute_2338 = None
    squeeze_494: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_493, 3);  squeeze_493 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1810: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_487, [512, 1, 16, 64, 1]);  squeeze_487 = None
    permute_2339: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1810, [0, 4, 1, 2, 3]);  view_1810 = None
    clone_179: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2339, memory_format = torch.contiguous_format);  permute_2339 = None
    view_1811: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_179, [1, 512, 1024]);  clone_179 = None
    permute_2340: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_84, [0, 2, 1]);  view_84 = None
    bmm_516: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2340, view_1811);  permute_2340 = None
    permute_2341: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_85, [0, 2, 1]);  view_85 = None
    bmm_517: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1811, permute_2341);  view_1811 = permute_2341 = None
    view_1812: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_516, [1024, 1, 16, 64, 1]);  bmm_516 = None
    permute_2342: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1812, [4, 1, 2, 3, 0]);  view_1812 = None
    view_1813: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_517, [512, 1024, 1, 1, 1]);  bmm_517 = None
    permute_2343: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1813, [0, 2, 3, 4, 1]);  view_1813 = None
    permute_2344: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2342, [4, 2, 3, 0, 1]);  permute_2342 = None
    squeeze_495: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2344, 4);  permute_2344 = None
    squeeze_496: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_495, 3);  squeeze_495 = None
    permute_2345: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2343, [0, 1, 4, 2, 3]);  permute_2343 = None
    squeeze_497: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2345, 4);  permute_2345 = None
    squeeze_498: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_497, 3);  squeeze_497 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_418: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_895, squeeze_498);  mul_895 = squeeze_498 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1814: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_491, [512, 1, 16, 64, 1]);  squeeze_491 = None
    permute_2346: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1814, [0, 4, 1, 2, 3]);  view_1814 = None
    view_1815: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2346, [1, 512, 1024]);  permute_2346 = None
    permute_2347: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_80, [0, 2, 1]);  view_80 = None
    bmm_518: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2347, view_1815);  permute_2347 = None
    permute_2348: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_81, [0, 2, 1]);  view_81 = None
    bmm_519: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1815, permute_2348);  view_1815 = permute_2348 = None
    view_1816: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_518, [1024, 1, 16, 64, 1]);  bmm_518 = None
    permute_2349: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1816, [4, 1, 2, 3, 0]);  view_1816 = None
    view_1817: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_519, [512, 1024, 1, 1, 1]);  bmm_519 = None
    permute_2350: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1817, [0, 2, 3, 4, 1]);  view_1817 = None
    permute_2351: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2349, [4, 2, 3, 0, 1]);  permute_2349 = None
    squeeze_499: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2351, 4);  permute_2351 = None
    squeeze_500: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_499, 3);  squeeze_499 = None
    permute_2352: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2350, [0, 1, 4, 2, 3]);  permute_2350 = None
    squeeze_501: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2352, 4);  permute_2352 = None
    squeeze_502: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_501, 3);  squeeze_501 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_419: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_418, squeeze_502);  add_418 = squeeze_502 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1818: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_417, [512, 1, 16, 64, 1]);  add_417 = None
    permute_2353: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1818, [0, 4, 1, 2, 3]);  view_1818 = None
    clone_180: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2353, memory_format = torch.contiguous_format);  permute_2353 = None
    view_1819: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_180, [1, 512, 1024]);  clone_180 = None
    permute_2354: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_76, [0, 2, 1]);  view_76 = None
    bmm_520: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2354, view_1819);  permute_2354 = None
    permute_2355: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_77, [0, 2, 1]);  view_77 = None
    bmm_521: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1819, permute_2355);  view_1819 = permute_2355 = None
    view_1820: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_520, [1024, 1, 16, 64, 1]);  bmm_520 = None
    permute_2356: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1820, [4, 1, 2, 3, 0]);  view_1820 = None
    view_1821: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_521, [512, 1024, 1, 1, 1]);  bmm_521 = None
    permute_2357: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1821, [0, 2, 3, 4, 1]);  view_1821 = None
    permute_2358: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2356, [4, 2, 3, 0, 1]);  permute_2356 = None
    squeeze_503: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2358, 4);  permute_2358 = None
    squeeze_504: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_503, 3);  squeeze_503 = None
    permute_2359: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2357, [0, 1, 4, 2, 3]);  permute_2357 = None
    squeeze_505: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2359, 4);  permute_2359 = None
    squeeze_506: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_505, 3);  squeeze_505 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_420: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_419, squeeze_506);  add_419 = squeeze_506 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_229: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_21, getitem_27);  add_21 = getitem_27 = None
    mul_904: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_229, rsqrt_3);  sub_229 = None
    mul_905: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_420, primals_184);  primals_184 = None
    mul_906: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_905, 1024)
    sum_316: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_905, [2], True)
    mul_907: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_905, mul_904);  mul_905 = None
    sum_317: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_907, [2], True);  mul_907 = None
    mul_908: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_904, sum_317);  sum_317 = None
    sub_230: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_906, sum_316);  mul_906 = sum_316 = None
    sub_231: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_230, mul_908);  sub_230 = mul_908 = None
    div_71: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_3, 1024);  rsqrt_3 = None
    mul_909: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_71, sub_231);  div_71 = sub_231 = None
    mul_910: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_420, mul_904);  mul_904 = None
    sum_318: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_910, [0, 1]);  mul_910 = None
    sum_319: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_420, [0, 1]);  add_420 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_95: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_25, torch.float32);  getitem_25 = None
    mul_911: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_95, 1.1111111111111112);  convert_element_type_95 = None
    mul_912: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_909, mul_911);  mul_911 = None
    clone_181: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_912, memory_format = torch.contiguous_format);  mul_912 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1822: "f32[512, 1024]" = torch.ops.aten.view.default(clone_181, [512, 1024]);  clone_181 = None
    permute_2360: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_86, [1, 0]);  permute_86 = None
    mm_90: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1822, permute_2360);  permute_2360 = None
    permute_2361: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1822, [1, 0])
    mm_91: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_2361, view_74);  permute_2361 = view_74 = None
    permute_2362: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_91, [1, 0]);  mm_91 = None
    sum_320: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1822, [0], True);  view_1822 = None
    view_1823: "f32[1024]" = torch.ops.aten.view.default(sum_320, [1024]);  sum_320 = None
    permute_2363: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_2362, [1, 0]);  permute_2362 = None
    view_1824: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_90, [512, 1, 4096]);  mm_90 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_96: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_23, torch.float32);  getitem_23 = None
    mul_913: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_96, 1.1111111111111112);  convert_element_type_96 = None
    mul_914: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1824, mul_913);  view_1824 = mul_913 = None
    clone_182: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_914, memory_format = torch.contiguous_format);  mul_914 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_915: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_73, 0.7071067811865476)
    erf_46: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_915);  mul_915 = None
    add_421: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_46, 1);  erf_46 = None
    mul_916: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_421, 0.5);  add_421 = None
    mul_917: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_73, view_73)
    mul_918: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_917, -0.5);  mul_917 = None
    exp_48: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_918);  mul_918 = None
    mul_919: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_48, 0.3989422804014327);  exp_48 = None
    mul_920: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_73, mul_919);  view_73 = mul_919 = None
    add_422: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_916, mul_920);  mul_916 = mul_920 = None
    mul_921: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_182, add_422);  clone_182 = add_422 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1825: "f32[512, 4096]" = torch.ops.aten.view.default(mul_921, [512, 4096]);  mul_921 = None
    permute_2364: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_85, [1, 0]);  permute_85 = None
    mm_92: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1825, permute_2364);  permute_2364 = None
    permute_2365: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1825, [1, 0])
    mm_93: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_2365, view_72);  permute_2365 = view_72 = None
    permute_2366: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_93, [1, 0]);  mm_93 = None
    sum_321: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1825, [0], True);  view_1825 = None
    view_1826: "f32[4096]" = torch.ops.aten.view.default(sum_321, [4096]);  sum_321 = None
    permute_2367: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_2366, [1, 0]);  permute_2366 = None
    view_1827: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_92, [512, 1, 1024]);  mm_92 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_423: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_909, view_1827);  mul_909 = view_1827 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_232: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_17, getitem_21);  add_17 = getitem_21 = None
    mul_922: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_232, rsqrt_2);  sub_232 = None
    mul_923: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_423, primals_178);  primals_178 = None
    mul_924: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_923, 1024)
    sum_322: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_923, [2], True)
    mul_925: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_923, mul_922);  mul_923 = None
    sum_323: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_925, [2], True);  mul_925 = None
    mul_926: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_922, sum_323);  sum_323 = None
    sub_233: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_924, sum_322);  mul_924 = sum_322 = None
    sub_234: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_233, mul_926);  sub_233 = mul_926 = None
    div_72: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_2, 1024);  rsqrt_2 = None
    mul_927: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_72, sub_234);  div_72 = sub_234 = None
    mul_928: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_423, mul_922);  mul_922 = None
    sum_324: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_928, [0, 1]);  mul_928 = None
    sum_325: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_423, [0, 1]);  add_423 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_97: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_19, torch.float32);  getitem_19 = None
    mul_929: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_97, 1.1111111111111112);  convert_element_type_97 = None
    mul_930: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_927, mul_929);  mul_929 = None
    clone_183: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_930, memory_format = torch.contiguous_format);  mul_930 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1828: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_183, [512, 1, 1024, 1, 1]);  clone_183 = None
    permute_2368: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1828, [0, 3, 4, 1, 2]);  view_1828 = None
    view_1829: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2368, [1, 512, 1024]);  permute_2368 = None
    permute_2369: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_68, [0, 2, 1]);  view_68 = None
    bmm_522: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2369, view_1829);  permute_2369 = None
    permute_2370: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_69, [0, 2, 1]);  view_69 = None
    bmm_523: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1829, permute_2370);  view_1829 = permute_2370 = None
    view_1830: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_522, [64, 16, 1, 1024, 1]);  bmm_522 = None
    permute_2371: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1830, [4, 2, 3, 0, 1]);  view_1830 = None
    view_1831: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_523, [512, 64, 16, 1, 1]);  bmm_523 = None
    permute_2372: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1831, [0, 3, 4, 1, 2]);  view_1831 = None
    permute_2373: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2371, [2, 4, 3, 0, 1]);  permute_2371 = None
    squeeze_507: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2373, 4);  permute_2373 = None
    squeeze_508: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_507, 3);  squeeze_507 = None
    permute_2374: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2372, [0, 1, 4, 3, 2]);  permute_2372 = None
    squeeze_509: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2374, 4);  permute_2374 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1832: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_509, [512, 1, 16, 64, 1]);  squeeze_509 = None
    permute_2375: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1832, [2, 0, 4, 1, 3]);  view_1832 = None
    view_1833: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_2375, [16, 512, 64]);  permute_2375 = None
    permute_2376: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_64, [0, 2, 1]);  view_64 = None
    bmm_524: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_2376, view_1833);  permute_2376 = None
    permute_2377: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_65, [0, 2, 1]);  view_65 = None
    bmm_525: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1833, permute_2377);  view_1833 = permute_2377 = None
    view_1834: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_524, [16, 512, 1, 64, 1]);  bmm_524 = None
    permute_2378: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1834, [4, 2, 0, 3, 1]);  view_1834 = None
    view_1835: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_525, [16, 512, 512, 1, 1]);  bmm_525 = None
    permute_2379: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1835, [1, 3, 0, 4, 2]);  view_1835 = None
    permute_2380: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2378, [4, 1, 2, 3, 0]);  permute_2378 = None
    squeeze_510: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2380, 4);  permute_2380 = None
    permute_2381: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_2379, [1, 2, 0, 4, 3]);  permute_2379 = None
    squeeze_511: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_2381, 4);  permute_2381 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_98: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_17, torch.float32);  getitem_17 = None
    mul_931: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_98, 1.1111111111111112);  convert_element_type_98 = None
    mul_932: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_511, mul_931);  squeeze_511 = mul_931 = None
    clone_184: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_932, memory_format = torch.contiguous_format);  mul_932 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_48: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
    mul_933: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_184, alias_48);  clone_184 = None
    sum_326: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_933, [3], True)
    mul_934: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_48, sum_326);  alias_48 = sum_326 = None
    sub_235: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_933, mul_934);  mul_933 = mul_934 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_935: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_235, 0.125);  sub_235 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_111: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_22: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_111, [None, None, None, iota_3], mul_935, True);  full_111 = iota_3 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1836: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_22, [1, 16, 1023, 512]);  index_put_22 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_112: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_88: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_112, view_1836, 3, 0, 9223372036854775807);  full_112 = view_1836 = None
    full_113: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_89: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_113, slice_scatter_88, 2, 1, 9223372036854775807);  full_113 = slice_scatter_88 = None
    full_114: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_90: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_114, slice_scatter_89, 1, 0, 9223372036854775807);  full_114 = slice_scatter_89 = None
    full_115: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_91: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_115, slice_scatter_90, 0, 0, 9223372036854775807);  full_115 = slice_scatter_90 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1837: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_91, [1, 16, 512, 1024]);  slice_scatter_91 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1838: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1837, [1, 16, 512, 1024, 1]);  view_1837 = None
    permute_2382: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1838, [1, 2, 4, 0, 3]);  view_1838 = None
    view_1839: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_2382, [16, 512, 1024]);  permute_2382 = None
    permute_2383: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_58, [0, 2, 1]);  view_58 = None
    bmm_526: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_2383, view_1839);  permute_2383 = None
    permute_2384: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_59, [0, 2, 1]);  view_59 = None
    bmm_527: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1839, permute_2384);  view_1839 = permute_2384 = None
    view_1840: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_526, [16, 64, 1, 1024, 1]);  bmm_526 = None
    permute_2385: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1840, [2, 0, 4, 3, 1]);  view_1840 = None
    view_1841: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_527, [16, 512, 64, 1, 1]);  bmm_527 = None
    permute_2386: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1841, [3, 0, 1, 4, 2]);  view_1841 = None
    permute_2387: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2385, [3, 0, 1, 4, 2]);  permute_2385 = None
    squeeze_512: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2387, 4);  permute_2387 = None
    permute_2388: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2386, [2, 0, 1, 4, 3]);  permute_2386 = None
    squeeze_513: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2388, 4);  permute_2388 = None
    sum_327: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_513, [0, 1], True)
    view_1842: "f32[16, 64]" = torch.ops.aten.view.default(sum_327, [16, 64]);  sum_327 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1843: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_935, [1, 16, 512, 512, 1]);  mul_935 = None
    permute_2389: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1843, [1, 2, 4, 0, 3]);  view_1843 = None
    view_1844: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_2389, [16, 512, 512]);  permute_2389 = None
    permute_2390: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_54, [0, 2, 1]);  view_54 = None
    bmm_528: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_2390, view_1844);  permute_2390 = None
    permute_2391: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_55, [0, 2, 1]);  view_55 = None
    bmm_529: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1844, permute_2391);  view_1844 = permute_2391 = None
    view_1845: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_528, [16, 64, 1, 512, 1]);  bmm_528 = None
    permute_2392: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1845, [2, 0, 4, 3, 1]);  view_1845 = None
    view_1846: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_529, [16, 512, 64, 1, 1]);  bmm_529 = None
    permute_2393: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1846, [3, 0, 1, 4, 2]);  view_1846 = None
    permute_2394: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2392, [3, 0, 1, 4, 2]);  permute_2392 = None
    squeeze_514: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2394, 4);  permute_2394 = None
    permute_2395: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2393, [2, 0, 1, 4, 3]);  permute_2393 = None
    squeeze_515: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2395, 4);  permute_2395 = None
    sum_328: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_515, [0, 1], True)
    view_1847: "f32[16, 64]" = torch.ops.aten.view.default(sum_328, [16, 64]);  sum_328 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_424: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_513, squeeze_515);  squeeze_513 = squeeze_515 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1848: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_512, [1024, 1, 16, 64, 1]);  squeeze_512 = None
    permute_2396: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1848, [0, 4, 1, 2, 3]);  view_1848 = None
    view_1849: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_2396, [1, 1024, 1024]);  permute_2396 = None
    permute_2397: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_50, [0, 2, 1]);  view_50 = None
    bmm_530: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2397, view_1849);  permute_2397 = view_1849 = None
    view_1850: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_530, [1024, 1, 16, 64, 1]);  bmm_530 = None
    permute_2398: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1850, [4, 1, 2, 3, 0]);  view_1850 = None
    permute_2399: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2398, [4, 2, 3, 0, 1]);  permute_2398 = None
    squeeze_516: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2399, 4);  permute_2399 = None
    squeeze_517: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_516, 3);  squeeze_516 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1851: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_510, [512, 1, 16, 64, 1]);  squeeze_510 = None
    permute_2400: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1851, [0, 4, 1, 2, 3]);  view_1851 = None
    clone_185: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2400, memory_format = torch.contiguous_format);  permute_2400 = None
    view_1852: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_185, [1, 512, 1024]);  clone_185 = None
    permute_2401: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_46, [0, 2, 1]);  view_46 = None
    bmm_531: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2401, view_1852);  permute_2401 = None
    permute_2402: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_47, [0, 2, 1]);  view_47 = None
    bmm_532: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1852, permute_2402);  view_1852 = permute_2402 = None
    view_1853: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_531, [1024, 1, 16, 64, 1]);  bmm_531 = None
    permute_2403: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1853, [4, 1, 2, 3, 0]);  view_1853 = None
    view_1854: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_532, [512, 1024, 1, 1, 1]);  bmm_532 = None
    permute_2404: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1854, [0, 2, 3, 4, 1]);  view_1854 = None
    permute_2405: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2403, [4, 2, 3, 0, 1]);  permute_2403 = None
    squeeze_518: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2405, 4);  permute_2405 = None
    squeeze_519: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_518, 3);  squeeze_518 = None
    permute_2406: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2404, [0, 1, 4, 2, 3]);  permute_2404 = None
    squeeze_520: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2406, 4);  permute_2406 = None
    squeeze_521: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_520, 3);  squeeze_520 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_425: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_927, squeeze_521);  mul_927 = squeeze_521 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1855: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_514, [512, 1, 16, 64, 1]);  squeeze_514 = None
    permute_2407: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1855, [0, 4, 1, 2, 3]);  view_1855 = None
    view_1856: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2407, [1, 512, 1024]);  permute_2407 = None
    permute_2408: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_42, [0, 2, 1]);  view_42 = None
    bmm_533: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2408, view_1856);  permute_2408 = None
    permute_2409: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_43, [0, 2, 1]);  view_43 = None
    bmm_534: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1856, permute_2409);  view_1856 = permute_2409 = None
    view_1857: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_533, [1024, 1, 16, 64, 1]);  bmm_533 = None
    permute_2410: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1857, [4, 1, 2, 3, 0]);  view_1857 = None
    view_1858: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_534, [512, 1024, 1, 1, 1]);  bmm_534 = None
    permute_2411: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1858, [0, 2, 3, 4, 1]);  view_1858 = None
    permute_2412: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2410, [4, 2, 3, 0, 1]);  permute_2410 = None
    squeeze_522: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2412, 4);  permute_2412 = None
    squeeze_523: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_522, 3);  squeeze_522 = None
    permute_2413: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2411, [0, 1, 4, 2, 3]);  permute_2411 = None
    squeeze_524: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2413, 4);  permute_2413 = None
    squeeze_525: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_524, 3);  squeeze_524 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_426: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_425, squeeze_525);  add_425 = squeeze_525 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1859: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_424, [512, 1, 16, 64, 1]);  add_424 = None
    permute_2414: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1859, [0, 4, 1, 2, 3]);  view_1859 = None
    clone_186: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2414, memory_format = torch.contiguous_format);  permute_2414 = None
    view_1860: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_186, [1, 512, 1024]);  clone_186 = None
    permute_2415: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_38, [0, 2, 1]);  view_38 = None
    bmm_535: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2415, view_1860);  permute_2415 = None
    permute_2416: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_39, [0, 2, 1]);  view_39 = None
    bmm_536: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1860, permute_2416);  view_1860 = permute_2416 = None
    view_1861: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_535, [1024, 1, 16, 64, 1]);  bmm_535 = None
    permute_2417: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1861, [4, 1, 2, 3, 0]);  view_1861 = None
    view_1862: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_536, [512, 1024, 1, 1, 1]);  bmm_536 = None
    permute_2418: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1862, [0, 2, 3, 4, 1]);  view_1862 = None
    permute_2419: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2417, [4, 2, 3, 0, 1]);  permute_2417 = None
    squeeze_526: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2419, 4);  permute_2419 = None
    squeeze_527: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_526, 3);  squeeze_526 = None
    permute_2420: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2418, [0, 1, 4, 2, 3]);  permute_2418 = None
    squeeze_528: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2420, 4);  permute_2420 = None
    squeeze_529: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_528, 3);  squeeze_528 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_427: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_426, squeeze_529);  add_426 = squeeze_529 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:482, code: output = self.layer_norm(output + inp)
    sub_236: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_10, getitem_15);  add_10 = getitem_15 = None
    mul_936: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_236, rsqrt_1);  sub_236 = None
    mul_937: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_427, primals_176);  primals_176 = None
    mul_938: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_937, 1024)
    sum_329: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_937, [2], True)
    mul_939: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_937, mul_936);  mul_937 = None
    sum_330: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_939, [2], True);  mul_939 = None
    mul_940: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_936, sum_330);  sum_330 = None
    sub_237: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_938, sum_329);  mul_938 = sum_329 = None
    sub_238: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_237, mul_940);  sub_237 = mul_940 = None
    div_73: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt_1, 1024);  rsqrt_1 = None
    mul_941: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_73, sub_238);  div_73 = sub_238 = None
    mul_942: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_427, mul_936);  mul_936 = None
    sum_331: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_942, [0, 1]);  mul_942 = None
    sum_332: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_427, [0, 1]);  add_427 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:481, code: output = self.dropout(output)
    convert_element_type_99: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_13, torch.float32);  getitem_13 = None
    mul_943: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_99, 1.1111111111111112);  convert_element_type_99 = None
    mul_944: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_941, mul_943);  mul_943 = None
    clone_187: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_944, memory_format = torch.contiguous_format);  mul_944 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:480, code: output = self.layer_2(output)
    view_1863: "f32[512, 1024]" = torch.ops.aten.view.default(clone_187, [512, 1024]);  clone_187 = None
    permute_2421: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None
    mm_94: "f32[512, 4096]" = torch.ops.aten.mm.default(view_1863, permute_2421);  permute_2421 = None
    permute_2422: "f32[1024, 512]" = torch.ops.aten.permute.default(view_1863, [1, 0])
    mm_95: "f32[1024, 4096]" = torch.ops.aten.mm.default(permute_2422, view_36);  permute_2422 = view_36 = None
    permute_2423: "f32[4096, 1024]" = torch.ops.aten.permute.default(mm_95, [1, 0]);  mm_95 = None
    sum_333: "f32[1, 1024]" = torch.ops.aten.sum.dim_IntList(view_1863, [0], True);  view_1863 = None
    view_1864: "f32[1024]" = torch.ops.aten.view.default(sum_333, [1024]);  sum_333 = None
    permute_2424: "f32[1024, 4096]" = torch.ops.aten.permute.default(permute_2423, [1, 0]);  permute_2423 = None
    view_1865: "f32[512, 1, 4096]" = torch.ops.aten.view.default(mm_94, [512, 1, 4096]);  mm_94 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:479, code: output = self.dropout(output)
    convert_element_type_100: "f32[512, 1, 4096]" = torch.ops.prims.convert_element_type.default(getitem_11, torch.float32);  getitem_11 = None
    mul_945: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_100, 1.1111111111111112);  convert_element_type_100 = None
    mul_946: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_1865, mul_945);  view_1865 = mul_945 = None
    clone_188: "f32[512, 1, 4096]" = torch.ops.aten.clone.default(mul_946, memory_format = torch.contiguous_format);  mul_946 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/activations.py:78, code: return self.act(input)
    mul_947: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_35, 0.7071067811865476)
    erf_47: "f32[512, 1, 4096]" = torch.ops.aten.erf.default(mul_947);  mul_947 = None
    add_428: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(erf_47, 1);  erf_47 = None
    mul_948: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(add_428, 0.5);  add_428 = None
    mul_949: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_35, view_35)
    mul_950: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(mul_949, -0.5);  mul_949 = None
    exp_49: "f32[512, 1, 4096]" = torch.ops.aten.exp.default(mul_950);  mul_950 = None
    mul_951: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(exp_49, 0.3989422804014327);  exp_49 = None
    mul_952: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(view_35, mul_951);  view_35 = mul_951 = None
    add_429: "f32[512, 1, 4096]" = torch.ops.aten.add.Tensor(mul_948, mul_952);  mul_948 = mul_952 = None
    mul_953: "f32[512, 1, 4096]" = torch.ops.aten.mul.Tensor(clone_188, add_429);  clone_188 = add_429 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    view_1866: "f32[512, 4096]" = torch.ops.aten.view.default(mul_953, [512, 4096]);  mul_953 = None
    permute_2425: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_43, [1, 0]);  permute_43 = None
    mm_96: "f32[512, 1024]" = torch.ops.aten.mm.default(view_1866, permute_2425);  permute_2425 = None
    permute_2426: "f32[4096, 512]" = torch.ops.aten.permute.default(view_1866, [1, 0])
    mm_97: "f32[4096, 1024]" = torch.ops.aten.mm.default(permute_2426, view_34);  permute_2426 = view_34 = None
    permute_2427: "f32[1024, 4096]" = torch.ops.aten.permute.default(mm_97, [1, 0]);  mm_97 = None
    sum_334: "f32[1, 4096]" = torch.ops.aten.sum.dim_IntList(view_1866, [0], True);  view_1866 = None
    view_1867: "f32[4096]" = torch.ops.aten.view.default(sum_334, [4096]);  sum_334 = None
    permute_2428: "f32[4096, 1024]" = torch.ops.aten.permute.default(permute_2427, [1, 0]);  permute_2427 = None
    view_1868: "f32[512, 1, 1024]" = torch.ops.aten.view.default(mm_96, [512, 1, 1024]);  mm_96 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:477, code: output = self.layer_1(output)
    add_430: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_941, view_1868);  mul_941 = view_1868 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:323, code: output = self.layer_norm(attn_out)
    sub_239: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(add_6, getitem_9);  add_6 = getitem_9 = None
    mul_954: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(sub_239, rsqrt);  sub_239 = None
    mul_955: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_430, primals_170);  primals_170 = None
    mul_956: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_955, 1024)
    sum_335: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_955, [2], True)
    mul_957: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_955, mul_954);  mul_955 = None
    sum_336: "f32[512, 1, 1]" = torch.ops.aten.sum.dim_IntList(mul_957, [2], True);  mul_957 = None
    mul_958: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_954, sum_336);  sum_336 = None
    sub_240: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(mul_956, sum_335);  mul_956 = sum_335 = None
    sub_241: "f32[512, 1, 1024]" = torch.ops.aten.sub.Tensor(sub_240, mul_958);  sub_240 = mul_958 = None
    div_74: "f32[512, 1, 1]" = torch.ops.aten.div.Tensor(rsqrt, 1024);  rsqrt = None
    mul_959: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(div_74, sub_241);  div_74 = sub_241 = None
    mul_960: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_430, mul_954);  mul_954 = None
    sum_337: "f32[1024]" = torch.ops.aten.sum.dim_IntList(mul_960, [0, 1]);  mul_960 = None
    sum_338: "f32[1024]" = torch.ops.aten.sum.dim_IntList(add_430, [0, 1]);  add_430 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:320, code: attn_out = self.dropout(attn_out)
    convert_element_type_101: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_7, torch.float32);  getitem_7 = None
    mul_961: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_101, 1.1111111111111112);  convert_element_type_101 = None
    mul_962: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(mul_959, mul_961);  mul_961 = None
    clone_189: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_962, memory_format = torch.contiguous_format);  mul_962 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:318, code: attn_out = torch.einsum("ibnd,hnd->ibh", attn_vec, self.o)
    view_1869: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.view.default(clone_189, [512, 1, 1024, 1, 1]);  clone_189 = None
    permute_2429: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1869, [0, 3, 4, 1, 2]);  view_1869 = None
    view_1870: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2429, [1, 512, 1024]);  permute_2429 = None
    permute_2430: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_30, [0, 2, 1]);  view_30 = None
    bmm_537: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2430, view_1870);  permute_2430 = None
    permute_2431: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_31, [0, 2, 1]);  view_31 = None
    bmm_538: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1870, permute_2431);  view_1870 = permute_2431 = None
    view_1871: "f32[64, 16, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_537, [64, 16, 1, 1024, 1]);  bmm_537 = None
    permute_2432: "f32[1, 1, 1024, 64, 16]" = torch.ops.aten.permute.default(view_1871, [4, 2, 3, 0, 1]);  view_1871 = None
    view_1872: "f32[512, 64, 16, 1, 1]" = torch.ops.aten.view.default(bmm_538, [512, 64, 16, 1, 1]);  bmm_538 = None
    permute_2433: "f32[512, 1, 1, 64, 16]" = torch.ops.aten.permute.default(view_1872, [0, 3, 4, 1, 2]);  view_1872 = None
    permute_2434: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2432, [2, 4, 3, 0, 1]);  permute_2432 = None
    squeeze_530: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2434, 4);  permute_2434 = None
    squeeze_531: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_530, 3);  squeeze_530 = None
    permute_2435: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2433, [0, 1, 4, 3, 2]);  permute_2433 = None
    squeeze_532: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2435, 4);  permute_2435 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:308, code: attn_vec = torch.einsum("bnij,jbnd->ibnd", attn_prob, v_head_h)
    view_1873: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_532, [512, 1, 16, 64, 1]);  squeeze_532 = None
    permute_2436: "f32[16, 512, 1, 1, 64]" = torch.ops.aten.permute.default(view_1873, [2, 0, 4, 1, 3]);  view_1873 = None
    view_1874: "f32[16, 512, 64]" = torch.ops.aten.view.default(permute_2436, [16, 512, 64]);  permute_2436 = None
    permute_2437: "f32[16, 512, 512]" = torch.ops.aten.permute.default(view_26, [0, 2, 1]);  view_26 = None
    bmm_539: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(permute_2437, view_1874);  permute_2437 = None
    permute_2438: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_27, [0, 2, 1]);  view_27 = None
    bmm_540: "f32[16, 512, 512]" = torch.ops.aten.bmm.default(view_1874, permute_2438);  view_1874 = permute_2438 = None
    view_1875: "f32[16, 512, 1, 64, 1]" = torch.ops.aten.view.default(bmm_539, [16, 512, 1, 64, 1]);  bmm_539 = None
    permute_2439: "f32[1, 1, 16, 64, 512]" = torch.ops.aten.permute.default(view_1875, [4, 2, 0, 3, 1]);  view_1875 = None
    view_1876: "f32[16, 512, 512, 1, 1]" = torch.ops.aten.view.default(bmm_540, [16, 512, 512, 1, 1]);  bmm_540 = None
    permute_2440: "f32[512, 1, 16, 1, 512]" = torch.ops.aten.permute.default(view_1876, [1, 3, 0, 4, 2]);  view_1876 = None
    permute_2441: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2439, [4, 1, 2, 3, 0]);  permute_2439 = None
    squeeze_533: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2441, 4);  permute_2441 = None
    permute_2442: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.permute.default(permute_2440, [1, 2, 0, 4, 3]);  permute_2440 = None
    squeeze_534: "f32[1, 16, 512, 512]" = torch.ops.aten.squeeze.dim(permute_2442, 4);  permute_2442 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:301, code: attn_prob = self.dropout(attn_prob)
    convert_element_type_102: "f32[1, 16, 512, 512]" = torch.ops.prims.convert_element_type.default(getitem_5, torch.float32);  getitem_5 = None
    mul_963: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(convert_element_type_102, 1.1111111111111112);  convert_element_type_102 = None
    mul_964: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(squeeze_534, mul_963);  squeeze_534 = mul_963 = None
    clone_190: "f32[1, 16, 512, 512]" = torch.ops.aten.clone.default(mul_964, memory_format = torch.contiguous_format);  mul_964 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:300, code: attn_prob = nn.functional.softmax(attn_score, dim=3)
    alias_49: "f32[1, 16, 512, 512]" = torch.ops.aten.alias.default(alias);  alias = None
    mul_965: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(clone_190, alias_49);  clone_190 = None
    sum_339: "f32[1, 16, 512, 1]" = torch.ops.aten.sum.dim_IntList(mul_965, [3], True)
    mul_966: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(alias_49, sum_339);  alias_49 = sum_339 = None
    sub_242: "f32[1, 16, 512, 512]" = torch.ops.aten.sub.Tensor(mul_965, mul_966);  mul_965 = mul_966 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:291, code: attn_score = (ac + bd + ef) * self.scale
    mul_967: "f32[1, 16, 512, 512]" = torch.ops.aten.mul.Tensor(sub_242, 0.125);  sub_242 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:258, code: x = torch.index_select(x, 3, torch.arange(klen, device=x.device, dtype=torch.long))
    full_116: "f32[1, 16, 512, 1023]" = torch.ops.aten.full.default([1, 16, 512, 1023], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    index_put_23: "f32[1, 16, 512, 1023]" = torch.ops.aten.index_put.default(full_116, [None, None, None, iota_2], mul_967, True);  full_116 = iota_2 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:254, code: x = x.reshape(x_size[0], x_size[1], x_size[2], x_size[3] - 1)
    view_1877: "f32[1, 16, 1023, 512]" = torch.ops.aten.view.default(index_put_23, [1, 16, 1023, 512]);  index_put_23 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:253, code: x = x[:, :, 1:, :]
    full_117: "f32[1, 16, 1023, 512]" = torch.ops.aten.full.default([1, 16, 1023, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_92: "f32[1, 16, 1023, 512]" = torch.ops.aten.slice_scatter.default(full_117, view_1877, 3, 0, 9223372036854775807);  full_117 = view_1877 = None
    full_118: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_93: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_118, slice_scatter_92, 2, 1, 9223372036854775807);  full_118 = slice_scatter_92 = None
    full_119: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_94: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_119, slice_scatter_93, 1, 0, 9223372036854775807);  full_119 = slice_scatter_93 = None
    full_120: "f32[1, 16, 1024, 512]" = torch.ops.aten.full.default([1, 16, 1024, 512], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    slice_scatter_95: "f32[1, 16, 1024, 512]" = torch.ops.aten.slice_scatter.default(full_120, slice_scatter_94, 0, 0, 9223372036854775807);  full_120 = slice_scatter_94 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:252, code: x = x.reshape(x_size[0], x_size[1], x_size[3], x_size[2])
    view_1878: "f32[1, 16, 512, 1024]" = torch.ops.aten.view.default(slice_scatter_95, [1, 16, 512, 1024]);  slice_scatter_95 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:280, code: bd = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_r_bias, k_head_r)
    view_1879: "f32[1, 16, 512, 1024, 1]" = torch.ops.aten.view.default(view_1878, [1, 16, 512, 1024, 1]);  view_1878 = None
    permute_2443: "f32[16, 512, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1879, [1, 2, 4, 0, 3]);  view_1879 = None
    view_1880: "f32[16, 512, 1024]" = torch.ops.aten.view.default(permute_2443, [16, 512, 1024]);  permute_2443 = None
    permute_2444: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_20, [0, 2, 1]);  view_20 = None
    bmm_541: "f32[16, 64, 1024]" = torch.ops.aten.bmm.default(permute_2444, view_1880);  permute_2444 = None
    permute_2445: "f32[16, 1024, 64]" = torch.ops.aten.permute.default(view_21, [0, 2, 1]);  view_21 = None
    bmm_542: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1880, permute_2445);  view_1880 = permute_2445 = None
    view_1881: "f32[16, 64, 1, 1024, 1]" = torch.ops.aten.view.default(bmm_541, [16, 64, 1, 1024, 1]);  bmm_541 = None
    permute_2446: "f32[1, 16, 1, 1024, 64]" = torch.ops.aten.permute.default(view_1881, [2, 0, 4, 3, 1]);  view_1881 = None
    view_1882: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_542, [16, 512, 64, 1, 1]);  bmm_542 = None
    permute_2447: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1882, [3, 0, 1, 4, 2]);  view_1882 = None
    permute_2448: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2446, [3, 0, 1, 4, 2]);  permute_2446 = None
    squeeze_535: "f32[1024, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2448, 4);  permute_2448 = None
    permute_2449: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2447, [2, 0, 1, 4, 3]);  permute_2447 = None
    squeeze_536: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2449, 4);  permute_2449 = None
    sum_340: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_536, [0, 1], True)
    view_1883: "f32[16, 64]" = torch.ops.aten.view.default(sum_340, [16, 64]);  sum_340 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    view_1884: "f32[1, 16, 512, 512, 1]" = torch.ops.aten.view.default(mul_967, [1, 16, 512, 512, 1]);  mul_967 = None
    permute_2450: "f32[16, 512, 1, 1, 512]" = torch.ops.aten.permute.default(view_1884, [1, 2, 4, 0, 3]);  view_1884 = None
    view_1885: "f32[16, 512, 512]" = torch.ops.aten.view.default(permute_2450, [16, 512, 512]);  permute_2450 = None
    permute_2451: "f32[16, 64, 512]" = torch.ops.aten.permute.default(view_16, [0, 2, 1]);  view_16 = None
    bmm_543: "f32[16, 64, 512]" = torch.ops.aten.bmm.default(permute_2451, view_1885);  permute_2451 = None
    permute_2452: "f32[16, 512, 64]" = torch.ops.aten.permute.default(view_17, [0, 2, 1]);  view_17 = None
    bmm_544: "f32[16, 512, 64]" = torch.ops.aten.bmm.default(view_1885, permute_2452);  view_1885 = permute_2452 = None
    view_1886: "f32[16, 64, 1, 512, 1]" = torch.ops.aten.view.default(bmm_543, [16, 64, 1, 512, 1]);  bmm_543 = None
    permute_2453: "f32[1, 16, 1, 512, 64]" = torch.ops.aten.permute.default(view_1886, [2, 0, 4, 3, 1]);  view_1886 = None
    view_1887: "f32[16, 512, 64, 1, 1]" = torch.ops.aten.view.default(bmm_544, [16, 512, 64, 1, 1]);  bmm_544 = None
    permute_2454: "f32[1, 16, 512, 1, 64]" = torch.ops.aten.permute.default(view_1887, [3, 0, 1, 4, 2]);  view_1887 = None
    permute_2455: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2453, [3, 0, 1, 4, 2]);  permute_2453 = None
    squeeze_537: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2455, 4);  permute_2455 = None
    permute_2456: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.permute.default(permute_2454, [2, 0, 1, 4, 3]);  permute_2454 = None
    squeeze_538: "f32[512, 1, 16, 64]" = torch.ops.aten.squeeze.dim(permute_2456, 4);  permute_2456 = None
    sum_341: "f32[1, 1, 16, 64]" = torch.ops.aten.sum.dim_IntList(squeeze_538, [0, 1], True)
    view_1888: "f32[16, 64]" = torch.ops.aten.view.default(sum_341, [16, 64]);  sum_341 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:277, code: ac = torch.einsum("ibnd,jbnd->bnij", q_head + self.r_w_bias, k_head_h)
    add_431: "f32[512, 1, 16, 64]" = torch.ops.aten.add.Tensor(squeeze_536, squeeze_538);  squeeze_536 = squeeze_538 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:436, code: k_head_r = torch.einsum("ibh,hnd->ibnd", r.type(self.r.dtype), self.r)
    view_1889: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_535, [1024, 1, 16, 64, 1]);  squeeze_535 = None
    permute_2457: "f32[1024, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1889, [0, 4, 1, 2, 3]);  view_1889 = None
    view_1890: "f32[1, 1024, 1024]" = torch.ops.aten.view.default(permute_2457, [1, 1024, 1024]);  permute_2457 = None
    permute_2458: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_12, [0, 2, 1]);  view_12 = None
    bmm_545: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2458, view_1890);  permute_2458 = view_1890 = None
    view_1891: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_545, [1024, 1, 16, 64, 1]);  bmm_545 = None
    permute_2459: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1891, [4, 1, 2, 3, 0]);  view_1891 = None
    permute_2460: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2459, [4, 2, 3, 0, 1]);  permute_2459 = None
    squeeze_539: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2460, 4);  permute_2460 = None
    squeeze_540: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_539, 3);  squeeze_539 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    view_1892: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_533, [512, 1, 16, 64, 1]);  squeeze_533 = None
    permute_2461: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1892, [0, 4, 1, 2, 3]);  view_1892 = None
    clone_191: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2461, memory_format = torch.contiguous_format);  permute_2461 = None
    view_1893: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_191, [1, 512, 1024]);  clone_191 = None
    permute_2462: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_8, [0, 2, 1]);  view_8 = None
    bmm_546: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2462, view_1893);  permute_2462 = None
    permute_2463: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_9, [0, 2, 1]);  view_9 = None
    bmm_547: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1893, permute_2463);  view_1893 = permute_2463 = None
    view_1894: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_546, [1024, 1, 16, 64, 1]);  bmm_546 = None
    permute_2464: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1894, [4, 1, 2, 3, 0]);  view_1894 = None
    view_1895: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_547, [512, 1024, 1, 1, 1]);  bmm_547 = None
    permute_2465: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1895, [0, 2, 3, 4, 1]);  view_1895 = None
    permute_2466: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2464, [4, 2, 3, 0, 1]);  permute_2464 = None
    squeeze_541: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2466, 4);  permute_2466 = None
    squeeze_542: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_541, 3);  squeeze_541 = None
    permute_2467: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2465, [0, 1, 4, 2, 3]);  permute_2465 = None
    squeeze_543: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2467, 4);  permute_2467 = None
    squeeze_544: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_543, 3);  squeeze_543 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:432, code: v_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.v)
    add_432: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(mul_959, squeeze_544);  mul_959 = squeeze_544 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    view_1896: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(squeeze_537, [512, 1, 16, 64, 1]);  squeeze_537 = None
    permute_2468: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1896, [0, 4, 1, 2, 3]);  view_1896 = None
    view_1897: "f32[1, 512, 1024]" = torch.ops.aten.view.default(permute_2468, [1, 512, 1024]);  permute_2468 = None
    permute_2469: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view_4, [0, 2, 1]);  view_4 = None
    bmm_548: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2469, view_1897);  permute_2469 = None
    permute_2470: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_5, [0, 2, 1]);  view_5 = None
    bmm_549: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1897, permute_2470);  view_1897 = permute_2470 = None
    view_1898: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_548, [1024, 1, 16, 64, 1]);  bmm_548 = None
    permute_2471: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1898, [4, 1, 2, 3, 0]);  view_1898 = None
    view_1899: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_549, [512, 1024, 1, 1, 1]);  bmm_549 = None
    permute_2472: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1899, [0, 2, 3, 4, 1]);  view_1899 = None
    permute_2473: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2471, [4, 2, 3, 0, 1]);  permute_2471 = None
    squeeze_545: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2473, 4);  permute_2473 = None
    squeeze_546: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_545, 3);  squeeze_545 = None
    permute_2474: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2472, [0, 1, 4, 2, 3]);  permute_2472 = None
    squeeze_547: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2474, 4);  permute_2474 = None
    squeeze_548: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_547, 3);  squeeze_547 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:431, code: k_head_h = torch.einsum("ibh,hnd->ibnd", cat, self.k)
    add_433: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_432, squeeze_548);  add_432 = squeeze_548 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    view_1900: "f32[512, 1, 16, 64, 1]" = torch.ops.aten.view.default(add_431, [512, 1, 16, 64, 1]);  add_431 = None
    permute_2475: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.permute.default(view_1900, [0, 4, 1, 2, 3]);  view_1900 = None
    clone_192: "f32[512, 1, 1, 16, 64]" = torch.ops.aten.clone.default(permute_2475, memory_format = torch.contiguous_format);  permute_2475 = None
    view_1901: "f32[1, 512, 1024]" = torch.ops.aten.view.default(clone_192, [1, 512, 1024]);  clone_192 = None
    permute_2476: "f32[1, 1024, 512]" = torch.ops.aten.permute.default(view, [0, 2, 1]);  view = None
    bmm_550: "f32[1, 1024, 1024]" = torch.ops.aten.bmm.default(permute_2476, view_1901);  permute_2476 = None
    permute_2477: "f32[1, 1024, 1024]" = torch.ops.aten.permute.default(view_1, [0, 2, 1]);  view_1 = None
    bmm_551: "f32[1, 512, 1024]" = torch.ops.aten.bmm.default(view_1901, permute_2477);  view_1901 = permute_2477 = None
    view_1902: "f32[1024, 1, 16, 64, 1]" = torch.ops.aten.view.default(bmm_550, [1024, 1, 16, 64, 1]);  bmm_550 = None
    permute_2478: "f32[1, 1, 16, 64, 1024]" = torch.ops.aten.permute.default(view_1902, [4, 1, 2, 3, 0]);  view_1902 = None
    view_1903: "f32[512, 1024, 1, 1, 1]" = torch.ops.aten.view.default(bmm_551, [512, 1024, 1, 1, 1]);  bmm_551 = None
    permute_2479: "f32[512, 1, 1, 1, 1024]" = torch.ops.aten.permute.default(view_1903, [0, 2, 3, 4, 1]);  view_1903 = None
    permute_2480: "f32[1024, 16, 64, 1, 1]" = torch.ops.aten.permute.default(permute_2478, [4, 2, 3, 0, 1]);  permute_2478 = None
    squeeze_549: "f32[1024, 16, 64, 1]" = torch.ops.aten.squeeze.dim(permute_2480, 4);  permute_2480 = None
    squeeze_550: "f32[1024, 16, 64]" = torch.ops.aten.squeeze.dim(squeeze_549, 3);  squeeze_549 = None
    permute_2481: "f32[512, 1, 1024, 1, 1]" = torch.ops.aten.permute.default(permute_2479, [0, 1, 4, 2, 3]);  permute_2479 = None
    squeeze_551: "f32[512, 1, 1024, 1]" = torch.ops.aten.squeeze.dim(permute_2481, 4);  permute_2481 = None
    squeeze_552: "f32[512, 1, 1024]" = torch.ops.aten.squeeze.dim(squeeze_551, 3);  squeeze_551 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:430, code: q_head_h = torch.einsum("ibh,hnd->ibnd", h, self.q)
    add_434: "f32[512, 1, 1024]" = torch.ops.aten.add.Tensor(add_433, squeeze_552);  add_433 = squeeze_552 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1177, code: output_h = self.dropout(word_emb_k)
    convert_element_type_103: "f32[512, 1, 1024]" = torch.ops.prims.convert_element_type.default(getitem_1, torch.float32);  getitem_1 = None
    mul_968: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(convert_element_type_103, 1.1111111111111112);  convert_element_type_103 = None
    mul_969: "f32[512, 1, 1024]" = torch.ops.aten.mul.Tensor(add_434, mul_968);  add_434 = mul_968 = None
    clone_193: "f32[512, 1, 1024]" = torch.ops.aten.clone.default(mul_969, memory_format = torch.contiguous_format);  mul_969 = None
    
    # File: /workspace/youkaichao/miniconda3/envs/build_torch/lib/python3.10/site-packages/transformers/models/xlnet/modeling_xlnet.py:1176, code: word_emb_k = self.word_embedding(input_ids)
    eq: "b8[512, 1]" = torch.ops.aten.eq.Scalar(permute, -1)
    unsqueeze_605: "b8[512, 1, 1]" = torch.ops.aten.unsqueeze.default(eq, -1);  eq = None
    scalar_tensor_4: "f32[]" = torch.ops.aten.scalar_tensor.default(0.0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
    where_4: "f32[512, 1, 1024]" = torch.ops.aten.where.self(unsqueeze_605, scalar_tensor_4, clone_193);  unsqueeze_605 = scalar_tensor_4 = clone_193 = None
    full_121: "f32[32000, 1024]" = torch.ops.aten.full.default([32000, 1024], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
    _unsafe_index_put: "f32[32000, 1024]" = torch.ops.aten._unsafe_index_put.default(full_121, [permute], where_4, True);  full_121 = permute = where_4 = None
    return pytree.tree_unflatten([div_25, view_913, squeeze_550, squeeze_546, squeeze_542, squeeze_540, view_1888, view_1883, squeeze_531, squeeze_527, squeeze_523, squeeze_519, squeeze_517, view_1847, view_1842, squeeze_508, squeeze_504, squeeze_500, squeeze_496, squeeze_494, view_1806, view_1801, squeeze_485, squeeze_481, squeeze_477, squeeze_473, squeeze_471, view_1765, view_1760, squeeze_462, squeeze_458, squeeze_454, squeeze_450, squeeze_448, view_1724, view_1719, squeeze_439, squeeze_435, squeeze_431, squeeze_427, squeeze_425, view_1683, view_1678, squeeze_416, squeeze_412, squeeze_408, squeeze_404, squeeze_402, view_1642, view_1637, squeeze_393, squeeze_389, squeeze_385, squeeze_381, squeeze_379, view_1601, view_1596, squeeze_370, squeeze_366, squeeze_362, squeeze_358, squeeze_356, view_1560, view_1555, squeeze_347, squeeze_343, squeeze_339, squeeze_335, squeeze_333, view_1519, view_1514, squeeze_324, squeeze_320, squeeze_316, squeeze_312, squeeze_310, view_1478, view_1473, squeeze_301, squeeze_297, squeeze_293, squeeze_289, squeeze_287, view_1437, view_1432, squeeze_278, squeeze_274, squeeze_270, squeeze_266, squeeze_264, view_1396, view_1391, squeeze_255, squeeze_251, squeeze_247, squeeze_243, squeeze_241, view_1355, view_1350, squeeze_232, squeeze_228, squeeze_224, squeeze_220, squeeze_218, view_1314, view_1309, squeeze_209, squeeze_205, squeeze_201, squeeze_197, squeeze_195, view_1273, view_1268, squeeze_186, squeeze_182, squeeze_178, squeeze_174, squeeze_172, view_1232, view_1227, squeeze_163, squeeze_159, squeeze_155, squeeze_151, squeeze_149, view_1191, view_1186, squeeze_140, squeeze_136, squeeze_132, squeeze_128, squeeze_126, view_1150, view_1145, squeeze_117, squeeze_113, squeeze_109, squeeze_105, squeeze_103, view_1109, view_1104, squeeze_94, squeeze_90, squeeze_86, squeeze_82, squeeze_80, view_1068, view_1063, squeeze_71, squeeze_67, squeeze_63, squeeze_59, squeeze_57, view_1027, view_1022, squeeze_48, squeeze_44, squeeze_40, squeeze_36, squeeze_34, view_986, view_981, squeeze_25, squeeze_21, squeeze_17, squeeze_13, squeeze_11, view_945, view_940, squeeze_2, _unsafe_index_put, sum_337, sum_338, permute_2428, view_1867, permute_2424, view_1864, sum_331, sum_332, sum_324, sum_325, permute_2367, view_1826, permute_2363, view_1823, sum_318, sum_319, sum_311, sum_312, permute_2306, view_1785, permute_2302, view_1782, sum_305, sum_306, sum_298, sum_299, permute_2245, view_1744, permute_2241, view_1741, sum_292, sum_293, sum_285, sum_286, permute_2184, view_1703, permute_2180, view_1700, sum_279, sum_280, sum_272, sum_273, permute_2123, view_1662, permute_2119, view_1659, sum_266, sum_267, sum_259, sum_260, permute_2062, view_1621, permute_2058, view_1618, sum_253, sum_254, sum_246, sum_247, permute_2001, view_1580, permute_1997, view_1577, sum_240, sum_241, sum_233, sum_234, permute_1940, view_1539, permute_1936, view_1536, sum_227, sum_228, sum_220, sum_221, permute_1879, view_1498, permute_1875, view_1495, sum_214, sum_215, sum_207, sum_208, permute_1818, view_1457, permute_1814, view_1454, sum_201, sum_202, sum_194, sum_195, permute_1757, view_1416, permute_1753, view_1413, sum_188, sum_189, sum_181, sum_182, permute_1696, view_1375, permute_1692, view_1372, sum_175, sum_176, sum_168, sum_169, permute_1635, view_1334, permute_1631, view_1331, sum_162, sum_163, sum_155, sum_156, permute_1574, view_1293, permute_1570, view_1290, sum_149, sum_150, sum_142, sum_143, permute_1513, view_1252, permute_1509, view_1249, sum_136, sum_137, sum_129, sum_130, permute_1452, view_1211, permute_1448, view_1208, sum_123, sum_124, sum_116, sum_117, permute_1391, view_1170, permute_1387, view_1167, sum_110, sum_111, sum_103, sum_104, permute_1330, view_1129, permute_1326, view_1126, sum_97, sum_98, sum_90, sum_91, permute_1269, view_1088, permute_1265, view_1085, sum_84, sum_85, sum_77, sum_78, permute_1208, view_1047, permute_1204, view_1044, sum_71, sum_72, sum_64, sum_65, permute_1147, view_1006, permute_1143, view_1003, sum_58, sum_59, sum_51, sum_52, permute_1086, view_965, permute_1082, view_962, sum_45, sum_46, sum_38, sum_39, permute_1025, view_924, permute_1021, view_921, sum_32, sum_33, permute_1016, view_918, None, None], self._out_spec)
    